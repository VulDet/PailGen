filepath,vul_code,diff_target,patch_code
1112----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53089/bad/timer.c----_kvm_save_timer,"static void _kvm_save_timer(struct kvm_vcpu *vcpu) { unsigned long ticks, delta, cfg; ktime_t expire; struct loongarch_csrs *csr = vcpu->arch.csr; cfg = kvm_read_sw_gcsr(csr, LOONGARCH_CSR_TCFG); ticks = kvm_read_sw_gcsr(csr, LOONGARCH_CSR_TVAL); if (ticks < cfg) delta = tick_to_ns(vcpu, ticks); else delta = 0; expire = ktime_add_ns(ktime_get(), delta); vcpu->arch.expire = expire; if (kvm_vcpu_is_blocking(vcpu)) { <S2SV_StartVul> hrtimer_start(&vcpu->arch.swtimer, expire, HRTIMER_MODE_ABS_PINNED); <S2SV_EndVul> } }","- hrtimer_start(&vcpu->arch.swtimer, expire, HRTIMER_MODE_ABS_PINNED);
+ hrtimer_start(&vcpu->arch.swtimer, expire, HRTIMER_MODE_ABS_PINNED_HARD);","static void _kvm_save_timer(struct kvm_vcpu *vcpu) { unsigned long ticks, delta, cfg; ktime_t expire; struct loongarch_csrs *csr = vcpu->arch.csr; cfg = kvm_read_sw_gcsr(csr, LOONGARCH_CSR_TCFG); ticks = kvm_read_sw_gcsr(csr, LOONGARCH_CSR_TVAL); if (ticks < cfg) delta = tick_to_ns(vcpu, ticks); else delta = 0; expire = ktime_add_ns(ktime_get(), delta); vcpu->arch.expire = expire; if (kvm_vcpu_is_blocking(vcpu)) { hrtimer_start(&vcpu->arch.swtimer, expire, HRTIMER_MODE_ABS_PINNED_HARD); } }"
327----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46791/bad/mcp251x.c----mcp251x_hw_wake,"static int mcp251x_hw_wake(struct spi_device *spi) { u8 value; int ret; <S2SV_StartVul> disable_irq(spi->irq); <S2SV_EndVul> mcp251x_write_2regs(spi, CANINTE, CANINTE_WAKIE, CANINTF_WAKIF); mdelay(MCP251X_OST_DELAY_MS); mcp251x_write_reg(spi, CANCTRL, CANCTRL_REQOP_CONF); ret = mcp251x_read_stat_poll_timeout(spi, value, value == CANCTRL_REQOP_CONF, MCP251X_OST_DELAY_MS * 1000, USEC_PER_SEC); if (ret) { dev_err(&spi->dev, ""MCP251x didn't enter in config mode\n""); return ret; } mcp251x_write_2regs(spi, CANINTE, 0x00, 0x00); enable_irq(spi->irq); return 0; }","- disable_irq(spi->irq);
+ disable_irq_nosync(spi->irq);","static int mcp251x_hw_wake(struct spi_device *spi) { u8 value; int ret; disable_irq_nosync(spi->irq); mcp251x_write_2regs(spi, CANINTE, CANINTE_WAKIE, CANINTF_WAKIF); mdelay(MCP251X_OST_DELAY_MS); mcp251x_write_reg(spi, CANCTRL, CANCTRL_REQOP_CONF); ret = mcp251x_read_stat_poll_timeout(spi, value, value == CANCTRL_REQOP_CONF, MCP251X_OST_DELAY_MS * 1000, USEC_PER_SEC); if (ret) { dev_err(&spi->dev, ""MCP251x didn't enter in config mode\n""); return ret; } mcp251x_write_2regs(spi, CANINTE, 0x00, 0x00); enable_irq(spi->irq); return 0; }"
930----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50179/bad/addr.c----ceph_set_page_dirty,"static int ceph_set_page_dirty(struct page *page) { struct address_space *mapping = page->mapping; struct inode *inode; struct ceph_inode_info *ci; struct ceph_snap_context *snapc; if (PageDirty(page)) { dout(""%p set_page_dirty %p idx %lu -- already dirty\n"", mapping->host, page, page->index); BUG_ON(!PagePrivate(page)); return 0; } inode = mapping->host; ci = ceph_inode(inode); spin_lock(&ci->i_ceph_lock); <S2SV_StartVul> BUG_ON(ci->i_wr_ref == 0); if (__ceph_have_pending_cap_snap(ci)) { struct ceph_cap_snap *capsnap = list_last_entry(&ci->i_cap_snaps, struct ceph_cap_snap, ci_item); snapc = ceph_get_snap_context(capsnap->context); capsnap->dirty_pages++; } else { BUG_ON(!ci->i_head_snapc); snapc = ceph_get_snap_context(ci->i_head_snapc); ++ci->i_wrbuffer_ref_head; } if (ci->i_wrbuffer_ref == 0) ihold(inode); ++ci->i_wrbuffer_ref; dout(""%p set_page_dirty %p idx %lu head %d/%d -> %d/%d "" ""snapc %p seq %lld (%d snaps)\n"", mapping->host, page, page->index, ci->i_wrbuffer_ref-1, ci->i_wrbuffer_ref_head-1, ci->i_wrbuffer_ref, ci->i_wrbuffer_ref_head, snapc, snapc->seq, snapc->num_snaps); spin_unlock(&ci->i_ceph_lock); BUG_ON(PagePrivate(page)); page->private = (unsigned long)snapc; SetPagePrivate(page); return __set_page_dirty_nobuffers(page); }",- BUG_ON(ci->i_wr_ref == 0); // caller should hold Fw reference,"static int ceph_set_page_dirty(struct page *page) { struct address_space *mapping = page->mapping; struct inode *inode; struct ceph_inode_info *ci; struct ceph_snap_context *snapc; if (PageDirty(page)) { dout(""%p set_page_dirty %p idx %lu -- already dirty\n"", mapping->host, page, page->index); BUG_ON(!PagePrivate(page)); return 0; } inode = mapping->host; ci = ceph_inode(inode); spin_lock(&ci->i_ceph_lock); if (__ceph_have_pending_cap_snap(ci)) { struct ceph_cap_snap *capsnap = list_last_entry(&ci->i_cap_snaps, struct ceph_cap_snap, ci_item); snapc = ceph_get_snap_context(capsnap->context); capsnap->dirty_pages++; } else { BUG_ON(!ci->i_head_snapc); snapc = ceph_get_snap_context(ci->i_head_snapc); ++ci->i_wrbuffer_ref_head; } if (ci->i_wrbuffer_ref == 0) ihold(inode); ++ci->i_wrbuffer_ref; dout(""%p set_page_dirty %p idx %lu head %d/%d -> %d/%d "" ""snapc %p seq %lld (%d snaps)\n"", mapping->host, page, page->index, ci->i_wrbuffer_ref-1, ci->i_wrbuffer_ref_head-1, ci->i_wrbuffer_ref, ci->i_wrbuffer_ref_head, snapc, snapc->seq, snapc->num_snaps); spin_unlock(&ci->i_ceph_lock); BUG_ON(PagePrivate(page)); page->private = (unsigned long)snapc; SetPagePrivate(page); return __set_page_dirty_nobuffers(page); }"
1052----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50293/bad/af_smc.c----__smc_create,"static int __smc_create(struct net *net, struct socket *sock, int protocol, int kern, struct socket *clcsock) { int family = (protocol == SMCPROTO_SMC6) ? PF_INET6 : PF_INET; struct smc_sock *smc; struct sock *sk; int rc; rc = -ESOCKTNOSUPPORT; if (sock->type != SOCK_STREAM) goto out; rc = -EPROTONOSUPPORT; if (protocol != SMCPROTO_SMC && protocol != SMCPROTO_SMC6) goto out; rc = -ENOBUFS; sock->ops = &smc_sock_ops; sock->state = SS_UNCONNECTED; sk = smc_sock_alloc(net, sock, protocol); if (!sk) goto out; smc = smc_sk(sk); rc = 0; if (clcsock) smc->clcsock = clcsock; else rc = smc_create_clcsk(net, sk, family); <S2SV_StartVul> if (rc) <S2SV_EndVul> sk_common_release(sk); out: return rc; }","- if (rc)
+ if (rc) {
+ sock->sk = NULL;
+ }
+ }","static int __smc_create(struct net *net, struct socket *sock, int protocol, int kern, struct socket *clcsock) { int family = (protocol == SMCPROTO_SMC6) ? PF_INET6 : PF_INET; struct smc_sock *smc; struct sock *sk; int rc; rc = -ESOCKTNOSUPPORT; if (sock->type != SOCK_STREAM) goto out; rc = -EPROTONOSUPPORT; if (protocol != SMCPROTO_SMC && protocol != SMCPROTO_SMC6) goto out; rc = -ENOBUFS; sock->ops = &smc_sock_ops; sock->state = SS_UNCONNECTED; sk = smc_sock_alloc(net, sock, protocol); if (!sk) goto out; smc = smc_sk(sk); rc = 0; if (clcsock) smc->clcsock = clcsock; else rc = smc_create_clcsk(net, sk, family); if (rc) { sk_common_release(sk); sock->sk = NULL; } out: return rc; }"
794----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50071/bad/pinctrl-ma35.c----ma35_pinctrl_dt_node_to_map_func,"static int ma35_pinctrl_dt_node_to_map_func(struct pinctrl_dev *pctldev, struct device_node *np, struct pinctrl_map **map, unsigned int *num_maps) { struct ma35_pinctrl *npctl = pinctrl_dev_get_drvdata(pctldev); struct ma35_pin_group *grp; struct pinctrl_map *new_map; struct device_node *parent; int map_num = 1; int i; grp = ma35_pinctrl_find_group_by_name(npctl, np->name); if (!grp) { dev_err(npctl->dev, ""unable to find group for node %s\n"", np->name); return -EINVAL; } map_num += grp->npins; <S2SV_StartVul> new_map = devm_kcalloc(pctldev->dev, map_num, sizeof(*new_map), GFP_KERNEL); <S2SV_EndVul> if (!new_map) return -ENOMEM; *map = new_map; *num_maps = map_num; parent = of_get_parent(np); if (!parent) return -EINVAL; new_map[0].type = PIN_MAP_TYPE_MUX_GROUP; new_map[0].data.mux.function = parent->name; new_map[0].data.mux.group = np->name; of_node_put(parent); new_map++; for (i = 0; i < grp->npins; i++) { new_map[i].type = PIN_MAP_TYPE_CONFIGS_PIN; new_map[i].data.configs.group_or_pin = pin_get_name(pctldev, grp->pins[i]); new_map[i].data.configs.configs = grp->settings[i].configs; new_map[i].data.configs.num_configs = grp->settings[i].nconfigs; } dev_dbg(pctldev->dev, ""maps: function %s group %s num %d\n"", (*map)->data.mux.function, (*map)->data.mux.group, map_num); return 0; }","- new_map = devm_kcalloc(pctldev->dev, map_num, sizeof(*new_map), GFP_KERNEL);
+ new_map = kcalloc(map_num, sizeof(*new_map), GFP_KERNEL);","static int ma35_pinctrl_dt_node_to_map_func(struct pinctrl_dev *pctldev, struct device_node *np, struct pinctrl_map **map, unsigned int *num_maps) { struct ma35_pinctrl *npctl = pinctrl_dev_get_drvdata(pctldev); struct ma35_pin_group *grp; struct pinctrl_map *new_map; struct device_node *parent; int map_num = 1; int i; grp = ma35_pinctrl_find_group_by_name(npctl, np->name); if (!grp) { dev_err(npctl->dev, ""unable to find group for node %s\n"", np->name); return -EINVAL; } map_num += grp->npins; new_map = kcalloc(map_num, sizeof(*new_map), GFP_KERNEL); if (!new_map) return -ENOMEM; *map = new_map; *num_maps = map_num; parent = of_get_parent(np); if (!parent) return -EINVAL; new_map[0].type = PIN_MAP_TYPE_MUX_GROUP; new_map[0].data.mux.function = parent->name; new_map[0].data.mux.group = np->name; of_node_put(parent); new_map++; for (i = 0; i < grp->npins; i++) { new_map[i].type = PIN_MAP_TYPE_CONFIGS_PIN; new_map[i].data.configs.group_or_pin = pin_get_name(pctldev, grp->pins[i]); new_map[i].data.configs.configs = grp->settings[i].configs; new_map[i].data.configs.num_configs = grp->settings[i].nconfigs; } dev_dbg(pctldev->dev, ""maps: function %s group %s num %d\n"", (*map)->data.mux.function, (*map)->data.mux.group, map_num); return 0; }"
607----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49921/bad/dcn10_hubp.c----hubp1_is_flip_pending,"bool hubp1_is_flip_pending(struct hubp *hubp) { uint32_t flip_pending = 0; struct dcn10_hubp *hubp1 = TO_DCN10_HUBP(hubp); struct dc_plane_address earliest_inuse_address; if (hubp && hubp->power_gated) return false; REG_GET(DCSURF_FLIP_CONTROL, SURFACE_FLIP_PENDING, &flip_pending); REG_GET(DCSURF_SURFACE_EARLIEST_INUSE, SURFACE_EARLIEST_INUSE_ADDRESS, &earliest_inuse_address.grph.addr.low_part); REG_GET(DCSURF_SURFACE_EARLIEST_INUSE_HIGH, SURFACE_EARLIEST_INUSE_ADDRESS_HIGH, &earliest_inuse_address.grph.addr.high_part); if (flip_pending) return true; <S2SV_StartVul> if (earliest_inuse_address.grph.addr.quad_part != hubp->request_address.grph.addr.quad_part) <S2SV_EndVul> return true; return false; }","- if (earliest_inuse_address.grph.addr.quad_part != hubp->request_address.grph.addr.quad_part)
+ if (hubp &&
+ earliest_inuse_address.grph.addr.quad_part != hubp->request_address.grph.addr.quad_part)","bool hubp1_is_flip_pending(struct hubp *hubp) { uint32_t flip_pending = 0; struct dcn10_hubp *hubp1 = TO_DCN10_HUBP(hubp); struct dc_plane_address earliest_inuse_address; if (hubp && hubp->power_gated) return false; REG_GET(DCSURF_FLIP_CONTROL, SURFACE_FLIP_PENDING, &flip_pending); REG_GET(DCSURF_SURFACE_EARLIEST_INUSE, SURFACE_EARLIEST_INUSE_ADDRESS, &earliest_inuse_address.grph.addr.low_part); REG_GET(DCSURF_SURFACE_EARLIEST_INUSE_HIGH, SURFACE_EARLIEST_INUSE_ADDRESS_HIGH, &earliest_inuse_address.grph.addr.high_part); if (flip_pending) return true; if (hubp && earliest_inuse_address.grph.addr.quad_part != hubp->request_address.grph.addr.quad_part) return true; return false; }"
1104----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53085/bad/tpm-chip.c----tpm_hwrng_read,"static int tpm_hwrng_read(struct hwrng *rng, void *data, size_t max, bool wait) { struct tpm_chip *chip = container_of(rng, struct tpm_chip, hwrng); <S2SV_StartVul> if (chip->flags & TPM_CHIP_FLAG_SUSPENDED) <S2SV_EndVul> <S2SV_StartVul> return 0; <S2SV_EndVul> return tpm_get_random(chip, data, max); }","- if (chip->flags & TPM_CHIP_FLAG_SUSPENDED)
- return 0;","static int tpm_hwrng_read(struct hwrng *rng, void *data, size_t max, bool wait) { struct tpm_chip *chip = container_of(rng, struct tpm_chip, hwrng); return tpm_get_random(chip, data, max); }"
1019----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50257/bad/x_tables.c----*xt_find_table_lock,"struct xt_table *xt_find_table_lock(struct net *net, u_int8_t af, const char *name) { struct xt_pernet *xt_net = net_generic(net, xt_pernet_id); struct module *owner = NULL; struct xt_template *tmpl; struct xt_table *t; mutex_lock(&xt[af].mutex); list_for_each_entry(t, &xt_net->tables[af], list) if (strcmp(t->name, name) == 0 && try_module_get(t->me)) return t; list_for_each_entry(tmpl, &xt_templates[af], list) { int err; if (strcmp(tmpl->name, name)) continue; if (!try_module_get(tmpl->me)) goto out; owner = tmpl->me; mutex_unlock(&xt[af].mutex); err = tmpl->table_init(net); if (err < 0) { module_put(owner); return ERR_PTR(err); } mutex_lock(&xt[af].mutex); break; } list_for_each_entry(t, &xt_net->tables[af], list) <S2SV_StartVul> if (strcmp(t->name, name) == 0) <S2SV_EndVul> return t; module_put(owner); out: mutex_unlock(&xt[af].mutex); return ERR_PTR(-ENOENT); }","- if (strcmp(t->name, name) == 0)
+ if (strcmp(t->name, name) == 0 && owner == t->me)","struct xt_table *xt_find_table_lock(struct net *net, u_int8_t af, const char *name) { struct xt_pernet *xt_net = net_generic(net, xt_pernet_id); struct module *owner = NULL; struct xt_template *tmpl; struct xt_table *t; mutex_lock(&xt[af].mutex); list_for_each_entry(t, &xt_net->tables[af], list) if (strcmp(t->name, name) == 0 && try_module_get(t->me)) return t; list_for_each_entry(tmpl, &xt_templates[af], list) { int err; if (strcmp(tmpl->name, name)) continue; if (!try_module_get(tmpl->me)) goto out; owner = tmpl->me; mutex_unlock(&xt[af].mutex); err = tmpl->table_init(net); if (err < 0) { module_put(owner); return ERR_PTR(err); } mutex_lock(&xt[af].mutex); break; } list_for_each_entry(t, &xt_net->tables[af], list) if (strcmp(t->name, name) == 0 && owner == t->me) return t; module_put(owner); out: mutex_unlock(&xt[af].mutex); return ERR_PTR(-ENOENT); }"
749----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50027/bad/thermal_core.c----thermal_zone_device_unregister,"void thermal_zone_device_unregister(struct thermal_zone_device *tz) { int tz_id; struct thermal_cooling_device *cdev; struct thermal_zone_device *pos = NULL; if (!tz) return; tz_id = tz->id; mutex_lock(&thermal_list_lock); list_for_each_entry(pos, &thermal_tz_list, node) if (pos == tz) break; if (pos != tz) { mutex_unlock(&thermal_list_lock); return; } mutex_lock(&tz->lock); list_del(&tz->node); mutex_unlock(&tz->lock); list_for_each_entry(cdev, &thermal_cdev_list, node) if (tz->ops->unbind) tz->ops->unbind(tz, cdev); mutex_unlock(&thermal_list_lock); cancel_delayed_work_sync(&tz->poll_queue); thermal_set_governor(tz, NULL); thermal_remove_hwmon_sysfs(tz); ida_free(&thermal_tz_ida, tz->id); ida_destroy(&tz->ida); device_del(&tz->device); <S2SV_StartVul> kfree(tz->tzp); <S2SV_EndVul> put_device(&tz->device); thermal_notify_tz_delete(tz_id); wait_for_completion(&tz->removal); kfree(tz); }","- kfree(tz->tzp);
+ kfree(tz->tzp);","void thermal_zone_device_unregister(struct thermal_zone_device *tz) { int tz_id; struct thermal_cooling_device *cdev; struct thermal_zone_device *pos = NULL; if (!tz) return; tz_id = tz->id; mutex_lock(&thermal_list_lock); list_for_each_entry(pos, &thermal_tz_list, node) if (pos == tz) break; if (pos != tz) { mutex_unlock(&thermal_list_lock); return; } mutex_lock(&tz->lock); list_del(&tz->node); mutex_unlock(&tz->lock); list_for_each_entry(cdev, &thermal_cdev_list, node) if (tz->ops->unbind) tz->ops->unbind(tz, cdev); mutex_unlock(&thermal_list_lock); cancel_delayed_work_sync(&tz->poll_queue); thermal_set_governor(tz, NULL); thermal_remove_hwmon_sysfs(tz); ida_free(&thermal_tz_ida, tz->id); ida_destroy(&tz->ida); device_del(&tz->device); put_device(&tz->device); thermal_notify_tz_delete(tz_id); wait_for_completion(&tz->removal); kfree(tz->tzp); kfree(tz); }"
1214----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53222/bad/zram_drv.c----zram_add,"static int zram_add(void) { struct queue_limits lim = { .logical_block_size = ZRAM_LOGICAL_BLOCK_SIZE, .physical_block_size = PAGE_SIZE, .io_min = PAGE_SIZE, .io_opt = PAGE_SIZE, .max_hw_discard_sectors = UINT_MAX, #if ZRAM_LOGICAL_BLOCK_SIZE == PAGE_SIZE .max_write_zeroes_sectors = UINT_MAX, #endif .features = BLK_FEAT_STABLE_WRITES | BLK_FEAT_SYNCHRONOUS, }; struct zram *zram; int ret, device_id; zram = kzalloc(sizeof(struct zram), GFP_KERNEL); if (!zram) return -ENOMEM; ret = idr_alloc(&zram_index_idr, zram, 0, 0, GFP_KERNEL); if (ret < 0) goto out_free_dev; device_id = ret; init_rwsem(&zram->init_lock); #ifdef CONFIG_ZRAM_WRITEBACK spin_lock_init(&zram->wb_limit_lock); #endif zram->disk = blk_alloc_disk(&lim, NUMA_NO_NODE); if (IS_ERR(zram->disk)) { pr_err(""Error allocating disk structure for device %d\n"", device_id); ret = PTR_ERR(zram->disk); goto out_free_idr; } zram->disk->major = zram_major; zram->disk->first_minor = device_id; zram->disk->minors = 1; zram->disk->flags |= GENHD_FL_NO_PART; zram->disk->fops = &zram_devops; zram->disk->private_data = zram; snprintf(zram->disk->disk_name, 16, ""zram%d"", device_id); atomic_set(&zram->pp_in_progress, 0); set_capacity(zram->disk, 0); ret = device_add_disk(NULL, zram->disk, zram_disk_groups); if (ret) goto out_cleanup_disk; <S2SV_StartVul> zram_comp_params_reset(zram); <S2SV_EndVul> <S2SV_StartVul> comp_algorithm_set(zram, ZRAM_PRIMARY_COMP, default_compressor); <S2SV_EndVul> zram_debugfs_register(zram); pr_info(""Added device: %s\n"", zram->disk->disk_name); return device_id; out_cleanup_disk: put_disk(zram->disk); out_free_idr: idr_remove(&zram_index_idr, device_id); out_free_dev: kfree(zram); return ret; }","- zram_comp_params_reset(zram);
- comp_algorithm_set(zram, ZRAM_PRIMARY_COMP, default_compressor);
+ zram_comp_params_reset(zram);
+ comp_algorithm_set(zram, ZRAM_PRIMARY_COMP, default_compressor);","static int zram_add(void) { struct queue_limits lim = { .logical_block_size = ZRAM_LOGICAL_BLOCK_SIZE, .physical_block_size = PAGE_SIZE, .io_min = PAGE_SIZE, .io_opt = PAGE_SIZE, .max_hw_discard_sectors = UINT_MAX, #if ZRAM_LOGICAL_BLOCK_SIZE == PAGE_SIZE .max_write_zeroes_sectors = UINT_MAX, #endif .features = BLK_FEAT_STABLE_WRITES | BLK_FEAT_SYNCHRONOUS, }; struct zram *zram; int ret, device_id; zram = kzalloc(sizeof(struct zram), GFP_KERNEL); if (!zram) return -ENOMEM; ret = idr_alloc(&zram_index_idr, zram, 0, 0, GFP_KERNEL); if (ret < 0) goto out_free_dev; device_id = ret; init_rwsem(&zram->init_lock); #ifdef CONFIG_ZRAM_WRITEBACK spin_lock_init(&zram->wb_limit_lock); #endif zram->disk = blk_alloc_disk(&lim, NUMA_NO_NODE); if (IS_ERR(zram->disk)) { pr_err(""Error allocating disk structure for device %d\n"", device_id); ret = PTR_ERR(zram->disk); goto out_free_idr; } zram->disk->major = zram_major; zram->disk->first_minor = device_id; zram->disk->minors = 1; zram->disk->flags |= GENHD_FL_NO_PART; zram->disk->fops = &zram_devops; zram->disk->private_data = zram; snprintf(zram->disk->disk_name, 16, ""zram%d"", device_id); atomic_set(&zram->pp_in_progress, 0); zram_comp_params_reset(zram); comp_algorithm_set(zram, ZRAM_PRIMARY_COMP, default_compressor); set_capacity(zram->disk, 0); ret = device_add_disk(NULL, zram->disk, zram_disk_groups); if (ret) goto out_cleanup_disk; zram_debugfs_register(zram); pr_info(""Added device: %s\n"", zram->disk->disk_name); return device_id; out_cleanup_disk: put_disk(zram->disk); out_free_idr: idr_remove(&zram_index_idr, device_id); out_free_dev: kfree(zram); return ret; }"
88----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44957/bad/privcmd.c----irqfd_wakeup,"irqfd_wakeup(wait_queue_entry_t *wait, unsigned int mode, int sync, void *key) { struct privcmd_kernel_irqfd *kirqfd = container_of(wait, struct privcmd_kernel_irqfd, wait); __poll_t flags = key_to_poll(key); if (flags & EPOLLIN) irqfd_inject(kirqfd); if (flags & EPOLLHUP) { <S2SV_StartVul> mutex_lock(&irqfds_lock); <S2SV_EndVul> irqfd_deactivate(kirqfd); <S2SV_StartVul> mutex_unlock(&irqfds_lock); <S2SV_EndVul> } return 0; }","- mutex_lock(&irqfds_lock);
- mutex_unlock(&irqfds_lock);
+ unsigned long flags;
+ spin_lock_irqsave(&irqfds_lock, flags);
+ spin_unlock_irqrestore(&irqfds_lock, flags);","irqfd_wakeup(wait_queue_entry_t *wait, unsigned int mode, int sync, void *key) { struct privcmd_kernel_irqfd *kirqfd = container_of(wait, struct privcmd_kernel_irqfd, wait); __poll_t flags = key_to_poll(key); if (flags & EPOLLIN) irqfd_inject(kirqfd); if (flags & EPOLLHUP) { unsigned long flags; spin_lock_irqsave(&irqfds_lock, flags); irqfd_deactivate(kirqfd); spin_unlock_irqrestore(&irqfds_lock, flags); } return 0; }"
983----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50221/bad/vangogh_ppt.c----vangogh_tables_init,"static int vangogh_tables_init(struct smu_context *smu) { struct smu_table_context *smu_table = &smu->smu_table; struct smu_table *tables = smu_table->tables; uint32_t if_version; uint32_t smu_version; uint32_t ret = 0; ret = smu_cmn_get_smc_version(smu, &if_version, &smu_version); if (ret) { return ret; } SMU_TABLE_INIT(tables, SMU_TABLE_WATERMARKS, sizeof(Watermarks_t), PAGE_SIZE, AMDGPU_GEM_DOMAIN_VRAM); SMU_TABLE_INIT(tables, SMU_TABLE_DPMCLOCKS, sizeof(DpmClocks_t), PAGE_SIZE, AMDGPU_GEM_DOMAIN_VRAM); SMU_TABLE_INIT(tables, SMU_TABLE_PMSTATUSLOG, SMU11_TOOL_SIZE, PAGE_SIZE, AMDGPU_GEM_DOMAIN_VRAM); SMU_TABLE_INIT(tables, SMU_TABLE_ACTIVITY_MONITOR_COEFF, sizeof(DpmActivityMonitorCoeffExt_t), PAGE_SIZE, AMDGPU_GEM_DOMAIN_VRAM); if (if_version < 0x3) { SMU_TABLE_INIT(tables, SMU_TABLE_SMU_METRICS, sizeof(SmuMetrics_legacy_t), PAGE_SIZE, AMDGPU_GEM_DOMAIN_VRAM); smu_table->metrics_table = kzalloc(sizeof(SmuMetrics_legacy_t), GFP_KERNEL); } else { SMU_TABLE_INIT(tables, SMU_TABLE_SMU_METRICS, sizeof(SmuMetrics_t), PAGE_SIZE, AMDGPU_GEM_DOMAIN_VRAM); smu_table->metrics_table = kzalloc(sizeof(SmuMetrics_t), GFP_KERNEL); } if (!smu_table->metrics_table) goto err0_out; smu_table->metrics_time = 0; <S2SV_StartVul> if (smu_version >= 0x043F3E00) <S2SV_EndVul> <S2SV_StartVul> smu_table->gpu_metrics_table_size = sizeof(struct gpu_metrics_v2_3); <S2SV_EndVul> <S2SV_StartVul> else <S2SV_EndVul> <S2SV_StartVul> smu_table->gpu_metrics_table_size = sizeof(struct gpu_metrics_v2_2); <S2SV_EndVul> smu_table->gpu_metrics_table = kzalloc(smu_table->gpu_metrics_table_size, GFP_KERNEL); if (!smu_table->gpu_metrics_table) goto err1_out; smu_table->watermarks_table = kzalloc(sizeof(Watermarks_t), GFP_KERNEL); if (!smu_table->watermarks_table) goto err2_out; smu_table->clocks_table = kzalloc(sizeof(DpmClocks_t), GFP_KERNEL); if (!smu_table->clocks_table) goto err3_out; return 0; err3_out: kfree(smu_table->watermarks_table); err2_out: kfree(smu_table->gpu_metrics_table); err1_out: kfree(smu_table->metrics_table); err0_out: return -ENOMEM; }","- if (smu_version >= 0x043F3E00)
- smu_table->gpu_metrics_table_size = sizeof(struct gpu_metrics_v2_3);
- else
- smu_table->gpu_metrics_table_size = sizeof(struct gpu_metrics_v2_2);
+ smu_table->gpu_metrics_table_size = sizeof(struct gpu_metrics_v2_2);
+ smu_table->gpu_metrics_table_size = max(smu_table->gpu_metrics_table_size, sizeof(struct gpu_metrics_v2_3));
+ smu_table->gpu_metrics_table_size = max(smu_table->gpu_metrics_table_size, sizeof(struct gpu_metrics_v2_4));","static int vangogh_tables_init(struct smu_context *smu) { struct smu_table_context *smu_table = &smu->smu_table; struct smu_table *tables = smu_table->tables; uint32_t if_version; uint32_t smu_version; uint32_t ret = 0; ret = smu_cmn_get_smc_version(smu, &if_version, &smu_version); if (ret) { return ret; } SMU_TABLE_INIT(tables, SMU_TABLE_WATERMARKS, sizeof(Watermarks_t), PAGE_SIZE, AMDGPU_GEM_DOMAIN_VRAM); SMU_TABLE_INIT(tables, SMU_TABLE_DPMCLOCKS, sizeof(DpmClocks_t), PAGE_SIZE, AMDGPU_GEM_DOMAIN_VRAM); SMU_TABLE_INIT(tables, SMU_TABLE_PMSTATUSLOG, SMU11_TOOL_SIZE, PAGE_SIZE, AMDGPU_GEM_DOMAIN_VRAM); SMU_TABLE_INIT(tables, SMU_TABLE_ACTIVITY_MONITOR_COEFF, sizeof(DpmActivityMonitorCoeffExt_t), PAGE_SIZE, AMDGPU_GEM_DOMAIN_VRAM); if (if_version < 0x3) { SMU_TABLE_INIT(tables, SMU_TABLE_SMU_METRICS, sizeof(SmuMetrics_legacy_t), PAGE_SIZE, AMDGPU_GEM_DOMAIN_VRAM); smu_table->metrics_table = kzalloc(sizeof(SmuMetrics_legacy_t), GFP_KERNEL); } else { SMU_TABLE_INIT(tables, SMU_TABLE_SMU_METRICS, sizeof(SmuMetrics_t), PAGE_SIZE, AMDGPU_GEM_DOMAIN_VRAM); smu_table->metrics_table = kzalloc(sizeof(SmuMetrics_t), GFP_KERNEL); } if (!smu_table->metrics_table) goto err0_out; smu_table->metrics_time = 0; smu_table->gpu_metrics_table_size = sizeof(struct gpu_metrics_v2_2); smu_table->gpu_metrics_table_size = max(smu_table->gpu_metrics_table_size, sizeof(struct gpu_metrics_v2_3)); smu_table->gpu_metrics_table_size = max(smu_table->gpu_metrics_table_size, sizeof(struct gpu_metrics_v2_4)); smu_table->gpu_metrics_table = kzalloc(smu_table->gpu_metrics_table_size, GFP_KERNEL); if (!smu_table->gpu_metrics_table) goto err1_out; smu_table->watermarks_table = kzalloc(sizeof(Watermarks_t), GFP_KERNEL); if (!smu_table->watermarks_table) goto err2_out; smu_table->clocks_table = kzalloc(sizeof(DpmClocks_t), GFP_KERNEL); if (!smu_table->clocks_table) goto err3_out; return 0; err3_out: kfree(smu_table->watermarks_table); err2_out: kfree(smu_table->gpu_metrics_table); err1_out: kfree(smu_table->metrics_table); err0_out: return -ENOMEM; }"
780----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50057/bad/core.c----tps6598x_remove,"static void tps6598x_remove(struct i2c_client *client) { struct tps6598x *tps = i2c_get_clientdata(client); if (!client->irq) cancel_delayed_work_sync(&tps->wq_poll); <S2SV_StartVul> devm_free_irq(tps->dev, client->irq, tps); <S2SV_EndVul> tps6598x_disconnect(tps, 0); typec_unregister_port(tps->port); usb_role_switch_put(tps->role_sw); tps->data->reset(tps); if (tps->reset) gpiod_set_value_cansleep(tps->reset, 1); }","- devm_free_irq(tps->dev, client->irq, tps);
+ else
+ devm_free_irq(tps->dev, client->irq, tps);","static void tps6598x_remove(struct i2c_client *client) { struct tps6598x *tps = i2c_get_clientdata(client); if (!client->irq) cancel_delayed_work_sync(&tps->wq_poll); else devm_free_irq(tps->dev, client->irq, tps); tps6598x_disconnect(tps, 0); typec_unregister_port(tps->port); usb_role_switch_put(tps->role_sw); tps->data->reset(tps); if (tps->reset) gpiod_set_value_cansleep(tps->reset, 1); }"
1426----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56754/bad/qi.c----caam_qi_init,"int caam_qi_init(struct platform_device *caam_pdev) { int err, i; struct device *ctrldev = &caam_pdev->dev, *qidev; struct caam_drv_private *ctrlpriv; const cpumask_t *cpus = qman_affine_cpus(); cpumask_var_t clean_mask; err = -ENOMEM; if (!zalloc_cpumask_var(&clean_mask, GFP_KERNEL)) goto fail_cpumask; ctrlpriv = dev_get_drvdata(ctrldev); qidev = ctrldev; err = init_cgr(qidev); if (err) { dev_err(qidev, ""CGR initialization failed: %d\n"", err); goto fail_cgr; } err = alloc_rsp_fqs(qidev); if (err) { dev_err(qidev, ""Can't allocate CAAM response FQs: %d\n"", err); goto fail_fqs; } for_each_cpu(i, cpus) { struct caam_qi_pcpu_priv *priv = per_cpu_ptr(&pcpu_qipriv, i); struct caam_napi *caam_napi = &priv->caam_napi; struct napi_struct *irqtask = &caam_napi->irqtask; struct net_device *net_dev; net_dev = alloc_netdev_dummy(0); if (!net_dev) { err = -ENOMEM; goto fail; } cpumask_set_cpu(i, clean_mask); priv->net_dev = net_dev; net_dev->dev = *qidev; netif_napi_add_tx_weight(net_dev, irqtask, caam_qi_poll, CAAM_NAPI_WEIGHT); napi_enable(irqtask); } qi_cache = kmem_cache_create(""caamqicache"", CAAM_QI_MEMCACHE_SIZE, dma_get_cache_alignment(), 0, NULL); if (!qi_cache) { dev_err(qidev, ""Can't allocate CAAM cache\n""); err = -ENOMEM; goto fail; } caam_debugfs_qi_init(ctrlpriv); <S2SV_StartVul> err = devm_add_action_or_reset(qidev, caam_qi_shutdown, ctrlpriv); <S2SV_EndVul> if (err) goto fail2; dev_info(qidev, ""Linux CAAM Queue I/F driver initialised\n""); goto free_cpumask; fail2: kmem_cache_destroy(qi_cache); fail: free_caam_qi_pcpu_netdev(clean_mask); fail_fqs: free_rsp_fqs(); qman_delete_cgr_safe(&qipriv.cgr); qman_release_cgrid(qipriv.cgr.cgrid); fail_cgr: free_cpumask: free_cpumask_var(clean_mask); fail_cpumask: return err; }","- err = devm_add_action_or_reset(qidev, caam_qi_shutdown, ctrlpriv);
+ err = devm_add_action_or_reset(qidev, caam_qi_shutdown, qidev);","int caam_qi_init(struct platform_device *caam_pdev) { int err, i; struct device *ctrldev = &caam_pdev->dev, *qidev; struct caam_drv_private *ctrlpriv; const cpumask_t *cpus = qman_affine_cpus(); cpumask_var_t clean_mask; err = -ENOMEM; if (!zalloc_cpumask_var(&clean_mask, GFP_KERNEL)) goto fail_cpumask; ctrlpriv = dev_get_drvdata(ctrldev); qidev = ctrldev; err = init_cgr(qidev); if (err) { dev_err(qidev, ""CGR initialization failed: %d\n"", err); goto fail_cgr; } err = alloc_rsp_fqs(qidev); if (err) { dev_err(qidev, ""Can't allocate CAAM response FQs: %d\n"", err); goto fail_fqs; } for_each_cpu(i, cpus) { struct caam_qi_pcpu_priv *priv = per_cpu_ptr(&pcpu_qipriv, i); struct caam_napi *caam_napi = &priv->caam_napi; struct napi_struct *irqtask = &caam_napi->irqtask; struct net_device *net_dev; net_dev = alloc_netdev_dummy(0); if (!net_dev) { err = -ENOMEM; goto fail; } cpumask_set_cpu(i, clean_mask); priv->net_dev = net_dev; net_dev->dev = *qidev; netif_napi_add_tx_weight(net_dev, irqtask, caam_qi_poll, CAAM_NAPI_WEIGHT); napi_enable(irqtask); } qi_cache = kmem_cache_create(""caamqicache"", CAAM_QI_MEMCACHE_SIZE, dma_get_cache_alignment(), 0, NULL); if (!qi_cache) { dev_err(qidev, ""Can't allocate CAAM cache\n""); err = -ENOMEM; goto fail; } caam_debugfs_qi_init(ctrlpriv); err = devm_add_action_or_reset(qidev, caam_qi_shutdown, qidev); if (err) goto fail2; dev_info(qidev, ""Linux CAAM Queue I/F driver initialised\n""); goto free_cpumask; fail2: kmem_cache_destroy(qi_cache); fail: free_caam_qi_pcpu_netdev(clean_mask); fail_fqs: free_rsp_fqs(); qman_delete_cgr_safe(&qipriv.cgr); qman_release_cgrid(qipriv.cgr.cgrid); fail_cgr: free_cpumask: free_cpumask_var(clean_mask); fail_cpumask: return err; }"
1024----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50263/bad/fork.c----dup_mmap,"static __latent_entropy int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm) { struct vm_area_struct *mpnt, *tmp; int retval; unsigned long charge = 0; LIST_HEAD(uf); VMA_ITERATOR(vmi, mm, 0); uprobe_start_dup_mmap(); if (mmap_write_lock_killable(oldmm)) { retval = -EINTR; goto fail_uprobe_end; } flush_cache_dup_mm(oldmm); uprobe_dup_mmap(oldmm, mm); mmap_write_lock_nested(mm, SINGLE_DEPTH_NESTING); dup_mm_exe_file(mm, oldmm); mm->total_vm = oldmm->total_vm; mm->data_vm = oldmm->data_vm; mm->exec_vm = oldmm->exec_vm; mm->stack_vm = oldmm->stack_vm; <S2SV_StartVul> retval = ksm_fork(mm, oldmm); <S2SV_EndVul> <S2SV_StartVul> if (retval) <S2SV_EndVul> <S2SV_StartVul> goto out; <S2SV_EndVul> <S2SV_StartVul> khugepaged_fork(mm, oldmm); <S2SV_EndVul> retval = __mt_dup(&oldmm->mm_mt, &mm->mm_mt, GFP_KERNEL); if (unlikely(retval)) goto out; mt_clear_in_rcu(vmi.mas.tree); for_each_vma(vmi, mpnt) { struct file *file; vma_start_write(mpnt); if (mpnt->vm_flags & VM_DONTCOPY) { retval = vma_iter_clear_gfp(&vmi, mpnt->vm_start, mpnt->vm_end, GFP_KERNEL); if (retval) goto loop_out; vm_stat_account(mm, mpnt->vm_flags, -vma_pages(mpnt)); continue; } charge = 0; if (fatal_signal_pending(current)) { retval = -EINTR; goto loop_out; } if (mpnt->vm_flags & VM_ACCOUNT) { unsigned long len = vma_pages(mpnt); if (security_vm_enough_memory_mm(oldmm, len)) goto fail_nomem; charge = len; } tmp = vm_area_dup(mpnt); if (!tmp) goto fail_nomem; retval = vma_dup_policy(mpnt, tmp); if (retval) goto fail_nomem_policy; tmp->vm_mm = mm; retval = dup_userfaultfd(tmp, &uf); if (retval) goto fail_nomem_anon_vma_fork; if (tmp->vm_flags & VM_WIPEONFORK) { tmp->anon_vma = NULL; } else if (anon_vma_fork(tmp, mpnt)) goto fail_nomem_anon_vma_fork; vm_flags_clear(tmp, VM_LOCKED_MASK); if (is_vm_hugetlb_page(tmp)) hugetlb_dup_vma_private(tmp); vma_iter_bulk_store(&vmi, tmp); mm->map_count++; if (tmp->vm_ops && tmp->vm_ops->open) tmp->vm_ops->open(tmp); file = tmp->vm_file; if (file) { struct address_space *mapping = file->f_mapping; get_file(file); i_mmap_lock_write(mapping); if (vma_is_shared_maywrite(tmp)) mapping_allow_writable(mapping); flush_dcache_mmap_lock(mapping); vma_interval_tree_insert_after(tmp, mpnt, &mapping->i_mmap); flush_dcache_mmap_unlock(mapping); i_mmap_unlock_write(mapping); } if (!(tmp->vm_flags & VM_WIPEONFORK)) retval = copy_page_range(tmp, mpnt); if (retval) { mpnt = vma_next(&vmi); goto loop_out; } } retval = arch_dup_mmap(oldmm, mm); loop_out: vma_iter_free(&vmi); if (!retval) { mt_set_in_rcu(vmi.mas.tree); } else if (mpnt) { mas_set_range(&vmi.mas, mpnt->vm_start, mpnt->vm_end - 1); mas_store(&vmi.mas, XA_ZERO_ENTRY); } out: mmap_write_unlock(mm); flush_tlb_mm(oldmm); mmap_write_unlock(oldmm); if (!retval) dup_userfaultfd_complete(&uf); else dup_userfaultfd_fail(&uf); fail_uprobe_end: uprobe_end_dup_mmap(); return retval; fail_nomem_anon_vma_fork: mpol_put(vma_policy(tmp)); fail_nomem_policy: vm_area_free(tmp); fail_nomem: retval = -ENOMEM; vm_unacct_memory(charge); goto loop_out; }","- retval = ksm_fork(mm, oldmm);
- if (retval)
- goto out;
- khugepaged_fork(mm, oldmm);
+ ksm_fork(mm, oldmm);
+ khugepaged_fork(mm, oldmm);","static __latent_entropy int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm) { struct vm_area_struct *mpnt, *tmp; int retval; unsigned long charge = 0; LIST_HEAD(uf); VMA_ITERATOR(vmi, mm, 0); uprobe_start_dup_mmap(); if (mmap_write_lock_killable(oldmm)) { retval = -EINTR; goto fail_uprobe_end; } flush_cache_dup_mm(oldmm); uprobe_dup_mmap(oldmm, mm); mmap_write_lock_nested(mm, SINGLE_DEPTH_NESTING); dup_mm_exe_file(mm, oldmm); mm->total_vm = oldmm->total_vm; mm->data_vm = oldmm->data_vm; mm->exec_vm = oldmm->exec_vm; mm->stack_vm = oldmm->stack_vm; retval = __mt_dup(&oldmm->mm_mt, &mm->mm_mt, GFP_KERNEL); if (unlikely(retval)) goto out; mt_clear_in_rcu(vmi.mas.tree); for_each_vma(vmi, mpnt) { struct file *file; vma_start_write(mpnt); if (mpnt->vm_flags & VM_DONTCOPY) { retval = vma_iter_clear_gfp(&vmi, mpnt->vm_start, mpnt->vm_end, GFP_KERNEL); if (retval) goto loop_out; vm_stat_account(mm, mpnt->vm_flags, -vma_pages(mpnt)); continue; } charge = 0; if (fatal_signal_pending(current)) { retval = -EINTR; goto loop_out; } if (mpnt->vm_flags & VM_ACCOUNT) { unsigned long len = vma_pages(mpnt); if (security_vm_enough_memory_mm(oldmm, len)) goto fail_nomem; charge = len; } tmp = vm_area_dup(mpnt); if (!tmp) goto fail_nomem; retval = vma_dup_policy(mpnt, tmp); if (retval) goto fail_nomem_policy; tmp->vm_mm = mm; retval = dup_userfaultfd(tmp, &uf); if (retval) goto fail_nomem_anon_vma_fork; if (tmp->vm_flags & VM_WIPEONFORK) { tmp->anon_vma = NULL; } else if (anon_vma_fork(tmp, mpnt)) goto fail_nomem_anon_vma_fork; vm_flags_clear(tmp, VM_LOCKED_MASK); if (is_vm_hugetlb_page(tmp)) hugetlb_dup_vma_private(tmp); vma_iter_bulk_store(&vmi, tmp); mm->map_count++; if (tmp->vm_ops && tmp->vm_ops->open) tmp->vm_ops->open(tmp); file = tmp->vm_file; if (file) { struct address_space *mapping = file->f_mapping; get_file(file); i_mmap_lock_write(mapping); if (vma_is_shared_maywrite(tmp)) mapping_allow_writable(mapping); flush_dcache_mmap_lock(mapping); vma_interval_tree_insert_after(tmp, mpnt, &mapping->i_mmap); flush_dcache_mmap_unlock(mapping); i_mmap_unlock_write(mapping); } if (!(tmp->vm_flags & VM_WIPEONFORK)) retval = copy_page_range(tmp, mpnt); if (retval) { mpnt = vma_next(&vmi); goto loop_out; } } retval = arch_dup_mmap(oldmm, mm); loop_out: vma_iter_free(&vmi); if (!retval) { mt_set_in_rcu(vmi.mas.tree); ksm_fork(mm, oldmm); khugepaged_fork(mm, oldmm); } else if (mpnt) { mas_set_range(&vmi.mas, mpnt->vm_start, mpnt->vm_end - 1); mas_store(&vmi.mas, XA_ZERO_ENTRY); } out: mmap_write_unlock(mm); flush_tlb_mm(oldmm); mmap_write_unlock(oldmm); if (!retval) dup_userfaultfd_complete(&uf); else dup_userfaultfd_fail(&uf); fail_uprobe_end: uprobe_end_dup_mmap(); return retval; fail_nomem_anon_vma_fork: mpol_put(vma_policy(tmp)); fail_nomem_policy: vm_area_free(tmp); fail_nomem: retval = -ENOMEM; vm_unacct_memory(charge); goto loop_out; }"
1500----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-57951/bad/hrtimer.c----hrtimers_prepare_cpu,"int hrtimers_prepare_cpu(unsigned int cpu) { struct hrtimer_cpu_base *cpu_base = &per_cpu(hrtimer_bases, cpu); int i; for (i = 0; i < HRTIMER_MAX_CLOCK_BASES; i++) { struct hrtimer_clock_base *clock_b = &cpu_base->clock_base[i]; clock_b->cpu_base = cpu_base; seqcount_raw_spinlock_init(&clock_b->seq, &cpu_base->lock); timerqueue_init_head(&clock_b->active); } cpu_base->cpu = cpu; cpu_base->active_bases = 0; cpu_base->hres_active = 0; cpu_base->hang_detected = 0; cpu_base->next_timer = NULL; cpu_base->softirq_next_timer = NULL; cpu_base->expires_next = KTIME_MAX; cpu_base->softirq_expires_next = KTIME_MAX; cpu_base->online = 1; <S2SV_StartVul> hrtimer_cpu_base_init_expiry_lock(cpu_base); <S2SV_EndVul> return 0; }","- hrtimer_cpu_base_init_expiry_lock(cpu_base);
+ hrtimer_cpu_base_init_expiry_lock(cpu_base);
+ return 0;
+ }","int hrtimers_prepare_cpu(unsigned int cpu) { struct hrtimer_cpu_base *cpu_base = &per_cpu(hrtimer_bases, cpu); int i; for (i = 0; i < HRTIMER_MAX_CLOCK_BASES; i++) { struct hrtimer_clock_base *clock_b = &cpu_base->clock_base[i]; clock_b->cpu_base = cpu_base; seqcount_raw_spinlock_init(&clock_b->seq, &cpu_base->lock); timerqueue_init_head(&clock_b->active); } cpu_base->cpu = cpu; hrtimer_cpu_base_init_expiry_lock(cpu_base); return 0; }"
1032----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50273/bad/delayed-ref.c----insert_delayed_ref,"static bool insert_delayed_ref(struct btrfs_delayed_ref_root *root, struct btrfs_delayed_ref_head *href, struct btrfs_delayed_ref_node *ref) { struct btrfs_delayed_ref_node *exist; int mod; spin_lock(&href->lock); exist = tree_insert(&href->ref_tree, ref); if (!exist) { if (ref->action == BTRFS_ADD_DELAYED_REF) list_add_tail(&ref->add_list, &href->ref_add_list); atomic_inc(&root->num_entries); spin_unlock(&href->lock); return false; } if (exist->action == ref->action) { mod = ref->ref_mod; } else { if (exist->ref_mod < ref->ref_mod) { exist->action = ref->action; mod = -exist->ref_mod; exist->ref_mod = ref->ref_mod; if (ref->action == BTRFS_ADD_DELAYED_REF) list_add_tail(&exist->add_list, &href->ref_add_list); else if (ref->action == BTRFS_DROP_DELAYED_REF) { ASSERT(!list_empty(&exist->add_list)); <S2SV_StartVul> list_del(&exist->add_list); <S2SV_EndVul> } else { ASSERT(0); } } else mod = -ref->ref_mod; } exist->ref_mod += mod; if (exist->ref_mod == 0) drop_delayed_ref(root, href, exist); spin_unlock(&href->lock); return true; }","- list_del(&exist->add_list);
+ list_del_init(&exist->add_list);","static bool insert_delayed_ref(struct btrfs_delayed_ref_root *root, struct btrfs_delayed_ref_head *href, struct btrfs_delayed_ref_node *ref) { struct btrfs_delayed_ref_node *exist; int mod; spin_lock(&href->lock); exist = tree_insert(&href->ref_tree, ref); if (!exist) { if (ref->action == BTRFS_ADD_DELAYED_REF) list_add_tail(&ref->add_list, &href->ref_add_list); atomic_inc(&root->num_entries); spin_unlock(&href->lock); return false; } if (exist->action == ref->action) { mod = ref->ref_mod; } else { if (exist->ref_mod < ref->ref_mod) { exist->action = ref->action; mod = -exist->ref_mod; exist->ref_mod = ref->ref_mod; if (ref->action == BTRFS_ADD_DELAYED_REF) list_add_tail(&exist->add_list, &href->ref_add_list); else if (ref->action == BTRFS_DROP_DELAYED_REF) { ASSERT(!list_empty(&exist->add_list)); list_del_init(&exist->add_list); } else { ASSERT(0); } } else mod = -ref->ref_mod; } exist->ref_mod += mod; if (exist->ref_mod == 0) drop_delayed_ref(root, href, exist); spin_unlock(&href->lock); return true; }"
1187----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53175/bad/namespace.c----*create_ipc_ns,"static struct ipc_namespace *create_ipc_ns(struct user_namespace *user_ns, struct ipc_namespace *old_ns) { struct ipc_namespace *ns; struct ucounts *ucounts; int err; err = -ENOSPC; again: ucounts = inc_ipc_namespaces(user_ns); if (!ucounts) { if (flush_work(&free_ipc_work)) goto again; goto fail; } err = -ENOMEM; ns = kzalloc(sizeof(struct ipc_namespace), GFP_KERNEL_ACCOUNT); if (ns == NULL) goto fail_dec; err = ns_alloc_inum(&ns->ns); if (err) goto fail_free; ns->ns.ops = &ipcns_operations; refcount_set(&ns->ns.count, 1); ns->user_ns = get_user_ns(user_ns); ns->ucounts = ucounts; err = mq_init_ns(ns); if (err) goto fail_put; err = -ENOMEM; if (!setup_mq_sysctls(ns)) goto fail_put; if (!setup_ipc_sysctls(ns)) goto fail_mq; err = msg_init_ns(ns); if (err) <S2SV_StartVul> goto fail_put; <S2SV_EndVul> sem_init_ns(ns); shm_init_ns(ns); return ns; fail_mq: retire_mq_sysctls(ns); fail_put: put_user_ns(ns->user_ns); ns_free_inum(&ns->ns); fail_free: kfree(ns); fail_dec: dec_ipc_namespaces(ucounts); fail: return ERR_PTR(err); }","- goto fail_put;
+ goto fail_ipc;
+ fail_ipc:
+ retire_ipc_sysctls(ns);","static struct ipc_namespace *create_ipc_ns(struct user_namespace *user_ns, struct ipc_namespace *old_ns) { struct ipc_namespace *ns; struct ucounts *ucounts; int err; err = -ENOSPC; again: ucounts = inc_ipc_namespaces(user_ns); if (!ucounts) { if (flush_work(&free_ipc_work)) goto again; goto fail; } err = -ENOMEM; ns = kzalloc(sizeof(struct ipc_namespace), GFP_KERNEL_ACCOUNT); if (ns == NULL) goto fail_dec; err = ns_alloc_inum(&ns->ns); if (err) goto fail_free; ns->ns.ops = &ipcns_operations; refcount_set(&ns->ns.count, 1); ns->user_ns = get_user_ns(user_ns); ns->ucounts = ucounts; err = mq_init_ns(ns); if (err) goto fail_put; err = -ENOMEM; if (!setup_mq_sysctls(ns)) goto fail_put; if (!setup_ipc_sysctls(ns)) goto fail_mq; err = msg_init_ns(ns); if (err) goto fail_ipc; sem_init_ns(ns); shm_init_ns(ns); return ns; fail_ipc: retire_ipc_sysctls(ns); fail_mq: retire_mq_sysctls(ns); fail_put: put_user_ns(ns->user_ns); ns_free_inum(&ns->ns); fail_free: kfree(ns); fail_dec: dec_ipc_namespaces(ucounts); fail: return ERR_PTR(err); }"
214----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46696/bad/nfs4state.c----nfsd4_cb_getattr_release,"nfsd4_cb_getattr_release(struct nfsd4_callback *cb) { struct nfs4_cb_fattr *ncf = container_of(cb, struct nfs4_cb_fattr, ncf_getattr); struct nfs4_delegation *dp = container_of(ncf, struct nfs4_delegation, dl_cb_fattr); <S2SV_StartVul> nfs4_put_stid(&dp->dl_stid); <S2SV_EndVul> clear_bit(CB_GETATTR_BUSY, &ncf->ncf_cb_flags); wake_up_bit(&ncf->ncf_cb_flags, CB_GETATTR_BUSY); }","- nfs4_put_stid(&dp->dl_stid);
+ nfs4_put_stid(&dp->dl_stid);","nfsd4_cb_getattr_release(struct nfsd4_callback *cb) { struct nfs4_cb_fattr *ncf = container_of(cb, struct nfs4_cb_fattr, ncf_getattr); struct nfs4_delegation *dp = container_of(ncf, struct nfs4_delegation, dl_cb_fattr); clear_bit(CB_GETATTR_BUSY, &ncf->ncf_cb_flags); wake_up_bit(&ncf->ncf_cb_flags, CB_GETATTR_BUSY); nfs4_put_stid(&dp->dl_stid); }"
368----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46832/bad/cevt-r4k.c----r4k_clockevent_init,"int r4k_clockevent_init(void) { unsigned long flags = IRQF_PERCPU | IRQF_TIMER | IRQF_SHARED; unsigned int cpu = smp_processor_id(); struct clock_event_device *cd; unsigned int irq, min_delta; if (!cpu_has_counter || !mips_hpt_frequency) return -ENXIO; if (!c0_compare_int_usable()) return -ENXIO; <S2SV_StartVul> irq = get_c0_compare_int(); <S2SV_EndVul> cd = &per_cpu(mips_clockevent_device, cpu); cd->name = ""MIPS""; cd->features = CLOCK_EVT_FEAT_ONESHOT | CLOCK_EVT_FEAT_C3STOP | CLOCK_EVT_FEAT_PERCPU; min_delta = calculate_min_delta(); cd->rating = 300; <S2SV_StartVul> cd->irq = irq; <S2SV_EndVul> cd->cpumask = cpumask_of(cpu); cd->set_next_event = mips_next_event; cd->event_handler = mips_event_handler; clockevents_config_and_register(cd, mips_hpt_frequency, min_delta, 0x7fffffff); if (cp0_timer_irq_installed) return 0; cp0_timer_irq_installed = 1; if (request_irq(irq, c0_compare_interrupt, flags, ""timer"", c0_compare_interrupt)) pr_err(""Failed to request irq %d (timer)\n"", irq); return 0; }","- irq = get_c0_compare_int();
- cd->irq = irq;
+ irq = get_c0_compare_int();","int r4k_clockevent_init(void) { unsigned long flags = IRQF_PERCPU | IRQF_TIMER | IRQF_SHARED; unsigned int cpu = smp_processor_id(); struct clock_event_device *cd; unsigned int irq, min_delta; if (!cpu_has_counter || !mips_hpt_frequency) return -ENXIO; if (!c0_compare_int_usable()) return -ENXIO; cd = &per_cpu(mips_clockevent_device, cpu); cd->name = ""MIPS""; cd->features = CLOCK_EVT_FEAT_ONESHOT | CLOCK_EVT_FEAT_C3STOP | CLOCK_EVT_FEAT_PERCPU; min_delta = calculate_min_delta(); cd->rating = 300; cd->cpumask = cpumask_of(cpu); cd->set_next_event = mips_next_event; cd->event_handler = mips_event_handler; clockevents_config_and_register(cd, mips_hpt_frequency, min_delta, 0x7fffffff); if (cp0_timer_irq_installed) return 0; cp0_timer_irq_installed = 1; irq = get_c0_compare_int(); if (request_irq(irq, c0_compare_interrupt, flags, ""timer"", c0_compare_interrupt)) pr_err(""Failed to request irq %d (timer)\n"", irq); return 0; }"
1337----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56655/bad/nf_tables_api.c----__nft_release_basechain,"int __nft_release_basechain(struct nft_ctx *ctx) { struct nft_rule *rule; if (WARN_ON_ONCE(!nft_is_base_chain(ctx->chain))) return 0; nf_tables_unregister_hook(ctx->net, ctx->chain->table, ctx->chain); list_for_each_entry(rule, &ctx->chain->rules, list) <S2SV_StartVul> nft_use_dec(&ctx->chain->use); <S2SV_EndVul> <S2SV_StartVul> nft_chain_del(ctx->chain); <S2SV_EndVul> <S2SV_StartVul> nft_use_dec(&ctx->table->use); <S2SV_EndVul> <S2SV_StartVul> if (maybe_get_net(ctx->net)) <S2SV_EndVul> <S2SV_StartVul> call_rcu(&ctx->chain->rcu_head, nft_release_basechain_rcu); <S2SV_EndVul> <S2SV_StartVul> else <S2SV_EndVul> <S2SV_StartVul> __nft_release_basechain_now(ctx); <S2SV_EndVul> return 0; <S2SV_StartVul> } <S2SV_EndVul>","- nft_use_dec(&ctx->chain->use);
- nft_chain_del(ctx->chain);
- nft_use_dec(&ctx->table->use);
- if (maybe_get_net(ctx->net))
- call_rcu(&ctx->chain->rcu_head, nft_release_basechain_rcu);
- else
- __nft_release_basechain_now(ctx);
- }
+ nft_use_dec(&ctx->chain->use);","int __nft_release_basechain(struct nft_ctx *ctx) { struct nft_rule *rule; if (WARN_ON_ONCE(!nft_is_base_chain(ctx->chain))) return 0; nf_tables_unregister_hook(ctx->net, ctx->chain->table, ctx->chain); list_for_each_entry(rule, &ctx->chain->rules, list) nft_use_dec(&ctx->chain->use); nft_chain_del(ctx->chain); nft_use_dec(&ctx->table->use); if (!maybe_get_net(ctx->net)) { __nft_release_basechain_now(ctx); return 0; } synchronize_rcu(); __nft_release_basechain_now(ctx); put_net(ctx->net); return 0; }"
1293----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56588/bad/hisi_sas_v3_hw.c----debugfs_init_v3_hw,"static void debugfs_init_v3_hw(struct hisi_hba *hisi_hba) { struct device *dev = hisi_hba->dev; hisi_hba->debugfs_dir = debugfs_create_dir(dev_name(dev), hisi_sas_debugfs_dir); debugfs_bist_init_v3_hw(hisi_hba); <S2SV_StartVul> hisi_hba->debugfs_dump_dentry = <S2SV_EndVul> <S2SV_StartVul> debugfs_create_dir(""dump"", hisi_hba->debugfs_dir); <S2SV_EndVul> debugfs_phy_down_cnt_init_v3_hw(hisi_hba); debugfs_fifo_init_v3_hw(hisi_hba); debugfs_create_file(""trigger_dump"", 0200, hisi_hba->debugfs_dir, hisi_hba, &debugfs_trigger_dump_v3_hw_fops); }","- hisi_hba->debugfs_dump_dentry =
- debugfs_create_dir(""dump"", hisi_hba->debugfs_dir);
+ debugfs_dump_init_v3_hw(hisi_hba);","static void debugfs_init_v3_hw(struct hisi_hba *hisi_hba) { struct device *dev = hisi_hba->dev; hisi_hba->debugfs_dir = debugfs_create_dir(dev_name(dev), hisi_sas_debugfs_dir); debugfs_bist_init_v3_hw(hisi_hba); debugfs_dump_init_v3_hw(hisi_hba); debugfs_phy_down_cnt_init_v3_hw(hisi_hba); debugfs_fifo_init_v3_hw(hisi_hba); debugfs_create_file(""trigger_dump"", 0200, hisi_hba->debugfs_dir, hisi_hba, &debugfs_trigger_dump_v3_hw_fops); }"
1298----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56588/bad/hisi_sas_v3_hw.c----debugfs_release_v3_hw,"static void debugfs_release_v3_hw(struct hisi_hba *hisi_hba, int dump_index) { struct device *dev = hisi_hba->dev; int i; devm_kfree(dev, hisi_hba->debugfs_iost_cache[dump_index].cache); devm_kfree(dev, hisi_hba->debugfs_itct_cache[dump_index].cache); devm_kfree(dev, hisi_hba->debugfs_iost[dump_index].iost); devm_kfree(dev, hisi_hba->debugfs_itct[dump_index].itct); <S2SV_StartVul> for (i = 0; i < hisi_hba->queue_count; i++) <S2SV_EndVul> devm_kfree(dev, hisi_hba->debugfs_dq[dump_index][i].hdr); <S2SV_StartVul> for (i = 0; i < hisi_hba->queue_count; i++) <S2SV_EndVul> devm_kfree(dev, hisi_hba->debugfs_cq[dump_index][i].complete_hdr); <S2SV_StartVul> for (i = 0; i < DEBUGFS_REGS_NUM; i++) <S2SV_EndVul> devm_kfree(dev, hisi_hba->debugfs_regs[dump_index][i].data); <S2SV_StartVul> for (i = 0; i < hisi_hba->n_phy; i++) <S2SV_EndVul> devm_kfree(dev, hisi_hba->debugfs_port_reg[dump_index][i].data); }","- for (i = 0; i < hisi_hba->queue_count; i++)
- for (i = 0; i < hisi_hba->queue_count; i++)
- for (i = 0; i < DEBUGFS_REGS_NUM; i++)
- for (i = 0; i < hisi_hba->n_phy; i++)
+ hisi_hba->debugfs_iost_cache[dump_index].cache = NULL;
+ hisi_hba->debugfs_itct_cache[dump_index].cache = NULL;
+ hisi_hba->debugfs_iost[dump_index].iost = NULL;
+ hisi_hba->debugfs_itct[dump_index].itct = NULL;
+ for (i = 0; i < hisi_hba->queue_count; i++) {
+ hisi_hba->debugfs_dq[dump_index][i].hdr = NULL;
+ }
+ for (i = 0; i < hisi_hba->queue_count; i++) {
+ hisi_hba->debugfs_cq[dump_index][i].complete_hdr = NULL;
+ }
+ for (i = 0; i < DEBUGFS_REGS_NUM; i++) {
+ hisi_hba->debugfs_regs[dump_index][i].data = NULL;
+ }
+ for (i = 0; i < hisi_hba->n_phy; i++) {
+ hisi_hba->debugfs_port_reg[dump_index][i].data = NULL;
+ }
+ }","static void debugfs_release_v3_hw(struct hisi_hba *hisi_hba, int dump_index) { struct device *dev = hisi_hba->dev; int i; devm_kfree(dev, hisi_hba->debugfs_iost_cache[dump_index].cache); hisi_hba->debugfs_iost_cache[dump_index].cache = NULL; devm_kfree(dev, hisi_hba->debugfs_itct_cache[dump_index].cache); hisi_hba->debugfs_itct_cache[dump_index].cache = NULL; devm_kfree(dev, hisi_hba->debugfs_iost[dump_index].iost); hisi_hba->debugfs_iost[dump_index].iost = NULL; devm_kfree(dev, hisi_hba->debugfs_itct[dump_index].itct); hisi_hba->debugfs_itct[dump_index].itct = NULL; for (i = 0; i < hisi_hba->queue_count; i++) { devm_kfree(dev, hisi_hba->debugfs_dq[dump_index][i].hdr); hisi_hba->debugfs_dq[dump_index][i].hdr = NULL; } for (i = 0; i < hisi_hba->queue_count; i++) { devm_kfree(dev, hisi_hba->debugfs_cq[dump_index][i].complete_hdr); hisi_hba->debugfs_cq[dump_index][i].complete_hdr = NULL; } for (i = 0; i < DEBUGFS_REGS_NUM; i++) { devm_kfree(dev, hisi_hba->debugfs_regs[dump_index][i].data); hisi_hba->debugfs_regs[dump_index][i].data = NULL; } for (i = 0; i < hisi_hba->n_phy; i++) { devm_kfree(dev, hisi_hba->debugfs_port_reg[dump_index][i].data); hisi_hba->debugfs_port_reg[dump_index][i].data = NULL; } }"
1304----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56602/bad/socket.c----ieee802154_create,"static int ieee802154_create(struct net *net, struct socket *sock, int protocol, int kern) { struct sock *sk; int rc; struct proto *proto; const struct proto_ops *ops; if (!net_eq(net, &init_net)) return -EAFNOSUPPORT; switch (sock->type) { case SOCK_RAW: rc = -EPERM; if (!capable(CAP_NET_RAW)) goto out; proto = &ieee802154_raw_prot; ops = &ieee802154_raw_ops; break; case SOCK_DGRAM: proto = &ieee802154_dgram_prot; ops = &ieee802154_dgram_ops; break; default: rc = -ESOCKTNOSUPPORT; goto out; } rc = -ENOMEM; sk = sk_alloc(net, PF_IEEE802154, GFP_KERNEL, proto, kern); if (!sk) goto out; rc = 0; sock->ops = ops; sock_init_data(sock, sk); sk->sk_destruct = ieee802154_sock_destruct; sk->sk_family = PF_IEEE802154; sock_set_flag(sk, SOCK_ZAPPED); if (sk->sk_prot->hash) { rc = sk->sk_prot->hash(sk); <S2SV_StartVul> if (rc) { <S2SV_EndVul> <S2SV_StartVul> sk_common_release(sk); <S2SV_EndVul> <S2SV_StartVul> goto out; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> if (sk->sk_prot->init) { rc = sk->sk_prot->init(sk); if (rc) <S2SV_StartVul> sk_common_release(sk); <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> out: return rc; <S2SV_StartVul> } <S2SV_EndVul>","- if (rc) {
- sk_common_release(sk);
- goto out;
- }
- }
- sk_common_release(sk);
- }
- }
+ if (rc)
+ goto out_sk_release;
+ if (rc)
+ goto out_sk_release;
+ out_sk_release:
+ sk_common_release(sk);
+ sock->sk = NULL;
+ goto out;","static int ieee802154_create(struct net *net, struct socket *sock, int protocol, int kern) { struct sock *sk; int rc; struct proto *proto; const struct proto_ops *ops; if (!net_eq(net, &init_net)) return -EAFNOSUPPORT; switch (sock->type) { case SOCK_RAW: rc = -EPERM; if (!capable(CAP_NET_RAW)) goto out; proto = &ieee802154_raw_prot; ops = &ieee802154_raw_ops; break; case SOCK_DGRAM: proto = &ieee802154_dgram_prot; ops = &ieee802154_dgram_ops; break; default: rc = -ESOCKTNOSUPPORT; goto out; } rc = -ENOMEM; sk = sk_alloc(net, PF_IEEE802154, GFP_KERNEL, proto, kern); if (!sk) goto out; rc = 0; sock->ops = ops; sock_init_data(sock, sk); sk->sk_destruct = ieee802154_sock_destruct; sk->sk_family = PF_IEEE802154; sock_set_flag(sk, SOCK_ZAPPED); if (sk->sk_prot->hash) { rc = sk->sk_prot->hash(sk); if (rc) goto out_sk_release; } if (sk->sk_prot->init) { rc = sk->sk_prot->init(sk); if (rc) goto out_sk_release; } out: return rc; out_sk_release: sk_common_release(sk); sock->sk = NULL; goto out; }"
7----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-43891/bad/trace_events.c----event_filter_read,"event_filter_read(struct file *filp, char __user *ubuf, size_t cnt, loff_t *ppos) { struct trace_event_file *file; struct trace_seq *s; int r = -ENODEV; if (*ppos) return 0; s = kmalloc(sizeof(*s), GFP_KERNEL); if (!s) return -ENOMEM; trace_seq_init(s); mutex_lock(&event_mutex); <S2SV_StartVul> file = event_file_data(filp); <S2SV_EndVul> <S2SV_StartVul> if (file && !(file->flags & EVENT_FILE_FL_FREED)) <S2SV_EndVul> print_event_filter(file, s); mutex_unlock(&event_mutex); if (file) r = simple_read_from_buffer(ubuf, cnt, ppos, s->buffer, trace_seq_used(s)); kfree(s); return r; }","- file = event_file_data(filp);
- if (file && !(file->flags & EVENT_FILE_FL_FREED))
+ file = event_file_file(filp);
+ if (file)","event_filter_read(struct file *filp, char __user *ubuf, size_t cnt, loff_t *ppos) { struct trace_event_file *file; struct trace_seq *s; int r = -ENODEV; if (*ppos) return 0; s = kmalloc(sizeof(*s), GFP_KERNEL); if (!s) return -ENOMEM; trace_seq_init(s); mutex_lock(&event_mutex); file = event_file_file(filp); if (file) print_event_filter(file, s); mutex_unlock(&event_mutex); if (file) r = simple_read_from_buffer(ubuf, cnt, ppos, s->buffer, trace_seq_used(s)); kfree(s); return r; }"
318----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46784/bad/mana_en.c----mana_destroy_rxq,"static void mana_destroy_rxq(struct mana_port_context *apc, struct mana_rxq *rxq, bool validate_state) { struct gdma_context *gc = apc->ac->gdma_dev->gdma_context; struct mana_recv_buf_oob *rx_oob; struct device *dev = gc->dev; struct napi_struct *napi; struct page *page; int i; if (!rxq) return; napi = &rxq->rx_cq.napi; <S2SV_StartVul> if (validate_state) <S2SV_EndVul> <S2SV_StartVul> napi_synchronize(napi); <S2SV_EndVul> <S2SV_StartVul> napi_disable(napi); <S2SV_EndVul> xdp_rxq_info_unreg(&rxq->xdp_rxq); <S2SV_StartVul> netif_napi_del(napi); <S2SV_EndVul> mana_destroy_wq_obj(apc, GDMA_RQ, rxq->rxobj); mana_deinit_cq(apc, &rxq->rx_cq); if (rxq->xdp_save_va) put_page(virt_to_head_page(rxq->xdp_save_va)); for (i = 0; i < rxq->num_rx_buf; i++) { rx_oob = &rxq->rx_oobs[i]; if (!rx_oob->buf_va) continue; dma_unmap_single(dev, rx_oob->sgl[0].address, rx_oob->sgl[0].size, DMA_FROM_DEVICE); page = virt_to_head_page(rx_oob->buf_va); if (rx_oob->from_pool) page_pool_put_full_page(rxq->page_pool, page, false); else put_page(page); rx_oob->buf_va = NULL; } page_pool_destroy(rxq->page_pool); if (rxq->gdma_rq) mana_gd_destroy_queue(gc, rxq->gdma_rq); kfree(rxq); }","- if (validate_state)
- napi_synchronize(napi);
- napi_disable(napi);
- netif_napi_del(napi);
+ if (napi_initialized) {
+ napi_synchronize(napi);
+ napi_disable(napi);
+ netif_napi_del(napi);
+ }","static void mana_destroy_rxq(struct mana_port_context *apc, struct mana_rxq *rxq, bool napi_initialized) { struct gdma_context *gc = apc->ac->gdma_dev->gdma_context; struct mana_recv_buf_oob *rx_oob; struct device *dev = gc->dev; struct napi_struct *napi; struct page *page; int i; if (!rxq) return; napi = &rxq->rx_cq.napi; if (napi_initialized) { napi_synchronize(napi); napi_disable(napi); netif_napi_del(napi); } xdp_rxq_info_unreg(&rxq->xdp_rxq); mana_destroy_wq_obj(apc, GDMA_RQ, rxq->rxobj); mana_deinit_cq(apc, &rxq->rx_cq); if (rxq->xdp_save_va) put_page(virt_to_head_page(rxq->xdp_save_va)); for (i = 0; i < rxq->num_rx_buf; i++) { rx_oob = &rxq->rx_oobs[i]; if (!rx_oob->buf_va) continue; dma_unmap_single(dev, rx_oob->sgl[0].address, rx_oob->sgl[0].size, DMA_FROM_DEVICE); page = virt_to_head_page(rx_oob->buf_va); if (rx_oob->from_pool) page_pool_put_full_page(rxq->page_pool, page, false); else put_page(page); rx_oob->buf_va = NULL; } page_pool_destroy(rxq->page_pool); if (rxq->gdma_rq) mana_gd_destroy_queue(gc, rxq->gdma_rq); kfree(rxq); }"
639----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49941/bad/gpiolib.c----*gpiod_get_label,"const char *gpiod_get_label(struct gpio_desc *desc) { struct gpio_desc_label *label; unsigned long flags; flags = READ_ONCE(desc->flags); label = srcu_dereference_check(desc->label, &desc->gdev->desc_srcu, srcu_read_lock_held(&desc->gdev->desc_srcu)); if (test_bit(FLAG_USED_AS_IRQ, &flags)) <S2SV_StartVul> return label->str ?: ""interrupt""; <S2SV_EndVul> if (!test_bit(FLAG_REQUESTED, &flags)) return NULL; <S2SV_StartVul> return label->str; <S2SV_EndVul> }","- return label->str ?: ""interrupt"";
- return label->str;
+ return label ? label->str : ""interrupt"";
+ return label ? label->str : NULL;","const char *gpiod_get_label(struct gpio_desc *desc) { struct gpio_desc_label *label; unsigned long flags; flags = READ_ONCE(desc->flags); label = srcu_dereference_check(desc->label, &desc->gdev->desc_srcu, srcu_read_lock_held(&desc->gdev->desc_srcu)); if (test_bit(FLAG_USED_AS_IRQ, &flags)) return label ? label->str : ""interrupt""; if (!test_bit(FLAG_REQUESTED, &flags)) return NULL; return label ? label->str : NULL; }"
608----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49921/bad/dcn10_hwseq.c----dcn10_init_hw,"void dcn10_init_hw(struct dc *dc) { int i; struct abm *abm = dc->res_pool->abm; struct dmcu *dmcu = dc->res_pool->dmcu; struct dce_hwseq *hws = dc->hwseq; struct dc_bios *dcb = dc->ctx->dc_bios; struct resource_pool *res_pool = dc->res_pool; uint32_t backlight = MAX_BACKLIGHT_LEVEL; uint32_t user_level = MAX_BACKLIGHT_LEVEL; bool is_optimized_init_done = false; if (dc->clk_mgr && dc->clk_mgr->funcs->init_clocks) dc->clk_mgr->funcs->init_clocks(dc->clk_mgr); <S2SV_StartVul> if (dc->clk_mgr->clks.dispclk_khz != 0 && dc->clk_mgr->clks.dppclk_khz != 0) { <S2SV_EndVul> dc->current_state->bw_ctx.bw.dcn.clk.dispclk_khz = dc->clk_mgr->clks.dispclk_khz; dc->current_state->bw_ctx.bw.dcn.clk.dppclk_khz = dc->clk_mgr->clks.dppclk_khz; } if (dc->res_pool->dccg && dc->res_pool->dccg->funcs->dccg_init) dc->res_pool->dccg->funcs->dccg_init(res_pool->dccg); if (!dcb->funcs->is_accelerated_mode(dcb)) hws->funcs.disable_vga(dc->hwseq); if (!dc_dmub_srv_optimized_init_done(dc->ctx->dmub_srv)) hws->funcs.bios_golden_init(dc); if (dc->ctx->dc_bios->fw_info_valid) { res_pool->ref_clocks.xtalin_clock_inKhz = dc->ctx->dc_bios->fw_info.pll_info.crystal_frequency; if (res_pool->dccg && res_pool->hubbub) { (res_pool->dccg->funcs->get_dccg_ref_freq)(res_pool->dccg, dc->ctx->dc_bios->fw_info.pll_info.crystal_frequency, &res_pool->ref_clocks.dccg_ref_clock_inKhz); (res_pool->hubbub->funcs->get_dchub_ref_freq)(res_pool->hubbub, res_pool->ref_clocks.dccg_ref_clock_inKhz, &res_pool->ref_clocks.dchub_ref_clock_inKhz); } else { res_pool->ref_clocks.dccg_ref_clock_inKhz = res_pool->ref_clocks.xtalin_clock_inKhz; res_pool->ref_clocks.dchub_ref_clock_inKhz = res_pool->ref_clocks.xtalin_clock_inKhz; } } else ASSERT_CRITICAL(false); for (i = 0; i < dc->link_count; i++) { struct dc_link *link = dc->links[i]; if (!is_optimized_init_done) link->link_enc->funcs->hw_init(link->link_enc); if (link->link_enc->funcs->is_dig_enabled && link->link_enc->funcs->is_dig_enabled(link->link_enc)) { link->link_status.link_active = true; if (link->link_enc->funcs->fec_is_active && link->link_enc->funcs->fec_is_active(link->link_enc)) link->fec_state = dc_link_fec_enabled; } } dc->link_srv->blank_all_dp_displays(dc); if (hws->funcs.enable_power_gating_plane) hws->funcs.enable_power_gating_plane(dc->hwseq, true); if (dcb->funcs->is_accelerated_mode(dcb) || !dc->config.seamless_boot_edp_requested) { if (!is_optimized_init_done) { hws->funcs.init_pipes(dc, dc->current_state); if (dc->res_pool->hubbub->funcs->allow_self_refresh_control) dc->res_pool->hubbub->funcs->allow_self_refresh_control(dc->res_pool->hubbub, !dc->res_pool->hubbub->ctx->dc->debug.disable_stutter); } } if (!is_optimized_init_done) { for (i = 0; i < res_pool->audio_count; i++) { struct audio *audio = res_pool->audios[i]; audio->funcs->hw_init(audio); } for (i = 0; i < dc->link_count; i++) { struct dc_link *link = dc->links[i]; if (link->panel_cntl) { backlight = link->panel_cntl->funcs->hw_init(link->panel_cntl); user_level = link->panel_cntl->stored_backlight_registers.USER_LEVEL; } } if (abm != NULL) abm->funcs->abm_init(abm, backlight, user_level); if (dmcu != NULL && !dmcu->auto_load_dmcu) dmcu->funcs->dmcu_init(dmcu); } if (abm != NULL && dmcu != NULL) abm->dmcu_is_running = dmcu->funcs->is_dmcu_initialized(dmcu); if (!is_optimized_init_done) REG_WRITE(DIO_MEM_PWR_CTRL, 0); if (!dc->debug.disable_clock_gate) { REG_WRITE(DCCG_GATE_DISABLE_CNTL, 0); REG_WRITE(DCCG_GATE_DISABLE_CNTL2, 0); REG_UPDATE(DCFCLK_CNTL, DCFCLK_GATE_DIS, 0); } <S2SV_StartVul> if (dc->clk_mgr->funcs->notify_wm_ranges) <S2SV_EndVul> dc->clk_mgr->funcs->notify_wm_ranges(dc->clk_mgr); }","- if (dc->clk_mgr->clks.dispclk_khz != 0 && dc->clk_mgr->clks.dppclk_khz != 0) {
- if (dc->clk_mgr->funcs->notify_wm_ranges)
+ if (dc->clk_mgr && dc->clk_mgr->clks.dispclk_khz != 0 && dc->clk_mgr->clks.dppclk_khz != 0) {
+ if (dc->clk_mgr && dc->clk_mgr->funcs->notify_wm_ranges)","void dcn10_init_hw(struct dc *dc) { int i; struct abm *abm = dc->res_pool->abm; struct dmcu *dmcu = dc->res_pool->dmcu; struct dce_hwseq *hws = dc->hwseq; struct dc_bios *dcb = dc->ctx->dc_bios; struct resource_pool *res_pool = dc->res_pool; uint32_t backlight = MAX_BACKLIGHT_LEVEL; uint32_t user_level = MAX_BACKLIGHT_LEVEL; bool is_optimized_init_done = false; if (dc->clk_mgr && dc->clk_mgr->funcs->init_clocks) dc->clk_mgr->funcs->init_clocks(dc->clk_mgr); if (dc->clk_mgr && dc->clk_mgr->clks.dispclk_khz != 0 && dc->clk_mgr->clks.dppclk_khz != 0) { dc->current_state->bw_ctx.bw.dcn.clk.dispclk_khz = dc->clk_mgr->clks.dispclk_khz; dc->current_state->bw_ctx.bw.dcn.clk.dppclk_khz = dc->clk_mgr->clks.dppclk_khz; } if (dc->res_pool->dccg && dc->res_pool->dccg->funcs->dccg_init) dc->res_pool->dccg->funcs->dccg_init(res_pool->dccg); if (!dcb->funcs->is_accelerated_mode(dcb)) hws->funcs.disable_vga(dc->hwseq); if (!dc_dmub_srv_optimized_init_done(dc->ctx->dmub_srv)) hws->funcs.bios_golden_init(dc); if (dc->ctx->dc_bios->fw_info_valid) { res_pool->ref_clocks.xtalin_clock_inKhz = dc->ctx->dc_bios->fw_info.pll_info.crystal_frequency; if (res_pool->dccg && res_pool->hubbub) { (res_pool->dccg->funcs->get_dccg_ref_freq)(res_pool->dccg, dc->ctx->dc_bios->fw_info.pll_info.crystal_frequency, &res_pool->ref_clocks.dccg_ref_clock_inKhz); (res_pool->hubbub->funcs->get_dchub_ref_freq)(res_pool->hubbub, res_pool->ref_clocks.dccg_ref_clock_inKhz, &res_pool->ref_clocks.dchub_ref_clock_inKhz); } else { res_pool->ref_clocks.dccg_ref_clock_inKhz = res_pool->ref_clocks.xtalin_clock_inKhz; res_pool->ref_clocks.dchub_ref_clock_inKhz = res_pool->ref_clocks.xtalin_clock_inKhz; } } else ASSERT_CRITICAL(false); for (i = 0; i < dc->link_count; i++) { struct dc_link *link = dc->links[i]; if (!is_optimized_init_done) link->link_enc->funcs->hw_init(link->link_enc); if (link->link_enc->funcs->is_dig_enabled && link->link_enc->funcs->is_dig_enabled(link->link_enc)) { link->link_status.link_active = true; if (link->link_enc->funcs->fec_is_active && link->link_enc->funcs->fec_is_active(link->link_enc)) link->fec_state = dc_link_fec_enabled; } } dc->link_srv->blank_all_dp_displays(dc); if (hws->funcs.enable_power_gating_plane) hws->funcs.enable_power_gating_plane(dc->hwseq, true); if (dcb->funcs->is_accelerated_mode(dcb) || !dc->config.seamless_boot_edp_requested) { if (!is_optimized_init_done) { hws->funcs.init_pipes(dc, dc->current_state); if (dc->res_pool->hubbub->funcs->allow_self_refresh_control) dc->res_pool->hubbub->funcs->allow_self_refresh_control(dc->res_pool->hubbub, !dc->res_pool->hubbub->ctx->dc->debug.disable_stutter); } } if (!is_optimized_init_done) { for (i = 0; i < res_pool->audio_count; i++) { struct audio *audio = res_pool->audios[i]; audio->funcs->hw_init(audio); } for (i = 0; i < dc->link_count; i++) { struct dc_link *link = dc->links[i]; if (link->panel_cntl) { backlight = link->panel_cntl->funcs->hw_init(link->panel_cntl); user_level = link->panel_cntl->stored_backlight_registers.USER_LEVEL; } } if (abm != NULL) abm->funcs->abm_init(abm, backlight, user_level); if (dmcu != NULL && !dmcu->auto_load_dmcu) dmcu->funcs->dmcu_init(dmcu); } if (abm != NULL && dmcu != NULL) abm->dmcu_is_running = dmcu->funcs->is_dmcu_initialized(dmcu); if (!is_optimized_init_done) REG_WRITE(DIO_MEM_PWR_CTRL, 0); if (!dc->debug.disable_clock_gate) { REG_WRITE(DCCG_GATE_DISABLE_CNTL, 0); REG_WRITE(DCCG_GATE_DISABLE_CNTL2, 0); REG_UPDATE(DCFCLK_CNTL, DCFCLK_GATE_DIS, 0); } if (dc->clk_mgr && dc->clk_mgr->funcs->notify_wm_ranges) dc->clk_mgr->funcs->notify_wm_ranges(dc->clk_mgr); }"
108----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44966/bad/binfmt_flat.c----load_flat_binary,"static int load_flat_binary(struct linux_binprm *bprm) { struct lib_info libinfo; struct pt_regs *regs = current_pt_regs(); unsigned long stack_len = 0; unsigned long start_addr; int res; int i, j; memset(&libinfo, 0, sizeof(libinfo)); #ifndef CONFIG_MMU stack_len += PAGE_SIZE * MAX_ARG_PAGES - bprm->p; #endif stack_len += (bprm->argc + 1) * sizeof(char *); stack_len += (bprm->envc + 1) * sizeof(char *); stack_len = ALIGN(stack_len, FLAT_STACK_ALIGN); res = load_flat_file(bprm, &libinfo, &stack_len); if (res < 0) return res; <S2SV_StartVul> for (i = 0; i < MAX_SHARED_LIBS; i++) { <S2SV_EndVul> if (!libinfo.lib_list[i].loaded) continue; for (j = 0; j < MAX_SHARED_LIBS; j++) { unsigned long val = libinfo.lib_list[j].loaded ? libinfo.lib_list[j].start_data : UNLOADED_LIB; unsigned long __user *p = (unsigned long __user *) libinfo.lib_list[i].start_data; p -= j + 1; if (put_user(val, p)) return -EFAULT; } } set_binfmt(&flat_format); #ifdef CONFIG_MMU res = setup_arg_pages(bprm, STACK_TOP, EXSTACK_DEFAULT); if (!res) res = create_flat_tables(bprm, bprm->p); #else current->mm->start_stack = ((current->mm->context.end_brk + stack_len + 3) & ~3) - 4; pr_debug(""sp=%lx\n"", current->mm->start_stack); res = transfer_args_to_stack(bprm, &current->mm->start_stack); if (!res) res = create_flat_tables(bprm, current->mm->start_stack); #endif if (res) return res; start_addr = libinfo.lib_list[0].entry; #ifdef FLAT_PLAT_INIT FLAT_PLAT_INIT(regs); #endif finalize_exec(bprm); pr_debug(""start_thread(regs=0x%p, entry=0x%lx, start_stack=0x%lx)\n"", regs, start_addr, current->mm->start_stack); start_thread(regs, start_addr, current->mm->start_stack); return 0; }","- for (i = 0; i < MAX_SHARED_LIBS; i++) {
+ for (i = 0; i < MAX_SHARED_LIBS_UPDATE; i++) {","static int load_flat_binary(struct linux_binprm *bprm) { struct lib_info libinfo; struct pt_regs *regs = current_pt_regs(); unsigned long stack_len = 0; unsigned long start_addr; int res; int i, j; memset(&libinfo, 0, sizeof(libinfo)); #ifndef CONFIG_MMU stack_len += PAGE_SIZE * MAX_ARG_PAGES - bprm->p; #endif stack_len += (bprm->argc + 1) * sizeof(char *); stack_len += (bprm->envc + 1) * sizeof(char *); stack_len = ALIGN(stack_len, FLAT_STACK_ALIGN); res = load_flat_file(bprm, &libinfo, &stack_len); if (res < 0) return res; for (i = 0; i < MAX_SHARED_LIBS_UPDATE; i++) { if (!libinfo.lib_list[i].loaded) continue; for (j = 0; j < MAX_SHARED_LIBS; j++) { unsigned long val = libinfo.lib_list[j].loaded ? libinfo.lib_list[j].start_data : UNLOADED_LIB; unsigned long __user *p = (unsigned long __user *) libinfo.lib_list[i].start_data; p -= j + 1; if (put_user(val, p)) return -EFAULT; } } set_binfmt(&flat_format); #ifdef CONFIG_MMU res = setup_arg_pages(bprm, STACK_TOP, EXSTACK_DEFAULT); if (!res) res = create_flat_tables(bprm, bprm->p); #else current->mm->start_stack = ((current->mm->context.end_brk + stack_len + 3) & ~3) - 4; pr_debug(""sp=%lx\n"", current->mm->start_stack); res = transfer_args_to_stack(bprm, &current->mm->start_stack); if (!res) res = create_flat_tables(bprm, current->mm->start_stack); #endif if (res) return res; start_addr = libinfo.lib_list[0].entry; #ifdef FLAT_PLAT_INIT FLAT_PLAT_INIT(regs); #endif finalize_exec(bprm); pr_debug(""start_thread(regs=0x%p, entry=0x%lx, start_stack=0x%lx)\n"", regs, start_addr, current->mm->start_stack); start_thread(regs, start_addr, current->mm->start_stack); return 0; }"
396----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46856/bad/dp83822.c----dp83822_probe,"static int dp83822_probe(struct phy_device *phydev) { struct dp83822_private *dp83822; int ret; dp83822 = devm_kzalloc(&phydev->mdio.dev, sizeof(*dp83822), GFP_KERNEL); if (!dp83822) return -ENOMEM; <S2SV_StartVul> phydev->priv = dp83822; <S2SV_EndVul> ret = dp83822_read_straps(phydev); if (ret) return ret; dp83822_of_init(phydev); if (dp83822->fx_enabled) phydev->port = PORT_FIBRE; return 0; }","- phydev->priv = dp83822;
+ static int dp83822_probe(struct phy_device *phydev)
+ {
+ struct dp83822_private *dp83822;
+ int ret;
+ ret = dp8382x_probe(phydev);
+ if (ret)
+ return ret;
+ dp83822 = phydev->priv;
+ if (ret)
+ return ret;",static int dp83822_probe(struct phy_device *phydev) { struct dp83822_private *dp83822; int ret; ret = dp8382x_probe(phydev); if (ret) return ret; dp83822 = phydev->priv; ret = dp83822_read_straps(phydev); if (ret) return ret; dp83822_of_init(phydev); if (dp83822->fx_enabled) phydev->port = PORT_FIBRE; return 0; }
866----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50124/bad/iso.c----iso_sock_timeout,"static void iso_sock_timeout(struct work_struct *work) { struct iso_conn *conn = container_of(work, struct iso_conn, timeout_work.work); struct sock *sk; iso_conn_lock(conn); <S2SV_StartVul> sk = conn->sk; <S2SV_EndVul> <S2SV_StartVul> if (sk) <S2SV_EndVul> <S2SV_StartVul> sock_hold(sk); <S2SV_EndVul> iso_conn_unlock(conn); if (!sk) return; BT_DBG(""sock %p state %d"", sk, sk->sk_state); lock_sock(sk); sk->sk_err = ETIMEDOUT; sk->sk_state_change(sk); release_sock(sk); sock_put(sk); }","- sk = conn->sk;
- if (sk)
- sock_hold(sk);
+ sk = iso_sock_hold(conn);","static void iso_sock_timeout(struct work_struct *work) { struct iso_conn *conn = container_of(work, struct iso_conn, timeout_work.work); struct sock *sk; iso_conn_lock(conn); sk = iso_sock_hold(conn); iso_conn_unlock(conn); if (!sk) return; BT_DBG(""sock %p state %d"", sk, sk->sk_state); lock_sock(sk); sk->sk_err = ETIMEDOUT; sk->sk_state_change(sk); release_sock(sk); sock_put(sk); }"
868----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50125/bad/sco.c----sco_sock_timeout,"static void sco_sock_timeout(struct work_struct *work) { struct sco_conn *conn = container_of(work, struct sco_conn, timeout_work.work); struct sock *sk; sco_conn_lock(conn); if (!conn->hcon) { sco_conn_unlock(conn); return; } <S2SV_StartVul> sk = conn->sk; <S2SV_EndVul> <S2SV_StartVul> if (sk) <S2SV_EndVul> <S2SV_StartVul> sock_hold(sk); <S2SV_EndVul> sco_conn_unlock(conn); if (!sk) return; BT_DBG(""sock %p state %d"", sk, sk->sk_state); lock_sock(sk); sk->sk_err = ETIMEDOUT; sk->sk_state_change(sk); release_sock(sk); sock_put(sk); }","- sk = conn->sk;
- if (sk)
- sock_hold(sk);
+ }
+ sk = sco_sock_hold(conn);","static void sco_sock_timeout(struct work_struct *work) { struct sco_conn *conn = container_of(work, struct sco_conn, timeout_work.work); struct sock *sk; sco_conn_lock(conn); if (!conn->hcon) { sco_conn_unlock(conn); return; } sk = sco_sock_hold(conn); sco_conn_unlock(conn); if (!sk) return; BT_DBG(""sock %p state %d"", sk, sk->sk_state); lock_sock(sk); sk->sk_err = ETIMEDOUT; sk->sk_state_change(sk); release_sock(sk); sock_put(sk); }"
1525----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2025-21682/bad/bnxt_xdp.c----bnxt_xdp_set,"static int bnxt_xdp_set(struct bnxt *bp, struct bpf_prog *prog) { struct net_device *dev = bp->dev; int tx_xdp = 0, tx_cp, rc, tc; struct bpf_prog *old; if (prog && !prog->aux->xdp_has_frags && bp->dev->mtu > BNXT_MAX_PAGE_MODE_MTU) { netdev_warn(dev, ""MTU %d larger than %d without XDP frag support.\n"", bp->dev->mtu, BNXT_MAX_PAGE_MODE_MTU); return -EOPNOTSUPP; } if (!(bp->flags & BNXT_FLAG_SHARED_RINGS)) { netdev_warn(dev, ""ethtool rx/tx channels must be combined to support XDP.\n""); return -EOPNOTSUPP; } if (prog) tx_xdp = bp->rx_nr_rings; tc = bp->num_tc; if (!tc) tc = 1; rc = bnxt_check_rings(bp, bp->tx_nr_rings_per_tc, bp->rx_nr_rings, true, tc, tx_xdp); if (rc) { netdev_warn(dev, ""Unable to reserve enough TX rings to support XDP.\n""); return rc; } if (netif_running(dev)) bnxt_close_nic(bp, true, false); old = xchg(&bp->xdp_prog, prog); if (old) bpf_prog_put(old); if (prog) { bnxt_set_rx_skb_mode(bp, true); xdp_features_set_redirect_target(dev, true); } else { <S2SV_StartVul> int rx, tx; <S2SV_EndVul> xdp_features_clear_redirect_target(dev); bnxt_set_rx_skb_mode(bp, false); <S2SV_StartVul> bnxt_get_max_rings(bp, &rx, &tx, true); <S2SV_EndVul> <S2SV_StartVul> if (rx > 1) { <S2SV_EndVul> <S2SV_StartVul> bp->flags &= ~BNXT_FLAG_NO_AGG_RINGS; <S2SV_EndVul> <S2SV_StartVul> bp->dev->hw_features |= NETIF_F_LRO; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> bp->tx_nr_rings_xdp = tx_xdp; bp->tx_nr_rings = bp->tx_nr_rings_per_tc * tc + tx_xdp; tx_cp = bnxt_num_tx_to_cp(bp, bp->tx_nr_rings); bp->cp_nr_rings = max_t(int, tx_cp, bp->rx_nr_rings); bnxt_set_tpa_flags(bp); bnxt_set_ring_params(bp); if (netif_running(dev)) return bnxt_open_nic(bp, true, false); return 0; }","- int rx, tx;
- bnxt_get_max_rings(bp, &rx, &tx, true);
- if (rx > 1) {
- bp->flags &= ~BNXT_FLAG_NO_AGG_RINGS;
- bp->dev->hw_features |= NETIF_F_LRO;
- }
- }","static int bnxt_xdp_set(struct bnxt *bp, struct bpf_prog *prog) { struct net_device *dev = bp->dev; int tx_xdp = 0, tx_cp, rc, tc; struct bpf_prog *old; if (prog && !prog->aux->xdp_has_frags && bp->dev->mtu > BNXT_MAX_PAGE_MODE_MTU) { netdev_warn(dev, ""MTU %d larger than %d without XDP frag support.\n"", bp->dev->mtu, BNXT_MAX_PAGE_MODE_MTU); return -EOPNOTSUPP; } if (!(bp->flags & BNXT_FLAG_SHARED_RINGS)) { netdev_warn(dev, ""ethtool rx/tx channels must be combined to support XDP.\n""); return -EOPNOTSUPP; } if (prog) tx_xdp = bp->rx_nr_rings; tc = bp->num_tc; if (!tc) tc = 1; rc = bnxt_check_rings(bp, bp->tx_nr_rings_per_tc, bp->rx_nr_rings, true, tc, tx_xdp); if (rc) { netdev_warn(dev, ""Unable to reserve enough TX rings to support XDP.\n""); return rc; } if (netif_running(dev)) bnxt_close_nic(bp, true, false); old = xchg(&bp->xdp_prog, prog); if (old) bpf_prog_put(old); if (prog) { bnxt_set_rx_skb_mode(bp, true); xdp_features_set_redirect_target(dev, true); } else { xdp_features_clear_redirect_target(dev); bnxt_set_rx_skb_mode(bp, false); } bp->tx_nr_rings_xdp = tx_xdp; bp->tx_nr_rings = bp->tx_nr_rings_per_tc * tc + tx_xdp; tx_cp = bnxt_num_tx_to_cp(bp, bp->tx_nr_rings); bp->cp_nr_rings = max_t(int, tx_cp, bp->rx_nr_rings); bnxt_set_tpa_flags(bp); bnxt_set_ring_params(bp); if (netif_running(dev)) return bnxt_open_nic(bp, true, false); return 0; }"
537----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49859/bad/file.c----f2fs_defragment_range,"static int f2fs_defragment_range(struct f2fs_sb_info *sbi, struct file *filp, struct f2fs_defragment *range) { struct inode *inode = file_inode(filp); struct f2fs_map_blocks map = { .m_next_extent = NULL, .m_seg_type = NO_CHECK_TYPE, .m_may_create = false }; struct extent_info ei = {}; pgoff_t pg_start, pg_end, next_pgofs; unsigned int total = 0, sec_num; block_t blk_end = 0; bool fragmented = false; int err; f2fs_balance_fs(sbi, true); inode_lock(inode); pg_start = range->start >> PAGE_SHIFT; pg_end = min_t(pgoff_t, (range->start + range->len) >> PAGE_SHIFT, DIV_ROUND_UP(i_size_read(inode), PAGE_SIZE)); <S2SV_StartVul> if (is_inode_flag_set(inode, FI_COMPRESS_RELEASED)) { <S2SV_EndVul> err = -EINVAL; goto unlock_out; } set_inode_flag(inode, FI_OPU_WRITE); if (f2fs_should_update_inplace(inode, NULL)) { err = -EINVAL; goto out; } err = filemap_write_and_wait_range(inode->i_mapping, pg_start << PAGE_SHIFT, (pg_end << PAGE_SHIFT) - 1); if (err) goto out; if (f2fs_lookup_read_extent_cache(inode, pg_start, &ei)) { if ((pgoff_t)ei.fofs + ei.len >= pg_end) goto out; } map.m_lblk = pg_start; map.m_next_pgofs = &next_pgofs; while (map.m_lblk < pg_end) { map.m_len = pg_end - map.m_lblk; err = f2fs_map_blocks(inode, &map, F2FS_GET_BLOCK_DEFAULT); if (err) goto out; if (!(map.m_flags & F2FS_MAP_FLAGS)) { map.m_lblk = next_pgofs; continue; } if (blk_end && blk_end != map.m_pblk) fragmented = true; total += map.m_len; blk_end = map.m_pblk + map.m_len; map.m_lblk += map.m_len; } if (!fragmented) { total = 0; goto out; } sec_num = DIV_ROUND_UP(total, CAP_BLKS_PER_SEC(sbi)); if (has_not_enough_free_secs(sbi, 0, sec_num)) { err = -EAGAIN; goto out; } map.m_lblk = pg_start; map.m_len = pg_end - pg_start; total = 0; while (map.m_lblk < pg_end) { pgoff_t idx; int cnt = 0; do_map: map.m_len = pg_end - map.m_lblk; err = f2fs_map_blocks(inode, &map, F2FS_GET_BLOCK_DEFAULT); if (err) goto clear_out; if (!(map.m_flags & F2FS_MAP_FLAGS)) { map.m_lblk = next_pgofs; goto check; } set_inode_flag(inode, FI_SKIP_WRITES); idx = map.m_lblk; while (idx < map.m_lblk + map.m_len && cnt < BLKS_PER_SEG(sbi)) { struct page *page; page = f2fs_get_lock_data_page(inode, idx, true); if (IS_ERR(page)) { err = PTR_ERR(page); goto clear_out; } f2fs_wait_on_page_writeback(page, DATA, true, true); set_page_dirty(page); set_page_private_gcing(page); f2fs_put_page(page, 1); idx++; cnt++; total++; } map.m_lblk = idx; check: if (map.m_lblk < pg_end && cnt < BLKS_PER_SEG(sbi)) goto do_map; clear_inode_flag(inode, FI_SKIP_WRITES); err = filemap_fdatawrite(inode->i_mapping); if (err) goto out; } clear_out: clear_inode_flag(inode, FI_SKIP_WRITES); out: clear_inode_flag(inode, FI_OPU_WRITE); unlock_out: inode_unlock(inode); if (!err) range->len = (u64)total << PAGE_SHIFT; return err; }","- if (is_inode_flag_set(inode, FI_COMPRESS_RELEASED)) {
+ if (is_inode_flag_set(inode, FI_COMPRESS_RELEASED) ||
+ f2fs_is_atomic_file(inode)) {
+ }","static int f2fs_defragment_range(struct f2fs_sb_info *sbi, struct file *filp, struct f2fs_defragment *range) { struct inode *inode = file_inode(filp); struct f2fs_map_blocks map = { .m_next_extent = NULL, .m_seg_type = NO_CHECK_TYPE, .m_may_create = false }; struct extent_info ei = {}; pgoff_t pg_start, pg_end, next_pgofs; unsigned int total = 0, sec_num; block_t blk_end = 0; bool fragmented = false; int err; f2fs_balance_fs(sbi, true); inode_lock(inode); pg_start = range->start >> PAGE_SHIFT; pg_end = min_t(pgoff_t, (range->start + range->len) >> PAGE_SHIFT, DIV_ROUND_UP(i_size_read(inode), PAGE_SIZE)); if (is_inode_flag_set(inode, FI_COMPRESS_RELEASED) || f2fs_is_atomic_file(inode)) { err = -EINVAL; goto unlock_out; } set_inode_flag(inode, FI_OPU_WRITE); if (f2fs_should_update_inplace(inode, NULL)) { err = -EINVAL; goto out; } err = filemap_write_and_wait_range(inode->i_mapping, pg_start << PAGE_SHIFT, (pg_end << PAGE_SHIFT) - 1); if (err) goto out; if (f2fs_lookup_read_extent_cache(inode, pg_start, &ei)) { if ((pgoff_t)ei.fofs + ei.len >= pg_end) goto out; } map.m_lblk = pg_start; map.m_next_pgofs = &next_pgofs; while (map.m_lblk < pg_end) { map.m_len = pg_end - map.m_lblk; err = f2fs_map_blocks(inode, &map, F2FS_GET_BLOCK_DEFAULT); if (err) goto out; if (!(map.m_flags & F2FS_MAP_FLAGS)) { map.m_lblk = next_pgofs; continue; } if (blk_end && blk_end != map.m_pblk) fragmented = true; total += map.m_len; blk_end = map.m_pblk + map.m_len; map.m_lblk += map.m_len; } if (!fragmented) { total = 0; goto out; } sec_num = DIV_ROUND_UP(total, CAP_BLKS_PER_SEC(sbi)); if (has_not_enough_free_secs(sbi, 0, sec_num)) { err = -EAGAIN; goto out; } map.m_lblk = pg_start; map.m_len = pg_end - pg_start; total = 0; while (map.m_lblk < pg_end) { pgoff_t idx; int cnt = 0; do_map: map.m_len = pg_end - map.m_lblk; err = f2fs_map_blocks(inode, &map, F2FS_GET_BLOCK_DEFAULT); if (err) goto clear_out; if (!(map.m_flags & F2FS_MAP_FLAGS)) { map.m_lblk = next_pgofs; goto check; } set_inode_flag(inode, FI_SKIP_WRITES); idx = map.m_lblk; while (idx < map.m_lblk + map.m_len && cnt < BLKS_PER_SEG(sbi)) { struct page *page; page = f2fs_get_lock_data_page(inode, idx, true); if (IS_ERR(page)) { err = PTR_ERR(page); goto clear_out; } f2fs_wait_on_page_writeback(page, DATA, true, true); set_page_dirty(page); set_page_private_gcing(page); f2fs_put_page(page, 1); idx++; cnt++; total++; } map.m_lblk = idx; check: if (map.m_lblk < pg_end && cnt < BLKS_PER_SEG(sbi)) goto do_map; clear_inode_flag(inode, FI_SKIP_WRITES); err = filemap_fdatawrite(inode->i_mapping); if (err) goto out; } clear_out: clear_inode_flag(inode, FI_SKIP_WRITES); out: clear_inode_flag(inode, FI_OPU_WRITE); unlock_out: inode_unlock(inode); if (!err) range->len = (u64)total << PAGE_SHIFT; return err; }"
867----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50125/bad/sco.c----sco_conn_del,"static void sco_conn_del(struct hci_conn *hcon, int err) { struct sco_conn *conn = hcon->sco_data; struct sock *sk; if (!conn) return; BT_DBG(""hcon %p conn %p, err %d"", hcon, conn, err); sco_conn_lock(conn); <S2SV_StartVul> sk = conn->sk; <S2SV_EndVul> <S2SV_StartVul> if (sk) <S2SV_EndVul> <S2SV_StartVul> sock_hold(sk); <S2SV_EndVul> sco_conn_unlock(conn); if (sk) { lock_sock(sk); sco_sock_clear_timer(sk); sco_chan_del(sk, err); release_sock(sk); sock_put(sk); } cancel_delayed_work_sync(&conn->timeout_work); hcon->sco_data = NULL; kfree(conn); }","- sk = conn->sk;
- if (sk)
- sock_hold(sk);
+ sk = sco_sock_hold(conn);","static void sco_conn_del(struct hci_conn *hcon, int err) { struct sco_conn *conn = hcon->sco_data; struct sock *sk; if (!conn) return; BT_DBG(""hcon %p conn %p, err %d"", hcon, conn, err); sco_conn_lock(conn); sk = sco_sock_hold(conn); sco_conn_unlock(conn); if (sk) { lock_sock(sk); sco_sock_clear_timer(sk); sco_chan_del(sk, err); release_sock(sk); sock_put(sk); } cancel_delayed_work_sync(&conn->timeout_work); hcon->sco_data = NULL; kfree(conn); }"
176----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-45026/bad/dasd_eckd.c----dasd_eckd_analysis_ccw,"dasd_eckd_analysis_ccw(struct dasd_device *device) { struct dasd_eckd_private *private = device->private; struct eckd_count *count_data; struct LO_eckd_data *LO_data; struct dasd_ccw_req *cqr; struct ccw1 *ccw; int cplength, datasize; int i; cplength = 8; datasize = sizeof(struct DE_eckd_data) + 2*sizeof(struct LO_eckd_data); cqr = dasd_smalloc_request(DASD_ECKD_MAGIC, cplength, datasize, device, NULL); if (IS_ERR(cqr)) return cqr; ccw = cqr->cpaddr; define_extent(ccw++, cqr->data, 0, 1, DASD_ECKD_CCW_READ_COUNT, device, 0); LO_data = cqr->data + sizeof(struct DE_eckd_data); ccw[-1].flags |= CCW_FLAG_CC; locate_record(ccw++, LO_data++, 0, 0, 4, DASD_ECKD_CCW_READ_COUNT, device, 0); count_data = private->count_area; for (i = 0; i < 4; i++) { ccw[-1].flags |= CCW_FLAG_CC; ccw->cmd_code = DASD_ECKD_CCW_READ_COUNT; ccw->flags = 0; ccw->count = 8; ccw->cda = virt_to_dma32(count_data); ccw++; count_data++; } ccw[-1].flags |= CCW_FLAG_CC; locate_record(ccw++, LO_data++, 1, 0, 1, DASD_ECKD_CCW_READ_COUNT, device, 0); ccw[-1].flags |= CCW_FLAG_CC; ccw->cmd_code = DASD_ECKD_CCW_READ_COUNT; ccw->flags = 0; ccw->count = 8; ccw->cda = virt_to_dma32(count_data); cqr->block = NULL; cqr->startdev = device; cqr->memdev = device; cqr->retries = 255; cqr->buildclk = get_tod_clock(); cqr->status = DASD_CQR_FILLED; set_bit(DASD_CQR_SUPPRESS_NRF, &cqr->flags); return cqr; <S2SV_StartVul> } <S2SV_EndVul>","- }
+ set_bit(DASD_CQR_SUPPRESS_IT, &cqr->flags);","dasd_eckd_analysis_ccw(struct dasd_device *device) { struct dasd_eckd_private *private = device->private; struct eckd_count *count_data; struct LO_eckd_data *LO_data; struct dasd_ccw_req *cqr; struct ccw1 *ccw; int cplength, datasize; int i; cplength = 8; datasize = sizeof(struct DE_eckd_data) + 2*sizeof(struct LO_eckd_data); cqr = dasd_smalloc_request(DASD_ECKD_MAGIC, cplength, datasize, device, NULL); if (IS_ERR(cqr)) return cqr; ccw = cqr->cpaddr; define_extent(ccw++, cqr->data, 0, 1, DASD_ECKD_CCW_READ_COUNT, device, 0); LO_data = cqr->data + sizeof(struct DE_eckd_data); ccw[-1].flags |= CCW_FLAG_CC; locate_record(ccw++, LO_data++, 0, 0, 4, DASD_ECKD_CCW_READ_COUNT, device, 0); count_data = private->count_area; for (i = 0; i < 4; i++) { ccw[-1].flags |= CCW_FLAG_CC; ccw->cmd_code = DASD_ECKD_CCW_READ_COUNT; ccw->flags = 0; ccw->count = 8; ccw->cda = virt_to_dma32(count_data); ccw++; count_data++; } ccw[-1].flags |= CCW_FLAG_CC; locate_record(ccw++, LO_data++, 1, 0, 1, DASD_ECKD_CCW_READ_COUNT, device, 0); ccw[-1].flags |= CCW_FLAG_CC; ccw->cmd_code = DASD_ECKD_CCW_READ_COUNT; ccw->flags = 0; ccw->count = 8; ccw->cda = virt_to_dma32(count_data); cqr->block = NULL; cqr->startdev = device; cqr->memdev = device; cqr->retries = 255; cqr->buildclk = get_tod_clock(); cqr->status = DASD_CQR_FILLED; set_bit(DASD_CQR_SUPPRESS_NRF, &cqr->flags); set_bit(DASD_CQR_SUPPRESS_IT, &cqr->flags); return cqr; }"
254----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46728/bad/link_dp_training.c----configure_lttpr_mode_non_transparent,"static enum dc_status configure_lttpr_mode_non_transparent( struct dc_link *link, const struct link_training_settings *lt_settings) { uint8_t repeater_cnt; uint32_t aux_interval_address; uint8_t repeater_id; enum dc_status result = DC_ERROR_UNEXPECTED; uint8_t repeater_mode = DP_PHY_REPEATER_MODE_TRANSPARENT; const struct dc *dc = link->dc; enum dp_link_encoding encoding = dc->link_srv->dp_get_encoding_format(&lt_settings->link_settings); if (encoding == DP_8b_10b_ENCODING) { DC_LOG_HW_LINK_TRAINING(""%s\n Set LTTPR to Transparent Mode\n"", __func__); result = core_link_write_dpcd(link, DP_PHY_REPEATER_MODE, (uint8_t *)&repeater_mode, sizeof(repeater_mode)); } if (result == DC_OK) { link->dpcd_caps.lttpr_caps.mode = repeater_mode; } if (lt_settings->lttpr_mode == LTTPR_MODE_NON_TRANSPARENT) { DC_LOG_HW_LINK_TRAINING(""%s\n Set LTTPR to Non Transparent Mode\n"", __func__); repeater_mode = DP_PHY_REPEATER_MODE_NON_TRANSPARENT; result = core_link_write_dpcd(link, DP_PHY_REPEATER_MODE, (uint8_t *)&repeater_mode, sizeof(repeater_mode)); if (result == DC_OK) { link->dpcd_caps.lttpr_caps.mode = repeater_mode; } if (encoding == DP_8b_10b_ENCODING) { repeater_cnt = dp_parse_lttpr_repeater_count(link->dpcd_caps.lttpr_caps.phy_repeater_cnt); <S2SV_StartVul> if (link->ep_type == DISPLAY_ENDPOINT_USB4_DPIA) <S2SV_EndVul> link->dpcd_caps.lttpr_caps.aux_rd_interval[--repeater_cnt] = 0; <S2SV_StartVul> for (repeater_id = repeater_cnt; repeater_id > 0; repeater_id--) { <S2SV_EndVul> aux_interval_address = DP_TRAINING_AUX_RD_INTERVAL_PHY_REPEATER1 + ((DP_REPEATER_CONFIGURATION_AND_STATUS_SIZE) * (repeater_id - 1)); core_link_read_dpcd( link, aux_interval_address, (uint8_t *)&link->dpcd_caps.lttpr_caps.aux_rd_interval[repeater_id - 1], sizeof(link->dpcd_caps.lttpr_caps.aux_rd_interval[repeater_id - 1])); link->dpcd_caps.lttpr_caps.aux_rd_interval[repeater_id - 1] &= 0x7F; } } } return result; }","- if (link->ep_type == DISPLAY_ENDPOINT_USB4_DPIA)
- for (repeater_id = repeater_cnt; repeater_id > 0; repeater_id--) {
+ if (link->ep_type == DISPLAY_ENDPOINT_USB4_DPIA && repeater_cnt > 0 && repeater_cnt < MAX_REPEATER_CNT)
+ for (repeater_id = repeater_cnt; repeater_id > 0 && repeater_id < MAX_REPEATER_CNT; repeater_id--) {","static enum dc_status configure_lttpr_mode_non_transparent( struct dc_link *link, const struct link_training_settings *lt_settings) { uint8_t repeater_cnt; uint32_t aux_interval_address; uint8_t repeater_id; enum dc_status result = DC_ERROR_UNEXPECTED; uint8_t repeater_mode = DP_PHY_REPEATER_MODE_TRANSPARENT; const struct dc *dc = link->dc; enum dp_link_encoding encoding = dc->link_srv->dp_get_encoding_format(&lt_settings->link_settings); if (encoding == DP_8b_10b_ENCODING) { DC_LOG_HW_LINK_TRAINING(""%s\n Set LTTPR to Transparent Mode\n"", __func__); result = core_link_write_dpcd(link, DP_PHY_REPEATER_MODE, (uint8_t *)&repeater_mode, sizeof(repeater_mode)); } if (result == DC_OK) { link->dpcd_caps.lttpr_caps.mode = repeater_mode; } if (lt_settings->lttpr_mode == LTTPR_MODE_NON_TRANSPARENT) { DC_LOG_HW_LINK_TRAINING(""%s\n Set LTTPR to Non Transparent Mode\n"", __func__); repeater_mode = DP_PHY_REPEATER_MODE_NON_TRANSPARENT; result = core_link_write_dpcd(link, DP_PHY_REPEATER_MODE, (uint8_t *)&repeater_mode, sizeof(repeater_mode)); if (result == DC_OK) { link->dpcd_caps.lttpr_caps.mode = repeater_mode; } if (encoding == DP_8b_10b_ENCODING) { repeater_cnt = dp_parse_lttpr_repeater_count(link->dpcd_caps.lttpr_caps.phy_repeater_cnt); if (link->ep_type == DISPLAY_ENDPOINT_USB4_DPIA && repeater_cnt > 0 && repeater_cnt < MAX_REPEATER_CNT) link->dpcd_caps.lttpr_caps.aux_rd_interval[--repeater_cnt] = 0; for (repeater_id = repeater_cnt; repeater_id > 0 && repeater_id < MAX_REPEATER_CNT; repeater_id--) { aux_interval_address = DP_TRAINING_AUX_RD_INTERVAL_PHY_REPEATER1 + ((DP_REPEATER_CONFIGURATION_AND_STATUS_SIZE) * (repeater_id - 1)); core_link_read_dpcd( link, aux_interval_address, (uint8_t *)&link->dpcd_caps.lttpr_caps.aux_rd_interval[repeater_id - 1], sizeof(link->dpcd_caps.lttpr_caps.aux_rd_interval[repeater_id - 1])); link->dpcd_caps.lttpr_caps.aux_rd_interval[repeater_id - 1] &= 0x7F; } } } return result; }"
1384----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56718/bad/smc_core.c----smc_link_down_work,"static void smc_link_down_work(struct work_struct *work) { struct smc_link *link = container_of(work, struct smc_link, link_down_wrk); struct smc_link_group *lgr = link->lgr; if (list_empty(&lgr->list)) <S2SV_StartVul> return; <S2SV_EndVul> wake_up_all(&lgr->llc_msg_waiter); down_write(&lgr->llc_conf_mutex); smcr_link_down(link); up_write(&lgr->llc_conf_mutex); }","- return;
+ goto out;
+ out:","static void smc_link_down_work(struct work_struct *work) { struct smc_link *link = container_of(work, struct smc_link, link_down_wrk); struct smc_link_group *lgr = link->lgr; if (list_empty(&lgr->list)) goto out; wake_up_all(&lgr->llc_msg_waiter); down_write(&lgr->llc_conf_mutex); smcr_link_down(link); up_write(&lgr->llc_conf_mutex); out: smcr_link_put(link); }"
931----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50180/bad/sis_main.c----sisfb_search_mode,"static void sisfb_search_mode(char *name, bool quiet) { unsigned int j = 0, xres = 0, yres = 0, depth = 0, rate = 0; int i = 0; <S2SV_StartVul> char strbuf[16], strbuf1[20]; <S2SV_EndVul> char *nameptr = name; if(name == NULL) { if(!quiet) printk(KERN_ERR ""sisfb: Internal error, using default mode.\n""); sisfb_mode_idx = DEFAULT_MODE; return; } if(!strncasecmp(name, sisbios_mode[MODE_INDEX_NONE].name, strlen(name))) { if(!quiet) printk(KERN_ERR ""sisfb: Mode 'none' not supported anymore. Using default.\n""); sisfb_mode_idx = DEFAULT_MODE; return; } if(strlen(name) <= 19) { strcpy(strbuf1, name); for(i = 0; i < strlen(strbuf1); i++) { if(strbuf1[i] < '0' || strbuf1[i] > '9') strbuf1[i] = ' '; } if(sscanf(strbuf1, ""%u %u %u %u"", &xres, &yres, &depth, &rate) == 4) { if((rate <= 32) || (depth > 32)) { j = rate; rate = depth; depth = j; } sprintf(strbuf, ""%ux%ux%u"", xres, yres, depth); nameptr = strbuf; sisfb_parm_rate = rate; } else if(sscanf(strbuf1, ""%u %u %u"", &xres, &yres, &depth) == 3) { sprintf(strbuf, ""%ux%ux%u"", xres, yres, depth); nameptr = strbuf; } else { xres = 0; if((sscanf(strbuf1, ""%u %u"", &xres, &yres) == 2) && (xres != 0)) { sprintf(strbuf, ""%ux%ux8"", xres, yres); nameptr = strbuf; } else { sisfb_search_vesamode(simple_strtoul(name, NULL, 0), quiet); return; } } } i = 0; j = 0; while(sisbios_mode[i].mode_no[0] != 0) { if(!strncasecmp(nameptr, sisbios_mode[i++].name, strlen(nameptr))) { if(sisfb_fstn) { if(sisbios_mode[i-1].mode_no[1] == 0x50 || sisbios_mode[i-1].mode_no[1] == 0x56 || sisbios_mode[i-1].mode_no[1] == 0x53) continue; } else { if(sisbios_mode[i-1].mode_no[1] == 0x5a || sisbios_mode[i-1].mode_no[1] == 0x5b) continue; } sisfb_mode_idx = i - 1; j = 1; break; } } if((!j) && !quiet) printk(KERN_ERR ""sisfb: Invalid mode '%s'\n"", nameptr); }","- char strbuf[16], strbuf1[20];
+ char strbuf[24], strbuf1[20];","static void sisfb_search_mode(char *name, bool quiet) { unsigned int j = 0, xres = 0, yres = 0, depth = 0, rate = 0; int i = 0; char strbuf[24], strbuf1[20]; char *nameptr = name; if(name == NULL) { if(!quiet) printk(KERN_ERR ""sisfb: Internal error, using default mode.\n""); sisfb_mode_idx = DEFAULT_MODE; return; } if(!strncasecmp(name, sisbios_mode[MODE_INDEX_NONE].name, strlen(name))) { if(!quiet) printk(KERN_ERR ""sisfb: Mode 'none' not supported anymore. Using default.\n""); sisfb_mode_idx = DEFAULT_MODE; return; } if(strlen(name) <= 19) { strcpy(strbuf1, name); for(i = 0; i < strlen(strbuf1); i++) { if(strbuf1[i] < '0' || strbuf1[i] > '9') strbuf1[i] = ' '; } if(sscanf(strbuf1, ""%u %u %u %u"", &xres, &yres, &depth, &rate) == 4) { if((rate <= 32) || (depth > 32)) { j = rate; rate = depth; depth = j; } sprintf(strbuf, ""%ux%ux%u"", xres, yres, depth); nameptr = strbuf; sisfb_parm_rate = rate; } else if(sscanf(strbuf1, ""%u %u %u"", &xres, &yres, &depth) == 3) { sprintf(strbuf, ""%ux%ux%u"", xres, yres, depth); nameptr = strbuf; } else { xres = 0; if((sscanf(strbuf1, ""%u %u"", &xres, &yres) == 2) && (xres != 0)) { sprintf(strbuf, ""%ux%ux8"", xres, yres); nameptr = strbuf; } else { sisfb_search_vesamode(simple_strtoul(name, NULL, 0), quiet); return; } } } i = 0; j = 0; while(sisbios_mode[i].mode_no[0] != 0) { if(!strncasecmp(nameptr, sisbios_mode[i++].name, strlen(nameptr))) { if(sisfb_fstn) { if(sisbios_mode[i-1].mode_no[1] == 0x50 || sisbios_mode[i-1].mode_no[1] == 0x56 || sisbios_mode[i-1].mode_no[1] == 0x53) continue; } else { if(sisbios_mode[i-1].mode_no[1] == 0x5a || sisbios_mode[i-1].mode_no[1] == 0x5b) continue; } sisfb_mode_idx = i - 1; j = 1; break; } } if((!j) && !quiet) printk(KERN_ERR ""sisfb: Invalid mode '%s'\n"", nameptr); }"
47----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44932/bad/idpf_lib.c----idpf_vport_stop,"static void idpf_vport_stop(struct idpf_vport *vport) { struct idpf_netdev_priv *np = netdev_priv(vport->netdev); if (np->state <= __IDPF_VPORT_DOWN) return; netif_carrier_off(vport->netdev); netif_tx_disable(vport->netdev); idpf_send_disable_vport_msg(vport); idpf_send_disable_queues_msg(vport); idpf_send_map_unmap_queue_vector_msg(vport, false); if (test_and_clear_bit(IDPF_VPORT_DEL_QUEUES, vport->flags)) idpf_send_delete_queues_msg(vport); idpf_remove_features(vport); vport->link_up = false; idpf_vport_intr_deinit(vport); <S2SV_StartVul> idpf_vport_intr_rel(vport); <S2SV_EndVul> idpf_vport_queues_rel(vport); np->state = __IDPF_VPORT_DOWN; }","- idpf_vport_intr_rel(vport);
+ idpf_vport_intr_rel(vport);","static void idpf_vport_stop(struct idpf_vport *vport) { struct idpf_netdev_priv *np = netdev_priv(vport->netdev); if (np->state <= __IDPF_VPORT_DOWN) return; netif_carrier_off(vport->netdev); netif_tx_disable(vport->netdev); idpf_send_disable_vport_msg(vport); idpf_send_disable_queues_msg(vport); idpf_send_map_unmap_queue_vector_msg(vport, false); if (test_and_clear_bit(IDPF_VPORT_DEL_QUEUES, vport->flags)) idpf_send_delete_queues_msg(vport); idpf_remove_features(vport); vport->link_up = false; idpf_vport_intr_deinit(vport); idpf_vport_queues_rel(vport); idpf_vport_intr_rel(vport); np->state = __IDPF_VPORT_DOWN; }"
1542----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2025-21695/bad/dell-uart-backlight.c----dell_uart_bl_serdev_probe,"static int dell_uart_bl_serdev_probe(struct serdev_device *serdev) { u8 get_version[GET_CMD_LEN], resp[MAX_RESP_LEN]; struct backlight_properties props = {}; struct dell_uart_backlight *dell_bl; struct device *dev = &serdev->dev; int ret; dell_bl = devm_kzalloc(dev, sizeof(*dell_bl), GFP_KERNEL); if (!dell_bl) return -ENOMEM; mutex_init(&dell_bl->mutex); init_waitqueue_head(&dell_bl->wait_queue); dell_bl->dev = dev; ret = devm_serdev_device_open(dev, serdev); if (ret) return dev_err_probe(dev, ret, ""opening UART device\n""); serdev_device_set_baudrate(serdev, 9600); serdev_device_set_flow_control(serdev, false); <S2SV_StartVul> serdev_device_set_drvdata(serdev, dell_bl); <S2SV_EndVul> <S2SV_StartVul> serdev_device_set_client_ops(serdev, &dell_uart_bl_serdev_ops); <S2SV_EndVul> get_version[0] = DELL_SOF(GET_CMD_LEN); get_version[1] = CMD_GET_VERSION; get_version[2] = dell_uart_checksum(get_version, 2); ret = dell_uart_bl_command(dell_bl, get_version, GET_CMD_LEN, resp, MAX_RESP_LEN); if (ret) return dev_err_probe(dev, ret, ""getting firmware version\n""); dev_dbg(dev, ""Firmware version: %.*s\n"", resp[RESP_LEN] - 3, resp + RESP_DATA); ret = dell_uart_set_bl_power(dell_bl, FB_BLANK_UNBLANK); if (ret) return ret; ret = dell_uart_get_brightness(dell_bl); if (ret < 0) return ret; props.type = BACKLIGHT_PLATFORM; props.brightness = ret; props.max_brightness = DELL_BL_MAX_BRIGHTNESS; props.power = dell_bl->power; dell_bl->bl = devm_backlight_device_register(dev, ""dell_uart_backlight"", dev, dell_bl, &dell_uart_backlight_ops, &props); return PTR_ERR_OR_ZERO(dell_bl->bl); }","- serdev_device_set_drvdata(serdev, dell_bl);
- serdev_device_set_client_ops(serdev, &dell_uart_bl_serdev_ops);
+ serdev_device_set_drvdata(serdev, dell_bl);
+ serdev_device_set_client_ops(serdev, &dell_uart_bl_serdev_ops);","static int dell_uart_bl_serdev_probe(struct serdev_device *serdev) { u8 get_version[GET_CMD_LEN], resp[MAX_RESP_LEN]; struct backlight_properties props = {}; struct dell_uart_backlight *dell_bl; struct device *dev = &serdev->dev; int ret; dell_bl = devm_kzalloc(dev, sizeof(*dell_bl), GFP_KERNEL); if (!dell_bl) return -ENOMEM; mutex_init(&dell_bl->mutex); init_waitqueue_head(&dell_bl->wait_queue); dell_bl->dev = dev; serdev_device_set_drvdata(serdev, dell_bl); serdev_device_set_client_ops(serdev, &dell_uart_bl_serdev_ops); ret = devm_serdev_device_open(dev, serdev); if (ret) return dev_err_probe(dev, ret, ""opening UART device\n""); serdev_device_set_baudrate(serdev, 9600); serdev_device_set_flow_control(serdev, false); get_version[0] = DELL_SOF(GET_CMD_LEN); get_version[1] = CMD_GET_VERSION; get_version[2] = dell_uart_checksum(get_version, 2); ret = dell_uart_bl_command(dell_bl, get_version, GET_CMD_LEN, resp, MAX_RESP_LEN); if (ret) return dev_err_probe(dev, ret, ""getting firmware version\n""); dev_dbg(dev, ""Firmware version: %.*s\n"", resp[RESP_LEN] - 3, resp + RESP_DATA); ret = dell_uart_set_bl_power(dell_bl, FB_BLANK_UNBLANK); if (ret) return ret; ret = dell_uart_get_brightness(dell_bl); if (ret < 0) return ret; props.type = BACKLIGHT_PLATFORM; props.brightness = ret; props.max_brightness = DELL_BL_MAX_BRIGHTNESS; props.power = dell_bl->power; dell_bl->bl = devm_backlight_device_register(dev, ""dell_uart_backlight"", dev, dell_bl, &dell_uart_backlight_ops, &props); return PTR_ERR_OR_ZERO(dell_bl->bl); }"
965----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50207/bad/ring_buffer.c----ring_buffer_subbuf_order_set,"int ring_buffer_subbuf_order_set(struct trace_buffer *buffer, int order) { struct ring_buffer_per_cpu *cpu_buffer; struct buffer_page *bpage, *tmp; int old_order, old_size; int nr_pages; int psize; int err; int cpu; if (!buffer || order < 0) return -EINVAL; if (buffer->subbuf_order == order) return 0; psize = (1 << order) * PAGE_SIZE; if (psize <= BUF_PAGE_HDR_SIZE) return -EINVAL; if (psize > RB_WRITE_MASK + 1) return -EINVAL; old_order = buffer->subbuf_order; old_size = buffer->subbuf_size; mutex_lock(&buffer->mutex); atomic_inc(&buffer->record_disabled); synchronize_rcu(); buffer->subbuf_order = order; buffer->subbuf_size = psize - BUF_PAGE_HDR_SIZE; for_each_buffer_cpu(buffer, cpu) { if (!cpumask_test_cpu(cpu, buffer->cpumask)) continue; cpu_buffer = buffer->buffers[cpu]; if (cpu_buffer->mapped) { err = -EBUSY; goto error; } nr_pages = old_size * buffer->buffers[cpu]->nr_pages; nr_pages = DIV_ROUND_UP(nr_pages, buffer->subbuf_size); if (nr_pages < 2) nr_pages = 2; cpu_buffer->nr_pages_to_update = nr_pages; nr_pages++; INIT_LIST_HEAD(&cpu_buffer->new_pages); if (__rb_allocate_pages(cpu_buffer, nr_pages, &cpu_buffer->new_pages)) { err = -ENOMEM; goto error; } } for_each_buffer_cpu(buffer, cpu) { if (!cpumask_test_cpu(cpu, buffer->cpumask)) continue; cpu_buffer = buffer->buffers[cpu]; rb_head_page_deactivate(cpu_buffer); <S2SV_StartVul> list_for_each_entry_safe(bpage, tmp, cpu_buffer->pages, list) { <S2SV_EndVul> <S2SV_StartVul> list_del_init(&bpage->list); <S2SV_EndVul> <S2SV_StartVul> free_buffer_page(bpage); <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> bpage = list_entry(cpu_buffer->pages, struct buffer_page, list); <S2SV_EndVul> <S2SV_StartVul> free_buffer_page(bpage); <S2SV_EndVul> <S2SV_StartVul> free_buffer_page(cpu_buffer->reader_page); <S2SV_EndVul> cpu_buffer->reader_page = list_entry(cpu_buffer->new_pages.next, struct buffer_page, list); list_del_init(&cpu_buffer->reader_page->list); cpu_buffer->pages = cpu_buffer->new_pages.next; <S2SV_StartVul> cpu_buffer->new_pages.next->prev = cpu_buffer->new_pages.prev; <S2SV_EndVul> <S2SV_StartVul> cpu_buffer->new_pages.prev->next = cpu_buffer->new_pages.next; <S2SV_EndVul> <S2SV_StartVul> INIT_LIST_HEAD(&cpu_buffer->new_pages); <S2SV_EndVul> cpu_buffer->head_page = list_entry(cpu_buffer->pages, struct buffer_page, list); cpu_buffer->tail_page = cpu_buffer->commit_page = cpu_buffer->head_page; cpu_buffer->nr_pages = cpu_buffer->nr_pages_to_update; cpu_buffer->nr_pages_to_update = 0; <S2SV_StartVul> free_pages((unsigned long)cpu_buffer->free_page, old_order); <S2SV_EndVul> cpu_buffer->free_page = NULL; rb_head_page_activate(cpu_buffer); rb_check_pages(cpu_buffer); <S2SV_StartVul> } <S2SV_EndVul> atomic_dec(&buffer->record_disabled); mutex_unlock(&buffer->mutex); return 0; error: buffer->subbuf_order = old_order; buffer->subbuf_size = old_size; atomic_dec(&buffer->record_disabled); mutex_unlock(&buffer->mutex); for_each_buffer_cpu(buffer, cpu) { cpu_buffer = buffer->buffers[cpu]; if (!cpu_buffer->nr_pages_to_update) continue; list_for_each_entry_safe(bpage, tmp, &cpu_buffer->new_pages, list) { list_del_init(&bpage->list); free_buffer_page(bpage); } } return err; }","- list_for_each_entry_safe(bpage, tmp, cpu_buffer->pages, list) {
- list_del_init(&bpage->list);
- free_buffer_page(bpage);
- }
- bpage = list_entry(cpu_buffer->pages, struct buffer_page, list);
- free_buffer_page(bpage);
- free_buffer_page(cpu_buffer->reader_page);
- cpu_buffer->new_pages.next->prev = cpu_buffer->new_pages.prev;
- cpu_buffer->new_pages.prev->next = cpu_buffer->new_pages.next;
- INIT_LIST_HEAD(&cpu_buffer->new_pages);
- free_pages((unsigned long)cpu_buffer->free_page, old_order);
- }
+ struct buffer_data_page *old_free_data_page;
+ struct list_head old_pages;
+ unsigned long flags;
+ raw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);
+ list_add(&old_pages, cpu_buffer->pages);
+ list_add(&cpu_buffer->reader_page->list, &old_pages);
+ list_del_init(&cpu_buffer->new_pages);
+ old_free_data_page = cpu_buffer->free_page;
+ raw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);
+ list_for_each_entry_safe(bpage, tmp, &old_pages, list) {
+ list_del_init(&bpage->list);
+ free_buffer_page(bpage);
+ }
+ free_pages((unsigned long)old_free_data_page, old_order);
+ }","int ring_buffer_subbuf_order_set(struct trace_buffer *buffer, int order) { struct ring_buffer_per_cpu *cpu_buffer; struct buffer_page *bpage, *tmp; int old_order, old_size; int nr_pages; int psize; int err; int cpu; if (!buffer || order < 0) return -EINVAL; if (buffer->subbuf_order == order) return 0; psize = (1 << order) * PAGE_SIZE; if (psize <= BUF_PAGE_HDR_SIZE) return -EINVAL; if (psize > RB_WRITE_MASK + 1) return -EINVAL; old_order = buffer->subbuf_order; old_size = buffer->subbuf_size; mutex_lock(&buffer->mutex); atomic_inc(&buffer->record_disabled); synchronize_rcu(); buffer->subbuf_order = order; buffer->subbuf_size = psize - BUF_PAGE_HDR_SIZE; for_each_buffer_cpu(buffer, cpu) { if (!cpumask_test_cpu(cpu, buffer->cpumask)) continue; cpu_buffer = buffer->buffers[cpu]; if (cpu_buffer->mapped) { err = -EBUSY; goto error; } nr_pages = old_size * buffer->buffers[cpu]->nr_pages; nr_pages = DIV_ROUND_UP(nr_pages, buffer->subbuf_size); if (nr_pages < 2) nr_pages = 2; cpu_buffer->nr_pages_to_update = nr_pages; nr_pages++; INIT_LIST_HEAD(&cpu_buffer->new_pages); if (__rb_allocate_pages(cpu_buffer, nr_pages, &cpu_buffer->new_pages)) { err = -ENOMEM; goto error; } } for_each_buffer_cpu(buffer, cpu) { struct buffer_data_page *old_free_data_page; struct list_head old_pages; unsigned long flags; if (!cpumask_test_cpu(cpu, buffer->cpumask)) continue; cpu_buffer = buffer->buffers[cpu]; raw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags); rb_head_page_deactivate(cpu_buffer); list_add(&old_pages, cpu_buffer->pages); list_add(&cpu_buffer->reader_page->list, &old_pages); cpu_buffer->reader_page = list_entry(cpu_buffer->new_pages.next, struct buffer_page, list); list_del_init(&cpu_buffer->reader_page->list); cpu_buffer->pages = cpu_buffer->new_pages.next; list_del_init(&cpu_buffer->new_pages); cpu_buffer->head_page = list_entry(cpu_buffer->pages, struct buffer_page, list); cpu_buffer->tail_page = cpu_buffer->commit_page = cpu_buffer->head_page; cpu_buffer->nr_pages = cpu_buffer->nr_pages_to_update; cpu_buffer->nr_pages_to_update = 0; old_free_data_page = cpu_buffer->free_page; cpu_buffer->free_page = NULL; rb_head_page_activate(cpu_buffer); raw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags); list_for_each_entry_safe(bpage, tmp, &old_pages, list) { list_del_init(&bpage->list); free_buffer_page(bpage); } free_pages((unsigned long)old_free_data_page, old_order); rb_check_pages(cpu_buffer); } atomic_dec(&buffer->record_disabled); mutex_unlock(&buffer->mutex); return 0; error: buffer->subbuf_order = old_order; buffer->subbuf_size = old_size; atomic_dec(&buffer->record_disabled); mutex_unlock(&buffer->mutex); for_each_buffer_cpu(buffer, cpu) { cpu_buffer = buffer->buffers[cpu]; if (!cpu_buffer->nr_pages_to_update) continue; list_for_each_entry_safe(bpage, tmp, &cpu_buffer->new_pages, list) { list_del_init(&bpage->list); free_buffer_page(bpage); } } return err; }"
107----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44965/bad/pti.c----pti_clone_pgtable,"pti_clone_pgtable(unsigned long start, unsigned long end, enum pti_clone_level level) { unsigned long addr; for (addr = start; addr < end;) { pte_t *pte, *target_pte; pmd_t *pmd, *target_pmd; pgd_t *pgd; p4d_t *p4d; pud_t *pud; if (addr < start) break; pgd = pgd_offset_k(addr); if (WARN_ON(pgd_none(*pgd))) return; p4d = p4d_offset(pgd, addr); if (WARN_ON(p4d_none(*p4d))) return; pud = pud_offset(p4d, addr); if (pud_none(*pud)) { WARN_ON_ONCE(addr & ~PUD_MASK); addr = round_up(addr + 1, PUD_SIZE); continue; } pmd = pmd_offset(pud, addr); if (pmd_none(*pmd)) { WARN_ON_ONCE(addr & ~PMD_MASK); addr = round_up(addr + 1, PMD_SIZE); continue; } if (pmd_leaf(*pmd) || level == PTI_CLONE_PMD) { target_pmd = pti_user_pagetable_walk_pmd(addr); if (WARN_ON(!target_pmd)) return; if (WARN_ON(!(pmd_flags(*pmd) & _PAGE_PRESENT))) return; if (boot_cpu_has(X86_FEATURE_PGE)) *pmd = pmd_set_flags(*pmd, _PAGE_GLOBAL); *target_pmd = *pmd; <S2SV_StartVul> addr += PMD_SIZE; <S2SV_EndVul> } else if (level == PTI_CLONE_PTE) { pte = pte_offset_kernel(pmd, addr); if (pte_none(*pte)) { <S2SV_StartVul> addr += PAGE_SIZE; <S2SV_EndVul> continue; } if (WARN_ON(!(pte_flags(*pte) & _PAGE_PRESENT))) return; target_pte = pti_user_pagetable_walk_pte(addr); if (WARN_ON(!target_pte)) return; if (boot_cpu_has(X86_FEATURE_PGE)) *pte = pte_set_flags(*pte, _PAGE_GLOBAL); *target_pte = *pte; <S2SV_StartVul> addr += PAGE_SIZE; <S2SV_EndVul> } else { BUG(); } } }","- addr += PMD_SIZE;
- addr += PAGE_SIZE;
- addr += PAGE_SIZE;
+ addr = round_up(addr + 1, PMD_SIZE);
+ addr = round_up(addr + 1, PAGE_SIZE);
+ addr = round_up(addr + 1, PAGE_SIZE);","pti_clone_pgtable(unsigned long start, unsigned long end, enum pti_clone_level level) { unsigned long addr; for (addr = start; addr < end;) { pte_t *pte, *target_pte; pmd_t *pmd, *target_pmd; pgd_t *pgd; p4d_t *p4d; pud_t *pud; if (addr < start) break; pgd = pgd_offset_k(addr); if (WARN_ON(pgd_none(*pgd))) return; p4d = p4d_offset(pgd, addr); if (WARN_ON(p4d_none(*p4d))) return; pud = pud_offset(p4d, addr); if (pud_none(*pud)) { WARN_ON_ONCE(addr & ~PUD_MASK); addr = round_up(addr + 1, PUD_SIZE); continue; } pmd = pmd_offset(pud, addr); if (pmd_none(*pmd)) { WARN_ON_ONCE(addr & ~PMD_MASK); addr = round_up(addr + 1, PMD_SIZE); continue; } if (pmd_leaf(*pmd) || level == PTI_CLONE_PMD) { target_pmd = pti_user_pagetable_walk_pmd(addr); if (WARN_ON(!target_pmd)) return; if (WARN_ON(!(pmd_flags(*pmd) & _PAGE_PRESENT))) return; if (boot_cpu_has(X86_FEATURE_PGE)) *pmd = pmd_set_flags(*pmd, _PAGE_GLOBAL); *target_pmd = *pmd; addr = round_up(addr + 1, PMD_SIZE); } else if (level == PTI_CLONE_PTE) { pte = pte_offset_kernel(pmd, addr); if (pte_none(*pte)) { addr = round_up(addr + 1, PAGE_SIZE); continue; } if (WARN_ON(!(pte_flags(*pte) & _PAGE_PRESENT))) return; target_pte = pti_user_pagetable_walk_pte(addr); if (WARN_ON(!target_pte)) return; if (boot_cpu_has(X86_FEATURE_PGE)) *pte = pte_set_flags(*pte, _PAGE_GLOBAL); *target_pte = *pte; addr = round_up(addr + 1, PAGE_SIZE); } else { BUG(); } } }"
1300----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56593/bad/bcmsdh.c----brcmf_sdiod_sgtable_alloc,"void brcmf_sdiod_sgtable_alloc(struct brcmf_sdio_dev *sdiodev) { struct sdio_func *func; struct mmc_host *host; uint max_blocks; uint nents; int err; func = sdiodev->func2; host = func->card->host; sdiodev->sg_support = host->max_segs > 1; max_blocks = min_t(uint, host->max_blk_count, 511u); sdiodev->max_request_size = min_t(uint, host->max_req_size, max_blocks * func->cur_blksize); sdiodev->max_segment_count = min_t(uint, host->max_segs, SG_MAX_SINGLE_ALLOC); sdiodev->max_segment_size = host->max_seg_size; if (!sdiodev->sg_support) return; nents = max_t(uint, BRCMF_DEFAULT_RXGLOM_SIZE, sdiodev->settings->bus.sdio.txglomsz); <S2SV_StartVul> nents += (nents >> 4) + 1; <S2SV_EndVul> WARN_ON(nents > sdiodev->max_segment_count); brcmf_dbg(TRACE, ""nents=%d\n"", nents); err = sg_alloc_table(&sdiodev->sgtable, nents, GFP_KERNEL); if (err < 0) { brcmf_err(""allocation failed: disable scatter-gather""); sdiodev->sg_support = false; } sdiodev->txglomsz = sdiodev->settings->bus.sdio.txglomsz; }","- nents += (nents >> 4) + 1;
+ nents *= 2;","void brcmf_sdiod_sgtable_alloc(struct brcmf_sdio_dev *sdiodev) { struct sdio_func *func; struct mmc_host *host; uint max_blocks; uint nents; int err; func = sdiodev->func2; host = func->card->host; sdiodev->sg_support = host->max_segs > 1; max_blocks = min_t(uint, host->max_blk_count, 511u); sdiodev->max_request_size = min_t(uint, host->max_req_size, max_blocks * func->cur_blksize); sdiodev->max_segment_count = min_t(uint, host->max_segs, SG_MAX_SINGLE_ALLOC); sdiodev->max_segment_size = host->max_seg_size; if (!sdiodev->sg_support) return; nents = max_t(uint, BRCMF_DEFAULT_RXGLOM_SIZE, sdiodev->settings->bus.sdio.txglomsz); nents *= 2; WARN_ON(nents > sdiodev->max_segment_count); brcmf_dbg(TRACE, ""nents=%d\n"", nents); err = sg_alloc_table(&sdiodev->sgtable, nents, GFP_KERNEL); if (err < 0) { brcmf_err(""allocation failed: disable scatter-gather""); sdiodev->sg_support = false; } sdiodev->txglomsz = sdiodev->settings->bus.sdio.txglomsz; }"
1269----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56551/bad/amdgpu_vce.c----amdgpu_vce_sw_fini,"int amdgpu_vce_sw_fini(struct amdgpu_device *adev) { unsigned int i; if (adev->vce.vcpu_bo == NULL) return 0; drm_sched_entity_destroy(&adev->vce.entity); <S2SV_StartVul> amdgpu_bo_free_kernel(&adev->vce.vcpu_bo, &adev->vce.gpu_addr, <S2SV_EndVul> <S2SV_StartVul> (void **)&adev->vce.cpu_addr); <S2SV_EndVul> for (i = 0; i < adev->vce.num_rings; i++) amdgpu_ring_fini(&adev->vce.ring[i]); amdgpu_ucode_release(&adev->vce.fw); mutex_destroy(&adev->vce.idle_mutex); return 0; }","- amdgpu_bo_free_kernel(&adev->vce.vcpu_bo, &adev->vce.gpu_addr,
- (void **)&adev->vce.cpu_addr);
+ amdgpu_bo_free_kernel(&adev->vce.vcpu_bo, &adev->vce.gpu_addr,
+ (void **)&adev->vce.cpu_addr);","int amdgpu_vce_sw_fini(struct amdgpu_device *adev) { unsigned int i; if (adev->vce.vcpu_bo == NULL) return 0; drm_sched_entity_destroy(&adev->vce.entity); for (i = 0; i < adev->vce.num_rings; i++) amdgpu_ring_fini(&adev->vce.ring[i]); amdgpu_ucode_release(&adev->vce.fw); mutex_destroy(&adev->vce.idle_mutex); amdgpu_bo_free_kernel(&adev->vce.vcpu_bo, &adev->vce.gpu_addr, (void **)&adev->vce.cpu_addr); return 0; }"
1225----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53237/bad/hci_sysfs.c----hci_conn_del_sysfs,"void hci_conn_del_sysfs(struct hci_conn *conn) { struct hci_dev *hdev = conn->hdev; bt_dev_dbg(hdev, ""conn %p"", conn); if (!device_is_registered(&conn->dev)) { put_device(&conn->dev); return; <S2SV_StartVul> } <S2SV_EndVul> while (1) { struct device *dev; <S2SV_StartVul> dev = device_find_child(&conn->dev, NULL, __match_tty); <S2SV_EndVul> if (!dev) break; device_move(dev, NULL, DPM_ORDER_DEV_LAST); put_device(dev); } device_unregister(&conn->dev); }","- }
- dev = device_find_child(&conn->dev, NULL, __match_tty);
+ dev = device_find_any_child(&conn->dev);","void hci_conn_del_sysfs(struct hci_conn *conn) { struct hci_dev *hdev = conn->hdev; bt_dev_dbg(hdev, ""conn %p"", conn); if (!device_is_registered(&conn->dev)) { put_device(&conn->dev); return; } while (1) { struct device *dev; dev = device_find_any_child(&conn->dev); if (!dev) break; device_move(dev, NULL, DPM_ORDER_DEV_LAST); put_device(dev); } device_unregister(&conn->dev); }"
907----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50153/bad/target_core_device.c----*target_alloc_device,"struct se_device *target_alloc_device(struct se_hba *hba, const char *name) { struct se_device *dev; struct se_lun *xcopy_lun; int i; dev = hba->backend->ops->alloc_device(hba, name); if (!dev) return NULL; dev->queues = kcalloc(nr_cpu_ids, sizeof(*dev->queues), GFP_KERNEL); if (!dev->queues) { <S2SV_StartVul> dev->transport->free_device(dev); <S2SV_EndVul> return NULL; } dev->queue_cnt = nr_cpu_ids; for (i = 0; i < dev->queue_cnt; i++) { struct se_device_queue *q; q = &dev->queues[i]; INIT_LIST_HEAD(&q->state_list); spin_lock_init(&q->lock); init_llist_head(&q->sq.cmd_list); INIT_WORK(&q->sq.work, target_queued_submit_work); } dev->se_hba = hba; dev->transport = hba->backend->ops; dev->transport_flags = dev->transport->transport_flags_default; dev->prot_length = sizeof(struct t10_pi_tuple); dev->hba_index = hba->hba_index; INIT_LIST_HEAD(&dev->dev_sep_list); INIT_LIST_HEAD(&dev->dev_tmr_list); INIT_LIST_HEAD(&dev->delayed_cmd_list); INIT_LIST_HEAD(&dev->qf_cmd_list); spin_lock_init(&dev->delayed_cmd_lock); spin_lock_init(&dev->dev_reservation_lock); spin_lock_init(&dev->se_port_lock); spin_lock_init(&dev->se_tmr_lock); spin_lock_init(&dev->qf_cmd_lock); sema_init(&dev->caw_sem, 1); INIT_LIST_HEAD(&dev->t10_wwn.t10_vpd_list); spin_lock_init(&dev->t10_wwn.t10_vpd_lock); INIT_LIST_HEAD(&dev->t10_pr.registration_list); INIT_LIST_HEAD(&dev->t10_pr.aptpl_reg_list); spin_lock_init(&dev->t10_pr.registration_lock); spin_lock_init(&dev->t10_pr.aptpl_reg_lock); INIT_LIST_HEAD(&dev->t10_alua.tg_pt_gps_list); spin_lock_init(&dev->t10_alua.tg_pt_gps_lock); INIT_LIST_HEAD(&dev->t10_alua.lba_map_list); spin_lock_init(&dev->t10_alua.lba_map_lock); INIT_WORK(&dev->delayed_cmd_work, target_do_delayed_work); mutex_init(&dev->lun_reset_mutex); dev->t10_wwn.t10_dev = dev; dev->t10_wwn.company_id = 0x001405; dev->t10_alua.t10_dev = dev; dev->dev_attrib.da_dev = dev; dev->dev_attrib.emulate_model_alias = DA_EMULATE_MODEL_ALIAS; dev->dev_attrib.emulate_dpo = 1; dev->dev_attrib.emulate_fua_write = 1; dev->dev_attrib.emulate_fua_read = 1; dev->dev_attrib.emulate_write_cache = DA_EMULATE_WRITE_CACHE; dev->dev_attrib.emulate_ua_intlck_ctrl = TARGET_UA_INTLCK_CTRL_CLEAR; dev->dev_attrib.emulate_tas = DA_EMULATE_TAS; dev->dev_attrib.emulate_tpu = DA_EMULATE_TPU; dev->dev_attrib.emulate_tpws = DA_EMULATE_TPWS; dev->dev_attrib.emulate_caw = DA_EMULATE_CAW; dev->dev_attrib.emulate_3pc = DA_EMULATE_3PC; dev->dev_attrib.emulate_pr = DA_EMULATE_PR; dev->dev_attrib.pi_prot_type = TARGET_DIF_TYPE0_PROT; dev->dev_attrib.enforce_pr_isids = DA_ENFORCE_PR_ISIDS; dev->dev_attrib.force_pr_aptpl = DA_FORCE_PR_APTPL; dev->dev_attrib.is_nonrot = DA_IS_NONROT; dev->dev_attrib.emulate_rest_reord = DA_EMULATE_REST_REORD; dev->dev_attrib.max_unmap_lba_count = DA_MAX_UNMAP_LBA_COUNT; dev->dev_attrib.max_unmap_block_desc_count = DA_MAX_UNMAP_BLOCK_DESC_COUNT; dev->dev_attrib.unmap_granularity = DA_UNMAP_GRANULARITY_DEFAULT; dev->dev_attrib.unmap_granularity_alignment = DA_UNMAP_GRANULARITY_ALIGNMENT_DEFAULT; dev->dev_attrib.unmap_zeroes_data = DA_UNMAP_ZEROES_DATA_DEFAULT; dev->dev_attrib.max_write_same_len = DA_MAX_WRITE_SAME_LEN; xcopy_lun = &dev->xcopy_lun; rcu_assign_pointer(xcopy_lun->lun_se_dev, dev); init_completion(&xcopy_lun->lun_shutdown_comp); INIT_LIST_HEAD(&xcopy_lun->lun_deve_list); INIT_LIST_HEAD(&xcopy_lun->lun_dev_link); mutex_init(&xcopy_lun->lun_tg_pt_md_mutex); xcopy_lun->lun_tpg = &xcopy_pt_tpg; strlcpy(dev->t10_wwn.vendor, ""LIO-ORG"", sizeof(dev->t10_wwn.vendor)); strlcpy(dev->t10_wwn.model, dev->transport->inquiry_prod, sizeof(dev->t10_wwn.model)); strlcpy(dev->t10_wwn.revision, dev->transport->inquiry_rev, sizeof(dev->t10_wwn.revision)); return dev; }","- dev->transport->free_device(dev);
+ hba->backend->ops->free_device(dev);","struct se_device *target_alloc_device(struct se_hba *hba, const char *name) { struct se_device *dev; struct se_lun *xcopy_lun; int i; dev = hba->backend->ops->alloc_device(hba, name); if (!dev) return NULL; dev->queues = kcalloc(nr_cpu_ids, sizeof(*dev->queues), GFP_KERNEL); if (!dev->queues) { hba->backend->ops->free_device(dev); return NULL; } dev->queue_cnt = nr_cpu_ids; for (i = 0; i < dev->queue_cnt; i++) { struct se_device_queue *q; q = &dev->queues[i]; INIT_LIST_HEAD(&q->state_list); spin_lock_init(&q->lock); init_llist_head(&q->sq.cmd_list); INIT_WORK(&q->sq.work, target_queued_submit_work); } dev->se_hba = hba; dev->transport = hba->backend->ops; dev->transport_flags = dev->transport->transport_flags_default; dev->prot_length = sizeof(struct t10_pi_tuple); dev->hba_index = hba->hba_index; INIT_LIST_HEAD(&dev->dev_sep_list); INIT_LIST_HEAD(&dev->dev_tmr_list); INIT_LIST_HEAD(&dev->delayed_cmd_list); INIT_LIST_HEAD(&dev->qf_cmd_list); spin_lock_init(&dev->delayed_cmd_lock); spin_lock_init(&dev->dev_reservation_lock); spin_lock_init(&dev->se_port_lock); spin_lock_init(&dev->se_tmr_lock); spin_lock_init(&dev->qf_cmd_lock); sema_init(&dev->caw_sem, 1); INIT_LIST_HEAD(&dev->t10_wwn.t10_vpd_list); spin_lock_init(&dev->t10_wwn.t10_vpd_lock); INIT_LIST_HEAD(&dev->t10_pr.registration_list); INIT_LIST_HEAD(&dev->t10_pr.aptpl_reg_list); spin_lock_init(&dev->t10_pr.registration_lock); spin_lock_init(&dev->t10_pr.aptpl_reg_lock); INIT_LIST_HEAD(&dev->t10_alua.tg_pt_gps_list); spin_lock_init(&dev->t10_alua.tg_pt_gps_lock); INIT_LIST_HEAD(&dev->t10_alua.lba_map_list); spin_lock_init(&dev->t10_alua.lba_map_lock); INIT_WORK(&dev->delayed_cmd_work, target_do_delayed_work); mutex_init(&dev->lun_reset_mutex); dev->t10_wwn.t10_dev = dev; dev->t10_wwn.company_id = 0x001405; dev->t10_alua.t10_dev = dev; dev->dev_attrib.da_dev = dev; dev->dev_attrib.emulate_model_alias = DA_EMULATE_MODEL_ALIAS; dev->dev_attrib.emulate_dpo = 1; dev->dev_attrib.emulate_fua_write = 1; dev->dev_attrib.emulate_fua_read = 1; dev->dev_attrib.emulate_write_cache = DA_EMULATE_WRITE_CACHE; dev->dev_attrib.emulate_ua_intlck_ctrl = TARGET_UA_INTLCK_CTRL_CLEAR; dev->dev_attrib.emulate_tas = DA_EMULATE_TAS; dev->dev_attrib.emulate_tpu = DA_EMULATE_TPU; dev->dev_attrib.emulate_tpws = DA_EMULATE_TPWS; dev->dev_attrib.emulate_caw = DA_EMULATE_CAW; dev->dev_attrib.emulate_3pc = DA_EMULATE_3PC; dev->dev_attrib.emulate_pr = DA_EMULATE_PR; dev->dev_attrib.pi_prot_type = TARGET_DIF_TYPE0_PROT; dev->dev_attrib.enforce_pr_isids = DA_ENFORCE_PR_ISIDS; dev->dev_attrib.force_pr_aptpl = DA_FORCE_PR_APTPL; dev->dev_attrib.is_nonrot = DA_IS_NONROT; dev->dev_attrib.emulate_rest_reord = DA_EMULATE_REST_REORD; dev->dev_attrib.max_unmap_lba_count = DA_MAX_UNMAP_LBA_COUNT; dev->dev_attrib.max_unmap_block_desc_count = DA_MAX_UNMAP_BLOCK_DESC_COUNT; dev->dev_attrib.unmap_granularity = DA_UNMAP_GRANULARITY_DEFAULT; dev->dev_attrib.unmap_granularity_alignment = DA_UNMAP_GRANULARITY_ALIGNMENT_DEFAULT; dev->dev_attrib.unmap_zeroes_data = DA_UNMAP_ZEROES_DATA_DEFAULT; dev->dev_attrib.max_write_same_len = DA_MAX_WRITE_SAME_LEN; xcopy_lun = &dev->xcopy_lun; rcu_assign_pointer(xcopy_lun->lun_se_dev, dev); init_completion(&xcopy_lun->lun_shutdown_comp); INIT_LIST_HEAD(&xcopy_lun->lun_deve_list); INIT_LIST_HEAD(&xcopy_lun->lun_dev_link); mutex_init(&xcopy_lun->lun_tg_pt_md_mutex); xcopy_lun->lun_tpg = &xcopy_pt_tpg; strlcpy(dev->t10_wwn.vendor, ""LIO-ORG"", sizeof(dev->t10_wwn.vendor)); strlcpy(dev->t10_wwn.model, dev->transport->inquiry_prod, sizeof(dev->t10_wwn.model)); strlcpy(dev->t10_wwn.revision, dev->transport->inquiry_rev, sizeof(dev->t10_wwn.revision)); return dev; }"
726----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49999/bad/fs_operation.c----afs_wait_for_operation,"void afs_wait_for_operation(struct afs_operation *op) { _enter(""""); while (afs_select_fileserver(op)) { op->call_responded = false; op->call_error = 0; op->call_abort_code = 0; if (test_bit(AFS_SERVER_FL_IS_YFS, &op->server->flags) && op->ops->issue_yfs_rpc) op->ops->issue_yfs_rpc(op); else if (op->ops->issue_afs_rpc) op->ops->issue_afs_rpc(op); else op->call_error = -ENOTSUPP; if (op->call) { afs_wait_for_call_to_complete(op->call); op->call_abort_code = op->call->abort_code; op->call_error = op->call->error; op->call_responded = op->call->responded; afs_put_call(op->call); } } <S2SV_StartVul> if (op->call_responded) <S2SV_EndVul> set_bit(AFS_SERVER_FL_RESPONDING, &op->server->flags); if (!afs_op_error(op)) { _debug(""success""); op->ops->success(op); } else if (op->cumul_error.aborted) { if (op->ops->aborted) op->ops->aborted(op); } else { if (op->ops->failed) op->ops->failed(op); } afs_end_vnode_operation(op); if (!afs_op_error(op) && op->ops->edit_dir) { _debug(""edit_dir""); op->ops->edit_dir(op); } _leave(""""); }","- if (op->call_responded)
+ if (op->call_responded && op->server)","void afs_wait_for_operation(struct afs_operation *op) { _enter(""""); while (afs_select_fileserver(op)) { op->call_responded = false; op->call_error = 0; op->call_abort_code = 0; if (test_bit(AFS_SERVER_FL_IS_YFS, &op->server->flags) && op->ops->issue_yfs_rpc) op->ops->issue_yfs_rpc(op); else if (op->ops->issue_afs_rpc) op->ops->issue_afs_rpc(op); else op->call_error = -ENOTSUPP; if (op->call) { afs_wait_for_call_to_complete(op->call); op->call_abort_code = op->call->abort_code; op->call_error = op->call->error; op->call_responded = op->call->responded; afs_put_call(op->call); } } if (op->call_responded && op->server) set_bit(AFS_SERVER_FL_RESPONDING, &op->server->flags); if (!afs_op_error(op)) { _debug(""success""); op->ops->success(op); } else if (op->cumul_error.aborted) { if (op->ops->aborted) op->ops->aborted(op); } else { if (op->ops->failed) op->ops->failed(op); } afs_end_vnode_operation(op); if (!afs_op_error(op) && op->ops->edit_dir) { _debug(""edit_dir""); op->ops->edit_dir(op); } _leave(""""); }"
1365----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56698/bad/gadget.c----dwc3_prepare_trbs_sg,"static int dwc3_prepare_trbs_sg(struct dwc3_ep *dep, struct dwc3_request *req) { struct scatterlist *sg = req->start_sg; struct scatterlist *s; int i; unsigned int length = req->request.length; <S2SV_StartVul> unsigned int remaining = req->request.num_mapped_sgs <S2SV_EndVul> <S2SV_StartVul> - req->num_queued_sgs; <S2SV_EndVul> unsigned int num_trbs = req->num_trbs; bool needs_extra_trb = dwc3_needs_extra_trb(dep, req); <S2SV_StartVul> for_each_sg(req->request.sg, s, req->num_queued_sgs, i) <S2SV_EndVul> length -= sg_dma_len(s); for_each_sg(sg, s, remaining, i) { unsigned int num_trbs_left = dwc3_calc_trbs_left(dep); unsigned int trb_length; bool must_interrupt = false; bool last_sg = false; trb_length = min_t(unsigned int, length, sg_dma_len(s)); length -= trb_length; if ((i == remaining - 1) || !length) last_sg = true; if (!num_trbs_left) break; if (last_sg) { if (!dwc3_prepare_last_sg(dep, req, trb_length, i)) break; } else { if (num_trbs_left == 1 || (needs_extra_trb && num_trbs_left <= 2 && sg_dma_len(sg_next(s)) >= length)) { struct dwc3_request *r; list_for_each_entry(r, &dep->started_list, list) { if (r != req && !r->request.no_interrupt) break; if (r == req) must_interrupt = true; } } dwc3_prepare_one_trb(dep, req, trb_length, 1, i, false, must_interrupt); } if (!last_sg) req->start_sg = sg_next(s); req->num_queued_sgs++; req->num_pending_sgs--; if (length == 0) { req->num_pending_sgs = 0; break; } if (must_interrupt) break; } return req->num_trbs - num_trbs; }","- unsigned int remaining = req->request.num_mapped_sgs
- - req->num_queued_sgs;
- for_each_sg(req->request.sg, s, req->num_queued_sgs, i)
+ unsigned int remaining = req->num_pending_sgs;
+ unsigned int num_queued_sgs = req->request.num_mapped_sgs - remaining;
+ for_each_sg(req->request.sg, s, num_queued_sgs, i)","static int dwc3_prepare_trbs_sg(struct dwc3_ep *dep, struct dwc3_request *req) { struct scatterlist *sg = req->start_sg; struct scatterlist *s; int i; unsigned int length = req->request.length; unsigned int remaining = req->num_pending_sgs; unsigned int num_queued_sgs = req->request.num_mapped_sgs - remaining; unsigned int num_trbs = req->num_trbs; bool needs_extra_trb = dwc3_needs_extra_trb(dep, req); for_each_sg(req->request.sg, s, num_queued_sgs, i) length -= sg_dma_len(s); for_each_sg(sg, s, remaining, i) { unsigned int num_trbs_left = dwc3_calc_trbs_left(dep); unsigned int trb_length; bool must_interrupt = false; bool last_sg = false; trb_length = min_t(unsigned int, length, sg_dma_len(s)); length -= trb_length; if ((i == remaining - 1) || !length) last_sg = true; if (!num_trbs_left) break; if (last_sg) { if (!dwc3_prepare_last_sg(dep, req, trb_length, i)) break; } else { if (num_trbs_left == 1 || (needs_extra_trb && num_trbs_left <= 2 && sg_dma_len(sg_next(s)) >= length)) { struct dwc3_request *r; list_for_each_entry(r, &dep->started_list, list) { if (r != req && !r->request.no_interrupt) break; if (r == req) must_interrupt = true; } } dwc3_prepare_one_trb(dep, req, trb_length, 1, i, false, must_interrupt); } if (!last_sg) req->start_sg = sg_next(s); req->num_queued_sgs++; req->num_pending_sgs--; if (length == 0) { req->num_pending_sgs = 0; break; } if (must_interrupt) break; } return req->num_trbs - num_trbs; }"
1486----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-57911/bad/iio_simple_dummy_buffer.c----iio_simple_dummy_trigger_h,"static irqreturn_t iio_simple_dummy_trigger_h(int irq, void *p) { struct iio_poll_func *pf = p; struct iio_dev *indio_dev = pf->indio_dev; int len = 0; u16 *data; <S2SV_StartVul> data = kmalloc(indio_dev->scan_bytes, GFP_KERNEL); <S2SV_EndVul> if (!data) goto done; if (!bitmap_empty(indio_dev->active_scan_mask, indio_dev->masklength)) { int i, j; for (i = 0, j = 0; i < bitmap_weight(indio_dev->active_scan_mask, indio_dev->masklength); i++, j++) { j = find_next_bit(indio_dev->active_scan_mask, indio_dev->masklength, j); data[i] = fakedata[j]; len += 2; } } iio_push_to_buffers_with_timestamp(indio_dev, data, iio_get_time_ns(indio_dev)); kfree(data); done: iio_trigger_notify_done(indio_dev->trig); return IRQ_HANDLED; }","- data = kmalloc(indio_dev->scan_bytes, GFP_KERNEL);
+ data = kzalloc(indio_dev->scan_bytes, GFP_KERNEL);","static irqreturn_t iio_simple_dummy_trigger_h(int irq, void *p) { struct iio_poll_func *pf = p; struct iio_dev *indio_dev = pf->indio_dev; int len = 0; u16 *data; data = kzalloc(indio_dev->scan_bytes, GFP_KERNEL); if (!data) goto done; if (!bitmap_empty(indio_dev->active_scan_mask, indio_dev->masklength)) { int i, j; for (i = 0, j = 0; i < bitmap_weight(indio_dev->active_scan_mask, indio_dev->masklength); i++, j++) { j = find_next_bit(indio_dev->active_scan_mask, indio_dev->masklength, j); data[i] = fakedata[j]; len += 2; } } iio_push_to_buffers_with_timestamp(indio_dev, data, iio_get_time_ns(indio_dev)); kfree(data); done: iio_trigger_notify_done(indio_dev->trig); return IRQ_HANDLED; }"
245----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46715/bad/inkern.c----iio_channel_read,"static int iio_channel_read(struct iio_channel *chan, int *val, int *val2, enum iio_chan_info_enum info) { int unused; int vals[INDIO_MAX_RAW_ELEMENTS]; int ret; int val_len = 2; if (!val2) val2 = &unused; if (!iio_channel_has_info(chan->channel, info)) return -EINVAL; <S2SV_StartVul> if (chan->indio_dev->info->read_raw_multi) { <S2SV_EndVul> <S2SV_StartVul> ret = chan->indio_dev->info->read_raw_multi(chan->indio_dev, <S2SV_EndVul> <S2SV_StartVul> chan->channel, INDIO_MAX_RAW_ELEMENTS, <S2SV_EndVul> <S2SV_StartVul> vals, &val_len, info); <S2SV_EndVul> *val = vals[0]; *val2 = vals[1]; } else { <S2SV_StartVul> ret = chan->indio_dev->info->read_raw(chan->indio_dev, <S2SV_EndVul> <S2SV_StartVul> chan->channel, val, val2, info); <S2SV_EndVul> } return ret; }","- if (chan->indio_dev->info->read_raw_multi) {
- ret = chan->indio_dev->info->read_raw_multi(chan->indio_dev,
- chan->channel, INDIO_MAX_RAW_ELEMENTS,
- vals, &val_len, info);
- ret = chan->indio_dev->info->read_raw(chan->indio_dev,
- chan->channel, val, val2, info);
+ const struct iio_info *iio_info = chan->indio_dev->info;
+ return -EINVAL;
+ if (iio_info->read_raw_multi) {
+ ret = iio_info->read_raw_multi(chan->indio_dev,
+ chan->channel,
+ INDIO_MAX_RAW_ELEMENTS,
+ vals, &val_len, info);
+ } else if (iio_info->read_raw) {
+ ret = iio_info->read_raw(chan->indio_dev,
+ chan->channel, val, val2, info);
+ return -EINVAL;","static int iio_channel_read(struct iio_channel *chan, int *val, int *val2, enum iio_chan_info_enum info) { const struct iio_info *iio_info = chan->indio_dev->info; int unused; int vals[INDIO_MAX_RAW_ELEMENTS]; int ret; int val_len = 2; if (!val2) val2 = &unused; if (!iio_channel_has_info(chan->channel, info)) return -EINVAL; if (iio_info->read_raw_multi) { ret = iio_info->read_raw_multi(chan->indio_dev, chan->channel, INDIO_MAX_RAW_ELEMENTS, vals, &val_len, info); *val = vals[0]; *val2 = vals[1]; } else if (iio_info->read_raw) { ret = iio_info->read_raw(chan->indio_dev, chan->channel, val, val2, info); } else { return -EINVAL; } return ret; }"
596----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49918/bad/dcn32_resource.c----*dcn32_acquire_idle_pipe_for_head_pipe_in_layer,"static struct pipe_ctx *dcn32_acquire_idle_pipe_for_head_pipe_in_layer( struct dc_state *state, const struct resource_pool *pool, struct dc_stream_state *stream, const struct pipe_ctx *head_pipe) { struct resource_context *res_ctx = &state->res_ctx; struct pipe_ctx *idle_pipe, *pipe; struct resource_context *old_ctx = &stream->ctx->dc->current_state->res_ctx; int head_index; <S2SV_StartVul> if (!head_pipe) <S2SV_EndVul> ASSERT(0); head_index = head_pipe->pipe_idx; pipe = &old_ctx->pipe_ctx[head_index]; if (pipe->bottom_pipe && res_ctx->pipe_ctx[pipe->bottom_pipe->pipe_idx].stream == NULL) { idle_pipe = &res_ctx->pipe_ctx[pipe->bottom_pipe->pipe_idx]; idle_pipe->pipe_idx = pipe->bottom_pipe->pipe_idx; } else { idle_pipe = find_idle_secondary_pipe_check_mpo(res_ctx, pool, head_pipe); if (!idle_pipe) return NULL; } idle_pipe->stream = head_pipe->stream; idle_pipe->stream_res.tg = head_pipe->stream_res.tg; idle_pipe->stream_res.opp = head_pipe->stream_res.opp; idle_pipe->plane_res.hubp = pool->hubps[idle_pipe->pipe_idx]; idle_pipe->plane_res.ipp = pool->ipps[idle_pipe->pipe_idx]; idle_pipe->plane_res.dpp = pool->dpps[idle_pipe->pipe_idx]; idle_pipe->plane_res.mpcc_inst = pool->dpps[idle_pipe->pipe_idx]->inst; return idle_pipe; }","- if (!head_pipe)
+ if (!head_pipe) {
+ return NULL;
+ }","static struct pipe_ctx *dcn32_acquire_idle_pipe_for_head_pipe_in_layer( struct dc_state *state, const struct resource_pool *pool, struct dc_stream_state *stream, const struct pipe_ctx *head_pipe) { struct resource_context *res_ctx = &state->res_ctx; struct pipe_ctx *idle_pipe, *pipe; struct resource_context *old_ctx = &stream->ctx->dc->current_state->res_ctx; int head_index; if (!head_pipe) { ASSERT(0); return NULL; } head_index = head_pipe->pipe_idx; pipe = &old_ctx->pipe_ctx[head_index]; if (pipe->bottom_pipe && res_ctx->pipe_ctx[pipe->bottom_pipe->pipe_idx].stream == NULL) { idle_pipe = &res_ctx->pipe_ctx[pipe->bottom_pipe->pipe_idx]; idle_pipe->pipe_idx = pipe->bottom_pipe->pipe_idx; } else { idle_pipe = find_idle_secondary_pipe_check_mpo(res_ctx, pool, head_pipe); if (!idle_pipe) return NULL; } idle_pipe->stream = head_pipe->stream; idle_pipe->stream_res.tg = head_pipe->stream_res.tg; idle_pipe->stream_res.opp = head_pipe->stream_res.opp; idle_pipe->plane_res.hubp = pool->hubps[idle_pipe->pipe_idx]; idle_pipe->plane_res.ipp = pool->ipps[idle_pipe->pipe_idx]; idle_pipe->plane_res.dpp = pool->dpps[idle_pipe->pipe_idx]; idle_pipe->plane_res.mpcc_inst = pool->dpps[idle_pipe->pipe_idx]->inst; return idle_pipe; }"
920----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50164/bad/verifier.c----check_mem_size_reg,"static int check_mem_size_reg(struct bpf_verifier_env *env, struct bpf_reg_state *reg, u32 regno, bool zero_size_allowed, struct bpf_call_arg_meta *meta) { int err; meta->msize_max_value = reg->umax_value; if (!tnum_is_const(reg->var_off)) meta = NULL; if (reg->smin_value < 0) { verbose(env, ""R%d min value is negative, either use unsigned or 'var &= const'\n"", regno); return -EACCES; } if (reg->umin_value == 0 && !zero_size_allowed) { verbose(env, ""R%d invalid zero-sized read: u64=[%lld,%lld]\n"", regno, reg->umin_value, reg->umax_value); return -EACCES; } if (reg->umax_value >= BPF_MAX_VAR_SIZ) { verbose(env, ""R%d unbounded memory access, use 'var &= const' or 'if (var < const)'\n"", regno); return -EACCES; } <S2SV_StartVul> err = check_helper_mem_access(env, regno - 1, <S2SV_EndVul> <S2SV_StartVul> reg->umax_value, <S2SV_EndVul> <S2SV_StartVul> zero_size_allowed, meta); <S2SV_EndVul> if (!err) err = mark_chain_precision(env, regno); return err; }","- err = check_helper_mem_access(env, regno - 1,
- reg->umax_value,
- zero_size_allowed, meta);
+ err = check_helper_mem_access(env, regno - 1, reg->umax_value,
+ access_type, zero_size_allowed, meta);","static int check_mem_size_reg(struct bpf_verifier_env *env, struct bpf_reg_state *reg, u32 regno, enum bpf_access_type access_type, bool zero_size_allowed, struct bpf_call_arg_meta *meta) { int err; meta->msize_max_value = reg->umax_value; if (!tnum_is_const(reg->var_off)) meta = NULL; if (reg->smin_value < 0) { verbose(env, ""R%d min value is negative, either use unsigned or 'var &= const'\n"", regno); return -EACCES; } if (reg->umin_value == 0 && !zero_size_allowed) { verbose(env, ""R%d invalid zero-sized read: u64=[%lld,%lld]\n"", regno, reg->umin_value, reg->umax_value); return -EACCES; } if (reg->umax_value >= BPF_MAX_VAR_SIZ) { verbose(env, ""R%d unbounded memory access, use 'var &= const' or 'if (var < const)'\n"", regno); return -EACCES; } err = check_helper_mem_access(env, regno - 1, reg->umax_value, access_type, zero_size_allowed, meta); if (!err) err = mark_chain_precision(env, regno); return err; }"
833----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50099/bad/decode-insn.c----arm_kprobe_decode_insn,"arm_kprobe_decode_insn(kprobe_opcode_t *addr, struct arch_specific_insn *asi) { enum probe_insn decoded; probe_opcode_t insn = le32_to_cpu(*addr); probe_opcode_t *scan_end = NULL; unsigned long size = 0, offset = 0; if (kallsyms_lookup_size_offset((unsigned long) addr, &size, &offset)) { if (offset < (MAX_ATOMIC_CONTEXT_SIZE*sizeof(kprobe_opcode_t))) scan_end = addr - (offset / sizeof(kprobe_opcode_t)); else scan_end = addr - MAX_ATOMIC_CONTEXT_SIZE; } <S2SV_StartVul> decoded = arm_probe_decode_insn(insn, &asi->api); <S2SV_EndVul> if (decoded != INSN_REJECTED && scan_end) if (is_probed_address_atomic(addr - 1, scan_end)) return INSN_REJECTED; return decoded; }","- decoded = arm_probe_decode_insn(insn, &asi->api);
+ struct arch_probe_insn *api = &asi->api;
+ if (aarch64_insn_is_ldr_lit(insn)) {
+ api->handler = simulate_ldr_literal;
+ decoded = INSN_GOOD_NO_SLOT;
+ } else if (aarch64_insn_is_ldrsw_lit(insn)) {
+ api->handler = simulate_ldrsw_literal;
+ decoded = INSN_GOOD_NO_SLOT;
+ } else {
+ decoded = arm_probe_decode_insn(insn, &asi->api);
+ }
+ }","arm_kprobe_decode_insn(kprobe_opcode_t *addr, struct arch_specific_insn *asi) { enum probe_insn decoded; probe_opcode_t insn = le32_to_cpu(*addr); probe_opcode_t *scan_end = NULL; unsigned long size = 0, offset = 0; struct arch_probe_insn *api = &asi->api; if (aarch64_insn_is_ldr_lit(insn)) { api->handler = simulate_ldr_literal; decoded = INSN_GOOD_NO_SLOT; } else if (aarch64_insn_is_ldrsw_lit(insn)) { api->handler = simulate_ldrsw_literal; decoded = INSN_GOOD_NO_SLOT; } else { decoded = arm_probe_decode_insn(insn, &asi->api); } if (kallsyms_lookup_size_offset((unsigned long) addr, &size, &offset)) { if (offset < (MAX_ATOMIC_CONTEXT_SIZE*sizeof(kprobe_opcode_t))) scan_end = addr - (offset / sizeof(kprobe_opcode_t)); else scan_end = addr - MAX_ATOMIC_CONTEXT_SIZE; } if (decoded != INSN_REJECTED && scan_end) if (is_probed_address_atomic(addr - 1, scan_end)) return INSN_REJECTED; return decoded; }"
648----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49950/bad/hci_event.c----hci_remote_features_evt,"static void hci_remote_features_evt(struct hci_dev *hdev, void *data, struct sk_buff *skb) { struct hci_ev_remote_features *ev = data; struct hci_conn *conn; bt_dev_dbg(hdev, ""status 0x%2.2x"", ev->status); hci_dev_lock(hdev); conn = hci_conn_hash_lookup_handle(hdev, __le16_to_cpu(ev->handle)); if (!conn) goto unlock; if (!ev->status) memcpy(conn->features[0], ev->features, 8); if (conn->state != BT_CONFIG) goto unlock; if (!ev->status && lmp_ext_feat_capable(hdev) && lmp_ext_feat_capable(conn)) { struct hci_cp_read_remote_ext_features cp; cp.handle = ev->handle; cp.page = 0x01; hci_send_cmd(hdev, HCI_OP_READ_REMOTE_EXT_FEATURES, sizeof(cp), &cp); goto unlock; } <S2SV_StartVul> if (!ev->status && !test_bit(HCI_CONN_MGMT_CONNECTED, &conn->flags)) { <S2SV_EndVul> struct hci_cp_remote_name_req cp; memset(&cp, 0, sizeof(cp)); bacpy(&cp.bdaddr, &conn->dst); cp.pscan_rep_mode = 0x02; hci_send_cmd(hdev, HCI_OP_REMOTE_NAME_REQ, sizeof(cp), &cp); } else { mgmt_device_connected(hdev, conn, NULL, 0); } if (!hci_outgoing_auth_needed(hdev, conn)) { conn->state = BT_CONNECTED; hci_connect_cfm(conn, ev->status); hci_conn_drop(conn); } unlock: hci_dev_unlock(hdev); }","- if (!ev->status && !test_bit(HCI_CONN_MGMT_CONNECTED, &conn->flags)) {
+ if (!ev->status) {","static void hci_remote_features_evt(struct hci_dev *hdev, void *data, struct sk_buff *skb) { struct hci_ev_remote_features *ev = data; struct hci_conn *conn; bt_dev_dbg(hdev, ""status 0x%2.2x"", ev->status); hci_dev_lock(hdev); conn = hci_conn_hash_lookup_handle(hdev, __le16_to_cpu(ev->handle)); if (!conn) goto unlock; if (!ev->status) memcpy(conn->features[0], ev->features, 8); if (conn->state != BT_CONFIG) goto unlock; if (!ev->status && lmp_ext_feat_capable(hdev) && lmp_ext_feat_capable(conn)) { struct hci_cp_read_remote_ext_features cp; cp.handle = ev->handle; cp.page = 0x01; hci_send_cmd(hdev, HCI_OP_READ_REMOTE_EXT_FEATURES, sizeof(cp), &cp); goto unlock; } if (!ev->status) { struct hci_cp_remote_name_req cp; memset(&cp, 0, sizeof(cp)); bacpy(&cp.bdaddr, &conn->dst); cp.pscan_rep_mode = 0x02; hci_send_cmd(hdev, HCI_OP_REMOTE_NAME_REQ, sizeof(cp), &cp); } else { mgmt_device_connected(hdev, conn, NULL, 0); } if (!hci_outgoing_auth_needed(hdev, conn)) { conn->state = BT_CONNECTED; hci_connect_cfm(conn, ev->status); hci_conn_drop(conn); } unlock: hci_dev_unlock(hdev); }"
1280----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56577/bad/mtk_jpeg_dec_hw.c----mtk_jpegdec_hw_probe,"static int mtk_jpegdec_hw_probe(struct platform_device *pdev) { struct mtk_jpegdec_clk *jpegdec_clk; struct mtk_jpeg_dev *master_dev; struct mtk_jpegdec_comp_dev *dev; int ret, i; struct device *decs = &pdev->dev; if (!decs->parent) return -EPROBE_DEFER; master_dev = dev_get_drvdata(decs->parent); if (!master_dev) return -EPROBE_DEFER; dev = devm_kzalloc(&pdev->dev, sizeof(*dev), GFP_KERNEL); if (!dev) return -ENOMEM; dev->plat_dev = pdev; dev->dev = &pdev->dev; <S2SV_StartVul> ret = devm_add_action_or_reset(&pdev->dev, <S2SV_EndVul> <S2SV_StartVul> mtk_jpegdec_destroy_workqueue, <S2SV_EndVul> <S2SV_StartVul> master_dev->workqueue); <S2SV_EndVul> <S2SV_StartVul> if (ret) <S2SV_EndVul> <S2SV_StartVul> return ret; <S2SV_EndVul> spin_lock_init(&dev->hw_lock); dev->hw_state = MTK_JPEG_HW_IDLE; INIT_DELAYED_WORK(&dev->job_timeout_work, mtk_jpegdec_timeout_work); jpegdec_clk = &dev->jdec_clk; jpegdec_clk->clk_num = devm_clk_bulk_get_all(&pdev->dev, &jpegdec_clk->clks); if (jpegdec_clk->clk_num < 0) return dev_err_probe(&pdev->dev, jpegdec_clk->clk_num, ""Failed to get jpegdec clock count.\n""); dev->reg_base = devm_platform_ioremap_resource(pdev, 0); if (IS_ERR(dev->reg_base)) return PTR_ERR(dev->reg_base); ret = mtk_jpegdec_hw_init_irq(dev); if (ret) return dev_err_probe(&pdev->dev, ret, ""Failed to register JPEGDEC irq handler.\n""); i = atomic_add_return(1, &master_dev->hw_index) - 1; master_dev->dec_hw_dev[i] = dev; master_dev->reg_decbase[i] = dev->reg_base; dev->master_dev = master_dev; platform_set_drvdata(pdev, dev); pm_runtime_enable(&pdev->dev); return 0; }","- ret = devm_add_action_or_reset(&pdev->dev,
- mtk_jpegdec_destroy_workqueue,
- master_dev->workqueue);
- if (ret)
- return ret;","static int mtk_jpegdec_hw_probe(struct platform_device *pdev) { struct mtk_jpegdec_clk *jpegdec_clk; struct mtk_jpeg_dev *master_dev; struct mtk_jpegdec_comp_dev *dev; int ret, i; struct device *decs = &pdev->dev; if (!decs->parent) return -EPROBE_DEFER; master_dev = dev_get_drvdata(decs->parent); if (!master_dev) return -EPROBE_DEFER; dev = devm_kzalloc(&pdev->dev, sizeof(*dev), GFP_KERNEL); if (!dev) return -ENOMEM; dev->plat_dev = pdev; dev->dev = &pdev->dev; spin_lock_init(&dev->hw_lock); dev->hw_state = MTK_JPEG_HW_IDLE; INIT_DELAYED_WORK(&dev->job_timeout_work, mtk_jpegdec_timeout_work); jpegdec_clk = &dev->jdec_clk; jpegdec_clk->clk_num = devm_clk_bulk_get_all(&pdev->dev, &jpegdec_clk->clks); if (jpegdec_clk->clk_num < 0) return dev_err_probe(&pdev->dev, jpegdec_clk->clk_num, ""Failed to get jpegdec clock count.\n""); dev->reg_base = devm_platform_ioremap_resource(pdev, 0); if (IS_ERR(dev->reg_base)) return PTR_ERR(dev->reg_base); ret = mtk_jpegdec_hw_init_irq(dev); if (ret) return dev_err_probe(&pdev->dev, ret, ""Failed to register JPEGDEC irq handler.\n""); i = atomic_add_return(1, &master_dev->hw_index) - 1; master_dev->dec_hw_dev[i] = dev; master_dev->reg_decbase[i] = dev->reg_base; dev->master_dev = master_dev; platform_set_drvdata(pdev, dev); pm_runtime_enable(&pdev->dev); return 0; }"
761----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50038/bad/xt_connbytes.c----connbytes_mt_check,"static int connbytes_mt_check(const struct xt_mtchk_param *par) { const struct xt_connbytes_info *sinfo = par->matchinfo; int ret; if (sinfo->what != XT_CONNBYTES_PKTS && sinfo->what != XT_CONNBYTES_BYTES && sinfo->what != XT_CONNBYTES_AVGPKT) return -EINVAL; if (sinfo->direction != XT_CONNBYTES_DIR_ORIGINAL && sinfo->direction != XT_CONNBYTES_DIR_REPLY && sinfo->direction != XT_CONNBYTES_DIR_BOTH) return -EINVAL; ret = nf_ct_netns_get(par->net, par->family); <S2SV_StartVul> if (ret < 0) <S2SV_EndVul> pr_info_ratelimited(""cannot load conntrack support for proto=%u\n"", par->family); if (!nf_ct_acct_enabled(par->net)) { pr_warn(""Forcing CT accounting to be enabled\n""); nf_ct_set_acct(par->net, true); } return ret; }","- if (ret < 0)
+ if (ret < 0) {
+ return ret;
+ }","static int connbytes_mt_check(const struct xt_mtchk_param *par) { const struct xt_connbytes_info *sinfo = par->matchinfo; int ret; if (sinfo->what != XT_CONNBYTES_PKTS && sinfo->what != XT_CONNBYTES_BYTES && sinfo->what != XT_CONNBYTES_AVGPKT) return -EINVAL; if (sinfo->direction != XT_CONNBYTES_DIR_ORIGINAL && sinfo->direction != XT_CONNBYTES_DIR_REPLY && sinfo->direction != XT_CONNBYTES_DIR_BOTH) return -EINVAL; ret = nf_ct_netns_get(par->net, par->family); if (ret < 0) { pr_info_ratelimited(""cannot load conntrack support for proto=%u\n"", par->family); return ret; } if (!nf_ct_acct_enabled(par->net)) { pr_warn(""Forcing CT accounting to be enabled\n""); nf_ct_set_acct(par->net, true); } return ret; }"
789----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50065/bad/namei.c----ntfs_d_hash,"static int ntfs_d_hash(const struct dentry *dentry, struct qstr *name) { struct ntfs_sb_info *sbi; const char *n = name->name; unsigned int len = name->len; unsigned long hash; struct cpu_str *uni; unsigned int c; int err; hash = init_name_hash(dentry); for (;;) { if (!len--) { name->hash = end_name_hash(hash); return 0; } c = *n++; if (c >= 0x80) break; hash = partial_name_hash(toupper(c), hash); } <S2SV_StartVul> uni = __getname(); <S2SV_EndVul> if (!uni) return -ENOMEM; sbi = dentry->d_sb->s_fs_info; err = ntfs_nls_to_utf16(sbi, name->name, name->len, uni, NTFS_NAME_LEN, UTF16_HOST_ENDIAN); if (err < 0) goto out; if (!err) { err = -EINVAL; goto out; } hash = ntfs_names_hash(uni->name, uni->len, sbi->upcase, init_name_hash(dentry)); name->hash = end_name_hash(hash); err = 0; out: <S2SV_StartVul> __putname(uni); <S2SV_EndVul> return err; }","- uni = __getname();
- __putname(uni);
+ uni = kmem_cache_alloc(names_cachep, GFP_NOWAIT);
+ kmem_cache_free(names_cachep, uni);","static int ntfs_d_hash(const struct dentry *dentry, struct qstr *name) { struct ntfs_sb_info *sbi; const char *n = name->name; unsigned int len = name->len; unsigned long hash; struct cpu_str *uni; unsigned int c; int err; hash = init_name_hash(dentry); for (;;) { if (!len--) { name->hash = end_name_hash(hash); return 0; } c = *n++; if (c >= 0x80) break; hash = partial_name_hash(toupper(c), hash); } uni = kmem_cache_alloc(names_cachep, GFP_NOWAIT); if (!uni) return -ENOMEM; sbi = dentry->d_sb->s_fs_info; err = ntfs_nls_to_utf16(sbi, name->name, name->len, uni, NTFS_NAME_LEN, UTF16_HOST_ENDIAN); if (err < 0) goto out; if (!err) { err = -EINVAL; goto out; } hash = ntfs_names_hash(uni->name, uni->len, sbi->upcase, init_name_hash(dentry)); name->hash = end_name_hash(hash); err = 0; out: kmem_cache_free(names_cachep, uni); return err; }"
1353----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56674/bad/virtio_net.c----virtnet_enable_queue_pair,"static int virtnet_enable_queue_pair(struct virtnet_info *vi, int qp_index) { struct net_device *dev = vi->dev; int err; err = xdp_rxq_info_reg(&vi->rq[qp_index].xdp_rxq, dev, qp_index, vi->rq[qp_index].napi.napi_id); if (err < 0) return err; err = xdp_rxq_info_reg_mem_model(&vi->rq[qp_index].xdp_rxq, MEM_TYPE_PAGE_SHARED, NULL); if (err < 0) goto err_xdp_reg_mem_model; <S2SV_StartVul> netdev_tx_reset_queue(netdev_get_tx_queue(vi->dev, qp_index)); <S2SV_EndVul> virtnet_napi_enable(vi->rq[qp_index].vq, &vi->rq[qp_index].napi); virtnet_napi_tx_enable(vi, vi->sq[qp_index].vq, &vi->sq[qp_index].napi); return 0; err_xdp_reg_mem_model: xdp_rxq_info_unreg(&vi->rq[qp_index].xdp_rxq); return err; }","- netdev_tx_reset_queue(netdev_get_tx_queue(vi->dev, qp_index));","static int virtnet_enable_queue_pair(struct virtnet_info *vi, int qp_index) { struct net_device *dev = vi->dev; int err; err = xdp_rxq_info_reg(&vi->rq[qp_index].xdp_rxq, dev, qp_index, vi->rq[qp_index].napi.napi_id); if (err < 0) return err; err = xdp_rxq_info_reg_mem_model(&vi->rq[qp_index].xdp_rxq, MEM_TYPE_PAGE_SHARED, NULL); if (err < 0) goto err_xdp_reg_mem_model; virtnet_napi_enable(vi->rq[qp_index].vq, &vi->rq[qp_index].napi); virtnet_napi_tx_enable(vi, vi->sq[qp_index].vq, &vi->sq[qp_index].napi); return 0; err_xdp_reg_mem_model: xdp_rxq_info_unreg(&vi->rq[qp_index].xdp_rxq); return err; }"
111----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44969/bad/sclp_sd.c----sclp_sd_store_data,"static int sclp_sd_store_data(struct sclp_sd_data *result, u8 di) { u32 dsize = 0, esize = 0; unsigned long page, asce = 0; void *data = NULL; int rc; page = __get_free_page(GFP_KERNEL | GFP_DMA); if (!page) return -ENOMEM; rc = sclp_sd_sync(page, SD_EQ_SIZE, di, 0, 0, &dsize, &esize); if (rc) goto out; if (dsize == 0) goto out_result; data = vzalloc(array_size((size_t)dsize, PAGE_SIZE)); if (!data) { rc = -ENOMEM; goto out; } asce = base_asce_alloc((unsigned long) data, dsize); if (!asce) { vfree(data); rc = -ENOMEM; goto out; } rc = sclp_sd_sync(page, SD_EQ_STORE_DATA, di, asce, (u64) data, &dsize, &esize); if (rc) { <S2SV_StartVul> if (rc == -ERESTARTSYS) <S2SV_EndVul> <S2SV_StartVul> sclp_sd_sync(page, SD_EQ_HALT, di, 0, 0, NULL, NULL); <S2SV_EndVul> vfree(data); goto out; } out_result: result->esize_bytes = (size_t) esize * PAGE_SIZE; result->dsize_bytes = (size_t) dsize * PAGE_SIZE; result->data = data; out: base_asce_free(asce); free_page(page); return rc; }","- if (rc == -ERESTARTSYS)
- sclp_sd_sync(page, SD_EQ_HALT, di, 0, 0, NULL, NULL);
+ if (rc == -ERESTARTSYS) {
+ if (sclp_sd_sync(page, SD_EQ_HALT, di, 0, 0, NULL, NULL)) {
+ pr_warn(""Could not stop Store Data request - leaking at least %zu bytes\n"",
+ (size_t)dsize * PAGE_SIZE);
+ data = NULL;
+ asce = 0;
+ }
+ }
+ }","static int sclp_sd_store_data(struct sclp_sd_data *result, u8 di) { u32 dsize = 0, esize = 0; unsigned long page, asce = 0; void *data = NULL; int rc; page = __get_free_page(GFP_KERNEL | GFP_DMA); if (!page) return -ENOMEM; rc = sclp_sd_sync(page, SD_EQ_SIZE, di, 0, 0, &dsize, &esize); if (rc) goto out; if (dsize == 0) goto out_result; data = vzalloc(array_size((size_t)dsize, PAGE_SIZE)); if (!data) { rc = -ENOMEM; goto out; } asce = base_asce_alloc((unsigned long) data, dsize); if (!asce) { vfree(data); rc = -ENOMEM; goto out; } rc = sclp_sd_sync(page, SD_EQ_STORE_DATA, di, asce, (u64) data, &dsize, &esize); if (rc) { if (rc == -ERESTARTSYS) { if (sclp_sd_sync(page, SD_EQ_HALT, di, 0, 0, NULL, NULL)) { pr_warn(""Could not stop Store Data request - leaking at least %zu bytes\n"", (size_t)dsize * PAGE_SIZE); data = NULL; asce = 0; } } vfree(data); goto out; } out_result: result->esize_bytes = (size_t) esize * PAGE_SIZE; result->dsize_bytes = (size_t) dsize * PAGE_SIZE; result->data = data; out: base_asce_free(asce); free_page(page); return rc; }"
1054----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50297/bad/xilinx_axienet_main.c----axienet_start_xmit_dmaengine,"axienet_start_xmit_dmaengine(struct sk_buff *skb, struct net_device *ndev) { struct dma_async_tx_descriptor *dma_tx_desc = NULL; struct axienet_local *lp = netdev_priv(ndev); u32 app_metadata[DMA_NUM_APP_WORDS] = {0}; struct skbuf_dma_descriptor *skbuf_dma; struct dma_device *dma_dev; struct netdev_queue *txq; u32 csum_start_off; u32 csum_index_off; int sg_len; int ret; dma_dev = lp->tx_chan->device; sg_len = skb_shinfo(skb)->nr_frags + 1; if (CIRC_SPACE(lp->tx_ring_head, lp->tx_ring_tail, TX_BD_NUM_MAX) <= sg_len) { netif_stop_queue(ndev); if (net_ratelimit()) netdev_warn(ndev, ""TX ring unexpectedly full\n""); return NETDEV_TX_BUSY; } skbuf_dma = axienet_get_tx_desc(lp, lp->tx_ring_head); if (!skbuf_dma) goto xmit_error_drop_skb; lp->tx_ring_head++; sg_init_table(skbuf_dma->sgl, sg_len); ret = skb_to_sgvec(skb, skbuf_dma->sgl, 0, skb->len); if (ret < 0) goto xmit_error_drop_skb; ret = dma_map_sg(lp->dev, skbuf_dma->sgl, sg_len, DMA_TO_DEVICE); if (!ret) goto xmit_error_drop_skb; if (skb->ip_summed == CHECKSUM_PARTIAL) { if (lp->features & XAE_FEATURE_FULL_TX_CSUM) { app_metadata[0] |= 2; } else if (lp->features & XAE_FEATURE_PARTIAL_TX_CSUM) { csum_start_off = skb_transport_offset(skb); csum_index_off = csum_start_off + skb->csum_offset; app_metadata[0] |= 1; app_metadata[1] = (csum_start_off << 16) | csum_index_off; } } else if (skb->ip_summed == CHECKSUM_UNNECESSARY) { app_metadata[0] |= 2; } dma_tx_desc = dma_dev->device_prep_slave_sg(lp->tx_chan, skbuf_dma->sgl, sg_len, DMA_MEM_TO_DEV, DMA_PREP_INTERRUPT, (void *)app_metadata); if (!dma_tx_desc) goto xmit_error_unmap_sg; skbuf_dma->skb = skb; skbuf_dma->sg_len = sg_len; dma_tx_desc->callback_param = lp; dma_tx_desc->callback_result = axienet_dma_tx_cb; <S2SV_StartVul> dmaengine_submit(dma_tx_desc); <S2SV_EndVul> <S2SV_StartVul> dma_async_issue_pending(lp->tx_chan); <S2SV_EndVul> txq = skb_get_tx_queue(lp->ndev, skb); netdev_tx_sent_queue(txq, skb->len); netif_txq_maybe_stop(txq, CIRC_SPACE(lp->tx_ring_head, lp->tx_ring_tail, TX_BD_NUM_MAX), MAX_SKB_FRAGS + 1, 2 * MAX_SKB_FRAGS); return NETDEV_TX_OK; xmit_error_unmap_sg: dma_unmap_sg(lp->dev, skbuf_dma->sgl, sg_len, DMA_TO_DEVICE); xmit_error_drop_skb: dev_kfree_skb_any(skb); return NETDEV_TX_OK; }","- dmaengine_submit(dma_tx_desc);
- dma_async_issue_pending(lp->tx_chan);
+ dmaengine_submit(dma_tx_desc);
+ dma_async_issue_pending(lp->tx_chan);","axienet_start_xmit_dmaengine(struct sk_buff *skb, struct net_device *ndev) { struct dma_async_tx_descriptor *dma_tx_desc = NULL; struct axienet_local *lp = netdev_priv(ndev); u32 app_metadata[DMA_NUM_APP_WORDS] = {0}; struct skbuf_dma_descriptor *skbuf_dma; struct dma_device *dma_dev; struct netdev_queue *txq; u32 csum_start_off; u32 csum_index_off; int sg_len; int ret; dma_dev = lp->tx_chan->device; sg_len = skb_shinfo(skb)->nr_frags + 1; if (CIRC_SPACE(lp->tx_ring_head, lp->tx_ring_tail, TX_BD_NUM_MAX) <= sg_len) { netif_stop_queue(ndev); if (net_ratelimit()) netdev_warn(ndev, ""TX ring unexpectedly full\n""); return NETDEV_TX_BUSY; } skbuf_dma = axienet_get_tx_desc(lp, lp->tx_ring_head); if (!skbuf_dma) goto xmit_error_drop_skb; lp->tx_ring_head++; sg_init_table(skbuf_dma->sgl, sg_len); ret = skb_to_sgvec(skb, skbuf_dma->sgl, 0, skb->len); if (ret < 0) goto xmit_error_drop_skb; ret = dma_map_sg(lp->dev, skbuf_dma->sgl, sg_len, DMA_TO_DEVICE); if (!ret) goto xmit_error_drop_skb; if (skb->ip_summed == CHECKSUM_PARTIAL) { if (lp->features & XAE_FEATURE_FULL_TX_CSUM) { app_metadata[0] |= 2; } else if (lp->features & XAE_FEATURE_PARTIAL_TX_CSUM) { csum_start_off = skb_transport_offset(skb); csum_index_off = csum_start_off + skb->csum_offset; app_metadata[0] |= 1; app_metadata[1] = (csum_start_off << 16) | csum_index_off; } } else if (skb->ip_summed == CHECKSUM_UNNECESSARY) { app_metadata[0] |= 2; } dma_tx_desc = dma_dev->device_prep_slave_sg(lp->tx_chan, skbuf_dma->sgl, sg_len, DMA_MEM_TO_DEV, DMA_PREP_INTERRUPT, (void *)app_metadata); if (!dma_tx_desc) goto xmit_error_unmap_sg; skbuf_dma->skb = skb; skbuf_dma->sg_len = sg_len; dma_tx_desc->callback_param = lp; dma_tx_desc->callback_result = axienet_dma_tx_cb; txq = skb_get_tx_queue(lp->ndev, skb); netdev_tx_sent_queue(txq, skb->len); netif_txq_maybe_stop(txq, CIRC_SPACE(lp->tx_ring_head, lp->tx_ring_tail, TX_BD_NUM_MAX), MAX_SKB_FRAGS + 1, 2 * MAX_SKB_FRAGS); dmaengine_submit(dma_tx_desc); dma_async_issue_pending(lp->tx_chan); return NETDEV_TX_OK; xmit_error_unmap_sg: dma_unmap_sg(lp->dev, skbuf_dma->sgl, sg_len, DMA_TO_DEVICE); xmit_error_drop_skb: dev_kfree_skb_any(skb); return NETDEV_TX_OK; }"
860----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50119/bad/cifsfs.c----cifs_init_netfs,"static int cifs_init_netfs(void) { cifs_io_request_cachep = kmem_cache_create(""cifs_io_request"", sizeof(struct cifs_io_request), 0, SLAB_HWCACHE_ALIGN, NULL); if (!cifs_io_request_cachep) goto nomem_req; if (mempool_init_slab_pool(&cifs_io_request_pool, 100, cifs_io_request_cachep) < 0) goto nomem_reqpool; cifs_io_subrequest_cachep = kmem_cache_create(""cifs_io_subrequest"", sizeof(struct cifs_io_subrequest), 0, SLAB_HWCACHE_ALIGN, NULL); if (!cifs_io_subrequest_cachep) goto nomem_subreq; if (mempool_init_slab_pool(&cifs_io_subrequest_pool, 100, cifs_io_subrequest_cachep) < 0) goto nomem_subreqpool; return 0; nomem_subreqpool: kmem_cache_destroy(cifs_io_subrequest_cachep); nomem_subreq: <S2SV_StartVul> mempool_destroy(&cifs_io_request_pool); <S2SV_EndVul> nomem_reqpool: kmem_cache_destroy(cifs_io_request_cachep); nomem_req: return -ENOMEM; }","- mempool_destroy(&cifs_io_request_pool);
+ mempool_exit(&cifs_io_request_pool);","static int cifs_init_netfs(void) { cifs_io_request_cachep = kmem_cache_create(""cifs_io_request"", sizeof(struct cifs_io_request), 0, SLAB_HWCACHE_ALIGN, NULL); if (!cifs_io_request_cachep) goto nomem_req; if (mempool_init_slab_pool(&cifs_io_request_pool, 100, cifs_io_request_cachep) < 0) goto nomem_reqpool; cifs_io_subrequest_cachep = kmem_cache_create(""cifs_io_subrequest"", sizeof(struct cifs_io_subrequest), 0, SLAB_HWCACHE_ALIGN, NULL); if (!cifs_io_subrequest_cachep) goto nomem_subreq; if (mempool_init_slab_pool(&cifs_io_subrequest_pool, 100, cifs_io_subrequest_cachep) < 0) goto nomem_subreqpool; return 0; nomem_subreqpool: kmem_cache_destroy(cifs_io_subrequest_cachep); nomem_subreq: mempool_exit(&cifs_io_request_pool); nomem_reqpool: kmem_cache_destroy(cifs_io_request_cachep); nomem_req: return -ENOMEM; }"
496----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47736/bad/zdata.c----z_erofs_submit_queue,"static void z_erofs_submit_queue(struct z_erofs_decompress_frontend *f, struct z_erofs_decompressqueue *fgq, bool *force_fg, bool readahead) { struct super_block *sb = f->inode->i_sb; struct address_space *mc = MNGD_MAPPING(EROFS_SB(sb)); z_erofs_next_pcluster_t qtail[NR_JOBQUEUES]; struct z_erofs_decompressqueue *q[NR_JOBQUEUES]; z_erofs_next_pcluster_t owned_head = f->owned_head; pgoff_t last_index; struct block_device *last_bdev; unsigned int nr_bios = 0; struct bio *bio = NULL; unsigned long pflags; int memstall = 0; q[JQ_BYPASS] = jobqueue_init(sb, fgq + JQ_BYPASS, NULL); q[JQ_SUBMIT] = jobqueue_init(sb, fgq + JQ_SUBMIT, force_fg); qtail[JQ_BYPASS] = &q[JQ_BYPASS]->head; qtail[JQ_SUBMIT] = &q[JQ_SUBMIT]->head; q[JQ_SUBMIT]->head = owned_head; do { struct erofs_map_dev mdev; struct z_erofs_pcluster *pcl; pgoff_t cur, end; unsigned int i = 0; bool bypass = true; DBG_BUGON(owned_head == Z_EROFS_PCLUSTER_NIL); pcl = container_of(owned_head, struct z_erofs_pcluster, next); owned_head = READ_ONCE(pcl->next); if (z_erofs_is_inline_pcluster(pcl)) { move_to_bypass_jobqueue(pcl, qtail, owned_head); continue; } mdev = (struct erofs_map_dev) { .m_pa = erofs_pos(sb, pcl->obj.index), }; (void)erofs_map_dev(sb, &mdev); cur = erofs_blknr(sb, mdev.m_pa); end = cur + pcl->pclusterpages; do { <S2SV_StartVul> struct page *page; <S2SV_EndVul> <S2SV_StartVul> page = pickup_page_for_submission(pcl, i++, <S2SV_EndVul> <S2SV_StartVul> &f->pagepool, mc); <S2SV_EndVul> <S2SV_StartVul> if (!page) <S2SV_EndVul> <S2SV_StartVul> continue; <S2SV_EndVul> if (bio && (cur != last_index + 1 || last_bdev != mdev.m_bdev)) { <S2SV_StartVul> submit_bio_retry: <S2SV_EndVul> submit_bio(bio); if (memstall) { psi_memstall_leave(&pflags); memstall = 0; } bio = NULL; } if (unlikely(PageWorkingset(page)) && !memstall) { psi_memstall_enter(&pflags); memstall = 1; } if (!bio) { bio = bio_alloc(mdev.m_bdev, BIO_MAX_VECS, REQ_OP_READ, GFP_NOIO); bio->bi_end_io = z_erofs_decompressqueue_endio; last_bdev = mdev.m_bdev; bio->bi_iter.bi_sector = (sector_t)cur << (sb->s_blocksize_bits - 9); bio->bi_private = q[JQ_SUBMIT]; if (readahead) bio->bi_opf |= REQ_RAHEAD; ++nr_bios; } if (bio_add_page(bio, page, PAGE_SIZE, 0) < PAGE_SIZE) <S2SV_StartVul> goto submit_bio_retry; <S2SV_EndVul> last_index = cur; bypass = false; } while (++cur < end); if (!bypass) qtail[JQ_SUBMIT] = &pcl->next; else move_to_bypass_jobqueue(pcl, qtail, owned_head); } while (owned_head != Z_EROFS_PCLUSTER_TAIL); if (bio) { submit_bio(bio); if (memstall) psi_memstall_leave(&pflags); } if (!*force_fg && !nr_bios) { kvfree(q[JQ_SUBMIT]); return; } z_erofs_decompress_kickoff(q[JQ_SUBMIT], nr_bios); }","- struct page *page;
- page = pickup_page_for_submission(pcl, i++,
- &f->pagepool, mc);
- if (!page)
- continue;
- submit_bio_retry:
- goto submit_bio_retry;
+ struct page *page = NULL;
+ drain_io:
+ }
+ if (!page) {
+ page = pickup_page_for_submission(pcl, i++,
+ &f->pagepool, mc);
+ if (!page)
+ continue;
+ }
+ goto drain_io;","static void z_erofs_submit_queue(struct z_erofs_decompress_frontend *f, struct z_erofs_decompressqueue *fgq, bool *force_fg, bool readahead) { struct super_block *sb = f->inode->i_sb; struct address_space *mc = MNGD_MAPPING(EROFS_SB(sb)); z_erofs_next_pcluster_t qtail[NR_JOBQUEUES]; struct z_erofs_decompressqueue *q[NR_JOBQUEUES]; z_erofs_next_pcluster_t owned_head = f->owned_head; pgoff_t last_index; struct block_device *last_bdev; unsigned int nr_bios = 0; struct bio *bio = NULL; unsigned long pflags; int memstall = 0; q[JQ_BYPASS] = jobqueue_init(sb, fgq + JQ_BYPASS, NULL); q[JQ_SUBMIT] = jobqueue_init(sb, fgq + JQ_SUBMIT, force_fg); qtail[JQ_BYPASS] = &q[JQ_BYPASS]->head; qtail[JQ_SUBMIT] = &q[JQ_SUBMIT]->head; q[JQ_SUBMIT]->head = owned_head; do { struct erofs_map_dev mdev; struct z_erofs_pcluster *pcl; pgoff_t cur, end; unsigned int i = 0; bool bypass = true; DBG_BUGON(owned_head == Z_EROFS_PCLUSTER_NIL); pcl = container_of(owned_head, struct z_erofs_pcluster, next); owned_head = READ_ONCE(pcl->next); if (z_erofs_is_inline_pcluster(pcl)) { move_to_bypass_jobqueue(pcl, qtail, owned_head); continue; } mdev = (struct erofs_map_dev) { .m_pa = erofs_pos(sb, pcl->obj.index), }; (void)erofs_map_dev(sb, &mdev); cur = erofs_blknr(sb, mdev.m_pa); end = cur + pcl->pclusterpages; do { struct page *page = NULL; if (bio && (cur != last_index + 1 || last_bdev != mdev.m_bdev)) { drain_io: submit_bio(bio); if (memstall) { psi_memstall_leave(&pflags); memstall = 0; } bio = NULL; } if (!page) { page = pickup_page_for_submission(pcl, i++, &f->pagepool, mc); if (!page) continue; } if (unlikely(PageWorkingset(page)) && !memstall) { psi_memstall_enter(&pflags); memstall = 1; } if (!bio) { bio = bio_alloc(mdev.m_bdev, BIO_MAX_VECS, REQ_OP_READ, GFP_NOIO); bio->bi_end_io = z_erofs_decompressqueue_endio; last_bdev = mdev.m_bdev; bio->bi_iter.bi_sector = (sector_t)cur << (sb->s_blocksize_bits - 9); bio->bi_private = q[JQ_SUBMIT]; if (readahead) bio->bi_opf |= REQ_RAHEAD; ++nr_bios; } if (bio_add_page(bio, page, PAGE_SIZE, 0) < PAGE_SIZE) goto drain_io; last_index = cur; bypass = false; } while (++cur < end); if (!bypass) qtail[JQ_SUBMIT] = &pcl->next; else move_to_bypass_jobqueue(pcl, qtail, owned_head); } while (owned_head != Z_EROFS_PCLUSTER_TAIL); if (bio) { submit_bio(bio); if (memstall) psi_memstall_leave(&pflags); } if (!*force_fg && !nr_bios) { kvfree(q[JQ_SUBMIT]); return; } z_erofs_decompress_kickoff(q[JQ_SUBMIT], nr_bios); }"
640----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49942/bad/xe_bo.c----xe_bo_move,"static int xe_bo_move(struct ttm_buffer_object *ttm_bo, bool evict, struct ttm_operation_ctx *ctx, struct ttm_resource *new_mem, struct ttm_place *hop) { struct xe_device *xe = ttm_to_xe_device(ttm_bo->bdev); struct xe_bo *bo = ttm_to_xe_bo(ttm_bo); struct ttm_resource *old_mem = ttm_bo->resource; u32 old_mem_type = old_mem ? old_mem->mem_type : XE_PL_SYSTEM; struct ttm_tt *ttm = ttm_bo->ttm; struct xe_migrate *migrate = NULL; struct dma_fence *fence; bool move_lacks_source; bool tt_has_data; bool needs_clear; bool handle_system_ccs = (!IS_DGFX(xe) && xe_bo_needs_ccs_pages(bo) && ttm && ttm_tt_is_populated(ttm)) ? true : false; int ret = 0; if ((!old_mem && ttm) && !handle_system_ccs) { if (new_mem->mem_type == XE_PL_TT) ret = xe_tt_map_sg(ttm); if (!ret) ttm_bo_move_null(ttm_bo, new_mem); goto out; } if (ttm_bo->type == ttm_bo_type_sg) { ret = xe_bo_move_notify(bo, ctx); if (!ret) ret = xe_bo_move_dmabuf(ttm_bo, new_mem); return ret; } tt_has_data = ttm && (ttm_tt_is_populated(ttm) || (ttm->page_flags & TTM_TT_FLAG_SWAPPED)); move_lacks_source = handle_system_ccs ? (!bo->ccs_cleared) : (!mem_type_is_vram(old_mem_type) && !tt_has_data); needs_clear = (ttm && ttm->page_flags & TTM_TT_FLAG_ZERO_ALLOC) || (!ttm && ttm_bo->type == ttm_bo_type_device); if (new_mem->mem_type == XE_PL_TT) { ret = xe_tt_map_sg(ttm); if (ret) goto out; } if ((move_lacks_source && !needs_clear)) { ttm_bo_move_null(ttm_bo, new_mem); goto out; } if (old_mem_type == XE_PL_SYSTEM && new_mem->mem_type == XE_PL_TT && !handle_system_ccs) { ttm_bo_move_null(ttm_bo, new_mem); goto out; } if (old_mem_type == XE_PL_TT && new_mem->mem_type == XE_PL_TT) { ttm_bo_move_null(ttm_bo, new_mem); goto out; } if (!move_lacks_source && !xe_bo_is_pinned(bo)) { ret = xe_bo_move_notify(bo, ctx); if (ret) goto out; } if (old_mem_type == XE_PL_TT && new_mem->mem_type == XE_PL_SYSTEM) { long timeout = dma_resv_wait_timeout(ttm_bo->base.resv, DMA_RESV_USAGE_BOOKKEEP, true, MAX_SCHEDULE_TIMEOUT); if (timeout < 0) { ret = timeout; goto out; } if (!handle_system_ccs) { ttm_bo_move_null(ttm_bo, new_mem); goto out; } } if (!move_lacks_source && ((old_mem_type == XE_PL_SYSTEM && resource_is_vram(new_mem)) || (mem_type_is_vram(old_mem_type) && new_mem->mem_type == XE_PL_SYSTEM))) { hop->fpfn = 0; hop->lpfn = 0; hop->mem_type = XE_PL_TT; hop->flags = TTM_PL_FLAG_TEMPORARY; ret = -EMULTIHOP; goto out; } if (bo->tile) migrate = bo->tile->migrate; else if (resource_is_vram(new_mem)) migrate = mem_type_to_migrate(xe, new_mem->mem_type); else if (mem_type_is_vram(old_mem_type)) migrate = mem_type_to_migrate(xe, old_mem_type); else migrate = xe->tiles[0].migrate; xe_assert(xe, migrate); trace_xe_bo_move(bo, new_mem->mem_type, old_mem_type, move_lacks_source); xe_pm_runtime_get_noresume(xe); if (xe_bo_is_pinned(bo) && !xe_bo_is_user(bo)) { ret = xe_bo_vmap(bo); if (!ret) { <S2SV_StartVul> ret = ttm_bo_move_memcpy(ttm_bo, ctx, new_mem); <S2SV_EndVul> if (!ret && resource_is_vram(new_mem)) { struct xe_mem_region *vram = res_to_mem_region(new_mem); void __iomem *new_addr = vram->mapping + (new_mem->start << PAGE_SHIFT); if (XE_WARN_ON(new_mem->start == XE_BO_INVALID_OFFSET)) { ret = -EINVAL; xe_pm_runtime_put(xe); goto out; } xe_assert(xe, new_mem->start == bo->placements->fpfn); iosys_map_set_vaddr_iomem(&bo->vmap, new_addr); } } } else { if (move_lacks_source) fence = xe_migrate_clear(migrate, bo, new_mem); else fence = xe_migrate_copy(migrate, bo, bo, old_mem, new_mem, handle_system_ccs); if (IS_ERR(fence)) { ret = PTR_ERR(fence); xe_pm_runtime_put(xe); goto out; } if (!move_lacks_source) { ret = ttm_bo_move_accel_cleanup(ttm_bo, fence, evict, true, new_mem); if (ret) { dma_fence_wait(fence, false); ttm_bo_move_null(ttm_bo, new_mem); ret = 0; } } else { dma_resv_add_fence(ttm_bo->base.resv, fence, DMA_RESV_USAGE_KERNEL); ttm_bo_move_null(ttm_bo, new_mem); } dma_fence_put(fence); } xe_pm_runtime_put(xe); out: if ((!ttm_bo->resource || ttm_bo->resource->mem_type == XE_PL_SYSTEM) && ttm_bo->ttm) xe_tt_unmap_sg(ttm_bo->ttm); return ret; }","- ret = ttm_bo_move_memcpy(ttm_bo, ctx, new_mem);
+ ret = ttm_bo_move_memcpy(ttm_bo, ctx, new_mem);","static int xe_bo_move(struct ttm_buffer_object *ttm_bo, bool evict, struct ttm_operation_ctx *ctx, struct ttm_resource *new_mem, struct ttm_place *hop) { struct xe_device *xe = ttm_to_xe_device(ttm_bo->bdev); struct xe_bo *bo = ttm_to_xe_bo(ttm_bo); struct ttm_resource *old_mem = ttm_bo->resource; u32 old_mem_type = old_mem ? old_mem->mem_type : XE_PL_SYSTEM; struct ttm_tt *ttm = ttm_bo->ttm; struct xe_migrate *migrate = NULL; struct dma_fence *fence; bool move_lacks_source; bool tt_has_data; bool needs_clear; bool handle_system_ccs = (!IS_DGFX(xe) && xe_bo_needs_ccs_pages(bo) && ttm && ttm_tt_is_populated(ttm)) ? true : false; int ret = 0; if ((!old_mem && ttm) && !handle_system_ccs) { if (new_mem->mem_type == XE_PL_TT) ret = xe_tt_map_sg(ttm); if (!ret) ttm_bo_move_null(ttm_bo, new_mem); goto out; } if (ttm_bo->type == ttm_bo_type_sg) { ret = xe_bo_move_notify(bo, ctx); if (!ret) ret = xe_bo_move_dmabuf(ttm_bo, new_mem); return ret; } tt_has_data = ttm && (ttm_tt_is_populated(ttm) || (ttm->page_flags & TTM_TT_FLAG_SWAPPED)); move_lacks_source = !old_mem || (handle_system_ccs ? (!bo->ccs_cleared) : (!mem_type_is_vram(old_mem_type) && !tt_has_data)); needs_clear = (ttm && ttm->page_flags & TTM_TT_FLAG_ZERO_ALLOC) || (!ttm && ttm_bo->type == ttm_bo_type_device); if (new_mem->mem_type == XE_PL_TT) { ret = xe_tt_map_sg(ttm); if (ret) goto out; } if ((move_lacks_source && !needs_clear)) { ttm_bo_move_null(ttm_bo, new_mem); goto out; } if (old_mem_type == XE_PL_SYSTEM && new_mem->mem_type == XE_PL_TT && !handle_system_ccs) { ttm_bo_move_null(ttm_bo, new_mem); goto out; } if (old_mem_type == XE_PL_TT && new_mem->mem_type == XE_PL_TT) { ttm_bo_move_null(ttm_bo, new_mem); goto out; } if (!move_lacks_source && !xe_bo_is_pinned(bo)) { ret = xe_bo_move_notify(bo, ctx); if (ret) goto out; } if (old_mem_type == XE_PL_TT && new_mem->mem_type == XE_PL_SYSTEM) { long timeout = dma_resv_wait_timeout(ttm_bo->base.resv, DMA_RESV_USAGE_BOOKKEEP, true, MAX_SCHEDULE_TIMEOUT); if (timeout < 0) { ret = timeout; goto out; } if (!handle_system_ccs) { ttm_bo_move_null(ttm_bo, new_mem); goto out; } } if (!move_lacks_source && ((old_mem_type == XE_PL_SYSTEM && resource_is_vram(new_mem)) || (mem_type_is_vram(old_mem_type) && new_mem->mem_type == XE_PL_SYSTEM))) { hop->fpfn = 0; hop->lpfn = 0; hop->mem_type = XE_PL_TT; hop->flags = TTM_PL_FLAG_TEMPORARY; ret = -EMULTIHOP; goto out; } if (bo->tile) migrate = bo->tile->migrate; else if (resource_is_vram(new_mem)) migrate = mem_type_to_migrate(xe, new_mem->mem_type); else if (mem_type_is_vram(old_mem_type)) migrate = mem_type_to_migrate(xe, old_mem_type); else migrate = xe->tiles[0].migrate; xe_assert(xe, migrate); trace_xe_bo_move(bo, new_mem->mem_type, old_mem_type, move_lacks_source); xe_pm_runtime_get_noresume(xe); if (xe_bo_is_pinned(bo) && !xe_bo_is_user(bo)) { ret = xe_bo_vmap(bo); if (!ret) { ret = ttm_bo_move_memcpy(ttm_bo, ctx, new_mem); if (!ret && resource_is_vram(new_mem)) { struct xe_mem_region *vram = res_to_mem_region(new_mem); void __iomem *new_addr = vram->mapping + (new_mem->start << PAGE_SHIFT); if (XE_WARN_ON(new_mem->start == XE_BO_INVALID_OFFSET)) { ret = -EINVAL; xe_pm_runtime_put(xe); goto out; } xe_assert(xe, new_mem->start == bo->placements->fpfn); iosys_map_set_vaddr_iomem(&bo->vmap, new_addr); } } } else { if (move_lacks_source) fence = xe_migrate_clear(migrate, bo, new_mem); else fence = xe_migrate_copy(migrate, bo, bo, old_mem, new_mem, handle_system_ccs); if (IS_ERR(fence)) { ret = PTR_ERR(fence); xe_pm_runtime_put(xe); goto out; } if (!move_lacks_source) { ret = ttm_bo_move_accel_cleanup(ttm_bo, fence, evict, true, new_mem); if (ret) { dma_fence_wait(fence, false); ttm_bo_move_null(ttm_bo, new_mem); ret = 0; } } else { dma_resv_add_fence(ttm_bo->base.resv, fence, DMA_RESV_USAGE_KERNEL); ttm_bo_move_null(ttm_bo, new_mem); } dma_fence_put(fence); } xe_pm_runtime_put(xe); out: if ((!ttm_bo->resource || ttm_bo->resource->mem_type == XE_PL_SYSTEM) && ttm_bo->ttm) xe_tt_unmap_sg(ttm_bo->ttm); return ret; }"
1126----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53097/bad/slab_common.c----__do_krealloc,"__do_krealloc(const void *p, size_t new_size, gfp_t flags) { void *ret; size_t ks; if (likely(!ZERO_OR_NULL_PTR(p))) { if (!kasan_check_byte(p)) return NULL; ks = kfence_ksize(p) ?: __ksize(p); } else ks = 0; if (ks >= new_size) { if (want_init_on_alloc(flags)) { kasan_disable_current(); <S2SV_StartVul> memset((void *)p + new_size, 0, ks - new_size); <S2SV_EndVul> kasan_enable_current(); } p = kasan_krealloc((void *)p, new_size, flags); return (void *)p; } ret = kmalloc_track_caller(new_size, flags); if (ret && p) { kasan_disable_current(); memcpy(ret, kasan_reset_tag(p), ks); kasan_enable_current(); } return ret; }","- memset((void *)p + new_size, 0, ks - new_size);
+ memset(kasan_reset_tag(p) + new_size, 0, ks - new_size);","__do_krealloc(const void *p, size_t new_size, gfp_t flags) { void *ret; size_t ks; if (likely(!ZERO_OR_NULL_PTR(p))) { if (!kasan_check_byte(p)) return NULL; ks = kfence_ksize(p) ?: __ksize(p); } else ks = 0; if (ks >= new_size) { if (want_init_on_alloc(flags)) { kasan_disable_current(); memset(kasan_reset_tag(p) + new_size, 0, ks - new_size); kasan_enable_current(); } p = kasan_krealloc((void *)p, new_size, flags); return (void *)p; } ret = kmalloc_track_caller(new_size, flags); if (ret && p) { kasan_disable_current(); memcpy(ret, kasan_reset_tag(p), ks); kasan_enable_current(); } return ret; }"
584----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49905/bad/amdgpu_dm_plane.c----amdgpu_dm_plane_handle_cursor_update,"void amdgpu_dm_plane_handle_cursor_update(struct drm_plane *plane, struct drm_plane_state *old_plane_state) { struct amdgpu_device *adev = drm_to_adev(plane->dev); struct amdgpu_framebuffer *afb = to_amdgpu_framebuffer(plane->state->fb); struct drm_crtc *crtc = afb ? plane->state->crtc : old_plane_state->crtc; struct dm_crtc_state *crtc_state = crtc ? to_dm_crtc_state(crtc->state) : NULL; struct amdgpu_crtc *amdgpu_crtc = to_amdgpu_crtc(crtc); uint64_t address = afb ? afb->address : 0; struct dc_cursor_position position = {0}; struct dc_cursor_attributes attributes; int ret; if (!plane->state->fb && !old_plane_state->fb) return; drm_dbg_atomic(plane->dev, ""crtc_id=%d with size %d to %d\n"", amdgpu_crtc->crtc_id, plane->state->crtc_w, plane->state->crtc_h); ret = amdgpu_dm_plane_get_cursor_position(plane, crtc, &position); if (ret) return; if (!position.enable) { if (crtc_state && crtc_state->stream) { mutex_lock(&adev->dm.dc_lock); dc_stream_program_cursor_position(crtc_state->stream, &position); mutex_unlock(&adev->dm.dc_lock); } return; } amdgpu_crtc->cursor_width = plane->state->crtc_w; amdgpu_crtc->cursor_height = plane->state->crtc_h; memset(&attributes, 0, sizeof(attributes)); attributes.address.high_part = upper_32_bits(address); attributes.address.low_part = lower_32_bits(address); attributes.width = plane->state->crtc_w; attributes.height = plane->state->crtc_h; attributes.color_format = CURSOR_MODE_COLOR_PRE_MULTIPLIED_ALPHA; attributes.rotation_angle = 0; attributes.attribute_flags.value = 0; if (crtc_state->cm_is_degamma_srgb && adev->dm.dc->caps.color.dpp.gamma_corr) attributes.attribute_flags.bits.ENABLE_CURSOR_DEGAMMA = 1; <S2SV_StartVul> attributes.pitch = afb->base.pitches[0] / afb->base.format->cpp[0]; <S2SV_EndVul> if (crtc_state->stream) { mutex_lock(&adev->dm.dc_lock); if (!dc_stream_program_cursor_attributes(crtc_state->stream, &attributes)) DRM_ERROR(""DC failed to set cursor attributes\n""); if (!dc_stream_program_cursor_position(crtc_state->stream, &position)) DRM_ERROR(""DC failed to set cursor position\n""); mutex_unlock(&adev->dm.dc_lock); } }","- attributes.pitch = afb->base.pitches[0] / afb->base.format->cpp[0];
+ if (afb)
+ attributes.pitch = afb->base.pitches[0] / afb->base.format->cpp[0];","void amdgpu_dm_plane_handle_cursor_update(struct drm_plane *plane, struct drm_plane_state *old_plane_state) { struct amdgpu_device *adev = drm_to_adev(plane->dev); struct amdgpu_framebuffer *afb = to_amdgpu_framebuffer(plane->state->fb); struct drm_crtc *crtc = afb ? plane->state->crtc : old_plane_state->crtc; struct dm_crtc_state *crtc_state = crtc ? to_dm_crtc_state(crtc->state) : NULL; struct amdgpu_crtc *amdgpu_crtc = to_amdgpu_crtc(crtc); uint64_t address = afb ? afb->address : 0; struct dc_cursor_position position = {0}; struct dc_cursor_attributes attributes; int ret; if (!plane->state->fb && !old_plane_state->fb) return; drm_dbg_atomic(plane->dev, ""crtc_id=%d with size %d to %d\n"", amdgpu_crtc->crtc_id, plane->state->crtc_w, plane->state->crtc_h); ret = amdgpu_dm_plane_get_cursor_position(plane, crtc, &position); if (ret) return; if (!position.enable) { if (crtc_state && crtc_state->stream) { mutex_lock(&adev->dm.dc_lock); dc_stream_program_cursor_position(crtc_state->stream, &position); mutex_unlock(&adev->dm.dc_lock); } return; } amdgpu_crtc->cursor_width = plane->state->crtc_w; amdgpu_crtc->cursor_height = plane->state->crtc_h; memset(&attributes, 0, sizeof(attributes)); attributes.address.high_part = upper_32_bits(address); attributes.address.low_part = lower_32_bits(address); attributes.width = plane->state->crtc_w; attributes.height = plane->state->crtc_h; attributes.color_format = CURSOR_MODE_COLOR_PRE_MULTIPLIED_ALPHA; attributes.rotation_angle = 0; attributes.attribute_flags.value = 0; if (crtc_state->cm_is_degamma_srgb && adev->dm.dc->caps.color.dpp.gamma_corr) attributes.attribute_flags.bits.ENABLE_CURSOR_DEGAMMA = 1; if (afb) attributes.pitch = afb->base.pitches[0] / afb->base.format->cpp[0]; if (crtc_state->stream) { mutex_lock(&adev->dm.dc_lock); if (!dc_stream_program_cursor_attributes(crtc_state->stream, &attributes)) DRM_ERROR(""DC failed to set cursor attributes\n""); if (!dc_stream_program_cursor_position(crtc_state->stream, &position)) DRM_ERROR(""DC failed to set cursor position\n""); mutex_unlock(&adev->dm.dc_lock); } }"
620----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49925/bad/efifb.c----efifb_remove,"static void efifb_remove(struct platform_device *pdev) { struct fb_info *info = platform_get_drvdata(pdev); unregister_framebuffer(info); <S2SV_StartVul> sysfs_remove_groups(&pdev->dev.kobj, efifb_groups); <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul>","- sysfs_remove_groups(&pdev->dev.kobj, efifb_groups);
- }",static void efifb_remove(struct platform_device *pdev) { struct fb_info *info = platform_get_drvdata(pdev); unregister_framebuffer(info); }
812----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50086/bad/smb2pdu.c----smb2_check_user_session,"int smb2_check_user_session(struct ksmbd_work *work) { struct smb2_hdr *req_hdr = ksmbd_req_buf_next(work); struct ksmbd_conn *conn = work->conn; unsigned int cmd = le16_to_cpu(req_hdr->Command); unsigned long long sess_id; if (cmd == SMB2_ECHO_HE || cmd == SMB2_NEGOTIATE_HE || cmd == SMB2_SESSION_SETUP_HE) return 0; if (!ksmbd_conn_good(conn)) return -EIO; sess_id = le64_to_cpu(req_hdr->SessionId); if (work->next_smb2_rcv_hdr_off) { if (!work->sess) { pr_err(""The first operation in the compound does not have sess\n""); return -EINVAL; } if (sess_id != ULLONG_MAX && work->sess->id != sess_id) { pr_err(""session id(%llu) is different with the first operation(%lld)\n"", sess_id, work->sess->id); return -EINVAL; } return 1; } work->sess = ksmbd_session_lookup_all(conn, sess_id); <S2SV_StartVul> if (work->sess) <S2SV_EndVul> return 1; ksmbd_debug(SMB, ""Invalid user session, Uid %llu\n"", sess_id); return -ENOENT; }","- if (work->sess)
+ if (work->sess) {
+ ksmbd_user_session_get(work->sess);
+ }
+ }","int smb2_check_user_session(struct ksmbd_work *work) { struct smb2_hdr *req_hdr = ksmbd_req_buf_next(work); struct ksmbd_conn *conn = work->conn; unsigned int cmd = le16_to_cpu(req_hdr->Command); unsigned long long sess_id; if (cmd == SMB2_ECHO_HE || cmd == SMB2_NEGOTIATE_HE || cmd == SMB2_SESSION_SETUP_HE) return 0; if (!ksmbd_conn_good(conn)) return -EIO; sess_id = le64_to_cpu(req_hdr->SessionId); if (work->next_smb2_rcv_hdr_off) { if (!work->sess) { pr_err(""The first operation in the compound does not have sess\n""); return -EINVAL; } if (sess_id != ULLONG_MAX && work->sess->id != sess_id) { pr_err(""session id(%llu) is different with the first operation(%lld)\n"", sess_id, work->sess->id); return -EINVAL; } return 1; } work->sess = ksmbd_session_lookup_all(conn, sess_id); if (work->sess) { ksmbd_user_session_get(work->sess); return 1; } ksmbd_debug(SMB, ""Invalid user session, Uid %llu\n"", sess_id); return -ENOENT; }"
358----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46820/bad/vcn_v5_0_0.c----vcn_v5_0_0_hw_fini,"static int vcn_v5_0_0_hw_fini(void *handle) { struct amdgpu_device *adev = (struct amdgpu_device *)handle; int i; cancel_delayed_work_sync(&adev->vcn.idle_work); for (i = 0; i < adev->vcn.num_vcn_inst; ++i) { if (adev->vcn.harvest_config & (1 << i)) continue; <S2SV_StartVul> amdgpu_irq_put(adev, &adev->vcn.inst[i].irq, 0); <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> return 0; <S2SV_EndVul> }","- amdgpu_irq_put(adev, &adev->vcn.inst[i].irq, 0);
- }
- return 0;",static int vcn_v5_0_0_hw_fini(void *handle) { struct amdgpu_device *adev = (struct amdgpu_device *)handle; int i; cancel_delayed_work_sync(&adev->vcn.idle_work); for (i = 0; i < adev->vcn.num_vcn_inst; ++i) { if (adev->vcn.harvest_config & (1 << i)) continue; } return 0; }
61----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44943/bad/gup.c----__gup_device_huge,"static int __gup_device_huge(unsigned long pfn, unsigned long addr, unsigned long end, unsigned int flags, struct page **pages, int *nr) { int nr_start = *nr; struct dev_pagemap *pgmap = NULL; do { struct page *page = pfn_to_page(pfn); pgmap = get_dev_pagemap(pfn, pgmap); if (unlikely(!pgmap)) { undo_dev_pagemap(nr, nr_start, flags, pages); break; } if (!(flags & FOLL_PCI_P2PDMA) && is_pci_p2pdma_page(page)) { undo_dev_pagemap(nr, nr_start, flags, pages); break; } SetPageReferenced(page); pages[*nr] = page; <S2SV_StartVul> if (unlikely(try_grab_page(page, flags))) { <S2SV_EndVul> undo_dev_pagemap(nr, nr_start, flags, pages); break; <S2SV_StartVul> } <S2SV_EndVul> (*nr)++; pfn++; } while (addr += PAGE_SIZE, addr != end); put_dev_pagemap(pgmap); return addr == end; }","- if (unlikely(try_grab_page(page, flags))) {
- }
+ if (unlikely(try_grab_folio(page_folio(page), 1, flags))) {
+ }","static int __gup_device_huge(unsigned long pfn, unsigned long addr, unsigned long end, unsigned int flags, struct page **pages, int *nr) { int nr_start = *nr; struct dev_pagemap *pgmap = NULL; do { struct page *page = pfn_to_page(pfn); pgmap = get_dev_pagemap(pfn, pgmap); if (unlikely(!pgmap)) { undo_dev_pagemap(nr, nr_start, flags, pages); break; } if (!(flags & FOLL_PCI_P2PDMA) && is_pci_p2pdma_page(page)) { undo_dev_pagemap(nr, nr_start, flags, pages); break; } SetPageReferenced(page); pages[*nr] = page; if (unlikely(try_grab_folio(page_folio(page), 1, flags))) { undo_dev_pagemap(nr, nr_start, flags, pages); break; } (*nr)++; pfn++; } while (addr += PAGE_SIZE, addr != end); put_dev_pagemap(pgmap); return addr == end; }"
10----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-43891/bad/trace_events_hist.c----hist_show,"static int hist_show(struct seq_file *m, void *v) { struct event_trigger_data *data; struct trace_event_file *event_file; int n = 0, ret = 0; mutex_lock(&event_mutex); <S2SV_StartVul> event_file = event_file_data(m->private); <S2SV_EndVul> if (unlikely(!event_file)) { ret = -ENODEV; goto out_unlock; } list_for_each_entry(data, &event_file->triggers, list) { if (data->cmd_ops->trigger_type == ETT_EVENT_HIST) hist_trigger_show(m, data, n++); } out_unlock: mutex_unlock(&event_mutex); return ret; }","- event_file = event_file_data(m->private);
+ event_file = event_file_file(m->private);","static int hist_show(struct seq_file *m, void *v) { struct event_trigger_data *data; struct trace_event_file *event_file; int n = 0, ret = 0; mutex_lock(&event_mutex); event_file = event_file_file(m->private); if (unlikely(!event_file)) { ret = -ENODEV; goto out_unlock; } list_for_each_entry(data, &event_file->triggers, list) { if (data->cmd_ops->trigger_type == ETT_EVENT_HIST) hist_trigger_show(m, data, n++); } out_unlock: mutex_unlock(&event_mutex); return ret; }"
1463----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-57807/bad/megaraid_sas_base.c----megasas_aen_polling,"megasas_aen_polling(struct work_struct *work) { struct megasas_aen_event *ev = container_of(work, struct megasas_aen_event, hotplug_work.work); struct megasas_instance *instance = ev->instance; union megasas_evt_class_locale class_locale; int event_type = 0; u32 seq_num; u16 ld_target_id; int error; u8 dcmd_ret = DCMD_SUCCESS; struct scsi_device *sdev1; if (!instance) { printk(KERN_ERR ""invalid instance!\n""); kfree(ev); return; } mutex_lock(&instance->reset_mutex); instance->ev = NULL; if (instance->evt_detail) { megasas_decode_evt(instance); switch (le32_to_cpu(instance->evt_detail->code)) { case MR_EVT_PD_INSERTED: case MR_EVT_PD_REMOVED: event_type = SCAN_PD_CHANNEL; break; case MR_EVT_LD_OFFLINE: case MR_EVT_LD_DELETED: ld_target_id = instance->evt_detail->args.ld.target_id; sdev1 = scsi_device_lookup(instance->host, MEGASAS_MAX_PD_CHANNELS + (ld_target_id / MEGASAS_MAX_DEV_PER_CHANNEL), (ld_target_id % MEGASAS_MAX_DEV_PER_CHANNEL), 0); <S2SV_StartVul> if (sdev1) <S2SV_EndVul> megasas_remove_scsi_device(sdev1); event_type = SCAN_VD_CHANNEL; break; case MR_EVT_LD_CREATED: event_type = SCAN_VD_CHANNEL; break; case MR_EVT_CFG_CLEARED: case MR_EVT_CTRL_HOST_BUS_SCAN_REQUESTED: case MR_EVT_FOREIGN_CFG_IMPORTED: case MR_EVT_LD_STATE_CHANGE: event_type = SCAN_PD_CHANNEL | SCAN_VD_CHANNEL; dev_info(&instance->pdev->dev, ""scanning for scsi%d...\n"", instance->host->host_no); break; case MR_EVT_CTRL_PROP_CHANGED: dcmd_ret = megasas_get_ctrl_info(instance); if (dcmd_ret == DCMD_SUCCESS && instance->snapdump_wait_time) { megasas_get_snapdump_properties(instance); dev_info(&instance->pdev->dev, ""Snap dump wait time\t: %d\n"", instance->snapdump_wait_time); } break; default: event_type = 0; break; } } else { dev_err(&instance->pdev->dev, ""invalid evt_detail!\n""); mutex_unlock(&instance->reset_mutex); kfree(ev); return; } if (event_type) dcmd_ret = megasas_update_device_list(instance, event_type); mutex_unlock(&instance->reset_mutex); if (event_type && dcmd_ret == DCMD_SUCCESS) megasas_add_remove_devices(instance, event_type); if (dcmd_ret == DCMD_SUCCESS) seq_num = le32_to_cpu(instance->evt_detail->seq_num) + 1; else seq_num = instance->last_seq_num; class_locale.members.reserved = 0; class_locale.members.locale = MR_EVT_LOCALE_ALL; class_locale.members.class = MR_EVT_CLASS_DEBUG; if (instance->aen_cmd != NULL) { kfree(ev); return; } mutex_lock(&instance->reset_mutex); error = megasas_register_aen(instance, seq_num, class_locale.word); if (error) dev_err(&instance->pdev->dev, ""register aen failed error %x\n"", error); mutex_unlock(&instance->reset_mutex); kfree(ev); }","- if (sdev1)
+ if (sdev1) {
+ mutex_unlock(&instance->reset_mutex);
+ mutex_lock(&instance->reset_mutex);
+ }","megasas_aen_polling(struct work_struct *work) { struct megasas_aen_event *ev = container_of(work, struct megasas_aen_event, hotplug_work.work); struct megasas_instance *instance = ev->instance; union megasas_evt_class_locale class_locale; int event_type = 0; u32 seq_num; u16 ld_target_id; int error; u8 dcmd_ret = DCMD_SUCCESS; struct scsi_device *sdev1; if (!instance) { printk(KERN_ERR ""invalid instance!\n""); kfree(ev); return; } mutex_lock(&instance->reset_mutex); instance->ev = NULL; if (instance->evt_detail) { megasas_decode_evt(instance); switch (le32_to_cpu(instance->evt_detail->code)) { case MR_EVT_PD_INSERTED: case MR_EVT_PD_REMOVED: event_type = SCAN_PD_CHANNEL; break; case MR_EVT_LD_OFFLINE: case MR_EVT_LD_DELETED: ld_target_id = instance->evt_detail->args.ld.target_id; sdev1 = scsi_device_lookup(instance->host, MEGASAS_MAX_PD_CHANNELS + (ld_target_id / MEGASAS_MAX_DEV_PER_CHANNEL), (ld_target_id % MEGASAS_MAX_DEV_PER_CHANNEL), 0); if (sdev1) { mutex_unlock(&instance->reset_mutex); megasas_remove_scsi_device(sdev1); mutex_lock(&instance->reset_mutex); } event_type = SCAN_VD_CHANNEL; break; case MR_EVT_LD_CREATED: event_type = SCAN_VD_CHANNEL; break; case MR_EVT_CFG_CLEARED: case MR_EVT_CTRL_HOST_BUS_SCAN_REQUESTED: case MR_EVT_FOREIGN_CFG_IMPORTED: case MR_EVT_LD_STATE_CHANGE: event_type = SCAN_PD_CHANNEL | SCAN_VD_CHANNEL; dev_info(&instance->pdev->dev, ""scanning for scsi%d...\n"", instance->host->host_no); break; case MR_EVT_CTRL_PROP_CHANGED: dcmd_ret = megasas_get_ctrl_info(instance); if (dcmd_ret == DCMD_SUCCESS && instance->snapdump_wait_time) { megasas_get_snapdump_properties(instance); dev_info(&instance->pdev->dev, ""Snap dump wait time\t: %d\n"", instance->snapdump_wait_time); } break; default: event_type = 0; break; } } else { dev_err(&instance->pdev->dev, ""invalid evt_detail!\n""); mutex_unlock(&instance->reset_mutex); kfree(ev); return; } if (event_type) dcmd_ret = megasas_update_device_list(instance, event_type); mutex_unlock(&instance->reset_mutex); if (event_type && dcmd_ret == DCMD_SUCCESS) megasas_add_remove_devices(instance, event_type); if (dcmd_ret == DCMD_SUCCESS) seq_num = le32_to_cpu(instance->evt_detail->seq_num) + 1; else seq_num = instance->last_seq_num; class_locale.members.reserved = 0; class_locale.members.locale = MR_EVT_LOCALE_ALL; class_locale.members.class = MR_EVT_CLASS_DEBUG; if (instance->aen_cmd != NULL) { kfree(ev); return; } mutex_lock(&instance->reset_mutex); error = megasas_register_aen(instance, seq_num, class_locale.word); if (error) dev_err(&instance->pdev->dev, ""register aen failed error %x\n"", error); mutex_unlock(&instance->reset_mutex); kfree(ev); }"
50----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44933/bad/bnxt.c----bnxt_need_reserve_rings,"static bool bnxt_need_reserve_rings(struct bnxt *bp) { struct bnxt_hw_resc *hw_resc = &bp->hw_resc; int cp = bnxt_cp_rings_in_use(bp); int nq = bnxt_nq_rings_in_use(bp); int rx = bp->rx_nr_rings, stat; int vnic, grp = rx; <S2SV_StartVul> if (hw_resc->resv_tx_rings != bp->tx_nr_rings && <S2SV_EndVul> <S2SV_StartVul> bp->hwrm_spec_code >= 0x10601) <S2SV_EndVul> <S2SV_StartVul> return true; <S2SV_EndVul> <S2SV_StartVul> if (!BNXT_NEW_RM(bp)) { <S2SV_EndVul> bnxt_check_rss_tbl_no_rmgr(bp); return false; <S2SV_StartVul> } <S2SV_EndVul> vnic = bnxt_get_total_vnics(bp, rx); if (bp->flags & BNXT_FLAG_AGG_RINGS) rx <<= 1; stat = bnxt_get_func_stat_ctxs(bp); if (hw_resc->resv_rx_rings != rx || hw_resc->resv_cp_rings != cp || hw_resc->resv_vnics != vnic || hw_resc->resv_stat_ctxs != stat || (hw_resc->resv_hw_ring_grps != grp && !(bp->flags & BNXT_FLAG_CHIP_P5_PLUS))) return true; if ((bp->flags & BNXT_FLAG_CHIP_P5_PLUS) && BNXT_PF(bp) && hw_resc->resv_irqs != nq) return true; return false; }","- if (hw_resc->resv_tx_rings != bp->tx_nr_rings &&
- bp->hwrm_spec_code >= 0x10601)
- return true;
- if (!BNXT_NEW_RM(bp)) {
- }
+ if (!BNXT_NEW_RM(bp))
+ if (hw_resc->resv_tx_rings != bp->tx_nr_rings &&
+ bp->hwrm_spec_code >= 0x10601)
+ return true;
+ if (!BNXT_NEW_RM(bp))","static bool bnxt_need_reserve_rings(struct bnxt *bp) { struct bnxt_hw_resc *hw_resc = &bp->hw_resc; int cp = bnxt_cp_rings_in_use(bp); int nq = bnxt_nq_rings_in_use(bp); int rx = bp->rx_nr_rings, stat; int vnic, grp = rx; if (!BNXT_NEW_RM(bp)) bnxt_check_rss_tbl_no_rmgr(bp); if (hw_resc->resv_tx_rings != bp->tx_nr_rings && bp->hwrm_spec_code >= 0x10601) return true; if (!BNXT_NEW_RM(bp)) return false; vnic = bnxt_get_total_vnics(bp, rx); if (bp->flags & BNXT_FLAG_AGG_RINGS) rx <<= 1; stat = bnxt_get_func_stat_ctxs(bp); if (hw_resc->resv_rx_rings != rx || hw_resc->resv_cp_rings != cp || hw_resc->resv_vnics != vnic || hw_resc->resv_stat_ctxs != stat || (hw_resc->resv_hw_ring_grps != grp && !(bp->flags & BNXT_FLAG_CHIP_P5_PLUS))) return true; if ((bp->flags & BNXT_FLAG_CHIP_P5_PLUS) && BNXT_PF(bp) && hw_resc->resv_irqs != nq) return true; return false; }"
270----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46750/bad/pci.c----pci_slot_lock,"static void pci_slot_lock(struct pci_slot *slot) { struct pci_dev *dev; list_for_each_entry(dev, &slot->bus->devices, bus_list) { if (!dev->slot || dev->slot != slot) continue; <S2SV_StartVul> pci_dev_lock(dev); <S2SV_EndVul> if (dev->subordinate) pci_bus_lock(dev->subordinate); <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul>","- pci_dev_lock(dev);
- }
- }
+ else
+ pci_dev_lock(dev);","static void pci_slot_lock(struct pci_slot *slot) { struct pci_dev *dev; list_for_each_entry(dev, &slot->bus->devices, bus_list) { if (!dev->slot || dev->slot != slot) continue; if (dev->subordinate) pci_bus_lock(dev->subordinate); else pci_dev_lock(dev); } }"
11----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-43891/bad/trace_events_hist.c----hist_debug_show,"static int hist_debug_show(struct seq_file *m, void *v) { struct event_trigger_data *data; struct trace_event_file *event_file; int n = 0, ret = 0; mutex_lock(&event_mutex); <S2SV_StartVul> event_file = event_file_data(m->private); <S2SV_EndVul> if (unlikely(!event_file)) { ret = -ENODEV; goto out_unlock; } list_for_each_entry(data, &event_file->triggers, list) { if (data->cmd_ops->trigger_type == ETT_EVENT_HIST) hist_trigger_debug_show(m, data, n++); } out_unlock: mutex_unlock(&event_mutex); return ret; }","- event_file = event_file_data(m->private);
+ event_file = event_file_file(m->private);","static int hist_debug_show(struct seq_file *m, void *v) { struct event_trigger_data *data; struct trace_event_file *event_file; int n = 0, ret = 0; mutex_lock(&event_mutex); event_file = event_file_file(m->private); if (unlikely(!event_file)) { ret = -ENODEV; goto out_unlock; } list_for_each_entry(data, &event_file->triggers, list) { if (data->cmd_ops->trigger_type == ETT_EVENT_HIST) hist_trigger_debug_show(m, data, n++); } out_unlock: mutex_unlock(&event_mutex); return ret; }"
307----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46776/bad/link_factory.c----construct_phy,"static bool construct_phy(struct dc_link *link, const struct link_init_data *init_params) { uint8_t i; struct ddc_service_init_data ddc_service_init_data = { 0 }; struct dc_context *dc_ctx = init_params->ctx; struct encoder_init_data enc_init_data = { 0 }; struct panel_cntl_init_data panel_cntl_init_data = { 0 }; struct dc_bios *bios = init_params->dc->ctx->dc_bios; const struct dc_vbios_funcs *bp_funcs = bios->funcs; struct bp_disp_connector_caps_info disp_connect_caps_info = { 0 }; DC_LOGGER_INIT(dc_ctx->logger); link->irq_source_hpd = DC_IRQ_SOURCE_INVALID; link->irq_source_hpd_rx = DC_IRQ_SOURCE_INVALID; link->link_status.dpcd_caps = &link->dpcd_caps; link->dc = init_params->dc; link->ctx = dc_ctx; link->link_index = init_params->link_index; memset(&link->preferred_training_settings, 0, sizeof(struct dc_link_training_overrides)); memset(&link->preferred_link_setting, 0, sizeof(struct dc_link_settings)); link->link_id = bios->funcs->get_connector_id(bios, init_params->connector_index); link->ep_type = DISPLAY_ENDPOINT_PHY; DC_LOG_DC(""BIOS object table - link_id: %d"", link->link_id.id); if (bios->funcs->get_disp_connector_caps_info) { bios->funcs->get_disp_connector_caps_info(bios, link->link_id, &disp_connect_caps_info); link->is_internal_display = disp_connect_caps_info.INTERNAL_DISPLAY; DC_LOG_DC(""BIOS object table - is_internal_display: %d"", link->is_internal_display); } if (link->link_id.type != OBJECT_TYPE_CONNECTOR) { dm_output_to_console(""%s: Invalid Connector ObjectID from Adapter Service for connector index:%d! type %d expected %d\n"", __func__, init_params->connector_index, link->link_id.type, OBJECT_TYPE_CONNECTOR); goto create_fail; } if (link->dc->res_pool->funcs->link_init) link->dc->res_pool->funcs->link_init(link); link->hpd_gpio = link_get_hpd_gpio(link->ctx->dc_bios, link->link_id, link->ctx->gpio_service); if (link->hpd_gpio) { dal_gpio_open(link->hpd_gpio, GPIO_MODE_INTERRUPT); dal_gpio_unlock_pin(link->hpd_gpio); link->irq_source_hpd = dal_irq_get_source(link->hpd_gpio); DC_LOG_DC(""BIOS object table - hpd_gpio id: %d"", link->hpd_gpio->id); DC_LOG_DC(""BIOS object table - hpd_gpio en: %d"", link->hpd_gpio->en); } switch (link->link_id.id) { case CONNECTOR_ID_HDMI_TYPE_A: link->connector_signal = SIGNAL_TYPE_HDMI_TYPE_A; break; case CONNECTOR_ID_SINGLE_LINK_DVID: case CONNECTOR_ID_SINGLE_LINK_DVII: link->connector_signal = SIGNAL_TYPE_DVI_SINGLE_LINK; break; case CONNECTOR_ID_DUAL_LINK_DVID: case CONNECTOR_ID_DUAL_LINK_DVII: link->connector_signal = SIGNAL_TYPE_DVI_DUAL_LINK; break; case CONNECTOR_ID_DISPLAY_PORT: case CONNECTOR_ID_USBC: link->connector_signal = SIGNAL_TYPE_DISPLAY_PORT; if (link->hpd_gpio) link->irq_source_hpd_rx = dal_irq_get_rx_source(link->hpd_gpio); break; case CONNECTOR_ID_EDP: link->connector_signal = SIGNAL_TYPE_EDP; if (link->hpd_gpio) { if (!link->dc->config.allow_edp_hotplug_detection) link->irq_source_hpd = DC_IRQ_SOURCE_INVALID; switch (link->dc->config.allow_edp_hotplug_detection) { case HPD_EN_FOR_ALL_EDP: link->irq_source_hpd_rx = dal_irq_get_rx_source(link->hpd_gpio); break; case HPD_EN_FOR_PRIMARY_EDP_ONLY: if (link->link_index == 0) link->irq_source_hpd_rx = dal_irq_get_rx_source(link->hpd_gpio); else link->irq_source_hpd = DC_IRQ_SOURCE_INVALID; break; case HPD_EN_FOR_SECONDARY_EDP_ONLY: if (link->link_index == 1) link->irq_source_hpd_rx = dal_irq_get_rx_source(link->hpd_gpio); else link->irq_source_hpd = DC_IRQ_SOURCE_INVALID; break; default: link->irq_source_hpd = DC_IRQ_SOURCE_INVALID; break; } } break; case CONNECTOR_ID_LVDS: link->connector_signal = SIGNAL_TYPE_LVDS; break; default: DC_LOG_WARNING(""Unsupported Connector type:%d!\n"", link->link_id.id); goto create_fail; } LINK_INFO(""Connector[%d] description: signal: %s\n"", init_params->connector_index, signal_type_to_string(link->connector_signal)); ddc_service_init_data.ctx = link->ctx; ddc_service_init_data.id = link->link_id; ddc_service_init_data.link = link; link->ddc = link_create_ddc_service(&ddc_service_init_data); if (!link->ddc) { DC_ERROR(""Failed to create ddc_service!\n""); goto ddc_create_fail; } if (!link->ddc->ddc_pin) { DC_ERROR(""Failed to get I2C info for connector!\n""); goto ddc_create_fail; } link->ddc_hw_inst = dal_ddc_get_line(get_ddc_pin(link->ddc)); enc_init_data.ctx = dc_ctx; bp_funcs->get_src_obj(dc_ctx->dc_bios, link->link_id, 0, &enc_init_data.encoder); enc_init_data.connector = link->link_id; enc_init_data.channel = get_ddc_line(link); enc_init_data.hpd_source = get_hpd_line(link); link->hpd_src = enc_init_data.hpd_source; enc_init_data.transmitter = translate_encoder_to_transmitter(enc_init_data.encoder); link->link_enc = link->dc->res_pool->funcs->link_enc_create(dc_ctx, &enc_init_data); <S2SV_StartVul> DC_LOG_DC(""BIOS object table - DP_IS_USB_C: %d"", link->link_enc->features.flags.bits.DP_IS_USB_C); <S2SV_EndVul> <S2SV_StartVul> DC_LOG_DC(""BIOS object table - IS_DP2_CAPABLE: %d"", link->link_enc->features.flags.bits.IS_DP2_CAPABLE); <S2SV_EndVul> if (!link->link_enc) { DC_ERROR(""Failed to create link encoder!\n""); goto link_enc_create_fail; } link->eng_id = link->link_enc->preferred_engine; link->dc->res_pool->link_encoders[link->eng_id - ENGINE_ID_DIGA] = link->link_enc; link->dc->res_pool->dig_link_enc_count++; link->link_enc_hw_inst = link->link_enc->transmitter; if (link->dc->res_pool->funcs->panel_cntl_create && (link->link_id.id == CONNECTOR_ID_EDP || link->link_id.id == CONNECTOR_ID_LVDS)) { panel_cntl_init_data.ctx = dc_ctx; panel_cntl_init_data.inst = panel_cntl_init_data.ctx->dc_edp_id_count; panel_cntl_init_data.eng_id = link->eng_id; link->panel_cntl = link->dc->res_pool->funcs->panel_cntl_create( &panel_cntl_init_data); panel_cntl_init_data.ctx->dc_edp_id_count++; if (link->panel_cntl == NULL) { DC_ERROR(""Failed to create link panel_cntl!\n""); goto panel_cntl_create_fail; } } for (i = 0; i < 4; i++) { if (bp_funcs->get_device_tag(dc_ctx->dc_bios, link->link_id, i, &link->device_tag) != BP_RESULT_OK) { DC_ERROR(""Failed to find device tag!\n""); goto device_tag_fail; } if (!bp_funcs->is_device_id_supported(dc_ctx->dc_bios, link->device_tag.dev_id)) continue; if (link->device_tag.dev_id.device_type == DEVICE_TYPE_CRT && link->connector_signal != SIGNAL_TYPE_RGB) continue; if (link->device_tag.dev_id.device_type == DEVICE_TYPE_LCD && link->connector_signal == SIGNAL_TYPE_RGB) continue; DC_LOG_DC(""BIOS object table - device_tag.acpi_device: %d"", link->device_tag.acpi_device); DC_LOG_DC(""BIOS object table - device_tag.dev_id.device_type: %d"", link->device_tag.dev_id.device_type); DC_LOG_DC(""BIOS object table - device_tag.dev_id.enum_id: %d"", link->device_tag.dev_id.enum_id); break; } if (bios->integrated_info) { for (i = 0; i < MAX_NUMBER_OF_EXT_DISPLAY_PATH; i++) { struct external_display_path *path = &bios->integrated_info->ext_disp_conn_info.path[i]; if (path->device_connector_id.enum_id == link->link_id.enum_id && path->device_connector_id.id == link->link_id.id && path->device_connector_id.type == link->link_id.type) { if (link->device_tag.acpi_device != 0 && path->device_acpi_enum == link->device_tag.acpi_device) { link->ddi_channel_mapping = path->channel_mapping; link->chip_caps = path->caps; DC_LOG_DC(""BIOS object table - ddi_channel_mapping: 0x%04X"", link->ddi_channel_mapping.raw); DC_LOG_DC(""BIOS object table - chip_caps: %d"", link->chip_caps); } else if (path->device_tag == link->device_tag.dev_id.raw_device_tag) { link->ddi_channel_mapping = path->channel_mapping; link->chip_caps = path->caps; DC_LOG_DC(""BIOS object table - ddi_channel_mapping: 0x%04X"", link->ddi_channel_mapping.raw); DC_LOG_DC(""BIOS object table - chip_caps: %d"", link->chip_caps); } if (link->chip_caps & EXT_DISPLAY_PATH_CAPS__DP_FIXED_VS_EN) { link->bios_forced_drive_settings.VOLTAGE_SWING = (bios->integrated_info->ext_disp_conn_info.fixdpvoltageswing & 0x3); link->bios_forced_drive_settings.PRE_EMPHASIS = ((bios->integrated_info->ext_disp_conn_info.fixdpvoltageswing >> 2) & 0x3); } break; } } } if (bios->funcs->get_atom_dc_golden_table) bios->funcs->get_atom_dc_golden_table(bios); program_hpd_filter(link); link->psr_settings.psr_vtotal_control_support = false; link->psr_settings.psr_version = DC_PSR_VERSION_UNSUPPORTED; DC_LOG_DC(""BIOS object table - %s finished successfully.\n"", __func__); return true; device_tag_fail: link->link_enc->funcs->destroy(&link->link_enc); link_enc_create_fail: if (link->panel_cntl != NULL) link->panel_cntl->funcs->destroy(&link->panel_cntl); panel_cntl_create_fail: link_destroy_ddc_service(&link->ddc); ddc_create_fail: create_fail: if (link->hpd_gpio) { dal_gpio_destroy_irq(&link->hpd_gpio); link->hpd_gpio = NULL; } DC_LOG_DC(""BIOS object table - %s failed.\n"", __func__); return false; }","- DC_LOG_DC(""BIOS object table - DP_IS_USB_C: %d"", link->link_enc->features.flags.bits.DP_IS_USB_C);
- DC_LOG_DC(""BIOS object table - IS_DP2_CAPABLE: %d"", link->link_enc->features.flags.bits.IS_DP2_CAPABLE);
+ DC_LOG_DC(""BIOS object table - DP_IS_USB_C: %d"", link->link_enc->features.flags.bits.DP_IS_USB_C);
+ DC_LOG_DC(""BIOS object table - IS_DP2_CAPABLE: %d"", link->link_enc->features.flags.bits.IS_DP2_CAPABLE);","static bool construct_phy(struct dc_link *link, const struct link_init_data *init_params) { uint8_t i; struct ddc_service_init_data ddc_service_init_data = { 0 }; struct dc_context *dc_ctx = init_params->ctx; struct encoder_init_data enc_init_data = { 0 }; struct panel_cntl_init_data panel_cntl_init_data = { 0 }; struct dc_bios *bios = init_params->dc->ctx->dc_bios; const struct dc_vbios_funcs *bp_funcs = bios->funcs; struct bp_disp_connector_caps_info disp_connect_caps_info = { 0 }; DC_LOGGER_INIT(dc_ctx->logger); link->irq_source_hpd = DC_IRQ_SOURCE_INVALID; link->irq_source_hpd_rx = DC_IRQ_SOURCE_INVALID; link->link_status.dpcd_caps = &link->dpcd_caps; link->dc = init_params->dc; link->ctx = dc_ctx; link->link_index = init_params->link_index; memset(&link->preferred_training_settings, 0, sizeof(struct dc_link_training_overrides)); memset(&link->preferred_link_setting, 0, sizeof(struct dc_link_settings)); link->link_id = bios->funcs->get_connector_id(bios, init_params->connector_index); link->ep_type = DISPLAY_ENDPOINT_PHY; DC_LOG_DC(""BIOS object table - link_id: %d"", link->link_id.id); if (bios->funcs->get_disp_connector_caps_info) { bios->funcs->get_disp_connector_caps_info(bios, link->link_id, &disp_connect_caps_info); link->is_internal_display = disp_connect_caps_info.INTERNAL_DISPLAY; DC_LOG_DC(""BIOS object table - is_internal_display: %d"", link->is_internal_display); } if (link->link_id.type != OBJECT_TYPE_CONNECTOR) { dm_output_to_console(""%s: Invalid Connector ObjectID from Adapter Service for connector index:%d! type %d expected %d\n"", __func__, init_params->connector_index, link->link_id.type, OBJECT_TYPE_CONNECTOR); goto create_fail; } if (link->dc->res_pool->funcs->link_init) link->dc->res_pool->funcs->link_init(link); link->hpd_gpio = link_get_hpd_gpio(link->ctx->dc_bios, link->link_id, link->ctx->gpio_service); if (link->hpd_gpio) { dal_gpio_open(link->hpd_gpio, GPIO_MODE_INTERRUPT); dal_gpio_unlock_pin(link->hpd_gpio); link->irq_source_hpd = dal_irq_get_source(link->hpd_gpio); DC_LOG_DC(""BIOS object table - hpd_gpio id: %d"", link->hpd_gpio->id); DC_LOG_DC(""BIOS object table - hpd_gpio en: %d"", link->hpd_gpio->en); } switch (link->link_id.id) { case CONNECTOR_ID_HDMI_TYPE_A: link->connector_signal = SIGNAL_TYPE_HDMI_TYPE_A; break; case CONNECTOR_ID_SINGLE_LINK_DVID: case CONNECTOR_ID_SINGLE_LINK_DVII: link->connector_signal = SIGNAL_TYPE_DVI_SINGLE_LINK; break; case CONNECTOR_ID_DUAL_LINK_DVID: case CONNECTOR_ID_DUAL_LINK_DVII: link->connector_signal = SIGNAL_TYPE_DVI_DUAL_LINK; break; case CONNECTOR_ID_DISPLAY_PORT: case CONNECTOR_ID_USBC: link->connector_signal = SIGNAL_TYPE_DISPLAY_PORT; if (link->hpd_gpio) link->irq_source_hpd_rx = dal_irq_get_rx_source(link->hpd_gpio); break; case CONNECTOR_ID_EDP: link->connector_signal = SIGNAL_TYPE_EDP; if (link->hpd_gpio) { if (!link->dc->config.allow_edp_hotplug_detection) link->irq_source_hpd = DC_IRQ_SOURCE_INVALID; switch (link->dc->config.allow_edp_hotplug_detection) { case HPD_EN_FOR_ALL_EDP: link->irq_source_hpd_rx = dal_irq_get_rx_source(link->hpd_gpio); break; case HPD_EN_FOR_PRIMARY_EDP_ONLY: if (link->link_index == 0) link->irq_source_hpd_rx = dal_irq_get_rx_source(link->hpd_gpio); else link->irq_source_hpd = DC_IRQ_SOURCE_INVALID; break; case HPD_EN_FOR_SECONDARY_EDP_ONLY: if (link->link_index == 1) link->irq_source_hpd_rx = dal_irq_get_rx_source(link->hpd_gpio); else link->irq_source_hpd = DC_IRQ_SOURCE_INVALID; break; default: link->irq_source_hpd = DC_IRQ_SOURCE_INVALID; break; } } break; case CONNECTOR_ID_LVDS: link->connector_signal = SIGNAL_TYPE_LVDS; break; default: DC_LOG_WARNING(""Unsupported Connector type:%d!\n"", link->link_id.id); goto create_fail; } LINK_INFO(""Connector[%d] description: signal: %s\n"", init_params->connector_index, signal_type_to_string(link->connector_signal)); ddc_service_init_data.ctx = link->ctx; ddc_service_init_data.id = link->link_id; ddc_service_init_data.link = link; link->ddc = link_create_ddc_service(&ddc_service_init_data); if (!link->ddc) { DC_ERROR(""Failed to create ddc_service!\n""); goto ddc_create_fail; } if (!link->ddc->ddc_pin) { DC_ERROR(""Failed to get I2C info for connector!\n""); goto ddc_create_fail; } link->ddc_hw_inst = dal_ddc_get_line(get_ddc_pin(link->ddc)); enc_init_data.ctx = dc_ctx; bp_funcs->get_src_obj(dc_ctx->dc_bios, link->link_id, 0, &enc_init_data.encoder); enc_init_data.connector = link->link_id; enc_init_data.channel = get_ddc_line(link); enc_init_data.hpd_source = get_hpd_line(link); link->hpd_src = enc_init_data.hpd_source; enc_init_data.transmitter = translate_encoder_to_transmitter(enc_init_data.encoder); link->link_enc = link->dc->res_pool->funcs->link_enc_create(dc_ctx, &enc_init_data); if (!link->link_enc) { DC_ERROR(""Failed to create link encoder!\n""); goto link_enc_create_fail; } DC_LOG_DC(""BIOS object table - DP_IS_USB_C: %d"", link->link_enc->features.flags.bits.DP_IS_USB_C); DC_LOG_DC(""BIOS object table - IS_DP2_CAPABLE: %d"", link->link_enc->features.flags.bits.IS_DP2_CAPABLE); link->eng_id = link->link_enc->preferred_engine; link->dc->res_pool->link_encoders[link->eng_id - ENGINE_ID_DIGA] = link->link_enc; link->dc->res_pool->dig_link_enc_count++; link->link_enc_hw_inst = link->link_enc->transmitter; if (link->dc->res_pool->funcs->panel_cntl_create && (link->link_id.id == CONNECTOR_ID_EDP || link->link_id.id == CONNECTOR_ID_LVDS)) { panel_cntl_init_data.ctx = dc_ctx; panel_cntl_init_data.inst = panel_cntl_init_data.ctx->dc_edp_id_count; panel_cntl_init_data.eng_id = link->eng_id; link->panel_cntl = link->dc->res_pool->funcs->panel_cntl_create( &panel_cntl_init_data); panel_cntl_init_data.ctx->dc_edp_id_count++; if (link->panel_cntl == NULL) { DC_ERROR(""Failed to create link panel_cntl!\n""); goto panel_cntl_create_fail; } } for (i = 0; i < 4; i++) { if (bp_funcs->get_device_tag(dc_ctx->dc_bios, link->link_id, i, &link->device_tag) != BP_RESULT_OK) { DC_ERROR(""Failed to find device tag!\n""); goto device_tag_fail; } if (!bp_funcs->is_device_id_supported(dc_ctx->dc_bios, link->device_tag.dev_id)) continue; if (link->device_tag.dev_id.device_type == DEVICE_TYPE_CRT && link->connector_signal != SIGNAL_TYPE_RGB) continue; if (link->device_tag.dev_id.device_type == DEVICE_TYPE_LCD && link->connector_signal == SIGNAL_TYPE_RGB) continue; DC_LOG_DC(""BIOS object table - device_tag.acpi_device: %d"", link->device_tag.acpi_device); DC_LOG_DC(""BIOS object table - device_tag.dev_id.device_type: %d"", link->device_tag.dev_id.device_type); DC_LOG_DC(""BIOS object table - device_tag.dev_id.enum_id: %d"", link->device_tag.dev_id.enum_id); break; } if (bios->integrated_info) { for (i = 0; i < MAX_NUMBER_OF_EXT_DISPLAY_PATH; i++) { struct external_display_path *path = &bios->integrated_info->ext_disp_conn_info.path[i]; if (path->device_connector_id.enum_id == link->link_id.enum_id && path->device_connector_id.id == link->link_id.id && path->device_connector_id.type == link->link_id.type) { if (link->device_tag.acpi_device != 0 && path->device_acpi_enum == link->device_tag.acpi_device) { link->ddi_channel_mapping = path->channel_mapping; link->chip_caps = path->caps; DC_LOG_DC(""BIOS object table - ddi_channel_mapping: 0x%04X"", link->ddi_channel_mapping.raw); DC_LOG_DC(""BIOS object table - chip_caps: %d"", link->chip_caps); } else if (path->device_tag == link->device_tag.dev_id.raw_device_tag) { link->ddi_channel_mapping = path->channel_mapping; link->chip_caps = path->caps; DC_LOG_DC(""BIOS object table - ddi_channel_mapping: 0x%04X"", link->ddi_channel_mapping.raw); DC_LOG_DC(""BIOS object table - chip_caps: %d"", link->chip_caps); } if (link->chip_caps & EXT_DISPLAY_PATH_CAPS__DP_FIXED_VS_EN) { link->bios_forced_drive_settings.VOLTAGE_SWING = (bios->integrated_info->ext_disp_conn_info.fixdpvoltageswing & 0x3); link->bios_forced_drive_settings.PRE_EMPHASIS = ((bios->integrated_info->ext_disp_conn_info.fixdpvoltageswing >> 2) & 0x3); } break; } } } if (bios->funcs->get_atom_dc_golden_table) bios->funcs->get_atom_dc_golden_table(bios); program_hpd_filter(link); link->psr_settings.psr_vtotal_control_support = false; link->psr_settings.psr_version = DC_PSR_VERSION_UNSUPPORTED; DC_LOG_DC(""BIOS object table - %s finished successfully.\n"", __func__); return true; device_tag_fail: link->link_enc->funcs->destroy(&link->link_enc); link_enc_create_fail: if (link->panel_cntl != NULL) link->panel_cntl->funcs->destroy(&link->panel_cntl); panel_cntl_create_fail: link_destroy_ddc_service(&link->ddc); ddc_create_fail: create_fail: if (link->hpd_gpio) { dal_gpio_destroy_irq(&link->hpd_gpio); link->hpd_gpio = NULL; } DC_LOG_DC(""BIOS object table - %s failed.\n"", __func__); return false; }"
395----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46856/bad/dp83822.c----dp83822_config_intr,"static int dp83822_config_intr(struct phy_device *phydev) { struct dp83822_private *dp83822 = phydev->priv; int misr_status; int physcr_status; int err; if (phydev->interrupts == PHY_INTERRUPT_ENABLED) { misr_status = phy_read(phydev, MII_DP83822_MISR1); if (misr_status < 0) return misr_status; misr_status |= (DP83822_LINK_STAT_INT_EN | DP83822_ENERGY_DET_INT_EN | DP83822_LINK_QUAL_INT_EN); <S2SV_StartVul> if (!dp83822 || !dp83822->fx_enabled) <S2SV_EndVul> misr_status |= DP83822_ANEG_COMPLETE_INT_EN | DP83822_DUP_MODE_CHANGE_INT_EN | DP83822_SPEED_CHANGED_INT_EN; err = phy_write(phydev, MII_DP83822_MISR1, misr_status); if (err < 0) return err; misr_status = phy_read(phydev, MII_DP83822_MISR2); if (misr_status < 0) return misr_status; misr_status |= (DP83822_JABBER_DET_INT_EN | DP83822_SLEEP_MODE_INT_EN | DP83822_LB_FIFO_INT_EN | DP83822_PAGE_RX_INT_EN | DP83822_EEE_ERROR_CHANGE_INT_EN); <S2SV_StartVul> if (!dp83822 || !dp83822->fx_enabled) <S2SV_EndVul> misr_status |= DP83822_ANEG_ERR_INT_EN | DP83822_WOL_PKT_INT_EN; err = phy_write(phydev, MII_DP83822_MISR2, misr_status); if (err < 0) return err; physcr_status = phy_read(phydev, MII_DP83822_PHYSCR); if (physcr_status < 0) return physcr_status; physcr_status |= DP83822_PHYSCR_INT_OE | DP83822_PHYSCR_INTEN; } else { err = phy_write(phydev, MII_DP83822_MISR1, 0); if (err < 0) return err; err = phy_write(phydev, MII_DP83822_MISR2, 0); if (err < 0) return err; physcr_status = phy_read(phydev, MII_DP83822_PHYSCR); if (physcr_status < 0) return physcr_status; physcr_status &= ~DP83822_PHYSCR_INTEN; } return phy_write(phydev, MII_DP83822_PHYSCR, physcr_status); }","- if (!dp83822 || !dp83822->fx_enabled)
- if (!dp83822 || !dp83822->fx_enabled)
+ if (!dp83822->fx_enabled)
+ if (!dp83822->fx_enabled)","static int dp83822_config_intr(struct phy_device *phydev) { struct dp83822_private *dp83822 = phydev->priv; int misr_status; int physcr_status; int err; if (phydev->interrupts == PHY_INTERRUPT_ENABLED) { misr_status = phy_read(phydev, MII_DP83822_MISR1); if (misr_status < 0) return misr_status; misr_status |= (DP83822_LINK_STAT_INT_EN | DP83822_ENERGY_DET_INT_EN | DP83822_LINK_QUAL_INT_EN); if (!dp83822->fx_enabled) misr_status |= DP83822_ANEG_COMPLETE_INT_EN | DP83822_DUP_MODE_CHANGE_INT_EN | DP83822_SPEED_CHANGED_INT_EN; err = phy_write(phydev, MII_DP83822_MISR1, misr_status); if (err < 0) return err; misr_status = phy_read(phydev, MII_DP83822_MISR2); if (misr_status < 0) return misr_status; misr_status |= (DP83822_JABBER_DET_INT_EN | DP83822_SLEEP_MODE_INT_EN | DP83822_LB_FIFO_INT_EN | DP83822_PAGE_RX_INT_EN | DP83822_EEE_ERROR_CHANGE_INT_EN); if (!dp83822->fx_enabled) misr_status |= DP83822_ANEG_ERR_INT_EN | DP83822_WOL_PKT_INT_EN; err = phy_write(phydev, MII_DP83822_MISR2, misr_status); if (err < 0) return err; physcr_status = phy_read(phydev, MII_DP83822_PHYSCR); if (physcr_status < 0) return physcr_status; physcr_status |= DP83822_PHYSCR_INT_OE | DP83822_PHYSCR_INTEN; } else { err = phy_write(phydev, MII_DP83822_MISR1, 0); if (err < 0) return err; err = phy_write(phydev, MII_DP83822_MISR2, 0); if (err < 0) return err; physcr_status = phy_read(phydev, MII_DP83822_PHYSCR); if (physcr_status < 0) return physcr_status; physcr_status &= ~DP83822_PHYSCR_INTEN; } return phy_write(phydev, MII_DP83822_PHYSCR, physcr_status); }"
974----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50213/bad/drm_hdmi_state_helper_test.c----drm_test_check_broadcast_rgb_auto_cea_mode_vic_1,"static void drm_test_check_broadcast_rgb_auto_cea_mode_vic_1(struct kunit *test) { struct drm_atomic_helper_connector_hdmi_priv *priv; struct drm_modeset_acquire_ctx *ctx; struct drm_connector_state *conn_state; struct drm_atomic_state *state; struct drm_display_mode *mode; struct drm_connector *conn; struct drm_device *drm; struct drm_crtc *crtc; int ret; priv = drm_atomic_helper_connector_hdmi_init(test, BIT(HDMI_COLORSPACE_RGB), 8); KUNIT_ASSERT_NOT_NULL(test, priv); drm = &priv->drm; conn = &priv->connector; KUNIT_ASSERT_TRUE(test, conn->display_info.is_hdmi); ctx = drm_kunit_helper_acquire_ctx_alloc(test); KUNIT_ASSERT_NOT_ERR_OR_NULL(test, ctx); <S2SV_StartVul> mode = drm_display_mode_from_cea_vic(drm, 1); <S2SV_EndVul> KUNIT_ASSERT_NOT_NULL(test, mode); drm = &priv->drm; crtc = priv->crtc; ret = light_up_connector(test, drm, crtc, conn, mode, ctx); KUNIT_ASSERT_EQ(test, ret, 0); state = drm_kunit_helper_atomic_state_alloc(test, drm, ctx); KUNIT_ASSERT_NOT_ERR_OR_NULL(test, state); conn_state = drm_atomic_get_connector_state(state, conn); KUNIT_ASSERT_NOT_ERR_OR_NULL(test, conn_state); KUNIT_ASSERT_EQ(test, conn_state->hdmi.broadcast_rgb, DRM_HDMI_BROADCAST_RGB_AUTO); ret = drm_atomic_check_only(state); KUNIT_ASSERT_EQ(test, ret, 0); conn_state = drm_atomic_get_connector_state(state, conn); KUNIT_ASSERT_NOT_ERR_OR_NULL(test, conn_state); KUNIT_EXPECT_FALSE(test, conn_state->hdmi.is_limited_range); }","- mode = drm_display_mode_from_cea_vic(drm, 1);
+ mode = drm_kunit_display_mode_from_cea_vic(test, drm, 1);","static void drm_test_check_broadcast_rgb_auto_cea_mode_vic_1(struct kunit *test) { struct drm_atomic_helper_connector_hdmi_priv *priv; struct drm_modeset_acquire_ctx *ctx; struct drm_connector_state *conn_state; struct drm_atomic_state *state; struct drm_display_mode *mode; struct drm_connector *conn; struct drm_device *drm; struct drm_crtc *crtc; int ret; priv = drm_atomic_helper_connector_hdmi_init(test, BIT(HDMI_COLORSPACE_RGB), 8); KUNIT_ASSERT_NOT_NULL(test, priv); drm = &priv->drm; conn = &priv->connector; KUNIT_ASSERT_TRUE(test, conn->display_info.is_hdmi); ctx = drm_kunit_helper_acquire_ctx_alloc(test); KUNIT_ASSERT_NOT_ERR_OR_NULL(test, ctx); mode = drm_kunit_display_mode_from_cea_vic(test, drm, 1); KUNIT_ASSERT_NOT_NULL(test, mode); drm = &priv->drm; crtc = priv->crtc; ret = light_up_connector(test, drm, crtc, conn, mode, ctx); KUNIT_ASSERT_EQ(test, ret, 0); state = drm_kunit_helper_atomic_state_alloc(test, drm, ctx); KUNIT_ASSERT_NOT_ERR_OR_NULL(test, state); conn_state = drm_atomic_get_connector_state(state, conn); KUNIT_ASSERT_NOT_ERR_OR_NULL(test, conn_state); KUNIT_ASSERT_EQ(test, conn_state->hdmi.broadcast_rgb, DRM_HDMI_BROADCAST_RGB_AUTO); ret = drm_atomic_check_only(state); KUNIT_ASSERT_EQ(test, ret, 0); conn_state = drm_atomic_get_connector_state(state, conn); KUNIT_ASSERT_NOT_ERR_OR_NULL(test, conn_state); KUNIT_EXPECT_FALSE(test, conn_state->hdmi.is_limited_range); }"
389----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46850/bad/dcn35_hwseq.c----dcn35_set_drr,"void dcn35_set_drr(struct pipe_ctx **pipe_ctx, int num_pipes, struct dc_crtc_timing_adjust adjust) { int i = 0; struct drr_params params = {0}; unsigned int event_triggers = 0x2; unsigned int num_frames = 2; params.vertical_total_max = adjust.v_total_max; params.vertical_total_min = adjust.v_total_min; params.vertical_total_mid = adjust.v_total_mid; params.vertical_total_mid_frame_num = adjust.v_total_mid_frame_num; for (i = 0; i < num_pipes; i++) { <S2SV_StartVul> if ((pipe_ctx[i]->stream_res.tg != NULL) && pipe_ctx[i]->stream_res.tg->funcs) { <S2SV_EndVul> struct dc_crtc_timing *timing = &pipe_ctx[i]->stream->timing; struct dc *dc = pipe_ctx[i]->stream->ctx->dc; if (dc->debug.static_screen_wait_frames) { unsigned int frame_rate = timing->pix_clk_100hz / (timing->h_total * timing->v_total); if (frame_rate >= 120 && dc->caps.ips_support && dc->config.disable_ips != DMUB_IPS_DISABLE_ALL) { num_frames = 2 * (frame_rate % 60); } } <S2SV_StartVul> if (pipe_ctx[i]->stream_res.tg->funcs->set_drr) <S2SV_EndVul> <S2SV_StartVul> pipe_ctx[i]->stream_res.tg->funcs->set_drr( <S2SV_EndVul> <S2SV_StartVul> pipe_ctx[i]->stream_res.tg, &params); <S2SV_EndVul> if (adjust.v_total_max != 0 && adjust.v_total_min != 0) <S2SV_StartVul> if (pipe_ctx[i]->stream_res.tg->funcs->set_static_screen_control) <S2SV_EndVul> <S2SV_StartVul> pipe_ctx[i]->stream_res.tg->funcs->set_static_screen_control( <S2SV_EndVul> <S2SV_StartVul> pipe_ctx[i]->stream_res.tg, <S2SV_EndVul> <S2SV_StartVul> event_triggers, num_frames); <S2SV_EndVul> } } }","- if ((pipe_ctx[i]->stream_res.tg != NULL) && pipe_ctx[i]->stream_res.tg->funcs) {
- if (pipe_ctx[i]->stream_res.tg->funcs->set_drr)
- pipe_ctx[i]->stream_res.tg->funcs->set_drr(
- pipe_ctx[i]->stream_res.tg, &params);
- if (pipe_ctx[i]->stream_res.tg->funcs->set_static_screen_control)
- pipe_ctx[i]->stream_res.tg->funcs->set_static_screen_control(
- pipe_ctx[i]->stream_res.tg,
- event_triggers, num_frames);
+ struct timing_generator *tg = pipe_ctx[i]->stream_res.tg;
+ if ((tg != NULL) && tg->funcs) {
+ if (tg->funcs->set_drr)
+ tg->funcs->set_drr(tg, &params);
+ if (tg->funcs->set_static_screen_control)
+ tg->funcs->set_static_screen_control(
+ tg, event_triggers, num_frames);","void dcn35_set_drr(struct pipe_ctx **pipe_ctx, int num_pipes, struct dc_crtc_timing_adjust adjust) { int i = 0; struct drr_params params = {0}; unsigned int event_triggers = 0x2; unsigned int num_frames = 2; params.vertical_total_max = adjust.v_total_max; params.vertical_total_min = adjust.v_total_min; params.vertical_total_mid = adjust.v_total_mid; params.vertical_total_mid_frame_num = adjust.v_total_mid_frame_num; for (i = 0; i < num_pipes; i++) { struct timing_generator *tg = pipe_ctx[i]->stream_res.tg; if ((tg != NULL) && tg->funcs) { struct dc_crtc_timing *timing = &pipe_ctx[i]->stream->timing; struct dc *dc = pipe_ctx[i]->stream->ctx->dc; if (dc->debug.static_screen_wait_frames) { unsigned int frame_rate = timing->pix_clk_100hz / (timing->h_total * timing->v_total); if (frame_rate >= 120 && dc->caps.ips_support && dc->config.disable_ips != DMUB_IPS_DISABLE_ALL) { num_frames = 2 * (frame_rate % 60); } } if (tg->funcs->set_drr) tg->funcs->set_drr(tg, &params); if (adjust.v_total_max != 0 && adjust.v_total_min != 0) if (tg->funcs->set_static_screen_control) tg->funcs->set_static_screen_control( tg, event_triggers, num_frames); } } }"
1455----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56783/bad/nft_socket.c----nft_socket_cgroup_subtree_level,"static noinline int nft_socket_cgroup_subtree_level(void) { struct cgroup *cgrp = cgroup_get_from_path(""/""); int level; if (IS_ERR(cgrp)) return PTR_ERR(cgrp); level = cgrp->level; cgroup_put(cgrp); <S2SV_StartVul> if (WARN_ON_ONCE(level > 255)) <S2SV_EndVul> return -ERANGE; if (WARN_ON_ONCE(level < 0)) return -EINVAL; return level; }","- if (WARN_ON_ONCE(level > 255))
+ if (level > 255)","static noinline int nft_socket_cgroup_subtree_level(void) { struct cgroup *cgrp = cgroup_get_from_path(""/""); int level; if (IS_ERR(cgrp)) return PTR_ERR(cgrp); level = cgrp->level; cgroup_put(cgrp); if (level > 255) return -ERANGE; if (WARN_ON_ONCE(level < 0)) return -EINVAL; return level; }"
802----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50075/bad/xhci-tegra.c----tegra_xusb_enter_elpg,"static int tegra_xusb_enter_elpg(struct tegra_xusb *tegra, bool runtime) { struct xhci_hcd *xhci = hcd_to_xhci(tegra->hcd); struct device *dev = tegra->dev; bool wakeup = runtime ? true : device_may_wakeup(dev); unsigned int i; int err; u32 usbcmd; u32 portsc; dev_dbg(dev, ""entering ELPG\n""); usbcmd = readl(&xhci->op_regs->command); usbcmd &= ~CMD_EIE; writel(usbcmd, &xhci->op_regs->command); err = tegra_xusb_check_ports(tegra); if (err < 0) { dev_err(tegra->dev, ""not all ports suspended: %d\n"", err); goto out; } <S2SV_StartVul> for (i = 0; i < tegra->num_usb_phys; i++) { <S2SV_EndVul> if (!xhci->usb2_rhub.ports[i]) continue; portsc = readl(xhci->usb2_rhub.ports[i]->addr); tegra->lp0_utmi_pad_mask &= ~BIT(i); if (((portsc & PORT_PLS_MASK) == XDEV_U3) || ((portsc & DEV_SPEED_MASK) == XDEV_FS)) tegra->lp0_utmi_pad_mask |= BIT(i); } err = xhci_suspend(xhci, wakeup); if (err < 0) { dev_err(tegra->dev, ""failed to suspend XHCI: %d\n"", err); goto out; } tegra_xusb_save_context(tegra); if (wakeup) tegra_xhci_enable_phy_sleepwalk_wake(tegra); tegra_xusb_powergate_partitions(tegra); for (i = 0; i < tegra->num_phys; i++) { if (!tegra->phys[i]) continue; phy_power_off(tegra->phys[i]); if (!wakeup) phy_exit(tegra->phys[i]); } tegra_xusb_clk_disable(tegra); out: if (!err) dev_dbg(tegra->dev, ""entering ELPG done\n""); else { usbcmd = readl(&xhci->op_regs->command); usbcmd |= CMD_EIE; writel(usbcmd, &xhci->op_regs->command); dev_dbg(tegra->dev, ""entering ELPG failed\n""); pm_runtime_mark_last_busy(tegra->dev); } return err; }","- for (i = 0; i < tegra->num_usb_phys; i++) {
+ for (i = 0; i < xhci->usb2_rhub.num_ports; i++) {","static int tegra_xusb_enter_elpg(struct tegra_xusb *tegra, bool runtime) { struct xhci_hcd *xhci = hcd_to_xhci(tegra->hcd); struct device *dev = tegra->dev; bool wakeup = runtime ? true : device_may_wakeup(dev); unsigned int i; int err; u32 usbcmd; u32 portsc; dev_dbg(dev, ""entering ELPG\n""); usbcmd = readl(&xhci->op_regs->command); usbcmd &= ~CMD_EIE; writel(usbcmd, &xhci->op_regs->command); err = tegra_xusb_check_ports(tegra); if (err < 0) { dev_err(tegra->dev, ""not all ports suspended: %d\n"", err); goto out; } for (i = 0; i < xhci->usb2_rhub.num_ports; i++) { if (!xhci->usb2_rhub.ports[i]) continue; portsc = readl(xhci->usb2_rhub.ports[i]->addr); tegra->lp0_utmi_pad_mask &= ~BIT(i); if (((portsc & PORT_PLS_MASK) == XDEV_U3) || ((portsc & DEV_SPEED_MASK) == XDEV_FS)) tegra->lp0_utmi_pad_mask |= BIT(i); } err = xhci_suspend(xhci, wakeup); if (err < 0) { dev_err(tegra->dev, ""failed to suspend XHCI: %d\n"", err); goto out; } tegra_xusb_save_context(tegra); if (wakeup) tegra_xhci_enable_phy_sleepwalk_wake(tegra); tegra_xusb_powergate_partitions(tegra); for (i = 0; i < tegra->num_phys; i++) { if (!tegra->phys[i]) continue; phy_power_off(tegra->phys[i]); if (!wakeup) phy_exit(tegra->phys[i]); } tegra_xusb_clk_disable(tegra); out: if (!err) dev_dbg(tegra->dev, ""entering ELPG done\n""); else { usbcmd = readl(&xhci->op_regs->command); usbcmd |= CMD_EIE; writel(usbcmd, &xhci->op_regs->command); dev_dbg(tegra->dev, ""entering ELPG failed\n""); pm_runtime_mark_last_busy(tegra->dev); } return err; }"
1212----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53221/bad/super.c----init_f2fs_fs,static int __init init_f2fs_fs(void) { int err; err = init_inodecache(); if (err) goto fail; err = f2fs_create_node_manager_caches(); if (err) goto free_inodecache; err = f2fs_create_segment_manager_caches(); if (err) goto free_node_manager_caches; err = f2fs_create_checkpoint_caches(); if (err) goto free_segment_manager_caches; err = f2fs_create_recovery_cache(); if (err) goto free_checkpoint_caches; err = f2fs_create_extent_cache(); if (err) goto free_recovery_cache; err = f2fs_create_garbage_collection_cache(); if (err) goto free_extent_cache; err = f2fs_init_sysfs(); if (err) goto free_garbage_collection_cache; err = f2fs_init_shrinker(); <S2SV_StartVul> if (err) <S2SV_EndVul> goto free_sysfs; <S2SV_StartVul> err = register_filesystem(&f2fs_fs_type); <S2SV_EndVul> <S2SV_StartVul> if (err) <S2SV_EndVul> <S2SV_StartVul> goto free_shrinker; <S2SV_EndVul> f2fs_create_root_stats(); err = f2fs_init_post_read_processing(); <S2SV_StartVul> if (err) <S2SV_EndVul> goto free_root_stats; err = f2fs_init_iostat_processing(); if (err) goto free_post_read; err = f2fs_init_bio_entry_cache(); if (err) goto free_iostat; err = f2fs_init_bioset(); if (err) goto free_bio_entry_cache; err = f2fs_init_compress_mempool(); if (err) goto free_bioset; err = f2fs_init_compress_cache(); if (err) goto free_compress_mempool; err = f2fs_create_casefold_cache(); <S2SV_StartVul> if (err) <S2SV_EndVul> goto free_compress_cache; return 0; free_compress_cache: f2fs_destroy_compress_cache(); free_compress_mempool: f2fs_destroy_compress_mempool(); free_bioset: f2fs_destroy_bioset(); free_bio_entry_cache: f2fs_destroy_bio_entry_cache(); free_iostat: f2fs_destroy_iostat_processing(); free_post_read: f2fs_destroy_post_read_processing(); free_root_stats: f2fs_destroy_root_stats(); <S2SV_StartVul> unregister_filesystem(&f2fs_fs_type); <S2SV_EndVul> <S2SV_StartVul> free_shrinker: <S2SV_EndVul> f2fs_exit_shrinker(); free_sysfs: f2fs_exit_sysfs(); free_garbage_collection_cache: f2fs_destroy_garbage_collection_cache(); free_extent_cache: f2fs_destroy_extent_cache(); free_recovery_cache: f2fs_destroy_recovery_cache(); free_checkpoint_caches: f2fs_destroy_checkpoint_caches(); free_segment_manager_caches: f2fs_destroy_segment_manager_caches(); free_node_manager_caches: f2fs_destroy_node_manager_caches(); free_inodecache: destroy_inodecache(); fail: return err; },"- if (err)
- err = register_filesystem(&f2fs_fs_type);
- if (err)
- goto free_shrinker;
- if (err)
- if (err)
- unregister_filesystem(&f2fs_fs_type);
- free_shrinker:
+ if (err)
+ if (err)
+ if (err)
+ err = register_filesystem(&f2fs_fs_type);
+ if (err)
+ goto free_casefold_cache;
+ free_casefold_cache:
+ f2fs_destroy_casefold_cache();",static int __init init_f2fs_fs(void) { int err; err = init_inodecache(); if (err) goto fail; err = f2fs_create_node_manager_caches(); if (err) goto free_inodecache; err = f2fs_create_segment_manager_caches(); if (err) goto free_node_manager_caches; err = f2fs_create_checkpoint_caches(); if (err) goto free_segment_manager_caches; err = f2fs_create_recovery_cache(); if (err) goto free_checkpoint_caches; err = f2fs_create_extent_cache(); if (err) goto free_recovery_cache; err = f2fs_create_garbage_collection_cache(); if (err) goto free_extent_cache; err = f2fs_init_sysfs(); if (err) goto free_garbage_collection_cache; err = f2fs_init_shrinker(); if (err) goto free_sysfs; f2fs_create_root_stats(); err = f2fs_init_post_read_processing(); if (err) goto free_root_stats; err = f2fs_init_iostat_processing(); if (err) goto free_post_read; err = f2fs_init_bio_entry_cache(); if (err) goto free_iostat; err = f2fs_init_bioset(); if (err) goto free_bio_entry_cache; err = f2fs_init_compress_mempool(); if (err) goto free_bioset; err = f2fs_init_compress_cache(); if (err) goto free_compress_mempool; err = f2fs_create_casefold_cache(); if (err) goto free_compress_cache; err = register_filesystem(&f2fs_fs_type); if (err) goto free_casefold_cache; return 0; free_casefold_cache: f2fs_destroy_casefold_cache(); free_compress_cache: f2fs_destroy_compress_cache(); free_compress_mempool: f2fs_destroy_compress_mempool(); free_bioset: f2fs_destroy_bioset(); free_bio_entry_cache: f2fs_destroy_bio_entry_cache(); free_iostat: f2fs_destroy_iostat_processing(); free_post_read: f2fs_destroy_post_read_processing(); free_root_stats: f2fs_destroy_root_stats(); f2fs_exit_shrinker(); free_sysfs: f2fs_exit_sysfs(); free_garbage_collection_cache: f2fs_destroy_garbage_collection_cache(); free_extent_cache: f2fs_destroy_extent_cache(); free_recovery_cache: f2fs_destroy_recovery_cache(); free_checkpoint_caches: f2fs_destroy_checkpoint_caches(); free_segment_manager_caches: f2fs_destroy_segment_manager_caches(); free_node_manager_caches: f2fs_destroy_node_manager_caches(); free_inodecache: destroy_inodecache(); fail: return err; }
709----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49991/bad/kfd_chardev.c----kfd_ioctl_create_queue,"static int kfd_ioctl_create_queue(struct file *filep, struct kfd_process *p, void *data) { struct kfd_ioctl_create_queue_args *args = data; struct kfd_node *dev; int err = 0; unsigned int queue_id; struct kfd_process_device *pdd; struct queue_properties q_properties; uint32_t doorbell_offset_in_process = 0; struct amdgpu_bo *wptr_bo = NULL; memset(&q_properties, 0, sizeof(struct queue_properties)); pr_debug(""Creating queue ioctl\n""); err = set_queue_properties_from_user(&q_properties, args); if (err) return err; pr_debug(""Looking for gpu id 0x%x\n"", args->gpu_id); mutex_lock(&p->mutex); pdd = kfd_process_device_data_by_id(p, args->gpu_id); if (!pdd) { pr_debug(""Could not find gpu id 0x%x\n"", args->gpu_id); err = -EINVAL; goto err_pdd; } dev = pdd->dev; pdd = kfd_bind_process_to_device(dev, p); if (IS_ERR(pdd)) { err = -ESRCH; goto err_bind_process; } if (!pdd->qpd.proc_doorbells) { err = kfd_alloc_process_doorbells(dev->kfd, pdd); if (err) { pr_debug(""failed to allocate process doorbells\n""); goto err_bind_process; } } if (dev->kfd->shared_resources.enable_mes && ((dev->adev->mes.sched_version & AMDGPU_MES_API_VERSION_MASK) >> AMDGPU_MES_API_VERSION_SHIFT) >= 2) { struct amdgpu_bo_va_mapping *wptr_mapping; struct amdgpu_vm *wptr_vm; wptr_vm = drm_priv_to_vm(pdd->drm_priv); err = amdgpu_bo_reserve(wptr_vm->root.bo, false); if (err) goto err_wptr_map_gart; wptr_mapping = amdgpu_vm_bo_lookup_mapping( wptr_vm, args->write_pointer_address >> PAGE_SHIFT); amdgpu_bo_unreserve(wptr_vm->root.bo); if (!wptr_mapping) { pr_err(""Failed to lookup wptr bo\n""); err = -EINVAL; goto err_wptr_map_gart; } wptr_bo = wptr_mapping->bo_va->base.bo; if (wptr_bo->tbo.base.size > PAGE_SIZE) { pr_err(""Requested GART mapping for wptr bo larger than one page\n""); err = -EINVAL; goto err_wptr_map_gart; } err = amdgpu_amdkfd_map_gtt_bo_to_gart(dev->adev, wptr_bo); if (err) { pr_err(""Failed to map wptr bo to GART\n""); goto err_wptr_map_gart; } } pr_debug(""Creating queue for PASID 0x%x on gpu 0x%x\n"", p->pasid, dev->id); err = pqm_create_queue(&p->pqm, dev, filep, &q_properties, &queue_id, wptr_bo, NULL, NULL, NULL, &doorbell_offset_in_process); if (err != 0) goto err_create_queue; args->queue_id = queue_id; args->doorbell_offset = KFD_MMAP_TYPE_DOORBELL; args->doorbell_offset |= KFD_MMAP_GPU_ID(args->gpu_id); if (KFD_IS_SOC15(dev)) args->doorbell_offset |= doorbell_offset_in_process; mutex_unlock(&p->mutex); pr_debug(""Queue id %d was created successfully\n"", args->queue_id); pr_debug(""Ring buffer address == 0x%016llX\n"", args->ring_base_address); pr_debug(""Read ptr address == 0x%016llX\n"", args->read_pointer_address); pr_debug(""Write ptr address == 0x%016llX\n"", args->write_pointer_address); kfd_dbg_ev_raise(KFD_EC_MASK(EC_QUEUE_NEW), p, dev, queue_id, false, NULL, 0); return 0; err_create_queue: if (wptr_bo) <S2SV_StartVul> amdgpu_amdkfd_free_gtt_mem(dev->adev, wptr_bo); <S2SV_EndVul> err_wptr_map_gart: err_bind_process: err_pdd: mutex_unlock(&p->mutex); return err; }","- amdgpu_amdkfd_free_gtt_mem(dev->adev, wptr_bo);
+ amdgpu_amdkfd_free_gtt_mem(dev->adev, (void **)&wptr_bo);","static int kfd_ioctl_create_queue(struct file *filep, struct kfd_process *p, void *data) { struct kfd_ioctl_create_queue_args *args = data; struct kfd_node *dev; int err = 0; unsigned int queue_id; struct kfd_process_device *pdd; struct queue_properties q_properties; uint32_t doorbell_offset_in_process = 0; struct amdgpu_bo *wptr_bo = NULL; memset(&q_properties, 0, sizeof(struct queue_properties)); pr_debug(""Creating queue ioctl\n""); err = set_queue_properties_from_user(&q_properties, args); if (err) return err; pr_debug(""Looking for gpu id 0x%x\n"", args->gpu_id); mutex_lock(&p->mutex); pdd = kfd_process_device_data_by_id(p, args->gpu_id); if (!pdd) { pr_debug(""Could not find gpu id 0x%x\n"", args->gpu_id); err = -EINVAL; goto err_pdd; } dev = pdd->dev; pdd = kfd_bind_process_to_device(dev, p); if (IS_ERR(pdd)) { err = -ESRCH; goto err_bind_process; } if (!pdd->qpd.proc_doorbells) { err = kfd_alloc_process_doorbells(dev->kfd, pdd); if (err) { pr_debug(""failed to allocate process doorbells\n""); goto err_bind_process; } } if (dev->kfd->shared_resources.enable_mes && ((dev->adev->mes.sched_version & AMDGPU_MES_API_VERSION_MASK) >> AMDGPU_MES_API_VERSION_SHIFT) >= 2) { struct amdgpu_bo_va_mapping *wptr_mapping; struct amdgpu_vm *wptr_vm; wptr_vm = drm_priv_to_vm(pdd->drm_priv); err = amdgpu_bo_reserve(wptr_vm->root.bo, false); if (err) goto err_wptr_map_gart; wptr_mapping = amdgpu_vm_bo_lookup_mapping( wptr_vm, args->write_pointer_address >> PAGE_SHIFT); amdgpu_bo_unreserve(wptr_vm->root.bo); if (!wptr_mapping) { pr_err(""Failed to lookup wptr bo\n""); err = -EINVAL; goto err_wptr_map_gart; } wptr_bo = wptr_mapping->bo_va->base.bo; if (wptr_bo->tbo.base.size > PAGE_SIZE) { pr_err(""Requested GART mapping for wptr bo larger than one page\n""); err = -EINVAL; goto err_wptr_map_gart; } err = amdgpu_amdkfd_map_gtt_bo_to_gart(dev->adev, wptr_bo); if (err) { pr_err(""Failed to map wptr bo to GART\n""); goto err_wptr_map_gart; } } pr_debug(""Creating queue for PASID 0x%x on gpu 0x%x\n"", p->pasid, dev->id); err = pqm_create_queue(&p->pqm, dev, filep, &q_properties, &queue_id, wptr_bo, NULL, NULL, NULL, &doorbell_offset_in_process); if (err != 0) goto err_create_queue; args->queue_id = queue_id; args->doorbell_offset = KFD_MMAP_TYPE_DOORBELL; args->doorbell_offset |= KFD_MMAP_GPU_ID(args->gpu_id); if (KFD_IS_SOC15(dev)) args->doorbell_offset |= doorbell_offset_in_process; mutex_unlock(&p->mutex); pr_debug(""Queue id %d was created successfully\n"", args->queue_id); pr_debug(""Ring buffer address == 0x%016llX\n"", args->ring_base_address); pr_debug(""Read ptr address == 0x%016llX\n"", args->read_pointer_address); pr_debug(""Write ptr address == 0x%016llX\n"", args->write_pointer_address); kfd_dbg_ev_raise(KFD_EC_MASK(EC_QUEUE_NEW), p, dev, queue_id, false, NULL, 0); return 0; err_create_queue: if (wptr_bo) amdgpu_amdkfd_free_gtt_mem(dev->adev, (void **)&wptr_bo); err_wptr_map_gart: err_bind_process: err_pdd: mutex_unlock(&p->mutex); return err; }"
1338----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56655/bad/nf_tables_api.c----nf_tables_newtable,"static int nf_tables_newtable(struct sk_buff *skb, const struct nfnl_info *info, const struct nlattr * const nla[]) { struct nftables_pernet *nft_net = nft_pernet(info->net); struct netlink_ext_ack *extack = info->extack; u8 genmask = nft_genmask_next(info->net); u8 family = info->nfmsg->nfgen_family; struct net *net = info->net; const struct nlattr *attr; struct nft_table *table; struct nft_ctx ctx; u32 flags = 0; int err; if (!nft_supported_family(family)) return -EOPNOTSUPP; lockdep_assert_held(&nft_net->commit_mutex); attr = nla[NFTA_TABLE_NAME]; table = nft_table_lookup(net, attr, family, genmask, NETLINK_CB(skb).portid); if (IS_ERR(table)) { if (PTR_ERR(table) != -ENOENT) return PTR_ERR(table); } else { if (info->nlh->nlmsg_flags & NLM_F_EXCL) { NL_SET_BAD_ATTR(extack, attr); return -EEXIST; } if (info->nlh->nlmsg_flags & NLM_F_REPLACE) return -EOPNOTSUPP; nft_ctx_init(&ctx, net, skb, info->nlh, family, table, NULL, nla); return nf_tables_updtable(&ctx); } if (nla[NFTA_TABLE_FLAGS]) { flags = ntohl(nla_get_be32(nla[NFTA_TABLE_FLAGS])); if (flags & ~NFT_TABLE_F_MASK) return -EOPNOTSUPP; } err = -ENOMEM; table = kzalloc(sizeof(*table), GFP_KERNEL_ACCOUNT); if (table == NULL) goto err_kzalloc; table->validate_state = nft_net->validate_state; table->name = nla_strdup(attr, GFP_KERNEL_ACCOUNT); if (table->name == NULL) goto err_strdup; if (nla[NFTA_TABLE_USERDATA]) { table->udata = nla_memdup(nla[NFTA_TABLE_USERDATA], GFP_KERNEL_ACCOUNT); if (table->udata == NULL) goto err_table_udata; table->udlen = nla_len(nla[NFTA_TABLE_USERDATA]); } err = rhltable_init(&table->chains_ht, &nft_chain_ht_params); if (err) goto err_chain_ht; INIT_LIST_HEAD(&table->chains); INIT_LIST_HEAD(&table->sets); INIT_LIST_HEAD(&table->objects); INIT_LIST_HEAD(&table->flowtables); <S2SV_StartVul> write_pnet(&table->net, net); <S2SV_EndVul> table->family = family; table->flags = flags; table->handle = ++nft_net->table_handle; if (table->flags & NFT_TABLE_F_OWNER) table->nlpid = NETLINK_CB(skb).portid; nft_ctx_init(&ctx, net, skb, info->nlh, family, table, NULL, nla); err = nft_trans_table_add(&ctx, NFT_MSG_NEWTABLE); if (err < 0) goto err_trans; list_add_tail_rcu(&table->list, &nft_net->tables); return 0; err_trans: rhltable_destroy(&table->chains_ht); err_chain_ht: kfree(table->udata); err_table_udata: kfree(table->name); err_strdup: kfree(table); err_kzalloc: return err; }","- write_pnet(&table->net, net);","static int nf_tables_newtable(struct sk_buff *skb, const struct nfnl_info *info, const struct nlattr * const nla[]) { struct nftables_pernet *nft_net = nft_pernet(info->net); struct netlink_ext_ack *extack = info->extack; u8 genmask = nft_genmask_next(info->net); u8 family = info->nfmsg->nfgen_family; struct net *net = info->net; const struct nlattr *attr; struct nft_table *table; struct nft_ctx ctx; u32 flags = 0; int err; if (!nft_supported_family(family)) return -EOPNOTSUPP; lockdep_assert_held(&nft_net->commit_mutex); attr = nla[NFTA_TABLE_NAME]; table = nft_table_lookup(net, attr, family, genmask, NETLINK_CB(skb).portid); if (IS_ERR(table)) { if (PTR_ERR(table) != -ENOENT) return PTR_ERR(table); } else { if (info->nlh->nlmsg_flags & NLM_F_EXCL) { NL_SET_BAD_ATTR(extack, attr); return -EEXIST; } if (info->nlh->nlmsg_flags & NLM_F_REPLACE) return -EOPNOTSUPP; nft_ctx_init(&ctx, net, skb, info->nlh, family, table, NULL, nla); return nf_tables_updtable(&ctx); } if (nla[NFTA_TABLE_FLAGS]) { flags = ntohl(nla_get_be32(nla[NFTA_TABLE_FLAGS])); if (flags & ~NFT_TABLE_F_MASK) return -EOPNOTSUPP; } err = -ENOMEM; table = kzalloc(sizeof(*table), GFP_KERNEL_ACCOUNT); if (table == NULL) goto err_kzalloc; table->validate_state = nft_net->validate_state; table->name = nla_strdup(attr, GFP_KERNEL_ACCOUNT); if (table->name == NULL) goto err_strdup; if (nla[NFTA_TABLE_USERDATA]) { table->udata = nla_memdup(nla[NFTA_TABLE_USERDATA], GFP_KERNEL_ACCOUNT); if (table->udata == NULL) goto err_table_udata; table->udlen = nla_len(nla[NFTA_TABLE_USERDATA]); } err = rhltable_init(&table->chains_ht, &nft_chain_ht_params); if (err) goto err_chain_ht; INIT_LIST_HEAD(&table->chains); INIT_LIST_HEAD(&table->sets); INIT_LIST_HEAD(&table->objects); INIT_LIST_HEAD(&table->flowtables); table->family = family; table->flags = flags; table->handle = ++nft_net->table_handle; if (table->flags & NFT_TABLE_F_OWNER) table->nlpid = NETLINK_CB(skb).portid; nft_ctx_init(&ctx, net, skb, info->nlh, family, table, NULL, nla); err = nft_trans_table_add(&ctx, NFT_MSG_NEWTABLE); if (err < 0) goto err_trans; list_add_tail_rcu(&table->list, &nft_net->tables); return 0; err_trans: rhltable_destroy(&table->chains_ht); err_chain_ht: kfree(table->udata); err_table_udata: kfree(table->name); err_strdup: kfree(table); err_kzalloc: return err; }"
630----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49936/bad/hash.c----xenvif_flush_hash,"static void xenvif_flush_hash(struct xenvif *vif) { struct xenvif_hash_cache_entry *entry; unsigned long flags; if (xenvif_hash_cache_size == 0) return; spin_lock_irqsave(&vif->hash.cache.lock, flags); <S2SV_StartVul> list_for_each_entry_rcu(entry, &vif->hash.cache.list, link, <S2SV_EndVul> <S2SV_StartVul> lockdep_is_held(&vif->hash.cache.lock)) { <S2SV_EndVul> list_del_rcu(&entry->link); vif->hash.cache.count--; kfree_rcu(entry, rcu); } spin_unlock_irqrestore(&vif->hash.cache.lock, flags); }","- list_for_each_entry_rcu(entry, &vif->hash.cache.list, link,
- lockdep_is_held(&vif->hash.cache.lock)) {
+ list_for_each_entry_safe(entry, n, &vif->hash.cache.list, link) {","static void xenvif_flush_hash(struct xenvif *vif) { struct xenvif_hash_cache_entry *entry, *n; unsigned long flags; if (xenvif_hash_cache_size == 0) return; spin_lock_irqsave(&vif->hash.cache.lock, flags); list_for_each_entry_safe(entry, n, &vif->hash.cache.list, link) { list_del_rcu(&entry->link); vif->hash.cache.count--; kfree_rcu(entry, rcu); } spin_unlock_irqrestore(&vif->hash.cache.lock, flags); }"
1164----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53138/bad/ktls_tx.c----mlx5e_ktls_tx_handle_ooo,"mlx5e_ktls_tx_handle_ooo(struct mlx5e_ktls_offload_context_tx *priv_tx, struct mlx5e_txqsq *sq, int datalen, u32 seq) { enum mlx5e_ktls_sync_retval ret; struct tx_sync_info info = {}; int i; ret = tx_sync_info_get(priv_tx, seq, datalen, &info); if (unlikely(ret != MLX5E_KTLS_SYNC_DONE)) return ret; tx_post_resync_params(sq, priv_tx, info.rcd_sn); for (i = 0; i < info.nr_frags; i++) { unsigned int orig_fsz, frag_offset = 0, n = 0; skb_frag_t *f = &info.frags[i]; orig_fsz = skb_frag_size(f); do { unsigned int fsz; n++; fsz = min_t(unsigned int, sq->hw_mtu, orig_fsz - frag_offset); skb_frag_size_set(f, fsz); if (tx_post_resync_dump(sq, f, priv_tx->tisn)) { page_ref_add(skb_frag_page(f), n - 1); goto err_out; } skb_frag_off_add(f, fsz); frag_offset += fsz; } while (frag_offset < orig_fsz); page_ref_add(skb_frag_page(f), n - 1); } return MLX5E_KTLS_SYNC_DONE; err_out: for (; i < info.nr_frags; i++) <S2SV_StartVul> put_page(skb_frag_page(&info.frags[i])); <S2SV_EndVul> return MLX5E_KTLS_SYNC_FAIL; }","- put_page(skb_frag_page(&info.frags[i]));
+ page_ref_dec(skb_frag_page(&info.frags[i]));","mlx5e_ktls_tx_handle_ooo(struct mlx5e_ktls_offload_context_tx *priv_tx, struct mlx5e_txqsq *sq, int datalen, u32 seq) { enum mlx5e_ktls_sync_retval ret; struct tx_sync_info info = {}; int i; ret = tx_sync_info_get(priv_tx, seq, datalen, &info); if (unlikely(ret != MLX5E_KTLS_SYNC_DONE)) return ret; tx_post_resync_params(sq, priv_tx, info.rcd_sn); for (i = 0; i < info.nr_frags; i++) { unsigned int orig_fsz, frag_offset = 0, n = 0; skb_frag_t *f = &info.frags[i]; orig_fsz = skb_frag_size(f); do { unsigned int fsz; n++; fsz = min_t(unsigned int, sq->hw_mtu, orig_fsz - frag_offset); skb_frag_size_set(f, fsz); if (tx_post_resync_dump(sq, f, priv_tx->tisn)) { page_ref_add(skb_frag_page(f), n - 1); goto err_out; } skb_frag_off_add(f, fsz); frag_offset += fsz; } while (frag_offset < orig_fsz); page_ref_add(skb_frag_page(f), n - 1); } return MLX5E_KTLS_SYNC_DONE; err_out: for (; i < info.nr_frags; i++) page_ref_dec(skb_frag_page(&info.frags[i])); return MLX5E_KTLS_SYNC_FAIL; }"
641----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49943/bad/xe_guc_submit.c----guc_submit_wedged_fini,"static void guc_submit_wedged_fini(void *arg) { struct xe_guc *guc = arg; struct xe_exec_queue *q; unsigned long index; <S2SV_StartVul> xa_for_each(&guc->submission_state.exec_queue_lookup, index, q) <S2SV_EndVul> <S2SV_StartVul> if (exec_queue_wedged(q)) <S2SV_EndVul> xe_exec_queue_put(q); }","- xa_for_each(&guc->submission_state.exec_queue_lookup, index, q)
- if (exec_queue_wedged(q))
+ mutex_lock(&guc->submission_state.lock);
+ xa_for_each(&guc->submission_state.exec_queue_lookup, index, q) {
+ if (exec_queue_wedged(q)) {
+ mutex_unlock(&guc->submission_state.lock);
+ mutex_lock(&guc->submission_state.lock);
+ }
+ }
+ mutex_unlock(&guc->submission_state.lock);
+ }","static void guc_submit_wedged_fini(void *arg) { struct xe_guc *guc = arg; struct xe_exec_queue *q; unsigned long index; mutex_lock(&guc->submission_state.lock); xa_for_each(&guc->submission_state.exec_queue_lookup, index, q) { if (exec_queue_wedged(q)) { mutex_unlock(&guc->submission_state.lock); xe_exec_queue_put(q); mutex_lock(&guc->submission_state.lock); } } mutex_unlock(&guc->submission_state.lock); }"
1372----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56704/bad/trans_xen.c----xen_9pfs_front_free,"static void xen_9pfs_front_free(struct xen_9pfs_front_priv *priv) { int i, j; write_lock(&xen_9pfs_lock); list_del(&priv->list); write_unlock(&xen_9pfs_lock); for (i = 0; i < priv->num_rings; i++) { struct xen_9pfs_dataring *ring = &priv->rings[i]; cancel_work_sync(&ring->work); if (!priv->rings[i].intf) break; if (priv->rings[i].irq > 0) <S2SV_StartVul> unbind_from_irqhandler(priv->rings[i].irq, priv->dev); <S2SV_EndVul> if (priv->rings[i].data.in) { for (j = 0; j < (1 << priv->rings[i].intf->ring_order); j++) { grant_ref_t ref; ref = priv->rings[i].intf->ref[j]; gnttab_end_foreign_access(ref, NULL); } free_pages_exact(priv->rings[i].data.in, 1UL << (priv->rings[i].intf->ring_order + XEN_PAGE_SHIFT)); } gnttab_end_foreign_access(priv->rings[i].ref, NULL); free_page((unsigned long)priv->rings[i].intf); } kfree(priv->rings); kfree(priv->tag); kfree(priv); }","- unbind_from_irqhandler(priv->rings[i].irq, priv->dev);
+ unbind_from_irqhandler(priv->rings[i].irq, ring);","static void xen_9pfs_front_free(struct xen_9pfs_front_priv *priv) { int i, j; write_lock(&xen_9pfs_lock); list_del(&priv->list); write_unlock(&xen_9pfs_lock); for (i = 0; i < priv->num_rings; i++) { struct xen_9pfs_dataring *ring = &priv->rings[i]; cancel_work_sync(&ring->work); if (!priv->rings[i].intf) break; if (priv->rings[i].irq > 0) unbind_from_irqhandler(priv->rings[i].irq, ring); if (priv->rings[i].data.in) { for (j = 0; j < (1 << priv->rings[i].intf->ring_order); j++) { grant_ref_t ref; ref = priv->rings[i].intf->ref[j]; gnttab_end_foreign_access(ref, NULL); } free_pages_exact(priv->rings[i].data.in, 1UL << (priv->rings[i].intf->ring_order + XEN_PAGE_SHIFT)); } gnttab_end_foreign_access(priv->rings[i].ref, NULL); free_page((unsigned long)priv->rings[i].intf); } kfree(priv->rings); kfree(priv->tag); kfree(priv); }"
486----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47729/bad/xe_migrate.c----*xe_migrate_init,"struct xe_migrate *xe_migrate_init(struct xe_tile *tile) { struct xe_device *xe = tile_to_xe(tile); struct xe_gt *primary_gt = tile->primary_gt; struct xe_migrate *m; struct xe_vm *vm; int err; m = drmm_kzalloc(&xe->drm, sizeof(*m), GFP_KERNEL); if (!m) return ERR_PTR(-ENOMEM); m->tile = tile; vm = xe_vm_create(xe, XE_VM_FLAG_MIGRATION | XE_VM_FLAG_SET_TILE_ID(tile)); if (IS_ERR(vm)) return ERR_CAST(vm); xe_vm_lock(vm, false); err = xe_migrate_prepare_vm(tile, m, vm); xe_vm_unlock(vm); if (err) { xe_vm_close_and_put(vm); return ERR_PTR(err); } if (xe->info.has_usm) { struct xe_hw_engine *hwe = xe_gt_hw_engine(primary_gt, XE_ENGINE_CLASS_COPY, primary_gt->usm.reserved_bcs_instance, false); u32 logical_mask = xe_migrate_usm_logical_mask(primary_gt); if (!hwe || !logical_mask) return ERR_PTR(-EINVAL); m->q = xe_exec_queue_create(xe, vm, logical_mask, 1, hwe, EXEC_QUEUE_FLAG_KERNEL | EXEC_QUEUE_FLAG_PERMANENT | EXEC_QUEUE_FLAG_HIGH_PRIORITY, 0); } else { m->q = xe_exec_queue_create_class(xe, primary_gt, vm, XE_ENGINE_CLASS_COPY, EXEC_QUEUE_FLAG_KERNEL | <S2SV_StartVul> EXEC_QUEUE_FLAG_PERMANENT); <S2SV_EndVul> } if (IS_ERR(m->q)) { xe_vm_close_and_put(vm); return ERR_CAST(m->q); } mutex_init(&m->job_mutex); fs_reclaim_acquire(GFP_KERNEL); might_lock(&m->job_mutex); fs_reclaim_release(GFP_KERNEL); err = drmm_add_action_or_reset(&xe->drm, xe_migrate_fini, m); if (err) return ERR_PTR(err); if (IS_DGFX(xe)) { if (xe_device_has_flat_ccs(xe)) m->min_chunk_size = SZ_4K * SZ_64K / xe_device_ccs_bytes(xe, SZ_64K); else m->min_chunk_size = SZ_64K; m->min_chunk_size = roundup_pow_of_two(m->min_chunk_size); drm_dbg(&xe->drm, ""Migrate min chunk size is 0x%08llx\n"", (unsigned long long)m->min_chunk_size); } return m; }","- EXEC_QUEUE_FLAG_PERMANENT);
+ EXEC_QUEUE_FLAG_PERMANENT, 0);","struct xe_migrate *xe_migrate_init(struct xe_tile *tile) { struct xe_device *xe = tile_to_xe(tile); struct xe_gt *primary_gt = tile->primary_gt; struct xe_migrate *m; struct xe_vm *vm; int err; m = drmm_kzalloc(&xe->drm, sizeof(*m), GFP_KERNEL); if (!m) return ERR_PTR(-ENOMEM); m->tile = tile; vm = xe_vm_create(xe, XE_VM_FLAG_MIGRATION | XE_VM_FLAG_SET_TILE_ID(tile)); if (IS_ERR(vm)) return ERR_CAST(vm); xe_vm_lock(vm, false); err = xe_migrate_prepare_vm(tile, m, vm); xe_vm_unlock(vm); if (err) { xe_vm_close_and_put(vm); return ERR_PTR(err); } if (xe->info.has_usm) { struct xe_hw_engine *hwe = xe_gt_hw_engine(primary_gt, XE_ENGINE_CLASS_COPY, primary_gt->usm.reserved_bcs_instance, false); u32 logical_mask = xe_migrate_usm_logical_mask(primary_gt); if (!hwe || !logical_mask) return ERR_PTR(-EINVAL); m->q = xe_exec_queue_create(xe, vm, logical_mask, 1, hwe, EXEC_QUEUE_FLAG_KERNEL | EXEC_QUEUE_FLAG_PERMANENT | EXEC_QUEUE_FLAG_HIGH_PRIORITY, 0); } else { m->q = xe_exec_queue_create_class(xe, primary_gt, vm, XE_ENGINE_CLASS_COPY, EXEC_QUEUE_FLAG_KERNEL | EXEC_QUEUE_FLAG_PERMANENT, 0); } if (IS_ERR(m->q)) { xe_vm_close_and_put(vm); return ERR_CAST(m->q); } mutex_init(&m->job_mutex); fs_reclaim_acquire(GFP_KERNEL); might_lock(&m->job_mutex); fs_reclaim_release(GFP_KERNEL); err = drmm_add_action_or_reset(&xe->drm, xe_migrate_fini, m); if (err) return ERR_PTR(err); if (IS_DGFX(xe)) { if (xe_device_has_flat_ccs(xe)) m->min_chunk_size = SZ_4K * SZ_64K / xe_device_ccs_bytes(xe, SZ_64K); else m->min_chunk_size = SZ_64K; m->min_chunk_size = roundup_pow_of_two(m->min_chunk_size); drm_dbg(&xe->drm, ""Migrate min chunk size is 0x%08llx\n"", (unsigned long long)m->min_chunk_size); } return m; }"
1413----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56742/bad/cmd.c----mlx5vf_add_migration_pages,"static int mlx5vf_add_migration_pages(struct mlx5_vhca_data_buffer *buf, unsigned int npages) { unsigned int to_alloc = npages; struct page **page_list; unsigned long filled; unsigned int to_fill; int ret; to_fill = min_t(unsigned int, npages, PAGE_SIZE / sizeof(*page_list)); page_list = kvzalloc(to_fill * sizeof(*page_list), GFP_KERNEL_ACCOUNT); if (!page_list) return -ENOMEM; do { filled = alloc_pages_bulk_array(GFP_KERNEL_ACCOUNT, to_fill, page_list); if (!filled) { ret = -ENOMEM; goto err; } to_alloc -= filled; ret = sg_alloc_append_table_from_pages( &buf->table, page_list, filled, 0, filled << PAGE_SHIFT, UINT_MAX, SG_MAX_SINGLE_ALLOC, GFP_KERNEL_ACCOUNT); if (ret) <S2SV_StartVul> goto err; <S2SV_EndVul> buf->allocated_length += filled * PAGE_SIZE; memset(page_list, 0, filled * sizeof(*page_list)); to_fill = min_t(unsigned int, to_alloc, PAGE_SIZE / sizeof(*page_list)); } while (to_alloc > 0); kvfree(page_list); return 0; err: kvfree(page_list); return ret; }","- goto err;
+ int i;
+ goto err_append;
+ err_append:
+ for (i = filled - 1; i >= 0; i--)
+ __free_page(page_list[i]);","static int mlx5vf_add_migration_pages(struct mlx5_vhca_data_buffer *buf, unsigned int npages) { unsigned int to_alloc = npages; struct page **page_list; unsigned long filled; unsigned int to_fill; int ret; int i; to_fill = min_t(unsigned int, npages, PAGE_SIZE / sizeof(*page_list)); page_list = kvzalloc(to_fill * sizeof(*page_list), GFP_KERNEL_ACCOUNT); if (!page_list) return -ENOMEM; do { filled = alloc_pages_bulk_array(GFP_KERNEL_ACCOUNT, to_fill, page_list); if (!filled) { ret = -ENOMEM; goto err; } to_alloc -= filled; ret = sg_alloc_append_table_from_pages( &buf->table, page_list, filled, 0, filled << PAGE_SHIFT, UINT_MAX, SG_MAX_SINGLE_ALLOC, GFP_KERNEL_ACCOUNT); if (ret) goto err_append; buf->allocated_length += filled * PAGE_SIZE; memset(page_list, 0, filled * sizeof(*page_list)); to_fill = min_t(unsigned int, to_alloc, PAGE_SIZE / sizeof(*page_list)); } while (to_alloc > 0); kvfree(page_list); return 0; err_append: for (i = filled - 1; i >= 0; i--) __free_page(page_list[i]); err: kvfree(page_list); return ret; }"
340----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46801/bad/libfs.c----path_from_stashed,"int path_from_stashed(struct dentry **stashed, struct vfsmount *mnt, void *data, struct path *path) { struct dentry *dentry; const struct stashed_operations *sops = mnt->mnt_sb->s_fs_info; <S2SV_StartVul> path->dentry = get_stashed_dentry(*stashed); <S2SV_EndVul> if (path->dentry) { sops->put_data(data); goto out_path; } dentry = prepare_anon_dentry(stashed, mnt->mnt_sb, data); if (IS_ERR(dentry)) return PTR_ERR(dentry); path->dentry = stash_dentry(stashed, dentry); if (path->dentry != dentry) dput(dentry); out_path: WARN_ON_ONCE(path->dentry->d_fsdata != stashed); WARN_ON_ONCE(d_inode(path->dentry)->i_private != data); path->mnt = mntget(mnt); return 0; }","- path->dentry = get_stashed_dentry(*stashed);
+ path->dentry = get_stashed_dentry(stashed);","int path_from_stashed(struct dentry **stashed, struct vfsmount *mnt, void *data, struct path *path) { struct dentry *dentry; const struct stashed_operations *sops = mnt->mnt_sb->s_fs_info; path->dentry = get_stashed_dentry(stashed); if (path->dentry) { sops->put_data(data); goto out_path; } dentry = prepare_anon_dentry(stashed, mnt->mnt_sb, data); if (IS_ERR(dentry)) return PTR_ERR(dentry); path->dentry = stash_dentry(stashed, dentry); if (path->dentry != dentry) dput(dentry); out_path: WARN_ON_ONCE(path->dentry->d_fsdata != stashed); WARN_ON_ONCE(d_inode(path->dentry)->i_private != data); path->mnt = mntget(mnt); return 0; }"
898----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50146/bad/en_main.c----_mlx5e_remove,"static void _mlx5e_remove(struct auxiliary_device *adev) { struct mlx5_adev *edev = container_of(adev, struct mlx5_adev, adev); struct mlx5e_dev *mlx5e_dev = auxiliary_get_drvdata(adev); struct mlx5e_priv *priv = mlx5e_dev->priv; struct mlx5_core_dev *mdev = edev->mdev; mlx5_core_uplink_netdev_set(mdev, NULL); mlx5e_dcbnl_delete_app(priv); unregister_netdev(priv->netdev); _mlx5e_suspend(adev, false); <S2SV_StartVul> priv->profile->cleanup(priv); <S2SV_EndVul> mlx5e_destroy_netdev(priv); mlx5e_devlink_port_unregister(mlx5e_dev); mlx5e_destroy_devlink(mlx5e_dev); }","- priv->profile->cleanup(priv);
+ if (priv->profile)
+ priv->profile->cleanup(priv);","static void _mlx5e_remove(struct auxiliary_device *adev) { struct mlx5_adev *edev = container_of(adev, struct mlx5_adev, adev); struct mlx5e_dev *mlx5e_dev = auxiliary_get_drvdata(adev); struct mlx5e_priv *priv = mlx5e_dev->priv; struct mlx5_core_dev *mdev = edev->mdev; mlx5_core_uplink_netdev_set(mdev, NULL); mlx5e_dcbnl_delete_app(priv); unregister_netdev(priv->netdev); _mlx5e_suspend(adev, false); if (priv->profile) priv->profile->cleanup(priv); mlx5e_destroy_netdev(priv); mlx5e_devlink_port_unregister(mlx5e_dev); mlx5e_destroy_devlink(mlx5e_dev); }"
835----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50100/bad/dummy_hcd.c----dummy_urb_enqueue,"static int dummy_urb_enqueue( struct usb_hcd *hcd, struct urb *urb, gfp_t mem_flags ) { struct dummy_hcd *dum_hcd; struct urbp *urbp; unsigned long flags; int rc; urbp = kmalloc(sizeof *urbp, mem_flags); if (!urbp) return -ENOMEM; urbp->urb = urb; urbp->miter_started = 0; dum_hcd = hcd_to_dummy_hcd(hcd); spin_lock_irqsave(&dum_hcd->dum->lock, flags); rc = dummy_validate_stream(dum_hcd, urb); if (rc) { kfree(urbp); goto done; } rc = usb_hcd_link_urb_to_ep(hcd, urb); if (rc) { kfree(urbp); goto done; } if (!dum_hcd->udev) { dum_hcd->udev = urb->dev; usb_get_dev(dum_hcd->udev); } else if (unlikely(dum_hcd->udev != urb->dev)) dev_err(dummy_dev(dum_hcd), ""usb_device address has changed!\n""); list_add_tail(&urbp->urbp_list, &dum_hcd->urbp_list); urb->hcpriv = urbp; if (!dum_hcd->next_frame_urbp) dum_hcd->next_frame_urbp = urbp; if (usb_pipetype(urb->pipe) == PIPE_CONTROL) urb->error_count = 1; <S2SV_StartVul> if (!hrtimer_active(&dum_hcd->timer)) <S2SV_EndVul> hrtimer_start(&dum_hcd->timer, ns_to_ktime(DUMMY_TIMER_INT_NSECS), HRTIMER_MODE_REL_SOFT); done: spin_unlock_irqrestore(&dum_hcd->dum->lock, flags); return rc; }","- if (!hrtimer_active(&dum_hcd->timer))
+ if (!dum_hcd->timer_pending) {
+ dum_hcd->timer_pending = 1;
+ }","static int dummy_urb_enqueue( struct usb_hcd *hcd, struct urb *urb, gfp_t mem_flags ) { struct dummy_hcd *dum_hcd; struct urbp *urbp; unsigned long flags; int rc; urbp = kmalloc(sizeof *urbp, mem_flags); if (!urbp) return -ENOMEM; urbp->urb = urb; urbp->miter_started = 0; dum_hcd = hcd_to_dummy_hcd(hcd); spin_lock_irqsave(&dum_hcd->dum->lock, flags); rc = dummy_validate_stream(dum_hcd, urb); if (rc) { kfree(urbp); goto done; } rc = usb_hcd_link_urb_to_ep(hcd, urb); if (rc) { kfree(urbp); goto done; } if (!dum_hcd->udev) { dum_hcd->udev = urb->dev; usb_get_dev(dum_hcd->udev); } else if (unlikely(dum_hcd->udev != urb->dev)) dev_err(dummy_dev(dum_hcd), ""usb_device address has changed!\n""); list_add_tail(&urbp->urbp_list, &dum_hcd->urbp_list); urb->hcpriv = urbp; if (!dum_hcd->next_frame_urbp) dum_hcd->next_frame_urbp = urbp; if (usb_pipetype(urb->pipe) == PIPE_CONTROL) urb->error_count = 1; if (!dum_hcd->timer_pending) { dum_hcd->timer_pending = 1; hrtimer_start(&dum_hcd->timer, ns_to_ktime(DUMMY_TIMER_INT_NSECS), HRTIMER_MODE_REL_SOFT); } done: spin_unlock_irqrestore(&dum_hcd->dum->lock, flags); return rc; }"
180----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-45029/bad/i2c-tegra.c----tegra_i2c_probe,"static int tegra_i2c_probe(struct platform_device *pdev) { struct tegra_i2c_dev *i2c_dev; struct resource *res; int err; i2c_dev = devm_kzalloc(&pdev->dev, sizeof(*i2c_dev), GFP_KERNEL); if (!i2c_dev) return -ENOMEM; platform_set_drvdata(pdev, i2c_dev); init_completion(&i2c_dev->msg_complete); init_completion(&i2c_dev->dma_complete); i2c_dev->hw = device_get_match_data(&pdev->dev); i2c_dev->cont_id = pdev->id; i2c_dev->dev = &pdev->dev; i2c_dev->base = devm_platform_get_and_ioremap_resource(pdev, 0, &res); if (IS_ERR(i2c_dev->base)) return PTR_ERR(i2c_dev->base); i2c_dev->base_phys = res->start; err = platform_get_irq(pdev, 0); if (err < 0) return err; i2c_dev->irq = err; irq_set_status_flags(i2c_dev->irq, IRQ_NOAUTOEN); err = devm_request_threaded_irq(i2c_dev->dev, i2c_dev->irq, NULL, tegra_i2c_isr, IRQF_NO_SUSPEND | IRQF_ONESHOT, dev_name(i2c_dev->dev), i2c_dev); if (err) return err; tegra_i2c_parse_dt(i2c_dev); err = tegra_i2c_init_reset(i2c_dev); if (err) return err; err = tegra_i2c_init_clocks(i2c_dev); if (err) return err; err = tegra_i2c_init_dma(i2c_dev); if (err) goto release_clocks; <S2SV_StartVul> if (!IS_VI(i2c_dev)) <S2SV_EndVul> pm_runtime_irq_safe(i2c_dev->dev); pm_runtime_enable(i2c_dev->dev); err = tegra_i2c_init_hardware(i2c_dev); if (err) goto release_rpm; i2c_set_adapdata(&i2c_dev->adapter, i2c_dev); i2c_dev->adapter.dev.of_node = i2c_dev->dev->of_node; i2c_dev->adapter.dev.parent = i2c_dev->dev; i2c_dev->adapter.retries = 1; i2c_dev->adapter.timeout = 6 * HZ; i2c_dev->adapter.quirks = i2c_dev->hw->quirks; i2c_dev->adapter.owner = THIS_MODULE; i2c_dev->adapter.class = I2C_CLASS_DEPRECATED; i2c_dev->adapter.algo = &tegra_i2c_algo; i2c_dev->adapter.nr = pdev->id; ACPI_COMPANION_SET(&i2c_dev->adapter.dev, ACPI_COMPANION(&pdev->dev)); if (i2c_dev->hw->supports_bus_clear) i2c_dev->adapter.bus_recovery_info = &tegra_i2c_recovery_info; strscpy(i2c_dev->adapter.name, dev_name(i2c_dev->dev), sizeof(i2c_dev->adapter.name)); err = i2c_add_numbered_adapter(&i2c_dev->adapter); if (err) goto release_rpm; return 0; release_rpm: pm_runtime_disable(i2c_dev->dev); tegra_i2c_release_dma(i2c_dev); release_clocks: tegra_i2c_release_clocks(i2c_dev); return err; }","- if (!IS_VI(i2c_dev))
+ if (!IS_VI(i2c_dev) && !has_acpi_companion(i2c_dev->dev))","static int tegra_i2c_probe(struct platform_device *pdev) { struct tegra_i2c_dev *i2c_dev; struct resource *res; int err; i2c_dev = devm_kzalloc(&pdev->dev, sizeof(*i2c_dev), GFP_KERNEL); if (!i2c_dev) return -ENOMEM; platform_set_drvdata(pdev, i2c_dev); init_completion(&i2c_dev->msg_complete); init_completion(&i2c_dev->dma_complete); i2c_dev->hw = device_get_match_data(&pdev->dev); i2c_dev->cont_id = pdev->id; i2c_dev->dev = &pdev->dev; i2c_dev->base = devm_platform_get_and_ioremap_resource(pdev, 0, &res); if (IS_ERR(i2c_dev->base)) return PTR_ERR(i2c_dev->base); i2c_dev->base_phys = res->start; err = platform_get_irq(pdev, 0); if (err < 0) return err; i2c_dev->irq = err; irq_set_status_flags(i2c_dev->irq, IRQ_NOAUTOEN); err = devm_request_threaded_irq(i2c_dev->dev, i2c_dev->irq, NULL, tegra_i2c_isr, IRQF_NO_SUSPEND | IRQF_ONESHOT, dev_name(i2c_dev->dev), i2c_dev); if (err) return err; tegra_i2c_parse_dt(i2c_dev); err = tegra_i2c_init_reset(i2c_dev); if (err) return err; err = tegra_i2c_init_clocks(i2c_dev); if (err) return err; err = tegra_i2c_init_dma(i2c_dev); if (err) goto release_clocks; if (!IS_VI(i2c_dev) && !has_acpi_companion(i2c_dev->dev)) pm_runtime_irq_safe(i2c_dev->dev); pm_runtime_enable(i2c_dev->dev); err = tegra_i2c_init_hardware(i2c_dev); if (err) goto release_rpm; i2c_set_adapdata(&i2c_dev->adapter, i2c_dev); i2c_dev->adapter.dev.of_node = i2c_dev->dev->of_node; i2c_dev->adapter.dev.parent = i2c_dev->dev; i2c_dev->adapter.retries = 1; i2c_dev->adapter.timeout = 6 * HZ; i2c_dev->adapter.quirks = i2c_dev->hw->quirks; i2c_dev->adapter.owner = THIS_MODULE; i2c_dev->adapter.class = I2C_CLASS_DEPRECATED; i2c_dev->adapter.algo = &tegra_i2c_algo; i2c_dev->adapter.nr = pdev->id; ACPI_COMPANION_SET(&i2c_dev->adapter.dev, ACPI_COMPANION(&pdev->dev)); if (i2c_dev->hw->supports_bus_clear) i2c_dev->adapter.bus_recovery_info = &tegra_i2c_recovery_info; strscpy(i2c_dev->adapter.name, dev_name(i2c_dev->dev), sizeof(i2c_dev->adapter.name)); err = i2c_add_numbered_adapter(&i2c_dev->adapter); if (err) goto release_rpm; return 0; release_rpm: pm_runtime_disable(i2c_dev->dev); tegra_i2c_release_dma(i2c_dev); release_clocks: tegra_i2c_release_clocks(i2c_dev); return err; }"
977----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50214/bad/drm_connector_test.c----drm_test_drm_hdmi_compute_mode_clock_rgb,"static void drm_test_drm_hdmi_compute_mode_clock_rgb(struct kunit *test) { struct drm_connector_init_priv *priv = test->priv; const struct drm_display_mode *mode; unsigned long long rate; struct drm_device *drm = &priv->drm; <S2SV_StartVul> mode = drm_display_mode_from_cea_vic(drm, 16); <S2SV_EndVul> KUNIT_ASSERT_NOT_NULL(test, mode); KUNIT_ASSERT_FALSE(test, mode->flags & DRM_MODE_FLAG_DBLCLK); rate = drm_hdmi_compute_mode_clock(mode, 8, HDMI_COLORSPACE_RGB); KUNIT_ASSERT_GT(test, rate, 0); KUNIT_EXPECT_EQ(test, mode->clock * 1000ULL, rate); }","- mode = drm_display_mode_from_cea_vic(drm, 16);
+ mode = drm_kunit_display_mode_from_cea_vic(test, drm, 16);","static void drm_test_drm_hdmi_compute_mode_clock_rgb(struct kunit *test) { struct drm_connector_init_priv *priv = test->priv; const struct drm_display_mode *mode; unsigned long long rate; struct drm_device *drm = &priv->drm; mode = drm_kunit_display_mode_from_cea_vic(test, drm, 16); KUNIT_ASSERT_NOT_NULL(test, mode); KUNIT_ASSERT_FALSE(test, mode->flags & DRM_MODE_FLAG_DBLCLK); rate = drm_hdmi_compute_mode_clock(mode, 8, HDMI_COLORSPACE_RGB); KUNIT_ASSERT_GT(test, rate, 0); KUNIT_EXPECT_EQ(test, mode->clock * 1000ULL, rate); }"
407----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46867/bad/xe_drm_client.c----show_meminfo,"static void show_meminfo(struct drm_printer *p, struct drm_file *file) { struct drm_memory_stats stats[TTM_NUM_MEM_TYPES] = {}; struct xe_file *xef = file->driver_priv; struct ttm_device *bdev = &xef->xe->ttm; struct ttm_resource_manager *man; struct xe_drm_client *client; struct drm_gem_object *obj; struct xe_bo *bo; unsigned int id; u32 mem_type; client = xef->client; spin_lock(&file->table_lock); idr_for_each_entry(&file->object_idr, obj, id) { struct xe_bo *bo = gem_to_xe_bo(obj); bo_meminfo(bo, stats); } spin_unlock(&file->table_lock); spin_lock(&client->bos_lock); list_for_each_entry(bo, &client->bos_list, client_link) { if (!kref_get_unless_zero(&bo->ttm.base.refcount)) continue; bo_meminfo(bo, stats); <S2SV_StartVul> xe_bo_put(bo); <S2SV_EndVul> } spin_unlock(&client->bos_lock); for (mem_type = XE_PL_SYSTEM; mem_type < TTM_NUM_MEM_TYPES; ++mem_type) { if (!xe_mem_type_to_name[mem_type]) continue; man = ttm_manager_type(bdev, mem_type); if (man) { drm_print_memory_stats(p, &stats[mem_type], DRM_GEM_OBJECT_RESIDENT | (mem_type != XE_PL_SYSTEM ? 0 : DRM_GEM_OBJECT_PURGEABLE), xe_mem_type_to_name[mem_type]); } } }","- xe_bo_put(bo);
+ LLIST_HEAD(deferred);
+ xe_bo_put_deferred(bo, &deferred);
+ xe_bo_put_commit(&deferred);","static void show_meminfo(struct drm_printer *p, struct drm_file *file) { struct drm_memory_stats stats[TTM_NUM_MEM_TYPES] = {}; struct xe_file *xef = file->driver_priv; struct ttm_device *bdev = &xef->xe->ttm; struct ttm_resource_manager *man; struct xe_drm_client *client; struct drm_gem_object *obj; struct xe_bo *bo; LLIST_HEAD(deferred); unsigned int id; u32 mem_type; client = xef->client; spin_lock(&file->table_lock); idr_for_each_entry(&file->object_idr, obj, id) { struct xe_bo *bo = gem_to_xe_bo(obj); bo_meminfo(bo, stats); } spin_unlock(&file->table_lock); spin_lock(&client->bos_lock); list_for_each_entry(bo, &client->bos_list, client_link) { if (!kref_get_unless_zero(&bo->ttm.base.refcount)) continue; bo_meminfo(bo, stats); xe_bo_put_deferred(bo, &deferred); } spin_unlock(&client->bos_lock); xe_bo_put_commit(&deferred); for (mem_type = XE_PL_SYSTEM; mem_type < TTM_NUM_MEM_TYPES; ++mem_type) { if (!xe_mem_type_to_name[mem_type]) continue; man = ttm_manager_type(bdev, mem_type); if (man) { drm_print_memory_stats(p, &stats[mem_type], DRM_GEM_OBJECT_RESIDENT | (mem_type != XE_PL_SYSTEM ? 0 : DRM_GEM_OBJECT_PURGEABLE), xe_mem_type_to_name[mem_type]); } } }"
185----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46674/bad/dwc3-st.c----st_dwc3_probe,"static int st_dwc3_probe(struct platform_device *pdev) { struct st_dwc3 *dwc3_data; struct resource *res; struct device *dev = &pdev->dev; struct device_node *node = dev->of_node, *child; struct platform_device *child_pdev; struct regmap *regmap; int ret; dwc3_data = devm_kzalloc(dev, sizeof(*dwc3_data), GFP_KERNEL); if (!dwc3_data) return -ENOMEM; dwc3_data->glue_base = devm_platform_ioremap_resource_byname(pdev, ""reg-glue""); if (IS_ERR(dwc3_data->glue_base)) return PTR_ERR(dwc3_data->glue_base); regmap = syscon_regmap_lookup_by_phandle(node, ""st,syscfg""); if (IS_ERR(regmap)) return PTR_ERR(regmap); dwc3_data->dev = dev; dwc3_data->regmap = regmap; res = platform_get_resource_byname(pdev, IORESOURCE_MEM, ""syscfg-reg""); <S2SV_StartVul> if (!res) { <S2SV_EndVul> <S2SV_StartVul> ret = -ENXIO; <S2SV_EndVul> <S2SV_StartVul> goto undo_platform_dev_alloc; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> dwc3_data->syscfg_reg_off = res->start; dev_vdbg(&pdev->dev, ""glue-logic addr 0x%pK, syscfg-reg offset 0x%x\n"", dwc3_data->glue_base, dwc3_data->syscfg_reg_off); dwc3_data->rstc_pwrdn = devm_reset_control_get_exclusive(dev, ""powerdown""); if (IS_ERR(dwc3_data->rstc_pwrdn)) { dev_err(&pdev->dev, ""could not get power controller\n""); <S2SV_StartVul> ret = PTR_ERR(dwc3_data->rstc_pwrdn); <S2SV_EndVul> <S2SV_StartVul> goto undo_platform_dev_alloc; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> reset_control_deassert(dwc3_data->rstc_pwrdn); dwc3_data->rstc_rst = devm_reset_control_get_shared(dev, ""softreset""); if (IS_ERR(dwc3_data->rstc_rst)) { dev_err(&pdev->dev, ""could not get reset controller\n""); ret = PTR_ERR(dwc3_data->rstc_rst); goto undo_powerdown; } reset_control_deassert(dwc3_data->rstc_rst); child = of_get_compatible_child(node, ""snps,dwc3""); if (!child) { dev_err(&pdev->dev, ""failed to find dwc3 core node\n""); ret = -ENODEV; goto err_node_put; } ret = of_platform_populate(node, NULL, NULL, dev); if (ret) { dev_err(dev, ""failed to add dwc3 core\n""); goto err_node_put; } child_pdev = of_find_device_by_node(child); if (!child_pdev) { dev_err(dev, ""failed to find dwc3 core device\n""); ret = -ENODEV; goto err_node_put; } dwc3_data->dr_mode = usb_get_dr_mode(&child_pdev->dev); of_node_put(child); platform_device_put(child_pdev); ret = st_dwc3_drd_init(dwc3_data); if (ret) { dev_err(dev, ""drd initialisation failed\n""); goto undo_softreset; } st_dwc3_init(dwc3_data); platform_set_drvdata(pdev, dwc3_data); return 0; err_node_put: of_node_put(child); undo_softreset: reset_control_assert(dwc3_data->rstc_rst); undo_powerdown: reset_control_assert(dwc3_data->rstc_pwrdn); <S2SV_StartVul> undo_platform_dev_alloc: <S2SV_EndVul> <S2SV_StartVul> platform_device_put(pdev); <S2SV_EndVul> return ret; <S2SV_StartVul> } <S2SV_EndVul>","- if (!res) {
- ret = -ENXIO;
- goto undo_platform_dev_alloc;
- }
- ret = PTR_ERR(dwc3_data->rstc_pwrdn);
- goto undo_platform_dev_alloc;
- }
- undo_platform_dev_alloc:
- platform_device_put(pdev);
- }
+ if (!res)
+ return -ENXIO;
+ return PTR_ERR(dwc3_data->rstc_pwrdn);","static int st_dwc3_probe(struct platform_device *pdev) { struct st_dwc3 *dwc3_data; struct resource *res; struct device *dev = &pdev->dev; struct device_node *node = dev->of_node, *child; struct platform_device *child_pdev; struct regmap *regmap; int ret; dwc3_data = devm_kzalloc(dev, sizeof(*dwc3_data), GFP_KERNEL); if (!dwc3_data) return -ENOMEM; dwc3_data->glue_base = devm_platform_ioremap_resource_byname(pdev, ""reg-glue""); if (IS_ERR(dwc3_data->glue_base)) return PTR_ERR(dwc3_data->glue_base); regmap = syscon_regmap_lookup_by_phandle(node, ""st,syscfg""); if (IS_ERR(regmap)) return PTR_ERR(regmap); dwc3_data->dev = dev; dwc3_data->regmap = regmap; res = platform_get_resource_byname(pdev, IORESOURCE_MEM, ""syscfg-reg""); if (!res) return -ENXIO; dwc3_data->syscfg_reg_off = res->start; dev_vdbg(&pdev->dev, ""glue-logic addr 0x%pK, syscfg-reg offset 0x%x\n"", dwc3_data->glue_base, dwc3_data->syscfg_reg_off); dwc3_data->rstc_pwrdn = devm_reset_control_get_exclusive(dev, ""powerdown""); if (IS_ERR(dwc3_data->rstc_pwrdn)) { dev_err(&pdev->dev, ""could not get power controller\n""); return PTR_ERR(dwc3_data->rstc_pwrdn); } reset_control_deassert(dwc3_data->rstc_pwrdn); dwc3_data->rstc_rst = devm_reset_control_get_shared(dev, ""softreset""); if (IS_ERR(dwc3_data->rstc_rst)) { dev_err(&pdev->dev, ""could not get reset controller\n""); ret = PTR_ERR(dwc3_data->rstc_rst); goto undo_powerdown; } reset_control_deassert(dwc3_data->rstc_rst); child = of_get_compatible_child(node, ""snps,dwc3""); if (!child) { dev_err(&pdev->dev, ""failed to find dwc3 core node\n""); ret = -ENODEV; goto err_node_put; } ret = of_platform_populate(node, NULL, NULL, dev); if (ret) { dev_err(dev, ""failed to add dwc3 core\n""); goto err_node_put; } child_pdev = of_find_device_by_node(child); if (!child_pdev) { dev_err(dev, ""failed to find dwc3 core device\n""); ret = -ENODEV; goto err_node_put; } dwc3_data->dr_mode = usb_get_dr_mode(&child_pdev->dev); of_node_put(child); platform_device_put(child_pdev); ret = st_dwc3_drd_init(dwc3_data); if (ret) { dev_err(dev, ""drd initialisation failed\n""); goto undo_softreset; } st_dwc3_init(dwc3_data); platform_set_drvdata(pdev, dwc3_data); return 0; err_node_put: of_node_put(child); undo_softreset: reset_control_assert(dwc3_data->rstc_rst); undo_powerdown: reset_control_assert(dwc3_data->rstc_pwrdn); return ret; }"
85----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44955/bad/amdgpu_dm_mst_types.c----is_dsc_need_re_compute,"static bool is_dsc_need_re_compute( struct drm_atomic_state *state, struct dc_state *dc_state, struct dc_link *dc_link) { int i, j; bool is_dsc_need_re_compute = false; struct amdgpu_dm_connector *stream_on_link[MAX_PIPES]; int new_stream_on_link_num = 0; struct amdgpu_dm_connector *aconnector; struct dc_stream_state *stream; const struct dc *dc = dc_link->dc; if (dc_link->type != dc_connection_mst_branch) return false; if (needs_dsc_aux_workaround(dc_link) && (!(dc_link->dpcd_caps.dsc_caps.dsc_basic_caps.fields.dsc_support.DSC_SUPPORT || dc_link->dpcd_caps.dsc_caps.dsc_basic_caps.fields.dsc_support.DSC_PASSTHROUGH_SUPPORT))) return false; for (i = 0; i < MAX_PIPES; i++) stream_on_link[i] = NULL; for (i = 0; i < dc_state->stream_count; i++) { struct drm_crtc_state *new_crtc_state; struct drm_connector_state *new_conn_state; stream = dc_state->streams[i]; if (!stream) continue; if (stream->link != dc_link) continue; aconnector = (struct amdgpu_dm_connector *) stream->dm_stream_context; if (!aconnector || !aconnector->dsc_aux) <S2SV_StartVul> continue; <S2SV_EndVul> <S2SV_StartVul> if (!(aconnector->dc_sink->dsc_caps.dsc_dec_caps.is_dsc_supported || <S2SV_EndVul> <S2SV_StartVul> aconnector->dc_link->dpcd_caps.dsc_caps.dsc_basic_caps.fields.dsc_support.DSC_PASSTHROUGH_SUPPORT)) <S2SV_EndVul> <S2SV_StartVul> continue; <S2SV_EndVul> stream_on_link[new_stream_on_link_num] = aconnector; new_stream_on_link_num++; new_conn_state = drm_atomic_get_new_connector_state(state, &aconnector->base); if (!new_conn_state) continue; if (IS_ERR(new_conn_state)) continue; if (!new_conn_state->crtc) continue; new_crtc_state = drm_atomic_get_new_crtc_state(state, new_conn_state->crtc); if (!new_crtc_state) continue; if (IS_ERR(new_crtc_state)) continue; if (new_crtc_state->enable && new_crtc_state->active) { if (new_crtc_state->mode_changed || new_crtc_state->active_changed || new_crtc_state->connectors_changed) return true; } } for (i = 0; i < dc->current_state->stream_count; i++) { stream = dc->current_state->streams[i]; if (stream->link != dc_link) continue; aconnector = (struct amdgpu_dm_connector *)stream->dm_stream_context; if (!aconnector) continue; for (j = 0; j < new_stream_on_link_num; j++) { if (stream_on_link[j]) { if (aconnector == stream_on_link[j]) break; } } if (j == new_stream_on_link_num) { is_dsc_need_re_compute = true; break; } } return is_dsc_need_re_compute; }","- continue;
- if (!(aconnector->dc_sink->dsc_caps.dsc_dec_caps.is_dsc_supported ||
- aconnector->dc_link->dpcd_caps.dsc_caps.dsc_basic_caps.fields.dsc_support.DSC_PASSTHROUGH_SUPPORT))
- continue;","static bool is_dsc_need_re_compute( struct drm_atomic_state *state, struct dc_state *dc_state, struct dc_link *dc_link) { int i, j; bool is_dsc_need_re_compute = false; struct amdgpu_dm_connector *stream_on_link[MAX_PIPES]; int new_stream_on_link_num = 0; struct amdgpu_dm_connector *aconnector; struct dc_stream_state *stream; const struct dc *dc = dc_link->dc; if (dc_link->type != dc_connection_mst_branch) return false; if (needs_dsc_aux_workaround(dc_link) && (!(dc_link->dpcd_caps.dsc_caps.dsc_basic_caps.fields.dsc_support.DSC_SUPPORT || dc_link->dpcd_caps.dsc_caps.dsc_basic_caps.fields.dsc_support.DSC_PASSTHROUGH_SUPPORT))) return false; for (i = 0; i < MAX_PIPES; i++) stream_on_link[i] = NULL; for (i = 0; i < dc_state->stream_count; i++) { struct drm_crtc_state *new_crtc_state; struct drm_connector_state *new_conn_state; stream = dc_state->streams[i]; if (!stream) continue; if (stream->link != dc_link) continue; aconnector = (struct amdgpu_dm_connector *) stream->dm_stream_context; if (!aconnector || !aconnector->dsc_aux) continue; stream_on_link[new_stream_on_link_num] = aconnector; new_stream_on_link_num++; new_conn_state = drm_atomic_get_new_connector_state(state, &aconnector->base); if (!new_conn_state) continue; if (IS_ERR(new_conn_state)) continue; if (!new_conn_state->crtc) continue; new_crtc_state = drm_atomic_get_new_crtc_state(state, new_conn_state->crtc); if (!new_crtc_state) continue; if (IS_ERR(new_crtc_state)) continue; if (new_crtc_state->enable && new_crtc_state->active) { if (new_crtc_state->mode_changed || new_crtc_state->active_changed || new_crtc_state->connectors_changed) return true; } } for (i = 0; i < dc->current_state->stream_count; i++) { stream = dc->current_state->streams[i]; if (stream->link != dc_link) continue; aconnector = (struct amdgpu_dm_connector *)stream->dm_stream_context; if (!aconnector) continue; for (j = 0; j < new_stream_on_link_num; j++) { if (stream_on_link[j]) { if (aconnector == stream_on_link[j]) break; } } if (j == new_stream_on_link_num) { is_dsc_need_re_compute = true; break; } } return is_dsc_need_re_compute; }"
1133----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53107/bad/task_mmu.c----pagemap_scan_get_args,"static int pagemap_scan_get_args(struct pm_scan_arg *arg, unsigned long uarg) { if (copy_from_user(arg, (void __user *)uarg, sizeof(*arg))) return -EFAULT; if (arg->size != sizeof(struct pm_scan_arg)) return -EINVAL; if (arg->flags & ~PM_SCAN_FLAGS) return -EINVAL; if ((arg->category_inverted | arg->category_mask | arg->category_anyof_mask | arg->return_mask) & ~PM_SCAN_CATEGORIES) return -EINVAL; arg->start = untagged_addr((unsigned long)arg->start); arg->end = untagged_addr((unsigned long)arg->end); arg->vec = untagged_addr((unsigned long)arg->vec); if (!IS_ALIGNED(arg->start, PAGE_SIZE)) return -EINVAL; if (!access_ok((void __user *)(long)arg->start, arg->end - arg->start)) return -EFAULT; if (!arg->vec && arg->vec_len) return -EINVAL; if (arg->vec && !access_ok((void __user *)(long)arg->vec, <S2SV_StartVul> arg->vec_len * sizeof(struct page_region))) <S2SV_EndVul> return -EFAULT; arg->end = ALIGN(arg->end, PAGE_SIZE); arg->walk_end = 0; if (!arg->max_pages) arg->max_pages = ULONG_MAX; return 0; }","- arg->vec_len * sizeof(struct page_region)))
+ return -EINVAL;
+ if (UINT_MAX == SIZE_MAX && arg->vec_len > SIZE_MAX)
+ return -EINVAL;
+ size_mul(arg->vec_len, sizeof(struct page_region))))","static int pagemap_scan_get_args(struct pm_scan_arg *arg, unsigned long uarg) { if (copy_from_user(arg, (void __user *)uarg, sizeof(*arg))) return -EFAULT; if (arg->size != sizeof(struct pm_scan_arg)) return -EINVAL; if (arg->flags & ~PM_SCAN_FLAGS) return -EINVAL; if ((arg->category_inverted | arg->category_mask | arg->category_anyof_mask | arg->return_mask) & ~PM_SCAN_CATEGORIES) return -EINVAL; arg->start = untagged_addr((unsigned long)arg->start); arg->end = untagged_addr((unsigned long)arg->end); arg->vec = untagged_addr((unsigned long)arg->vec); if (!IS_ALIGNED(arg->start, PAGE_SIZE)) return -EINVAL; if (!access_ok((void __user *)(long)arg->start, arg->end - arg->start)) return -EFAULT; if (!arg->vec && arg->vec_len) return -EINVAL; if (UINT_MAX == SIZE_MAX && arg->vec_len > SIZE_MAX) return -EINVAL; if (arg->vec && !access_ok((void __user *)(long)arg->vec, size_mul(arg->vec_len, sizeof(struct page_region)))) return -EFAULT; arg->end = ALIGN(arg->end, PAGE_SIZE); arg->walk_end = 0; if (!arg->max_pages) arg->max_pages = ULONG_MAX; return 0; }"
1479----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-57900/bad/ila_xlat.c----ila_add_mapping,"static int ila_add_mapping(struct net *net, struct ila_xlat_params *xp) { struct ila_net *ilan = net_generic(net, ila_net_id); struct ila_map *ila, *head; spinlock_t *lock = ila_get_lock(ilan, xp->ip.locator_match); int err = 0, order; <S2SV_StartVul> if (!ilan->xlat.hooks_registered) { <S2SV_EndVul> <S2SV_StartVul> err = nf_register_net_hooks(net, ila_nf_hook_ops, <S2SV_EndVul> <S2SV_StartVul> ARRAY_SIZE(ila_nf_hook_ops)); <S2SV_EndVul> if (err) return err; <S2SV_StartVul> ilan->xlat.hooks_registered = true; <S2SV_EndVul> } ila = kzalloc(sizeof(*ila), GFP_KERNEL); if (!ila) return -ENOMEM; ila_init_saved_csum(&xp->ip); ila->xp = *xp; order = ila_order(ila); spin_lock(lock); head = rhashtable_lookup_fast(&ilan->xlat.rhash_table, &xp->ip.locator_match, rht_params); if (!head) { err = rhashtable_lookup_insert_fast(&ilan->xlat.rhash_table, &ila->node, rht_params); } else { struct ila_map *tila = head, *prev = NULL; do { if (!ila_cmp_params(tila, xp)) { err = -EEXIST; goto out; } if (order > ila_order(tila)) break; prev = tila; tila = rcu_dereference_protected(tila->next, lockdep_is_held(lock)); } while (tila); if (prev) { RCU_INIT_POINTER(ila->next, tila); rcu_assign_pointer(prev->next, ila); } else { RCU_INIT_POINTER(ila->next, head); err = rhashtable_replace_fast(&ilan->xlat.rhash_table, &head->node, &ila->node, rht_params); if (err) goto out; } } out: spin_unlock(lock); if (err) kfree(ila); return err; }","- if (!ilan->xlat.hooks_registered) {
- err = nf_register_net_hooks(net, ila_nf_hook_ops,
- ARRAY_SIZE(ila_nf_hook_ops));
- ilan->xlat.hooks_registered = true;
+ if (!READ_ONCE(ilan->xlat.hooks_registered)) {
+ mutex_lock(&ila_mutex);
+ if (!ilan->xlat.hooks_registered) {
+ err = nf_register_net_hooks(net, ila_nf_hook_ops,
+ ARRAY_SIZE(ila_nf_hook_ops));
+ if (!err)
+ WRITE_ONCE(ilan->xlat.hooks_registered, true);
+ }
+ mutex_unlock(&ila_mutex);
+ }","static int ila_add_mapping(struct net *net, struct ila_xlat_params *xp) { struct ila_net *ilan = net_generic(net, ila_net_id); struct ila_map *ila, *head; spinlock_t *lock = ila_get_lock(ilan, xp->ip.locator_match); int err = 0, order; if (!READ_ONCE(ilan->xlat.hooks_registered)) { mutex_lock(&ila_mutex); if (!ilan->xlat.hooks_registered) { err = nf_register_net_hooks(net, ila_nf_hook_ops, ARRAY_SIZE(ila_nf_hook_ops)); if (!err) WRITE_ONCE(ilan->xlat.hooks_registered, true); } mutex_unlock(&ila_mutex); if (err) return err; } ila = kzalloc(sizeof(*ila), GFP_KERNEL); if (!ila) return -ENOMEM; ila_init_saved_csum(&xp->ip); ila->xp = *xp; order = ila_order(ila); spin_lock(lock); head = rhashtable_lookup_fast(&ilan->xlat.rhash_table, &xp->ip.locator_match, rht_params); if (!head) { err = rhashtable_lookup_insert_fast(&ilan->xlat.rhash_table, &ila->node, rht_params); } else { struct ila_map *tila = head, *prev = NULL; do { if (!ila_cmp_params(tila, xp)) { err = -EEXIST; goto out; } if (order > ila_order(tila)) break; prev = tila; tila = rcu_dereference_protected(tila->next, lockdep_is_held(lock)); } while (tila); if (prev) { RCU_INIT_POINTER(ila->next, tila); rcu_assign_pointer(prev->next, ila); } else { RCU_INIT_POINTER(ila->next, head); err = rhashtable_replace_fast(&ilan->xlat.rhash_table, &head->node, &ila->node, rht_params); if (err) goto out; } } out: spin_unlock(lock); if (err) kfree(ila); return err; }"
1153----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53130/bad/gcinode.c----nilfs_gccache_submit_read_data,"int nilfs_gccache_submit_read_data(struct inode *inode, sector_t blkoff, sector_t pbn, __u64 vbn, struct buffer_head **out_bh) { struct buffer_head *bh; int err; bh = nilfs_grab_buffer(inode, inode->i_mapping, blkoff, 0); if (unlikely(!bh)) return -ENOMEM; if (buffer_uptodate(bh)) goto out; if (pbn == 0) { struct the_nilfs *nilfs = inode->i_sb->s_fs_info; err = nilfs_dat_translate(nilfs->ns_dat, vbn, &pbn); if (unlikely(err)) goto failed; } lock_buffer(bh); if (buffer_uptodate(bh)) { unlock_buffer(bh); goto out; <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> if (!buffer_mapped(bh)) { <S2SV_EndVul> <S2SV_StartVul> bh->b_bdev = inode->i_sb->s_bdev; <S2SV_EndVul> set_buffer_mapped(bh); <S2SV_StartVul> } <S2SV_EndVul> bh->b_blocknr = pbn; bh->b_end_io = end_buffer_read_sync; get_bh(bh); submit_bh(REQ_OP_READ, 0, bh); if (vbn) bh->b_blocknr = vbn; out: err = 0; *out_bh = bh; failed: unlock_page(bh->b_page); put_page(bh->b_page); if (unlikely(err)) brelse(bh); return err; }","- }
- if (!buffer_mapped(bh)) {
- bh->b_bdev = inode->i_sb->s_bdev;
- }
+ if (!buffer_mapped(bh))","int nilfs_gccache_submit_read_data(struct inode *inode, sector_t blkoff, sector_t pbn, __u64 vbn, struct buffer_head **out_bh) { struct buffer_head *bh; int err; bh = nilfs_grab_buffer(inode, inode->i_mapping, blkoff, 0); if (unlikely(!bh)) return -ENOMEM; if (buffer_uptodate(bh)) goto out; if (pbn == 0) { struct the_nilfs *nilfs = inode->i_sb->s_fs_info; err = nilfs_dat_translate(nilfs->ns_dat, vbn, &pbn); if (unlikely(err)) goto failed; } lock_buffer(bh); if (buffer_uptodate(bh)) { unlock_buffer(bh); goto out; } if (!buffer_mapped(bh)) set_buffer_mapped(bh); bh->b_blocknr = pbn; bh->b_end_io = end_buffer_read_sync; get_bh(bh); submit_bh(REQ_OP_READ, 0, bh); if (vbn) bh->b_blocknr = vbn; out: err = 0; *out_bh = bh; failed: unlock_page(bh->b_page); put_page(bh->b_page); if (unlikely(err)) brelse(bh); return err; }"
1277----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56572/bad/allegro-core.c----allocate_buffers_internal,"static int allocate_buffers_internal(struct allegro_channel *channel, struct list_head *list, size_t n, size_t size) { struct allegro_dev *dev = channel->dev; unsigned int i; int err; struct allegro_buffer *buffer, *tmp; for (i = 0; i < n; i++) { buffer = kmalloc(sizeof(*buffer), GFP_KERNEL); if (!buffer) { err = -ENOMEM; goto err; } INIT_LIST_HEAD(&buffer->head); err = allegro_alloc_buffer(dev, buffer, size); <S2SV_StartVul> if (err) <S2SV_EndVul> goto err; list_add(&buffer->head, list); } return 0; err: list_for_each_entry_safe(buffer, tmp, list, head) { list_del(&buffer->head); allegro_free_buffer(dev, buffer); kfree(buffer); } return err; }","- if (err)
+ if (err) {
+ kfree(buffer);
+ }
+ }","static int allocate_buffers_internal(struct allegro_channel *channel, struct list_head *list, size_t n, size_t size) { struct allegro_dev *dev = channel->dev; unsigned int i; int err; struct allegro_buffer *buffer, *tmp; for (i = 0; i < n; i++) { buffer = kmalloc(sizeof(*buffer), GFP_KERNEL); if (!buffer) { err = -ENOMEM; goto err; } INIT_LIST_HEAD(&buffer->head); err = allegro_alloc_buffer(dev, buffer, size); if (err) { kfree(buffer); goto err; } list_add(&buffer->head, list); } return 0; err: list_for_each_entry_safe(buffer, tmp, list, head) { list_del(&buffer->head); allegro_free_buffer(dev, buffer); kfree(buffer); } return err; }"
500----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47738/bad/scan.c----ieee80211_send_scan_probe_req,"static void ieee80211_send_scan_probe_req(struct ieee80211_sub_if_data *sdata, const u8 *src, const u8 *dst, const u8 *ssid, size_t ssid_len, const u8 *ie, size_t ie_len, u32 ratemask, u32 flags, u32 tx_flags, struct ieee80211_channel *channel) { struct sk_buff *skb; skb = ieee80211_build_probe_req(sdata, src, dst, ratemask, channel, ssid, ssid_len, ie, ie_len, flags); if (skb) { if (flags & IEEE80211_PROBE_FLAG_RANDOM_SN) { struct ieee80211_hdr *hdr = (void *)skb->data; struct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb); u16 sn = get_random_u16(); info->control.flags |= IEEE80211_TX_CTRL_NO_SEQNO; hdr->seq_ctrl = cpu_to_le16(IEEE80211_SN_TO_SEQ(sn)); } IEEE80211_SKB_CB(skb)->flags |= tx_flags; <S2SV_StartVul> IEEE80211_SKB_CB(skb)->control.flags |= IEEE80211_TX_CTRL_SCAN_TX; <S2SV_EndVul> ieee80211_tx_skb_tid_band(sdata, skb, 7, channel->band); } }","- IEEE80211_SKB_CB(skb)->control.flags |= IEEE80211_TX_CTRL_SCAN_TX;
+ IEEE80211_SKB_CB(skb)->control.flags |= IEEE80211_TX_CTRL_DONT_USE_RATE_MASK;","static void ieee80211_send_scan_probe_req(struct ieee80211_sub_if_data *sdata, const u8 *src, const u8 *dst, const u8 *ssid, size_t ssid_len, const u8 *ie, size_t ie_len, u32 ratemask, u32 flags, u32 tx_flags, struct ieee80211_channel *channel) { struct sk_buff *skb; skb = ieee80211_build_probe_req(sdata, src, dst, ratemask, channel, ssid, ssid_len, ie, ie_len, flags); if (skb) { if (flags & IEEE80211_PROBE_FLAG_RANDOM_SN) { struct ieee80211_hdr *hdr = (void *)skb->data; struct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb); u16 sn = get_random_u16(); info->control.flags |= IEEE80211_TX_CTRL_NO_SEQNO; hdr->seq_ctrl = cpu_to_le16(IEEE80211_SN_TO_SEQ(sn)); } IEEE80211_SKB_CB(skb)->flags |= tx_flags; IEEE80211_SKB_CB(skb)->control.flags |= IEEE80211_TX_CTRL_DONT_USE_RATE_MASK; ieee80211_tx_skb_tid_band(sdata, skb, 7, channel->band); } }"
736----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50010/bad/exec.c----SYSCALL_DEFINE1,"SYSCALL_DEFINE1(uselib, const char __user *, library) { struct linux_binfmt *fmt; struct file *file; struct filename *tmp = getname(library); int error = PTR_ERR(tmp); static const struct open_flags uselib_flags = { .open_flag = O_LARGEFILE | O_RDONLY | __FMODE_EXEC, .acc_mode = MAY_READ | MAY_EXEC, .intent = LOOKUP_OPEN, .lookup_flags = LOOKUP_FOLLOW, }; if (IS_ERR(tmp)) goto out; file = do_filp_open(AT_FDCWD, tmp, &uselib_flags); putname(tmp); error = PTR_ERR(file); if (IS_ERR(file)) goto out; error = -EACCES; <S2SV_StartVul> if (WARN_ON_ONCE(!S_ISREG(file_inode(file)->i_mode) || <S2SV_EndVul> <S2SV_StartVul> path_noexec(&file->f_path))) <S2SV_EndVul> goto exit; error = -ENOEXEC; read_lock(&binfmt_lock); list_for_each_entry(fmt, &formats, lh) { if (!fmt->load_shlib) continue; if (!try_module_get(fmt->module)) continue; read_unlock(&binfmt_lock); error = fmt->load_shlib(file); read_lock(&binfmt_lock); put_binfmt(fmt); if (error != -ENOEXEC) break; } read_unlock(&binfmt_lock); exit: fput(file); out: return error; }","- if (WARN_ON_ONCE(!S_ISREG(file_inode(file)->i_mode) ||
- path_noexec(&file->f_path)))
+ if (WARN_ON_ONCE(!S_ISREG(file_inode(file)->i_mode)) ||
+ path_noexec(&file->f_path))","SYSCALL_DEFINE1(uselib, const char __user *, library) { struct linux_binfmt *fmt; struct file *file; struct filename *tmp = getname(library); int error = PTR_ERR(tmp); static const struct open_flags uselib_flags = { .open_flag = O_LARGEFILE | O_RDONLY | __FMODE_EXEC, .acc_mode = MAY_READ | MAY_EXEC, .intent = LOOKUP_OPEN, .lookup_flags = LOOKUP_FOLLOW, }; if (IS_ERR(tmp)) goto out; file = do_filp_open(AT_FDCWD, tmp, &uselib_flags); putname(tmp); error = PTR_ERR(file); if (IS_ERR(file)) goto out; error = -EACCES; if (WARN_ON_ONCE(!S_ISREG(file_inode(file)->i_mode)) || path_noexec(&file->f_path)) goto exit; error = -ENOEXEC; read_lock(&binfmt_lock); list_for_each_entry(fmt, &formats, lh) { if (!fmt->load_shlib) continue; if (!try_module_get(fmt->module)) continue; read_unlock(&binfmt_lock); error = fmt->load_shlib(file); read_lock(&binfmt_lock); put_binfmt(fmt); if (error != -ENOEXEC) break; } read_unlock(&binfmt_lock); exit: fput(file); out: return error; }"
1274----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56567/bad/ad7780.c----ad7780_write_raw,"static int ad7780_write_raw(struct iio_dev *indio_dev, struct iio_chan_spec const *chan, int val, int val2, long m) { struct ad7780_state *st = iio_priv(indio_dev); const struct ad7780_chip_info *chip_info = st->chip_info; unsigned long long vref; unsigned int full_scale, gain; if (!chip_info->is_ad778x) return -EINVAL; switch (m) { case IIO_CHAN_INFO_SCALE: <S2SV_StartVul> if (val != 0) <S2SV_EndVul> return -EINVAL; vref = st->int_vref_mv * 1000000LL; full_scale = 1 << (chip_info->channel.scan_type.realbits - 1); gain = DIV_ROUND_CLOSEST_ULL(vref, full_scale); gain = DIV_ROUND_CLOSEST(gain, val2); st->gain = gain; if (gain < AD7780_GAIN_MIDPOINT) gain = 0; else gain = 1; gpiod_set_value(st->gain_gpio, gain); break; case IIO_CHAN_INFO_SAMP_FREQ: if (1000*val + val2/1000 < AD7780_FILTER_MIDPOINT) val = 0; else val = 1; st->odr = ad778x_odr_avail[val]; gpiod_set_value(st->filter_gpio, val); break; default: break; } return 0; }","- if (val != 0)
+ if (val != 0 || val2 == 0)","static int ad7780_write_raw(struct iio_dev *indio_dev, struct iio_chan_spec const *chan, int val, int val2, long m) { struct ad7780_state *st = iio_priv(indio_dev); const struct ad7780_chip_info *chip_info = st->chip_info; unsigned long long vref; unsigned int full_scale, gain; if (!chip_info->is_ad778x) return -EINVAL; switch (m) { case IIO_CHAN_INFO_SCALE: if (val != 0 || val2 == 0) return -EINVAL; vref = st->int_vref_mv * 1000000LL; full_scale = 1 << (chip_info->channel.scan_type.realbits - 1); gain = DIV_ROUND_CLOSEST_ULL(vref, full_scale); gain = DIV_ROUND_CLOSEST(gain, val2); st->gain = gain; if (gain < AD7780_GAIN_MIDPOINT) gain = 0; else gain = 1; gpiod_set_value(st->gain_gpio, gain); break; case IIO_CHAN_INFO_SAMP_FREQ: if (1000*val + val2/1000 < AD7780_FILTER_MIDPOINT) val = 0; else val = 1; st->odr = ad778x_odr_avail[val]; gpiod_set_value(st->filter_gpio, val); break; default: break; } return 0; }"
821----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50090/bad/xe_bb.c----__xe_bb_create_job,"__xe_bb_create_job(struct xe_exec_queue *q, struct xe_bb *bb, u64 *addr) { u32 size = drm_suballoc_size(bb->bo); <S2SV_StartVul> bb->cs[bb->len++] = MI_BATCH_BUFFER_END; <S2SV_EndVul> xe_gt_assert(q->gt, bb->len * 4 + bb_prefetch(q->gt) <= size); xe_sa_bo_flush_write(bb->bo); return xe_sched_job_create(q, addr); }","- bb->cs[bb->len++] = MI_BATCH_BUFFER_END;
+ if (bb->len == 0 || bb->cs[bb->len - 1] != MI_BATCH_BUFFER_END)
+ bb->cs[bb->len++] = MI_BATCH_BUFFER_END;","__xe_bb_create_job(struct xe_exec_queue *q, struct xe_bb *bb, u64 *addr) { u32 size = drm_suballoc_size(bb->bo); if (bb->len == 0 || bb->cs[bb->len - 1] != MI_BATCH_BUFFER_END) bb->cs[bb->len++] = MI_BATCH_BUFFER_END; xe_gt_assert(q->gt, bb->len * 4 + bb_prefetch(q->gt) <= size); xe_sa_bo_flush_write(bb->bo); return xe_sched_job_create(q, addr); }"
275----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46758/bad/lm95234.c----offset_store,"static ssize_t offset_store(struct device *dev, struct device_attribute *attr, const char *buf, size_t count) { struct lm95234_data *data = dev_get_drvdata(dev); int index = to_sensor_dev_attr(attr)->index; int ret = lm95234_update_device(data); long val; if (ret) return ret; ret = kstrtol(buf, 10, &val); if (ret < 0) return ret; <S2SV_StartVul> val = clamp_val(DIV_ROUND_CLOSEST(val, 500), -128, 127); <S2SV_EndVul> mutex_lock(&data->update_lock); data->toffset[index] = val; i2c_smbus_write_byte_data(data->client, LM95234_REG_OFFSET(index), val); mutex_unlock(&data->update_lock); return count; }","- val = clamp_val(DIV_ROUND_CLOSEST(val, 500), -128, 127);
+ val = DIV_ROUND_CLOSEST(clamp_val(val, -64000, 63500), 500);","static ssize_t offset_store(struct device *dev, struct device_attribute *attr, const char *buf, size_t count) { struct lm95234_data *data = dev_get_drvdata(dev); int index = to_sensor_dev_attr(attr)->index; int ret = lm95234_update_device(data); long val; if (ret) return ret; ret = kstrtol(buf, 10, &val); if (ret < 0) return ret; val = DIV_ROUND_CLOSEST(clamp_val(val, -64000, 63500), 500); mutex_lock(&data->update_lock); data->toffset[index] = val; i2c_smbus_write_byte_data(data->client, LM95234_REG_OFFSET(index), val); mutex_unlock(&data->update_lock); return count; }"
70----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44943/bad/huge_memory.c----*follow_devmap_pmd,"struct page *follow_devmap_pmd(struct vm_area_struct *vma, unsigned long addr, pmd_t *pmd, int flags, struct dev_pagemap **pgmap) { unsigned long pfn = pmd_pfn(*pmd); struct mm_struct *mm = vma->vm_mm; struct page *page; int ret; assert_spin_locked(pmd_lockptr(mm, pmd)); if (flags & FOLL_WRITE && !pmd_write(*pmd)) return NULL; if (pmd_present(*pmd) && pmd_devmap(*pmd)) ; else return NULL; if (flags & FOLL_TOUCH) touch_pmd(vma, addr, pmd, flags & FOLL_WRITE); if (!(flags & (FOLL_GET | FOLL_PIN))) return ERR_PTR(-EEXIST); pfn += (addr & ~PMD_MASK) >> PAGE_SHIFT; *pgmap = get_dev_pagemap(pfn, *pgmap); if (!*pgmap) return ERR_PTR(-EFAULT); page = pfn_to_page(pfn); <S2SV_StartVul> ret = try_grab_page(page, flags); <S2SV_EndVul> if (ret) page = ERR_PTR(ret); return page; }","- ret = try_grab_page(page, flags);
+ ret = try_grab_folio(page_folio(page), 1, flags);","struct page *follow_devmap_pmd(struct vm_area_struct *vma, unsigned long addr, pmd_t *pmd, int flags, struct dev_pagemap **pgmap) { unsigned long pfn = pmd_pfn(*pmd); struct mm_struct *mm = vma->vm_mm; struct page *page; int ret; assert_spin_locked(pmd_lockptr(mm, pmd)); if (flags & FOLL_WRITE && !pmd_write(*pmd)) return NULL; if (pmd_present(*pmd) && pmd_devmap(*pmd)) ; else return NULL; if (flags & FOLL_TOUCH) touch_pmd(vma, addr, pmd, flags & FOLL_WRITE); if (!(flags & (FOLL_GET | FOLL_PIN))) return ERR_PTR(-EEXIST); pfn += (addr & ~PMD_MASK) >> PAGE_SHIFT; *pgmap = get_dev_pagemap(pfn, *pgmap); if (!*pgmap) return ERR_PTR(-EFAULT); page = pfn_to_page(pfn); ret = try_grab_folio(page_folio(page), 1, flags); if (ret) page = ERR_PTR(ret); return page; }"
1392----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56722/bad/hns_roce_hw_v2.c----hns_roce_v2_query_cqc,"static int hns_roce_v2_query_cqc(struct hns_roce_dev *hr_dev, u32 cqn, void *buffer) { struct hns_roce_v2_cq_context *context; struct hns_roce_cmd_mailbox *mailbox; int ret; mailbox = hns_roce_alloc_cmd_mailbox(hr_dev); if (IS_ERR(mailbox)) return PTR_ERR(mailbox); context = mailbox->buf; ret = hns_roce_cmd_mbox(hr_dev, 0, mailbox->dma, HNS_ROCE_CMD_QUERY_CQC, cqn); if (ret) { <S2SV_StartVul> ibdev_err(&hr_dev->ib_dev, <S2SV_EndVul> <S2SV_StartVul> ""failed to process cmd when querying CQ, ret = %d.\n"", <S2SV_EndVul> <S2SV_StartVul> ret); <S2SV_EndVul> goto err_mailbox; <S2SV_StartVul> } <S2SV_EndVul> memcpy(buffer, context, sizeof(*context)); err_mailbox: hns_roce_free_cmd_mailbox(hr_dev, mailbox); return ret; }","- ibdev_err(&hr_dev->ib_dev,
- ""failed to process cmd when querying CQ, ret = %d.\n"",
- ret);
- }
+ ibdev_err_ratelimited(&hr_dev->ib_dev,
+ ""failed to process cmd when querying CQ, ret = %d.\n"",
+ ret);","static int hns_roce_v2_query_cqc(struct hns_roce_dev *hr_dev, u32 cqn, void *buffer) { struct hns_roce_v2_cq_context *context; struct hns_roce_cmd_mailbox *mailbox; int ret; mailbox = hns_roce_alloc_cmd_mailbox(hr_dev); if (IS_ERR(mailbox)) return PTR_ERR(mailbox); context = mailbox->buf; ret = hns_roce_cmd_mbox(hr_dev, 0, mailbox->dma, HNS_ROCE_CMD_QUERY_CQC, cqn); if (ret) { ibdev_err_ratelimited(&hr_dev->ib_dev, ""failed to process cmd when querying CQ, ret = %d.\n"", ret); goto err_mailbox; } memcpy(buffer, context, sizeof(*context)); err_mailbox: hns_roce_free_cmd_mailbox(hr_dev, mailbox); return ret; }"
1509----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2025-21665/bad/filemap.c----folio_seek_hole_data,"static inline loff_t folio_seek_hole_data(struct xa_state *xas, struct address_space *mapping, struct folio *folio, loff_t start, loff_t end, bool seek_data) { const struct address_space_operations *ops = mapping->a_ops; size_t offset, bsz = i_blocksize(mapping->host); if (xa_is_value(folio) || folio_test_uptodate(folio)) return seek_data ? start : end; if (!ops->is_partially_uptodate) return seek_data ? end : start; xas_pause(xas); rcu_read_unlock(); folio_lock(folio); if (unlikely(folio->mapping != mapping)) goto unlock; offset = offset_in_folio(folio, start) & ~(bsz - 1); do { if (ops->is_partially_uptodate(folio, offset, bsz) == seek_data) break; <S2SV_StartVul> start = (start + bsz) & ~(bsz - 1); <S2SV_EndVul> offset += bsz; } while (offset < folio_size(folio)); unlock: folio_unlock(folio); rcu_read_lock(); return start; }","- start = (start + bsz) & ~(bsz - 1);
+ start = (start + bsz) & ~((u64)bsz - 1);","static inline loff_t folio_seek_hole_data(struct xa_state *xas, struct address_space *mapping, struct folio *folio, loff_t start, loff_t end, bool seek_data) { const struct address_space_operations *ops = mapping->a_ops; size_t offset, bsz = i_blocksize(mapping->host); if (xa_is_value(folio) || folio_test_uptodate(folio)) return seek_data ? start : end; if (!ops->is_partially_uptodate) return seek_data ? end : start; xas_pause(xas); rcu_read_unlock(); folio_lock(folio); if (unlikely(folio->mapping != mapping)) goto unlock; offset = offset_in_folio(folio, start) & ~(bsz - 1); do { if (ops->is_partially_uptodate(folio, offset, bsz) == seek_data) break; start = (start + bsz) & ~((u64)bsz - 1); offset += bsz; } while (offset < folio_size(folio)); unlock: folio_unlock(folio); rcu_read_lock(); return start; }"
1320----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56629/bad/wacom_sys.c----wacom_update_name,"static void wacom_update_name(struct wacom *wacom, const char *suffix) { struct wacom_wac *wacom_wac = &wacom->wacom_wac; struct wacom_features *features = &wacom_wac->features; char name[WACOM_NAME_MAX - 20]; if ((features->type == HID_GENERIC) && !strcmp(""Wacom HID"", features->name)) { char *product_name = wacom->hdev->name; if (hid_is_usb(wacom->hdev)) { struct usb_interface *intf = to_usb_interface(wacom->hdev->dev.parent); struct usb_device *dev = interface_to_usbdev(intf); <S2SV_StartVul> product_name = dev->product; <S2SV_EndVul> } if (wacom->hdev->bus == BUS_I2C) { snprintf(name, sizeof(name), ""%s %X"", features->name, wacom->hdev->product); } else if (strstr(product_name, ""Wacom"") || strstr(product_name, ""wacom"") || strstr(product_name, ""WACOM"")) { if (strscpy(name, product_name, sizeof(name)) < 0) { hid_warn(wacom->hdev, ""String overflow while assembling device name""); } } else { snprintf(name, sizeof(name), ""Wacom %s"", product_name); } while (1) { char *gap = strstr(name, "" ""); if (gap == NULL) break; memmove(gap, gap+1, strlen(gap)); } if (name[strlen(name)-1] == ' ') name[strlen(name)-1] = '\0'; } else { if (strscpy(name, features->name, sizeof(name)) < 0) { hid_warn(wacom->hdev, ""String overflow while assembling device name""); } } snprintf(wacom_wac->name, sizeof(wacom_wac->name), ""%s%s"", name, suffix); snprintf(wacom_wac->pen_name, sizeof(wacom_wac->pen_name), ""%s%s Pen"", name, suffix); snprintf(wacom_wac->touch_name, sizeof(wacom_wac->touch_name), ""%s%s Finger"", name, suffix); snprintf(wacom_wac->pad_name, sizeof(wacom_wac->pad_name), ""%s%s Pad"", name, suffix); }","- product_name = dev->product;
+ if (dev->product != NULL)
+ product_name = dev->product;","static void wacom_update_name(struct wacom *wacom, const char *suffix) { struct wacom_wac *wacom_wac = &wacom->wacom_wac; struct wacom_features *features = &wacom_wac->features; char name[WACOM_NAME_MAX - 20]; if ((features->type == HID_GENERIC) && !strcmp(""Wacom HID"", features->name)) { char *product_name = wacom->hdev->name; if (hid_is_usb(wacom->hdev)) { struct usb_interface *intf = to_usb_interface(wacom->hdev->dev.parent); struct usb_device *dev = interface_to_usbdev(intf); if (dev->product != NULL) product_name = dev->product; } if (wacom->hdev->bus == BUS_I2C) { snprintf(name, sizeof(name), ""%s %X"", features->name, wacom->hdev->product); } else if (strstr(product_name, ""Wacom"") || strstr(product_name, ""wacom"") || strstr(product_name, ""WACOM"")) { if (strscpy(name, product_name, sizeof(name)) < 0) { hid_warn(wacom->hdev, ""String overflow while assembling device name""); } } else { snprintf(name, sizeof(name), ""Wacom %s"", product_name); } while (1) { char *gap = strstr(name, "" ""); if (gap == NULL) break; memmove(gap, gap+1, strlen(gap)); } if (name[strlen(name)-1] == ' ') name[strlen(name)-1] = '\0'; } else { if (strscpy(name, features->name, sizeof(name)) < 0) { hid_warn(wacom->hdev, ""String overflow while assembling device name""); } } snprintf(wacom_wac->name, sizeof(wacom_wac->name), ""%s%s"", name, suffix); snprintf(wacom_wac->pen_name, sizeof(wacom_wac->pen_name), ""%s%s Pen"", name, suffix); snprintf(wacom_wac->touch_name, sizeof(wacom_wac->touch_name), ""%s%s Finger"", name, suffix); snprintf(wacom_wac->pad_name, sizeof(wacom_wac->pad_name), ""%s%s Pad"", name, suffix); }"
199----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46683/bad/xe_exec_queue.c----xe_exec_queue_create_ioctl,"int xe_exec_queue_create_ioctl(struct drm_device *dev, void *data, struct drm_file *file) { struct xe_device *xe = to_xe_device(dev); struct xe_file *xef = to_xe_file(file); struct drm_xe_exec_queue_create *args = data; struct drm_xe_engine_class_instance eci[XE_HW_ENGINE_MAX_INSTANCE]; struct drm_xe_engine_class_instance __user *user_eci = u64_to_user_ptr(args->instances); struct xe_hw_engine *hwe; struct xe_vm *vm, *migrate_vm; struct xe_gt *gt; struct xe_exec_queue *q = NULL; u32 logical_mask; u32 id; u32 len; int err; if (XE_IOCTL_DBG(xe, args->flags) || XE_IOCTL_DBG(xe, args->reserved[0] || args->reserved[1])) return -EINVAL; len = args->width * args->num_placements; if (XE_IOCTL_DBG(xe, !len || len > XE_HW_ENGINE_MAX_INSTANCE)) return -EINVAL; err = __copy_from_user(eci, user_eci, sizeof(struct drm_xe_engine_class_instance) * len); if (XE_IOCTL_DBG(xe, err)) return -EFAULT; if (XE_IOCTL_DBG(xe, eci[0].gt_id >= xe->info.gt_count)) return -EINVAL; if (eci[0].engine_class == DRM_XE_ENGINE_CLASS_VM_BIND) { for_each_gt(gt, xe, id) { struct xe_exec_queue *new; u32 flags; if (xe_gt_is_media_type(gt)) continue; eci[0].gt_id = gt->info.id; logical_mask = bind_exec_queue_logical_mask(xe, gt, eci, args->width, args->num_placements); if (XE_IOCTL_DBG(xe, !logical_mask)) return -EINVAL; hwe = find_hw_engine(xe, eci[0]); if (XE_IOCTL_DBG(xe, !hwe)) return -EINVAL; xe_pm_runtime_get_noresume(xe); flags = EXEC_QUEUE_FLAG_VM | (id ? EXEC_QUEUE_FLAG_BIND_ENGINE_CHILD : 0); migrate_vm = xe_migrate_get_vm(gt_to_tile(gt)->migrate); new = xe_exec_queue_create(xe, migrate_vm, logical_mask, args->width, hwe, flags, args->extensions); xe_pm_runtime_put(xe); xe_vm_put(migrate_vm); if (IS_ERR(new)) { err = PTR_ERR(new); if (q) goto put_exec_queue; return err; } if (id == 0) q = new; else list_add_tail(&new->multi_gt_list, &q->multi_gt_link); } } else { gt = xe_device_get_gt(xe, eci[0].gt_id); logical_mask = calc_validate_logical_mask(xe, gt, eci, args->width, args->num_placements); if (XE_IOCTL_DBG(xe, !logical_mask)) return -EINVAL; hwe = find_hw_engine(xe, eci[0]); if (XE_IOCTL_DBG(xe, !hwe)) return -EINVAL; vm = xe_vm_lookup(xef, args->vm_id); if (XE_IOCTL_DBG(xe, !vm)) return -ENOENT; err = down_read_interruptible(&vm->lock); if (err) { xe_vm_put(vm); return err; } if (XE_IOCTL_DBG(xe, xe_vm_is_closed_or_banned(vm))) { up_read(&vm->lock); xe_vm_put(vm); return -ENOENT; } q = xe_exec_queue_create(xe, vm, logical_mask, args->width, hwe, 0, args->extensions); up_read(&vm->lock); xe_vm_put(vm); if (IS_ERR(q)) return PTR_ERR(q); if (xe_vm_in_preempt_fence_mode(vm)) { q->lr.context = dma_fence_context_alloc(1); <S2SV_StartVul> spin_lock_init(&q->lr.lock); <S2SV_EndVul> err = xe_vm_add_compute_exec_queue(vm, q); if (XE_IOCTL_DBG(xe, err)) goto put_exec_queue; } } mutex_lock(&xef->exec_queue.lock); err = xa_alloc(&xef->exec_queue.xa, &id, q, xa_limit_32b, GFP_KERNEL); mutex_unlock(&xef->exec_queue.lock); if (err) goto kill_exec_queue; args->exec_queue_id = id; q->xef = xe_file_get(xef); return 0; kill_exec_queue: xe_exec_queue_kill(q); put_exec_queue: xe_exec_queue_put(q); return err; }",- spin_lock_init(&q->lr.lock);,"int xe_exec_queue_create_ioctl(struct drm_device *dev, void *data, struct drm_file *file) { struct xe_device *xe = to_xe_device(dev); struct xe_file *xef = to_xe_file(file); struct drm_xe_exec_queue_create *args = data; struct drm_xe_engine_class_instance eci[XE_HW_ENGINE_MAX_INSTANCE]; struct drm_xe_engine_class_instance __user *user_eci = u64_to_user_ptr(args->instances); struct xe_hw_engine *hwe; struct xe_vm *vm, *migrate_vm; struct xe_gt *gt; struct xe_exec_queue *q = NULL; u32 logical_mask; u32 id; u32 len; int err; if (XE_IOCTL_DBG(xe, args->flags) || XE_IOCTL_DBG(xe, args->reserved[0] || args->reserved[1])) return -EINVAL; len = args->width * args->num_placements; if (XE_IOCTL_DBG(xe, !len || len > XE_HW_ENGINE_MAX_INSTANCE)) return -EINVAL; err = __copy_from_user(eci, user_eci, sizeof(struct drm_xe_engine_class_instance) * len); if (XE_IOCTL_DBG(xe, err)) return -EFAULT; if (XE_IOCTL_DBG(xe, eci[0].gt_id >= xe->info.gt_count)) return -EINVAL; if (eci[0].engine_class == DRM_XE_ENGINE_CLASS_VM_BIND) { for_each_gt(gt, xe, id) { struct xe_exec_queue *new; u32 flags; if (xe_gt_is_media_type(gt)) continue; eci[0].gt_id = gt->info.id; logical_mask = bind_exec_queue_logical_mask(xe, gt, eci, args->width, args->num_placements); if (XE_IOCTL_DBG(xe, !logical_mask)) return -EINVAL; hwe = find_hw_engine(xe, eci[0]); if (XE_IOCTL_DBG(xe, !hwe)) return -EINVAL; xe_pm_runtime_get_noresume(xe); flags = EXEC_QUEUE_FLAG_VM | (id ? EXEC_QUEUE_FLAG_BIND_ENGINE_CHILD : 0); migrate_vm = xe_migrate_get_vm(gt_to_tile(gt)->migrate); new = xe_exec_queue_create(xe, migrate_vm, logical_mask, args->width, hwe, flags, args->extensions); xe_pm_runtime_put(xe); xe_vm_put(migrate_vm); if (IS_ERR(new)) { err = PTR_ERR(new); if (q) goto put_exec_queue; return err; } if (id == 0) q = new; else list_add_tail(&new->multi_gt_list, &q->multi_gt_link); } } else { gt = xe_device_get_gt(xe, eci[0].gt_id); logical_mask = calc_validate_logical_mask(xe, gt, eci, args->width, args->num_placements); if (XE_IOCTL_DBG(xe, !logical_mask)) return -EINVAL; hwe = find_hw_engine(xe, eci[0]); if (XE_IOCTL_DBG(xe, !hwe)) return -EINVAL; vm = xe_vm_lookup(xef, args->vm_id); if (XE_IOCTL_DBG(xe, !vm)) return -ENOENT; err = down_read_interruptible(&vm->lock); if (err) { xe_vm_put(vm); return err; } if (XE_IOCTL_DBG(xe, xe_vm_is_closed_or_banned(vm))) { up_read(&vm->lock); xe_vm_put(vm); return -ENOENT; } q = xe_exec_queue_create(xe, vm, logical_mask, args->width, hwe, 0, args->extensions); up_read(&vm->lock); xe_vm_put(vm); if (IS_ERR(q)) return PTR_ERR(q); if (xe_vm_in_preempt_fence_mode(vm)) { q->lr.context = dma_fence_context_alloc(1); err = xe_vm_add_compute_exec_queue(vm, q); if (XE_IOCTL_DBG(xe, err)) goto put_exec_queue; } } mutex_lock(&xef->exec_queue.lock); err = xa_alloc(&xef->exec_queue.xa, &id, q, xa_limit_32b, GFP_KERNEL); mutex_unlock(&xef->exec_queue.lock); if (err) goto kill_exec_queue; args->exec_queue_id = id; q->xef = xe_file_get(xef); return 0; kill_exec_queue: xe_exec_queue_kill(q); put_exec_queue: xe_exec_queue_put(q); return err; }"
129----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44988/bad/global1_atu.c----mv88e6xxx_g1_atu_prob_irq_thread_fn,"static irqreturn_t mv88e6xxx_g1_atu_prob_irq_thread_fn(int irq, void *dev_id) { struct mv88e6xxx_chip *chip = dev_id; struct mv88e6xxx_atu_entry entry; int err, spid; u16 val, fid; mv88e6xxx_reg_lock(chip); err = mv88e6xxx_g1_read_atu_violation(chip); if (err) goto out; err = mv88e6xxx_g1_read(chip, MV88E6XXX_G1_ATU_OP, &val); if (err) goto out; err = mv88e6xxx_g1_atu_fid_read(chip, &fid); if (err) goto out; err = mv88e6xxx_g1_atu_data_read(chip, &entry); if (err) goto out; err = mv88e6xxx_g1_atu_mac_read(chip, &entry); if (err) goto out; spid = entry.state; if (val & MV88E6XXX_G1_ATU_OP_AGE_OUT_VIOLATION) { dev_err_ratelimited(chip->dev, ""ATU age out violation for %pM\n"", entry.mac); } if (val & MV88E6XXX_G1_ATU_OP_MEMBER_VIOLATION) { trace_mv88e6xxx_atu_member_violation(chip->dev, spid, entry.portvec, entry.mac, fid); chip->ports[spid].atu_member_violation++; } if (val & MV88E6XXX_G1_ATU_OP_MISS_VIOLATION) { trace_mv88e6xxx_atu_miss_violation(chip->dev, spid, entry.portvec, entry.mac, fid); chip->ports[spid].atu_miss_violation++; } if (val & MV88E6XXX_G1_ATU_OP_FULL_VIOLATION) { trace_mv88e6xxx_atu_full_violation(chip->dev, spid, entry.portvec, entry.mac, fid); <S2SV_StartVul> chip->ports[spid].atu_full_violation++; <S2SV_EndVul> } mv88e6xxx_reg_unlock(chip); return IRQ_HANDLED; out: mv88e6xxx_reg_unlock(chip); dev_err(chip->dev, ""ATU problem: error %d while handling interrupt\n"", err); return IRQ_HANDLED; }","- chip->ports[spid].atu_full_violation++;
+ if (spid < ARRAY_SIZE(chip->ports))
+ chip->ports[spid].atu_full_violation++;","static irqreturn_t mv88e6xxx_g1_atu_prob_irq_thread_fn(int irq, void *dev_id) { struct mv88e6xxx_chip *chip = dev_id; struct mv88e6xxx_atu_entry entry; int err, spid; u16 val, fid; mv88e6xxx_reg_lock(chip); err = mv88e6xxx_g1_read_atu_violation(chip); if (err) goto out; err = mv88e6xxx_g1_read(chip, MV88E6XXX_G1_ATU_OP, &val); if (err) goto out; err = mv88e6xxx_g1_atu_fid_read(chip, &fid); if (err) goto out; err = mv88e6xxx_g1_atu_data_read(chip, &entry); if (err) goto out; err = mv88e6xxx_g1_atu_mac_read(chip, &entry); if (err) goto out; spid = entry.state; if (val & MV88E6XXX_G1_ATU_OP_AGE_OUT_VIOLATION) { dev_err_ratelimited(chip->dev, ""ATU age out violation for %pM\n"", entry.mac); } if (val & MV88E6XXX_G1_ATU_OP_MEMBER_VIOLATION) { trace_mv88e6xxx_atu_member_violation(chip->dev, spid, entry.portvec, entry.mac, fid); chip->ports[spid].atu_member_violation++; } if (val & MV88E6XXX_G1_ATU_OP_MISS_VIOLATION) { trace_mv88e6xxx_atu_miss_violation(chip->dev, spid, entry.portvec, entry.mac, fid); chip->ports[spid].atu_miss_violation++; } if (val & MV88E6XXX_G1_ATU_OP_FULL_VIOLATION) { trace_mv88e6xxx_atu_full_violation(chip->dev, spid, entry.portvec, entry.mac, fid); if (spid < ARRAY_SIZE(chip->ports)) chip->ports[spid].atu_full_violation++; } mv88e6xxx_reg_unlock(chip); return IRQ_HANDLED; out: mv88e6xxx_reg_unlock(chip); dev_err(chip->dev, ""ATU problem: error %d while handling interrupt\n"", err); return IRQ_HANDLED; }"
635----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49940/bad/l2tp_core.c----l2tp_recv_common,"void l2tp_recv_common(struct l2tp_session *session, struct sk_buff *skb, unsigned char *ptr, unsigned char *optr, u16 hdrflags, int length) { struct l2tp_tunnel *tunnel = session->tunnel; int offset; if (session->peer_cookie_len > 0) { if (memcmp(ptr, &session->peer_cookie[0], session->peer_cookie_len)) { pr_debug_ratelimited(""%s: cookie mismatch (%u/%u). Discarding.\n"", tunnel->name, tunnel->tunnel_id, session->session_id); atomic_long_inc(&session->stats.rx_cookie_discards); goto discard; } ptr += session->peer_cookie_len; } L2TP_SKB_CB(skb)->has_seq = 0; if (tunnel->version == L2TP_HDR_VER_2) { if (hdrflags & L2TP_HDRFLAG_S) { L2TP_SKB_CB(skb)->ns = ntohs(*(__be16 *)ptr); L2TP_SKB_CB(skb)->has_seq = 1; ptr += 2; ptr += 2; } } else if (session->l2specific_type == L2TP_L2SPECTYPE_DEFAULT) { u32 l2h = ntohl(*(__be32 *)ptr); if (l2h & 0x40000000) { L2TP_SKB_CB(skb)->ns = l2h & 0x00ffffff; L2TP_SKB_CB(skb)->has_seq = 1; } ptr += 4; } if (L2TP_SKB_CB(skb)->has_seq) { if (!session->lns_mode && !session->send_seq) { trace_session_seqnum_lns_enable(session); session->send_seq = 1; <S2SV_StartVul> l2tp_session_set_header_len(session, tunnel->version); <S2SV_EndVul> } } else { if (session->recv_seq) { pr_debug_ratelimited(""%s: recv data has no seq numbers when required. Discarding.\n"", session->name); atomic_long_inc(&session->stats.rx_seq_discards); goto discard; } if (!session->lns_mode && session->send_seq) { trace_session_seqnum_lns_disable(session); session->send_seq = 0; <S2SV_StartVul> l2tp_session_set_header_len(session, tunnel->version); <S2SV_EndVul> } else if (session->send_seq) { pr_debug_ratelimited(""%s: recv data has no seq numbers when required. Discarding.\n"", session->name); atomic_long_inc(&session->stats.rx_seq_discards); goto discard; } } if (tunnel->version == L2TP_HDR_VER_2) { if (hdrflags & L2TP_HDRFLAG_O) { offset = ntohs(*(__be16 *)ptr); ptr += 2 + offset; } } offset = ptr - optr; if (!pskb_may_pull(skb, offset)) goto discard; __skb_pull(skb, offset); L2TP_SKB_CB(skb)->length = length; L2TP_SKB_CB(skb)->expires = jiffies + (session->reorder_timeout ? session->reorder_timeout : HZ); if (L2TP_SKB_CB(skb)->has_seq) { if (l2tp_recv_data_seq(session, skb)) goto discard; } else { skb_queue_tail(&session->reorder_q, skb); } l2tp_recv_dequeue(session); return; discard: atomic_long_inc(&session->stats.rx_errors); kfree_skb(skb); }","- l2tp_session_set_header_len(session, tunnel->version);
- l2tp_session_set_header_len(session, tunnel->version);
+ l2tp_session_set_header_len(session, tunnel->version,
+ tunnel->encap);
+ l2tp_session_set_header_len(session, tunnel->version,
+ tunnel->encap);","void l2tp_recv_common(struct l2tp_session *session, struct sk_buff *skb, unsigned char *ptr, unsigned char *optr, u16 hdrflags, int length) { struct l2tp_tunnel *tunnel = session->tunnel; int offset; if (session->peer_cookie_len > 0) { if (memcmp(ptr, &session->peer_cookie[0], session->peer_cookie_len)) { pr_debug_ratelimited(""%s: cookie mismatch (%u/%u). Discarding.\n"", tunnel->name, tunnel->tunnel_id, session->session_id); atomic_long_inc(&session->stats.rx_cookie_discards); goto discard; } ptr += session->peer_cookie_len; } L2TP_SKB_CB(skb)->has_seq = 0; if (tunnel->version == L2TP_HDR_VER_2) { if (hdrflags & L2TP_HDRFLAG_S) { L2TP_SKB_CB(skb)->ns = ntohs(*(__be16 *)ptr); L2TP_SKB_CB(skb)->has_seq = 1; ptr += 2; ptr += 2; } } else if (session->l2specific_type == L2TP_L2SPECTYPE_DEFAULT) { u32 l2h = ntohl(*(__be32 *)ptr); if (l2h & 0x40000000) { L2TP_SKB_CB(skb)->ns = l2h & 0x00ffffff; L2TP_SKB_CB(skb)->has_seq = 1; } ptr += 4; } if (L2TP_SKB_CB(skb)->has_seq) { if (!session->lns_mode && !session->send_seq) { trace_session_seqnum_lns_enable(session); session->send_seq = 1; l2tp_session_set_header_len(session, tunnel->version, tunnel->encap); } } else { if (session->recv_seq) { pr_debug_ratelimited(""%s: recv data has no seq numbers when required. Discarding.\n"", session->name); atomic_long_inc(&session->stats.rx_seq_discards); goto discard; } if (!session->lns_mode && session->send_seq) { trace_session_seqnum_lns_disable(session); session->send_seq = 0; l2tp_session_set_header_len(session, tunnel->version, tunnel->encap); } else if (session->send_seq) { pr_debug_ratelimited(""%s: recv data has no seq numbers when required. Discarding.\n"", session->name); atomic_long_inc(&session->stats.rx_seq_discards); goto discard; } } if (tunnel->version == L2TP_HDR_VER_2) { if (hdrflags & L2TP_HDRFLAG_O) { offset = ntohs(*(__be16 *)ptr); ptr += 2 + offset; } } offset = ptr - optr; if (!pskb_may_pull(skb, offset)) goto discard; __skb_pull(skb, offset); L2TP_SKB_CB(skb)->length = length; L2TP_SKB_CB(skb)->expires = jiffies + (session->reorder_timeout ? session->reorder_timeout : HZ); if (L2TP_SKB_CB(skb)->has_seq) { if (l2tp_recv_data_seq(session, skb)) goto discard; } else { skb_queue_tail(&session->reorder_q, skb); } l2tp_recv_dequeue(session); return; discard: atomic_long_inc(&session->stats.rx_errors); kfree_skb(skb); }"
1422----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56750/bad/super.c----erofs_fc_fill_super,"static int erofs_fc_fill_super(struct super_block *sb, struct fs_context *fc) { struct inode *inode; struct erofs_sb_info *sbi = EROFS_SB(sb); int err; sb->s_magic = EROFS_SUPER_MAGIC; sb->s_flags |= SB_RDONLY | SB_NOATIME; sb->s_maxbytes = MAX_LFS_FILESIZE; sb->s_op = &erofs_sops; sbi->blkszbits = PAGE_SHIFT; if (!sb->s_bdev) { sb->s_blocksize = PAGE_SIZE; sb->s_blocksize_bits = PAGE_SHIFT; if (erofs_is_fscache_mode(sb)) { err = erofs_fscache_register_fs(sb); if (err) return err; } err = super_setup_bdi(sb); if (err) return err; } else { if (!sb_set_blocksize(sb, PAGE_SIZE)) { errorfc(fc, ""failed to set initial blksize""); return -EINVAL; } sbi->dax_dev = fs_dax_get_by_bdev(sb->s_bdev, &sbi->dax_part_off, NULL, NULL); } err = erofs_read_superblock(sb); if (err) return err; if (sb->s_blocksize_bits != sbi->blkszbits) { if (erofs_is_fscache_mode(sb)) { errorfc(fc, ""unsupported blksize for fscache mode""); return -EINVAL; } <S2SV_StartVul> if (!sb_set_blocksize(sb, 1 << sbi->blkszbits)) { <S2SV_EndVul> errorfc(fc, ""failed to set erofs blksize""); return -EINVAL; } } if (test_opt(&sbi->opt, DAX_ALWAYS)) { if (!sbi->dax_dev) { errorfc(fc, ""DAX unsupported by block device. Turning off DAX.""); clear_opt(&sbi->opt, DAX_ALWAYS); } else if (sbi->blkszbits != PAGE_SHIFT) { errorfc(fc, ""unsupported blocksize for DAX""); clear_opt(&sbi->opt, DAX_ALWAYS); } } sb->s_time_gran = 1; sb->s_xattr = erofs_xattr_handlers; sb->s_export_op = &erofs_export_ops; if (test_opt(&sbi->opt, POSIX_ACL)) sb->s_flags |= SB_POSIXACL; else sb->s_flags &= ~SB_POSIXACL; #ifdef CONFIG_EROFS_FS_ZIP xa_init(&sbi->managed_pslots); #endif inode = erofs_iget(sb, sbi->root_nid); if (IS_ERR(inode)) return PTR_ERR(inode); if (!S_ISDIR(inode->i_mode)) { erofs_err(sb, ""rootino(nid %llu) is not a directory(i_mode %o)"", sbi->root_nid, inode->i_mode); iput(inode); return -EINVAL; } sb->s_root = d_make_root(inode); if (!sb->s_root) return -ENOMEM; erofs_shrinker_register(sb); if (erofs_sb_has_fragments(sbi) && sbi->packed_nid) { sbi->packed_inode = erofs_iget(sb, sbi->packed_nid); if (IS_ERR(sbi->packed_inode)) { err = PTR_ERR(sbi->packed_inode); sbi->packed_inode = NULL; return err; } } err = erofs_init_managed_cache(sb); if (err) return err; err = erofs_xattr_prefixes_init(sb); if (err) return err; erofs_set_sysfs_name(sb); err = erofs_register_sysfs(sb); if (err) return err; erofs_info(sb, ""mounted with root inode @ nid %llu."", sbi->root_nid); return 0; }","- if (!sb_set_blocksize(sb, 1 << sbi->blkszbits)) {
+ if (erofs_is_fileio_mode(sbi)) {
+ sb->s_blocksize = 1 << sbi->blkszbits;
+ sb->s_blocksize_bits = sbi->blkszbits;
+ } else if (!sb_set_blocksize(sb, 1 << sbi->blkszbits)) {","static int erofs_fc_fill_super(struct super_block *sb, struct fs_context *fc) { struct inode *inode; struct erofs_sb_info *sbi = EROFS_SB(sb); int err; sb->s_magic = EROFS_SUPER_MAGIC; sb->s_flags |= SB_RDONLY | SB_NOATIME; sb->s_maxbytes = MAX_LFS_FILESIZE; sb->s_op = &erofs_sops; sbi->blkszbits = PAGE_SHIFT; if (!sb->s_bdev) { sb->s_blocksize = PAGE_SIZE; sb->s_blocksize_bits = PAGE_SHIFT; if (erofs_is_fscache_mode(sb)) { err = erofs_fscache_register_fs(sb); if (err) return err; } err = super_setup_bdi(sb); if (err) return err; } else { if (!sb_set_blocksize(sb, PAGE_SIZE)) { errorfc(fc, ""failed to set initial blksize""); return -EINVAL; } sbi->dax_dev = fs_dax_get_by_bdev(sb->s_bdev, &sbi->dax_part_off, NULL, NULL); } err = erofs_read_superblock(sb); if (err) return err; if (sb->s_blocksize_bits != sbi->blkszbits) { if (erofs_is_fscache_mode(sb)) { errorfc(fc, ""unsupported blksize for fscache mode""); return -EINVAL; } if (erofs_is_fileio_mode(sbi)) { sb->s_blocksize = 1 << sbi->blkszbits; sb->s_blocksize_bits = sbi->blkszbits; } else if (!sb_set_blocksize(sb, 1 << sbi->blkszbits)) { errorfc(fc, ""failed to set erofs blksize""); return -EINVAL; } } if (test_opt(&sbi->opt, DAX_ALWAYS)) { if (!sbi->dax_dev) { errorfc(fc, ""DAX unsupported by block device. Turning off DAX.""); clear_opt(&sbi->opt, DAX_ALWAYS); } else if (sbi->blkszbits != PAGE_SHIFT) { errorfc(fc, ""unsupported blocksize for DAX""); clear_opt(&sbi->opt, DAX_ALWAYS); } } sb->s_time_gran = 1; sb->s_xattr = erofs_xattr_handlers; sb->s_export_op = &erofs_export_ops; if (test_opt(&sbi->opt, POSIX_ACL)) sb->s_flags |= SB_POSIXACL; else sb->s_flags &= ~SB_POSIXACL; #ifdef CONFIG_EROFS_FS_ZIP xa_init(&sbi->managed_pslots); #endif inode = erofs_iget(sb, sbi->root_nid); if (IS_ERR(inode)) return PTR_ERR(inode); if (!S_ISDIR(inode->i_mode)) { erofs_err(sb, ""rootino(nid %llu) is not a directory(i_mode %o)"", sbi->root_nid, inode->i_mode); iput(inode); return -EINVAL; } sb->s_root = d_make_root(inode); if (!sb->s_root) return -ENOMEM; erofs_shrinker_register(sb); if (erofs_sb_has_fragments(sbi) && sbi->packed_nid) { sbi->packed_inode = erofs_iget(sb, sbi->packed_nid); if (IS_ERR(sbi->packed_inode)) { err = PTR_ERR(sbi->packed_inode); sbi->packed_inode = NULL; return err; } } err = erofs_init_managed_cache(sb); if (err) return err; err = erofs_xattr_prefixes_init(sb); if (err) return err; erofs_set_sysfs_name(sb); err = erofs_register_sysfs(sb); if (err) return err; erofs_info(sb, ""mounted with root inode @ nid %llu."", sbi->root_nid); return 0; }"
93----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44962/bad/btnxpuart.c----ps_cancel_timer,static void ps_cancel_timer(struct btnxpuart_dev *nxpdev) { struct ps_data *psdata = &nxpdev->psdata; flush_work(&psdata->work); <S2SV_StartVul> del_timer_sync(&psdata->ps_timer); <S2SV_EndVul> },"- del_timer_sync(&psdata->ps_timer);
+ timer_shutdown_sync(&psdata->ps_timer);",static void ps_cancel_timer(struct btnxpuart_dev *nxpdev) { struct ps_data *psdata = &nxpdev->psdata; flush_work(&psdata->work); timer_shutdown_sync(&psdata->ps_timer); }
1163----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53138/bad/ktls_tx.c----mlx5e_ktls_tx_handle_resync_dump_comp,"void mlx5e_ktls_tx_handle_resync_dump_comp(struct mlx5e_txqsq *sq, struct mlx5e_tx_wqe_info *wi, u32 *dma_fifo_cc) { struct mlx5e_sq_stats *stats; struct mlx5e_sq_dma *dma; dma = mlx5e_dma_get(sq, (*dma_fifo_cc)++); stats = sq->stats; mlx5e_tx_dma_unmap(sq->pdev, dma); <S2SV_StartVul> put_page(wi->resync_dump_frag_page); <S2SV_EndVul> stats->tls_dump_packets++; stats->tls_dump_bytes += wi->num_bytes; }","- put_page(wi->resync_dump_frag_page);
+ page_ref_dec(wi->resync_dump_frag_page);","void mlx5e_ktls_tx_handle_resync_dump_comp(struct mlx5e_txqsq *sq, struct mlx5e_tx_wqe_info *wi, u32 *dma_fifo_cc) { struct mlx5e_sq_stats *stats; struct mlx5e_sq_dma *dma; dma = mlx5e_dma_get(sq, (*dma_fifo_cc)++); stats = sq->stats; mlx5e_tx_dma_unmap(sq->pdev, dma); page_ref_dec(wi->resync_dump_frag_page); stats->tls_dump_packets++; stats->tls_dump_bytes += wi->num_bytes; }"
1184----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53170/bad/genhd.c----del_gendisk,"void del_gendisk(struct gendisk *disk) { struct request_queue *q = disk->queue; struct block_device *part; unsigned long idx; bool start_drain, queue_dying; might_sleep(); if (WARN_ON_ONCE(!disk_live(disk) && !(disk->flags & GENHD_FL_HIDDEN))) return; disk_del_events(disk); mutex_lock(&disk->open_mutex); xa_for_each(&disk->part_tbl, idx, part) bdev_unhash(part); mutex_unlock(&disk->open_mutex); if (!test_bit(GD_DEAD, &disk->state)) blk_report_disk_dead(disk, false); mutex_lock(&disk->open_mutex); start_drain = __blk_mark_disk_dead(disk); queue_dying = blk_queue_dying(q); if (start_drain) blk_freeze_acquire_lock(q, true, queue_dying); xa_for_each_start(&disk->part_tbl, idx, part, 1) drop_partition(part); mutex_unlock(&disk->open_mutex); if (!(disk->flags & GENHD_FL_HIDDEN)) { sysfs_remove_link(&disk_to_dev(disk)->kobj, ""bdi""); bdi_unregister(disk->bdi); } blk_unregister_queue(disk); kobject_put(disk->part0->bd_holder_dir); kobject_put(disk->slave_dir); disk->slave_dir = NULL; part_stat_set_all(disk->part0, 0); disk->part0->bd_stamp = 0; sysfs_remove_link(block_depr, dev_name(disk_to_dev(disk))); pm_runtime_set_memalloc_noio(disk_to_dev(disk), false); device_del(disk_to_dev(disk)); blk_mq_freeze_queue_wait(q); blk_throtl_cancel_bios(disk); blk_sync_queue(q); blk_flush_integrity(); if (queue_is_mq(q)) blk_mq_cancel_work_sync(q); blk_mq_quiesce_queue(q); if (q->elevator) { mutex_lock(&q->sysfs_lock); elevator_exit(q); mutex_unlock(&q->sysfs_lock); } rq_qos_exit(q); blk_mq_unquiesce_queue(q); <S2SV_StartVul> if (!test_bit(GD_OWNS_QUEUE, &disk->state)) { <S2SV_EndVul> <S2SV_StartVul> blk_queue_flag_clear(QUEUE_FLAG_INIT_DONE, q); <S2SV_EndVul> __blk_mq_unfreeze_queue(q, true); <S2SV_StartVul> } else { <S2SV_EndVul> <S2SV_StartVul> if (queue_is_mq(q)) <S2SV_EndVul> <S2SV_StartVul> blk_mq_exit_queue(q); <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> if (start_drain) blk_unfreeze_release_lock(q, true, queue_dying); }","- if (!test_bit(GD_OWNS_QUEUE, &disk->state)) {
- blk_queue_flag_clear(QUEUE_FLAG_INIT_DONE, q);
- } else {
- if (queue_is_mq(q))
- blk_mq_exit_queue(q);
- }
+ if (!test_bit(GD_OWNS_QUEUE, &disk->state))
+ else if (queue_is_mq(q))
+ blk_mq_exit_queue(q);","void del_gendisk(struct gendisk *disk) { struct request_queue *q = disk->queue; struct block_device *part; unsigned long idx; bool start_drain, queue_dying; might_sleep(); if (WARN_ON_ONCE(!disk_live(disk) && !(disk->flags & GENHD_FL_HIDDEN))) return; disk_del_events(disk); mutex_lock(&disk->open_mutex); xa_for_each(&disk->part_tbl, idx, part) bdev_unhash(part); mutex_unlock(&disk->open_mutex); if (!test_bit(GD_DEAD, &disk->state)) blk_report_disk_dead(disk, false); mutex_lock(&disk->open_mutex); start_drain = __blk_mark_disk_dead(disk); queue_dying = blk_queue_dying(q); if (start_drain) blk_freeze_acquire_lock(q, true, queue_dying); xa_for_each_start(&disk->part_tbl, idx, part, 1) drop_partition(part); mutex_unlock(&disk->open_mutex); if (!(disk->flags & GENHD_FL_HIDDEN)) { sysfs_remove_link(&disk_to_dev(disk)->kobj, ""bdi""); bdi_unregister(disk->bdi); } blk_unregister_queue(disk); kobject_put(disk->part0->bd_holder_dir); kobject_put(disk->slave_dir); disk->slave_dir = NULL; part_stat_set_all(disk->part0, 0); disk->part0->bd_stamp = 0; sysfs_remove_link(block_depr, dev_name(disk_to_dev(disk))); pm_runtime_set_memalloc_noio(disk_to_dev(disk), false); device_del(disk_to_dev(disk)); blk_mq_freeze_queue_wait(q); blk_throtl_cancel_bios(disk); blk_sync_queue(q); blk_flush_integrity(); if (queue_is_mq(q)) blk_mq_cancel_work_sync(q); blk_mq_quiesce_queue(q); if (q->elevator) { mutex_lock(&q->sysfs_lock); elevator_exit(q); mutex_unlock(&q->sysfs_lock); } rq_qos_exit(q); blk_mq_unquiesce_queue(q); if (!test_bit(GD_OWNS_QUEUE, &disk->state)) __blk_mq_unfreeze_queue(q, true); else if (queue_is_mq(q)) blk_mq_exit_queue(q); if (start_drain) blk_unfreeze_release_lock(q, true, queue_dying); }"
495----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47735/bad/hns_roce_qp.c----hns_roce_unlock_cqs,"void hns_roce_unlock_cqs(struct hns_roce_cq *send_cq, struct hns_roce_cq *recv_cq) __releases(&send_cq->lock) __releases(&recv_cq->lock) { if (unlikely(send_cq == NULL && recv_cq == NULL)) { __release(&recv_cq->lock); __release(&send_cq->lock); } else if (unlikely(send_cq != NULL && recv_cq == NULL)) { __release(&recv_cq->lock); spin_unlock(&send_cq->lock); } else if (unlikely(send_cq == NULL && recv_cq != NULL)) { __release(&send_cq->lock); spin_unlock(&recv_cq->lock); } else if (send_cq == recv_cq) { __release(&recv_cq->lock); <S2SV_StartVul> spin_unlock_irq(&send_cq->lock); <S2SV_EndVul> } else if (send_cq->cqn < recv_cq->cqn) { spin_unlock(&recv_cq->lock); <S2SV_StartVul> spin_unlock_irq(&send_cq->lock); <S2SV_EndVul> } else { spin_unlock(&send_cq->lock); <S2SV_StartVul> spin_unlock_irq(&recv_cq->lock); <S2SV_EndVul> } }","- spin_unlock_irq(&send_cq->lock);
- spin_unlock_irq(&send_cq->lock);
- spin_unlock_irq(&recv_cq->lock);
+ spin_unlock(&send_cq->lock);
+ spin_unlock(&recv_cq->lock);
+ spin_unlock(&send_cq->lock);
+ spin_unlock(&send_cq->lock);
+ spin_unlock(&recv_cq->lock);","void hns_roce_unlock_cqs(struct hns_roce_cq *send_cq, struct hns_roce_cq *recv_cq) __releases(&send_cq->lock) __releases(&recv_cq->lock) { if (unlikely(send_cq == NULL && recv_cq == NULL)) { __release(&recv_cq->lock); __release(&send_cq->lock); } else if (unlikely(send_cq != NULL && recv_cq == NULL)) { __release(&recv_cq->lock); spin_unlock(&send_cq->lock); } else if (unlikely(send_cq == NULL && recv_cq != NULL)) { __release(&send_cq->lock); spin_unlock(&recv_cq->lock); } else if (send_cq == recv_cq) { __release(&recv_cq->lock); spin_unlock(&send_cq->lock); } else if (send_cq->cqn < recv_cq->cqn) { spin_unlock(&recv_cq->lock); spin_unlock(&send_cq->lock); } else { spin_unlock(&send_cq->lock); spin_unlock(&recv_cq->lock); } }"
172----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-45026/bad/dasd.c----dasd_int_handler,"void dasd_int_handler(struct ccw_device *cdev, unsigned long intparm, struct irb *irb) { struct dasd_ccw_req *cqr, *next, *fcqr; struct dasd_device *device; unsigned long now; int nrf_suppressed = 0; <S2SV_StartVul> int fp_suppressed = 0; <S2SV_EndVul> struct request *req; u8 *sense = NULL; int expires; cqr = (struct dasd_ccw_req *) intparm; if (IS_ERR(irb)) { switch (PTR_ERR(irb)) { case -EIO: if (cqr && cqr->status == DASD_CQR_CLEAR_PENDING) { device = cqr->startdev; cqr->status = DASD_CQR_CLEARED; dasd_device_clear_timer(device); wake_up(&dasd_flush_wq); dasd_schedule_device_bh(device); return; } break; case -ETIMEDOUT: DBF_EVENT_DEVID(DBF_WARNING, cdev, ""%s: "" ""request timed out\n"", __func__); break; default: DBF_EVENT_DEVID(DBF_WARNING, cdev, ""%s: "" ""unknown error %ld\n"", __func__, PTR_ERR(irb)); } dasd_handle_killed_request(cdev, intparm); return; } now = get_tod_clock(); if (!cqr || !(scsw_dstat(&irb->scsw) == (DEV_STAT_CHN_END | DEV_STAT_DEV_END) && scsw_cstat(&irb->scsw) == 0)) { if (cqr) memcpy(&cqr->irb, irb, sizeof(*irb)); device = dasd_device_from_cdev_locked(cdev); if (IS_ERR(device)) return; if (device->discipline == dasd_diag_discipline_pointer) { dasd_put_device(device); return; } sense = dasd_get_sense(irb); if (sense) { <S2SV_StartVul> fp_suppressed = (sense[1] & SNS1_FILE_PROTECTED) && <S2SV_EndVul> <S2SV_StartVul> test_bit(DASD_CQR_SUPPRESS_FP, &cqr->flags); <S2SV_EndVul> nrf_suppressed = (sense[1] & SNS1_NO_REC_FOUND) && test_bit(DASD_CQR_SUPPRESS_NRF, &cqr->flags); if (dasd_ese_oos_cond(sense)) { dasd_generic_space_exhaust(device, cqr); device->discipline->ext_pool_exhaust(device, cqr); dasd_put_device(device); return; } } <S2SV_StartVul> if (!(fp_suppressed || nrf_suppressed)) <S2SV_EndVul> device->discipline->dump_sense_dbf(device, irb, ""int""); if (device->features & DASD_FEATURE_ERPLOG) device->discipline->dump_sense(device, cqr, irb); device->discipline->check_for_device_change(device, cqr, irb); dasd_put_device(device); } if (scsw_dstat(&irb->scsw) & DEV_STAT_ATTENTION) { device = dasd_device_from_cdev_locked(cdev); if (!IS_ERR(device)) { device->discipline->check_attention(device, irb->esw.esw1.lpum); dasd_put_device(device); } } if (!cqr) return; device = (struct dasd_device *) cqr->startdev; if (!device || strncmp(device->discipline->ebcname, (char *) &cqr->magic, 4)) { DBF_EVENT_DEVID(DBF_DEBUG, cdev, ""%s"", ""invalid device in request""); return; } if (dasd_ese_needs_format(cqr->block, irb)) { req = dasd_get_callback_data(cqr); if (!req) { cqr->status = DASD_CQR_ERROR; return; } if (rq_data_dir(req) == READ) { device->discipline->ese_read(cqr, irb); cqr->status = DASD_CQR_SUCCESS; cqr->stopclk = now; dasd_device_clear_timer(device); dasd_schedule_device_bh(device); return; } fcqr = device->discipline->ese_format(device, cqr, irb); if (IS_ERR(fcqr)) { if (PTR_ERR(fcqr) == -EINVAL) { cqr->status = DASD_CQR_ERROR; return; } cqr->status = DASD_CQR_QUEUED; dasd_schedule_device_bh(device); return; } else { fcqr->status = DASD_CQR_QUEUED; cqr->status = DASD_CQR_QUEUED; list_add(&fcqr->devlist, &device->ccw_queue); dasd_schedule_device_bh(device); return; } } if (cqr->status == DASD_CQR_CLEAR_PENDING && scsw_fctl(&irb->scsw) & SCSW_FCTL_CLEAR_FUNC) { cqr->status = DASD_CQR_CLEARED; dasd_device_clear_timer(device); wake_up(&dasd_flush_wq); dasd_schedule_device_bh(device); return; } if (cqr->status != DASD_CQR_IN_IO) { DBF_DEV_EVENT(DBF_DEBUG, device, ""invalid status: bus_id %s, "" ""status %02x"", dev_name(&cdev->dev), cqr->status); return; } next = NULL; expires = 0; if (scsw_dstat(&irb->scsw) == (DEV_STAT_CHN_END | DEV_STAT_DEV_END) && scsw_cstat(&irb->scsw) == 0) { cqr->status = DASD_CQR_SUCCESS; cqr->stopclk = now; if (cqr->devlist.next != &device->ccw_queue) { next = list_entry(cqr->devlist.next, struct dasd_ccw_req, devlist); } } else { if (cqr->cpmode && dasd_check_hpf_error(irb) && device->discipline->handle_hpf_error) device->discipline->handle_hpf_error(device, irb); if (!test_bit(DASD_CQR_FLAGS_USE_ERP, &cqr->flags) && cqr->retries > 0) { if (cqr->lpm == dasd_path_get_opm(device)) DBF_DEV_EVENT(DBF_DEBUG, device, ""default ERP in fastpath "" ""(%i retries left)"", cqr->retries); if (!test_bit(DASD_CQR_VERIFY_PATH, &cqr->flags)) cqr->lpm = dasd_path_get_opm(device); cqr->status = DASD_CQR_QUEUED; next = cqr; } else cqr->status = DASD_CQR_ERROR; } if (next && (next->status == DASD_CQR_QUEUED) && (!device->stopped)) { if (device->discipline->start_IO(next) == 0) expires = next->expires; } if (expires != 0) dasd_device_set_timer(device, expires); else dasd_device_clear_timer(device); dasd_schedule_device_bh(device); }","- int fp_suppressed = 0;
- fp_suppressed = (sense[1] & SNS1_FILE_PROTECTED) &&
- test_bit(DASD_CQR_SUPPRESS_FP, &cqr->flags);
- if (!(fp_suppressed || nrf_suppressed))
+ int it_suppressed = 0;
+ it_suppressed = (sense[1] & SNS1_INV_TRACK_FORMAT) &&
+ !(sense[2] & SNS2_ENV_DATA_PRESENT) &&
+ test_bit(DASD_CQR_SUPPRESS_IT, &cqr->flags);
+ if (!(it_suppressed || nrf_suppressed))","void dasd_int_handler(struct ccw_device *cdev, unsigned long intparm, struct irb *irb) { struct dasd_ccw_req *cqr, *next, *fcqr; struct dasd_device *device; unsigned long now; int nrf_suppressed = 0; int it_suppressed = 0; struct request *req; u8 *sense = NULL; int expires; cqr = (struct dasd_ccw_req *) intparm; if (IS_ERR(irb)) { switch (PTR_ERR(irb)) { case -EIO: if (cqr && cqr->status == DASD_CQR_CLEAR_PENDING) { device = cqr->startdev; cqr->status = DASD_CQR_CLEARED; dasd_device_clear_timer(device); wake_up(&dasd_flush_wq); dasd_schedule_device_bh(device); return; } break; case -ETIMEDOUT: DBF_EVENT_DEVID(DBF_WARNING, cdev, ""%s: "" ""request timed out\n"", __func__); break; default: DBF_EVENT_DEVID(DBF_WARNING, cdev, ""%s: "" ""unknown error %ld\n"", __func__, PTR_ERR(irb)); } dasd_handle_killed_request(cdev, intparm); return; } now = get_tod_clock(); if (!cqr || !(scsw_dstat(&irb->scsw) == (DEV_STAT_CHN_END | DEV_STAT_DEV_END) && scsw_cstat(&irb->scsw) == 0)) { if (cqr) memcpy(&cqr->irb, irb, sizeof(*irb)); device = dasd_device_from_cdev_locked(cdev); if (IS_ERR(device)) return; if (device->discipline == dasd_diag_discipline_pointer) { dasd_put_device(device); return; } sense = dasd_get_sense(irb); if (sense) { it_suppressed = (sense[1] & SNS1_INV_TRACK_FORMAT) && !(sense[2] & SNS2_ENV_DATA_PRESENT) && test_bit(DASD_CQR_SUPPRESS_IT, &cqr->flags); nrf_suppressed = (sense[1] & SNS1_NO_REC_FOUND) && test_bit(DASD_CQR_SUPPRESS_NRF, &cqr->flags); if (dasd_ese_oos_cond(sense)) { dasd_generic_space_exhaust(device, cqr); device->discipline->ext_pool_exhaust(device, cqr); dasd_put_device(device); return; } } if (!(it_suppressed || nrf_suppressed)) device->discipline->dump_sense_dbf(device, irb, ""int""); if (device->features & DASD_FEATURE_ERPLOG) device->discipline->dump_sense(device, cqr, irb); device->discipline->check_for_device_change(device, cqr, irb); dasd_put_device(device); } if (scsw_dstat(&irb->scsw) & DEV_STAT_ATTENTION) { device = dasd_device_from_cdev_locked(cdev); if (!IS_ERR(device)) { device->discipline->check_attention(device, irb->esw.esw1.lpum); dasd_put_device(device); } } if (!cqr) return; device = (struct dasd_device *) cqr->startdev; if (!device || strncmp(device->discipline->ebcname, (char *) &cqr->magic, 4)) { DBF_EVENT_DEVID(DBF_DEBUG, cdev, ""%s"", ""invalid device in request""); return; } if (dasd_ese_needs_format(cqr->block, irb)) { req = dasd_get_callback_data(cqr); if (!req) { cqr->status = DASD_CQR_ERROR; return; } if (rq_data_dir(req) == READ) { device->discipline->ese_read(cqr, irb); cqr->status = DASD_CQR_SUCCESS; cqr->stopclk = now; dasd_device_clear_timer(device); dasd_schedule_device_bh(device); return; } fcqr = device->discipline->ese_format(device, cqr, irb); if (IS_ERR(fcqr)) { if (PTR_ERR(fcqr) == -EINVAL) { cqr->status = DASD_CQR_ERROR; return; } cqr->status = DASD_CQR_QUEUED; dasd_schedule_device_bh(device); return; } else { fcqr->status = DASD_CQR_QUEUED; cqr->status = DASD_CQR_QUEUED; list_add(&fcqr->devlist, &device->ccw_queue); dasd_schedule_device_bh(device); return; } } if (cqr->status == DASD_CQR_CLEAR_PENDING && scsw_fctl(&irb->scsw) & SCSW_FCTL_CLEAR_FUNC) { cqr->status = DASD_CQR_CLEARED; dasd_device_clear_timer(device); wake_up(&dasd_flush_wq); dasd_schedule_device_bh(device); return; } if (cqr->status != DASD_CQR_IN_IO) { DBF_DEV_EVENT(DBF_DEBUG, device, ""invalid status: bus_id %s, "" ""status %02x"", dev_name(&cdev->dev), cqr->status); return; } next = NULL; expires = 0; if (scsw_dstat(&irb->scsw) == (DEV_STAT_CHN_END | DEV_STAT_DEV_END) && scsw_cstat(&irb->scsw) == 0) { cqr->status = DASD_CQR_SUCCESS; cqr->stopclk = now; if (cqr->devlist.next != &device->ccw_queue) { next = list_entry(cqr->devlist.next, struct dasd_ccw_req, devlist); } } else { if (cqr->cpmode && dasd_check_hpf_error(irb) && device->discipline->handle_hpf_error) device->discipline->handle_hpf_error(device, irb); if (!test_bit(DASD_CQR_FLAGS_USE_ERP, &cqr->flags) && cqr->retries > 0) { if (cqr->lpm == dasd_path_get_opm(device)) DBF_DEV_EVENT(DBF_DEBUG, device, ""default ERP in fastpath "" ""(%i retries left)"", cqr->retries); if (!test_bit(DASD_CQR_VERIFY_PATH, &cqr->flags)) cqr->lpm = dasd_path_get_opm(device); cqr->status = DASD_CQR_QUEUED; next = cqr; } else cqr->status = DASD_CQR_ERROR; } if (next && (next->status == DASD_CQR_QUEUED) && (!device->stopped)) { if (device->discipline->start_IO(next) == 0) expires = next->expires; } if (expires != 0) dasd_device_set_timer(device, expires); else dasd_device_clear_timer(device); dasd_schedule_device_bh(device); }"
1303----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56599/bad/sdio.c----ath10k_sdio_remove,"static void ath10k_sdio_remove(struct sdio_func *func) { struct ath10k_sdio *ar_sdio = sdio_get_drvdata(func); struct ath10k *ar = ar_sdio->ar; ath10k_dbg(ar, ATH10K_DBG_BOOT, ""sdio removed func %d vendor 0x%x device 0x%x\n"", func->num, func->vendor, func->device); ath10k_core_unregister(ar); netif_napi_del(&ar->napi); <S2SV_StartVul> ath10k_core_destroy(ar); <S2SV_EndVul> destroy_workqueue(ar_sdio->workqueue); }","- ath10k_core_destroy(ar);
+ ath10k_core_destroy(ar);","static void ath10k_sdio_remove(struct sdio_func *func) { struct ath10k_sdio *ar_sdio = sdio_get_drvdata(func); struct ath10k *ar = ar_sdio->ar; ath10k_dbg(ar, ATH10K_DBG_BOOT, ""sdio removed func %d vendor 0x%x device 0x%x\n"", func->num, func->vendor, func->device); ath10k_core_unregister(ar); netif_napi_del(&ar->napi); destroy_workqueue(ar_sdio->workqueue); ath10k_core_destroy(ar); }"
314----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46780/bad/sysfs.c----nilfs_dev_uuid_show,"ssize_t nilfs_dev_uuid_show(struct nilfs_dev_attr *attr, struct the_nilfs *nilfs, char *buf) { <S2SV_StartVul> struct nilfs_super_block **sbp = nilfs->ns_sbp; <S2SV_EndVul> <S2SV_StartVul> return sysfs_emit(buf, ""%pUb\n"", sbp[0]->s_uuid); <S2SV_EndVul> }","- struct nilfs_super_block **sbp = nilfs->ns_sbp;
- return sysfs_emit(buf, ""%pUb\n"", sbp[0]->s_uuid);
+ struct nilfs_super_block *raw_sb;
+ ssize_t len;
+ down_read(&nilfs->ns_sem);
+ raw_sb = nilfs->ns_sbp[0];
+ len = sysfs_emit(buf, ""%pUb\n"", raw_sb->s_uuid);
+ up_read(&nilfs->ns_sem);
+ return len;","ssize_t nilfs_dev_uuid_show(struct nilfs_dev_attr *attr, struct the_nilfs *nilfs, char *buf) { struct nilfs_super_block *raw_sb; ssize_t len; down_read(&nilfs->ns_sem); raw_sb = nilfs->ns_sbp[0]; len = sysfs_emit(buf, ""%pUb\n"", raw_sb->s_uuid); up_read(&nilfs->ns_sem); return len; }"
804----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50077/bad/iso.c----iso_init,"int iso_init(void) { int err; BUILD_BUG_ON(sizeof(struct sockaddr_iso) > sizeof(struct sockaddr)); if (iso_inited) return -EALREADY; err = proto_register(&iso_proto, 0); if (err < 0) return err; err = bt_sock_register(BTPROTO_ISO, &iso_sock_family_ops); if (err < 0) { BT_ERR(""ISO socket registration failed""); goto error; } err = bt_procfs_init(&init_net, ""iso"", &iso_sk_list, NULL); if (err < 0) { BT_ERR(""Failed to create ISO proc file""); bt_sock_unregister(BTPROTO_ISO); goto error; } BT_INFO(""ISO socket layer initialized""); hci_register_cb(&iso_cb); <S2SV_StartVul> if (IS_ERR_OR_NULL(bt_debugfs)) <S2SV_EndVul> <S2SV_StartVul> return 0; <S2SV_EndVul> <S2SV_StartVul> if (!iso_debugfs) { <S2SV_EndVul> iso_debugfs = debugfs_create_file(""iso"", 0444, bt_debugfs, NULL, &iso_debugfs_fops); <S2SV_StartVul> } <S2SV_EndVul> iso_inited = true; return 0; error: proto_unregister(&iso_proto); return err; }","- if (IS_ERR_OR_NULL(bt_debugfs))
- return 0;
- if (!iso_debugfs) {
- }
+ if (!IS_ERR_OR_NULL(bt_debugfs))","int iso_init(void) { int err; BUILD_BUG_ON(sizeof(struct sockaddr_iso) > sizeof(struct sockaddr)); if (iso_inited) return -EALREADY; err = proto_register(&iso_proto, 0); if (err < 0) return err; err = bt_sock_register(BTPROTO_ISO, &iso_sock_family_ops); if (err < 0) { BT_ERR(""ISO socket registration failed""); goto error; } err = bt_procfs_init(&init_net, ""iso"", &iso_sk_list, NULL); if (err < 0) { BT_ERR(""Failed to create ISO proc file""); bt_sock_unregister(BTPROTO_ISO); goto error; } BT_INFO(""ISO socket layer initialized""); hci_register_cb(&iso_cb); if (!IS_ERR_OR_NULL(bt_debugfs)) iso_debugfs = debugfs_create_file(""iso"", 0444, bt_debugfs, NULL, &iso_debugfs_fops); iso_inited = true; return 0; error: proto_unregister(&iso_proto); return err; }"
489----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47730/bad/qm.c----qm_controller_reset_prepare,"static int qm_controller_reset_prepare(struct hisi_qm *qm) { struct pci_dev *pdev = qm->pdev; int ret; ret = qm_reset_prepare_ready(qm); if (ret) { pci_err(pdev, ""Controller reset not ready!\n""); return ret; <S2SV_StartVul> } <S2SV_EndVul> qm_cmd_uninit(qm); ret = qm_try_stop_vfs(qm, QM_PF_SRST_PREPARE, QM_SOFT_RESET); if (ret) pci_err(pdev, ""failed to stop vfs by pf in soft reset.\n""); ret = hisi_qm_stop(qm, QM_SOFT_RESET); if (ret) { pci_err(pdev, ""Fails to stop QM!\n""); qm_reset_bit_clear(qm); return ret; } if (qm->use_sva) { ret = qm_hw_err_isolate(qm); if (ret) pci_err(pdev, ""failed to isolate hw err!\n""); } ret = qm_wait_vf_prepare_finish(qm); if (ret) pci_err(pdev, ""failed to stop by vfs in soft reset!\n""); clear_bit(QM_RST_SCHED, &qm->misc_ctl); return 0; }","- }
+ }
+ qm_dev_ecc_mbit_handle(qm);","static int qm_controller_reset_prepare(struct hisi_qm *qm) { struct pci_dev *pdev = qm->pdev; int ret; ret = qm_reset_prepare_ready(qm); if (ret) { pci_err(pdev, ""Controller reset not ready!\n""); return ret; } qm_dev_ecc_mbit_handle(qm); qm_cmd_uninit(qm); ret = qm_try_stop_vfs(qm, QM_PF_SRST_PREPARE, QM_SOFT_RESET); if (ret) pci_err(pdev, ""failed to stop vfs by pf in soft reset.\n""); ret = hisi_qm_stop(qm, QM_SOFT_RESET); if (ret) { pci_err(pdev, ""Fails to stop QM!\n""); qm_reset_bit_clear(qm); return ret; } if (qm->use_sva) { ret = qm_hw_err_isolate(qm); if (ret) pci_err(pdev, ""failed to isolate hw err!\n""); } ret = qm_wait_vf_prepare_finish(qm); if (ret) pci_err(pdev, ""failed to stop by vfs in soft reset!\n""); clear_bit(QM_RST_SCHED, &qm->misc_ctl); return 0; }"
830----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50097/bad/fec_main.c----fec_restart,"fec_restart(struct net_device *ndev) { struct fec_enet_private *fep = netdev_priv(ndev); u32 temp_mac[2]; u32 rcntl = OPT_FRAME_SIZE | 0x04; u32 ecntl = FEC_ECR_ETHEREN; <S2SV_StartVul> fec_ptp_save_state(fep); <S2SV_EndVul> if (fep->quirks & FEC_QUIRK_HAS_MULTI_QUEUES || ((fep->quirks & FEC_QUIRK_NO_HARD_RESET) && fep->link)) { writel(0, fep->hwp + FEC_ECNTRL); } else { writel(1, fep->hwp + FEC_ECNTRL); udelay(10); } memcpy(&temp_mac, ndev->dev_addr, ETH_ALEN); writel((__force u32)cpu_to_be32(temp_mac[0]), fep->hwp + FEC_ADDR_LOW); writel((__force u32)cpu_to_be32(temp_mac[1]), fep->hwp + FEC_ADDR_HIGH); writel((0xffffffff & ~FEC_ENET_MII), fep->hwp + FEC_IEVENT); fec_enet_bd_init(ndev); fec_enet_enable_ring(ndev); if (fep->full_duplex == DUPLEX_FULL) { writel(0x04, fep->hwp + FEC_X_CNTRL); } else { rcntl |= 0x02; writel(0x0, fep->hwp + FEC_X_CNTRL); } writel(fep->phy_speed, fep->hwp + FEC_MII_SPEED); #if !defined(CONFIG_M5272) if (fep->quirks & FEC_QUIRK_HAS_RACC) { u32 val = readl(fep->hwp + FEC_RACC); val |= FEC_RACC_SHIFT16; if (fep->csum_flags & FLAG_RX_CSUM_ENABLED) val |= FEC_RACC_OPTIONS; else val &= ~FEC_RACC_OPTIONS; writel(val, fep->hwp + FEC_RACC); writel(PKT_MAXBUF_SIZE, fep->hwp + FEC_FTRL); } #endif if (fep->quirks & FEC_QUIRK_ENET_MAC) { rcntl |= 0x40000000 | 0x00000020; if (fep->phy_interface == PHY_INTERFACE_MODE_RGMII || fep->phy_interface == PHY_INTERFACE_MODE_RGMII_ID || fep->phy_interface == PHY_INTERFACE_MODE_RGMII_RXID || fep->phy_interface == PHY_INTERFACE_MODE_RGMII_TXID) rcntl |= (1 << 6); else if (fep->phy_interface == PHY_INTERFACE_MODE_RMII) rcntl |= FEC_RCR_RMII; else rcntl &= ~FEC_RCR_RMII; if (ndev->phydev) { if (ndev->phydev->speed == SPEED_1000) ecntl |= (1 << 5); else if (ndev->phydev->speed == SPEED_100) rcntl &= ~FEC_RCR_10BASET; else rcntl |= FEC_RCR_10BASET; } } else { #ifdef FEC_MIIGSK_ENR if (fep->quirks & FEC_QUIRK_USE_GASKET) { u32 cfgr; writel(0, fep->hwp + FEC_MIIGSK_ENR); while (readl(fep->hwp + FEC_MIIGSK_ENR) & 4) udelay(1); cfgr = (fep->phy_interface == PHY_INTERFACE_MODE_RMII) ? BM_MIIGSK_CFGR_RMII : BM_MIIGSK_CFGR_MII; if (ndev->phydev && ndev->phydev->speed == SPEED_10) cfgr |= BM_MIIGSK_CFGR_FRCONT_10M; writel(cfgr, fep->hwp + FEC_MIIGSK_CFGR); writel(2, fep->hwp + FEC_MIIGSK_ENR); } #endif } #if !defined(CONFIG_M5272) if ((fep->pause_flag & FEC_PAUSE_FLAG_ENABLE) || ((fep->pause_flag & FEC_PAUSE_FLAG_AUTONEG) && ndev->phydev && ndev->phydev->pause)) { rcntl |= FEC_ENET_FCE; writel(FEC_ENET_RSEM_V, fep->hwp + FEC_R_FIFO_RSEM); writel(FEC_ENET_RSFL_V, fep->hwp + FEC_R_FIFO_RSFL); writel(FEC_ENET_RAEM_V, fep->hwp + FEC_R_FIFO_RAEM); writel(FEC_ENET_RAFL_V, fep->hwp + FEC_R_FIFO_RAFL); writel(FEC_ENET_OPD_V, fep->hwp + FEC_OPD); } else { rcntl &= ~FEC_ENET_FCE; } #endif writel(rcntl, fep->hwp + FEC_R_CNTRL); set_multicast_list(ndev); #ifndef CONFIG_M5272 writel(0, fep->hwp + FEC_HASH_TABLE_HIGH); writel(0, fep->hwp + FEC_HASH_TABLE_LOW); #endif if (fep->quirks & FEC_QUIRK_ENET_MAC) { ecntl |= FEC_ECR_BYTESWP; writel(FEC_TXWMRK_STRFWD, fep->hwp + FEC_X_WMRK); } if (fep->bufdesc_ex) ecntl |= FEC_ECR_EN1588; if (fep->quirks & FEC_QUIRK_DELAYED_CLKS_SUPPORT && fep->rgmii_txc_dly) ecntl |= FEC_ENET_TXC_DLY; if (fep->quirks & FEC_QUIRK_DELAYED_CLKS_SUPPORT && fep->rgmii_rxc_dly) ecntl |= FEC_ENET_RXC_DLY; #ifndef CONFIG_M5272 writel(0 << 31, fep->hwp + FEC_MIB_CTRLSTAT); #endif writel(ecntl, fep->hwp + FEC_ECNTRL); fec_enet_active_rxring(ndev); if (fep->bufdesc_ex) { fec_ptp_start_cyclecounter(ndev); fec_ptp_restore_state(fep); } if (fep->link) writel(FEC_DEFAULT_IMASK, fep->hwp + FEC_IMASK); else writel(0, fep->hwp + FEC_IMASK); if (fep->quirks & FEC_QUIRK_HAS_COALESCE) fec_enet_itr_coal_set(ndev); }","- fec_ptp_save_state(fep);
+ if (fep->bufdesc_ex)
+ fec_ptp_save_state(fep);","fec_restart(struct net_device *ndev) { struct fec_enet_private *fep = netdev_priv(ndev); u32 temp_mac[2]; u32 rcntl = OPT_FRAME_SIZE | 0x04; u32 ecntl = FEC_ECR_ETHEREN; if (fep->bufdesc_ex) fec_ptp_save_state(fep); if (fep->quirks & FEC_QUIRK_HAS_MULTI_QUEUES || ((fep->quirks & FEC_QUIRK_NO_HARD_RESET) && fep->link)) { writel(0, fep->hwp + FEC_ECNTRL); } else { writel(1, fep->hwp + FEC_ECNTRL); udelay(10); } memcpy(&temp_mac, ndev->dev_addr, ETH_ALEN); writel((__force u32)cpu_to_be32(temp_mac[0]), fep->hwp + FEC_ADDR_LOW); writel((__force u32)cpu_to_be32(temp_mac[1]), fep->hwp + FEC_ADDR_HIGH); writel((0xffffffff & ~FEC_ENET_MII), fep->hwp + FEC_IEVENT); fec_enet_bd_init(ndev); fec_enet_enable_ring(ndev); if (fep->full_duplex == DUPLEX_FULL) { writel(0x04, fep->hwp + FEC_X_CNTRL); } else { rcntl |= 0x02; writel(0x0, fep->hwp + FEC_X_CNTRL); } writel(fep->phy_speed, fep->hwp + FEC_MII_SPEED); #if !defined(CONFIG_M5272) if (fep->quirks & FEC_QUIRK_HAS_RACC) { u32 val = readl(fep->hwp + FEC_RACC); val |= FEC_RACC_SHIFT16; if (fep->csum_flags & FLAG_RX_CSUM_ENABLED) val |= FEC_RACC_OPTIONS; else val &= ~FEC_RACC_OPTIONS; writel(val, fep->hwp + FEC_RACC); writel(PKT_MAXBUF_SIZE, fep->hwp + FEC_FTRL); } #endif if (fep->quirks & FEC_QUIRK_ENET_MAC) { rcntl |= 0x40000000 | 0x00000020; if (fep->phy_interface == PHY_INTERFACE_MODE_RGMII || fep->phy_interface == PHY_INTERFACE_MODE_RGMII_ID || fep->phy_interface == PHY_INTERFACE_MODE_RGMII_RXID || fep->phy_interface == PHY_INTERFACE_MODE_RGMII_TXID) rcntl |= (1 << 6); else if (fep->phy_interface == PHY_INTERFACE_MODE_RMII) rcntl |= FEC_RCR_RMII; else rcntl &= ~FEC_RCR_RMII; if (ndev->phydev) { if (ndev->phydev->speed == SPEED_1000) ecntl |= (1 << 5); else if (ndev->phydev->speed == SPEED_100) rcntl &= ~FEC_RCR_10BASET; else rcntl |= FEC_RCR_10BASET; } } else { #ifdef FEC_MIIGSK_ENR if (fep->quirks & FEC_QUIRK_USE_GASKET) { u32 cfgr; writel(0, fep->hwp + FEC_MIIGSK_ENR); while (readl(fep->hwp + FEC_MIIGSK_ENR) & 4) udelay(1); cfgr = (fep->phy_interface == PHY_INTERFACE_MODE_RMII) ? BM_MIIGSK_CFGR_RMII : BM_MIIGSK_CFGR_MII; if (ndev->phydev && ndev->phydev->speed == SPEED_10) cfgr |= BM_MIIGSK_CFGR_FRCONT_10M; writel(cfgr, fep->hwp + FEC_MIIGSK_CFGR); writel(2, fep->hwp + FEC_MIIGSK_ENR); } #endif } #if !defined(CONFIG_M5272) if ((fep->pause_flag & FEC_PAUSE_FLAG_ENABLE) || ((fep->pause_flag & FEC_PAUSE_FLAG_AUTONEG) && ndev->phydev && ndev->phydev->pause)) { rcntl |= FEC_ENET_FCE; writel(FEC_ENET_RSEM_V, fep->hwp + FEC_R_FIFO_RSEM); writel(FEC_ENET_RSFL_V, fep->hwp + FEC_R_FIFO_RSFL); writel(FEC_ENET_RAEM_V, fep->hwp + FEC_R_FIFO_RAEM); writel(FEC_ENET_RAFL_V, fep->hwp + FEC_R_FIFO_RAFL); writel(FEC_ENET_OPD_V, fep->hwp + FEC_OPD); } else { rcntl &= ~FEC_ENET_FCE; } #endif writel(rcntl, fep->hwp + FEC_R_CNTRL); set_multicast_list(ndev); #ifndef CONFIG_M5272 writel(0, fep->hwp + FEC_HASH_TABLE_HIGH); writel(0, fep->hwp + FEC_HASH_TABLE_LOW); #endif if (fep->quirks & FEC_QUIRK_ENET_MAC) { ecntl |= FEC_ECR_BYTESWP; writel(FEC_TXWMRK_STRFWD, fep->hwp + FEC_X_WMRK); } if (fep->bufdesc_ex) ecntl |= FEC_ECR_EN1588; if (fep->quirks & FEC_QUIRK_DELAYED_CLKS_SUPPORT && fep->rgmii_txc_dly) ecntl |= FEC_ENET_TXC_DLY; if (fep->quirks & FEC_QUIRK_DELAYED_CLKS_SUPPORT && fep->rgmii_rxc_dly) ecntl |= FEC_ENET_RXC_DLY; #ifndef CONFIG_M5272 writel(0 << 31, fep->hwp + FEC_MIB_CTRLSTAT); #endif writel(ecntl, fep->hwp + FEC_ECNTRL); fec_enet_active_rxring(ndev); if (fep->bufdesc_ex) { fec_ptp_start_cyclecounter(ndev); fec_ptp_restore_state(fep); } if (fep->link) writel(FEC_DEFAULT_IMASK, fep->hwp + FEC_IMASK); else writel(0, fep->hwp + FEC_IMASK); if (fep->quirks & FEC_QUIRK_HAS_COALESCE) fec_enet_itr_coal_set(ndev); }"
606----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49921/bad/dce110_hwseq.c----dce110_edp_backlight_control,"void dce110_edp_backlight_control( struct dc_link *link, bool enable) { struct dc_context *ctx = link->ctx; struct bp_transmitter_control cntl = { 0 }; <S2SV_StartVul> uint8_t pwrseq_instance; <S2SV_EndVul> unsigned int pre_T11_delay = OLED_PRE_T11_DELAY; unsigned int post_T7_delay = OLED_POST_T7_DELAY; if (dal_graphics_object_id_get_connector_id(link->link_enc->connector) != CONNECTOR_ID_EDP) { BREAK_TO_DEBUGGER(); return; } if (link->panel_cntl && !(link->dpcd_sink_ext_caps.bits.oled || link->dpcd_sink_ext_caps.bits.hdr_aux_backlight_control == 1 || link->dpcd_sink_ext_caps.bits.sdr_aux_backlight_control == 1)) { bool is_backlight_on = link->panel_cntl->funcs->is_panel_backlight_on(link->panel_cntl); if ((enable && is_backlight_on) || (!enable && !is_backlight_on)) { DC_LOG_HW_RESUME_S3( ""%s: panel already powered up/off. Do nothing.\n"", __func__); return; } } DC_LOG_HW_RESUME_S3( ""%s: backlight action: %s\n"", __func__, (enable ? ""On"":""Off"")); cntl.action = enable ? TRANSMITTER_CONTROL_BACKLIGHT_ON : TRANSMITTER_CONTROL_BACKLIGHT_OFF; cntl.transmitter = link->link_enc->transmitter; cntl.connector_obj_id = link->link_enc->connector; cntl.lanes_number = LANE_COUNT_FOUR; cntl.hpd_sel = link->link_enc->hpd_source; cntl.signal = SIGNAL_TYPE_EDP; <S2SV_StartVul> pwrseq_instance = link->panel_cntl->pwrseq_inst; <S2SV_EndVul> if (cntl.action == TRANSMITTER_CONTROL_BACKLIGHT_ON) { if (!link->dc->config.edp_no_power_sequencing) ctx->dc->link_srv->edp_receiver_ready_T7(link); else DC_LOG_DC(""edp_receiver_ready_T7 skipped\n""); } if (ctx->dc->ctx->dmub_srv && ctx->dc->debug.dmub_command_table) { if (cntl.action == TRANSMITTER_CONTROL_BACKLIGHT_ON) ctx->dc_bios->funcs->enable_lvtma_control(ctx->dc_bios, LVTMA_CONTROL_LCD_BLON, pwrseq_instance, link->link_powered_externally); else ctx->dc_bios->funcs->enable_lvtma_control(ctx->dc_bios, LVTMA_CONTROL_LCD_BLOFF, pwrseq_instance, link->link_powered_externally); } link_transmitter_control(ctx->dc_bios, &cntl); if (enable && link->dpcd_sink_ext_caps.bits.oled && !link->dc->config.edp_no_power_sequencing) { post_T7_delay += link->panel_config.pps.extra_post_t7_ms; msleep(post_T7_delay); } if (link->dpcd_sink_ext_caps.bits.oled || link->dpcd_sink_ext_caps.bits.hdr_aux_backlight_control == 1 || link->dpcd_sink_ext_caps.bits.sdr_aux_backlight_control == 1) ctx->dc->link_srv->edp_backlight_enable_aux(link, enable); if (cntl.action == TRANSMITTER_CONTROL_BACKLIGHT_OFF) { if (!link->dc->config.edp_no_power_sequencing) ctx->dc->link_srv->edp_add_delay_for_T9(link); else DC_LOG_DC(""edp_receiver_ready_T9 skipped\n""); } if (!enable && link->dpcd_sink_ext_caps.bits.oled) { pre_T11_delay += link->panel_config.pps.extra_pre_t11_ms; msleep(pre_T11_delay); } }","- uint8_t pwrseq_instance;
- pwrseq_instance = link->panel_cntl->pwrseq_inst;
+ uint8_t pwrseq_instance = 0;
+ if (link->panel_cntl)
+ pwrseq_instance = link->panel_cntl->pwrseq_inst;","void dce110_edp_backlight_control( struct dc_link *link, bool enable) { struct dc_context *ctx = link->ctx; struct bp_transmitter_control cntl = { 0 }; uint8_t pwrseq_instance = 0; unsigned int pre_T11_delay = OLED_PRE_T11_DELAY; unsigned int post_T7_delay = OLED_POST_T7_DELAY; if (dal_graphics_object_id_get_connector_id(link->link_enc->connector) != CONNECTOR_ID_EDP) { BREAK_TO_DEBUGGER(); return; } if (link->panel_cntl && !(link->dpcd_sink_ext_caps.bits.oled || link->dpcd_sink_ext_caps.bits.hdr_aux_backlight_control == 1 || link->dpcd_sink_ext_caps.bits.sdr_aux_backlight_control == 1)) { bool is_backlight_on = link->panel_cntl->funcs->is_panel_backlight_on(link->panel_cntl); if ((enable && is_backlight_on) || (!enable && !is_backlight_on)) { DC_LOG_HW_RESUME_S3( ""%s: panel already powered up/off. Do nothing.\n"", __func__); return; } } DC_LOG_HW_RESUME_S3( ""%s: backlight action: %s\n"", __func__, (enable ? ""On"":""Off"")); cntl.action = enable ? TRANSMITTER_CONTROL_BACKLIGHT_ON : TRANSMITTER_CONTROL_BACKLIGHT_OFF; cntl.transmitter = link->link_enc->transmitter; cntl.connector_obj_id = link->link_enc->connector; cntl.lanes_number = LANE_COUNT_FOUR; cntl.hpd_sel = link->link_enc->hpd_source; cntl.signal = SIGNAL_TYPE_EDP; if (link->panel_cntl) pwrseq_instance = link->panel_cntl->pwrseq_inst; if (cntl.action == TRANSMITTER_CONTROL_BACKLIGHT_ON) { if (!link->dc->config.edp_no_power_sequencing) ctx->dc->link_srv->edp_receiver_ready_T7(link); else DC_LOG_DC(""edp_receiver_ready_T7 skipped\n""); } if (ctx->dc->ctx->dmub_srv && ctx->dc->debug.dmub_command_table) { if (cntl.action == TRANSMITTER_CONTROL_BACKLIGHT_ON) ctx->dc_bios->funcs->enable_lvtma_control(ctx->dc_bios, LVTMA_CONTROL_LCD_BLON, pwrseq_instance, link->link_powered_externally); else ctx->dc_bios->funcs->enable_lvtma_control(ctx->dc_bios, LVTMA_CONTROL_LCD_BLOFF, pwrseq_instance, link->link_powered_externally); } link_transmitter_control(ctx->dc_bios, &cntl); if (enable && link->dpcd_sink_ext_caps.bits.oled && !link->dc->config.edp_no_power_sequencing) { post_T7_delay += link->panel_config.pps.extra_post_t7_ms; msleep(post_T7_delay); } if (link->dpcd_sink_ext_caps.bits.oled || link->dpcd_sink_ext_caps.bits.hdr_aux_backlight_control == 1 || link->dpcd_sink_ext_caps.bits.sdr_aux_backlight_control == 1) ctx->dc->link_srv->edp_backlight_enable_aux(link, enable); if (cntl.action == TRANSMITTER_CONTROL_BACKLIGHT_OFF) { if (!link->dc->config.edp_no_power_sequencing) ctx->dc->link_srv->edp_add_delay_for_T9(link); else DC_LOG_DC(""edp_receiver_ready_T9 skipped\n""); } if (!enable && link->dpcd_sink_ext_caps.bits.oled) { pre_T11_delay += link->panel_config.pps.extra_pre_t11_ms; msleep(pre_T11_delay); } }"
589----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49911/bad/dcn20_hwseq.c----dcn20_set_output_transfer_func,"bool dcn20_set_output_transfer_func(struct dc *dc, struct pipe_ctx *pipe_ctx, const struct dc_stream_state *stream) { int mpcc_id = pipe_ctx->plane_res.hubp->inst; struct mpc *mpc = pipe_ctx->stream_res.opp->ctx->dc->res_pool->mpc; const struct pwl_params *params = NULL; if (mpc->funcs->power_on_mpc_mem_pwr) mpc->funcs->power_on_mpc_mem_pwr(mpc, mpcc_id, true); if (pipe_ctx->top_pipe == NULL && mpc->funcs->set_output_gamma) { if (stream->out_transfer_func.type == TF_TYPE_HWPWL) params = &stream->out_transfer_func.pwl; else if (pipe_ctx->stream->out_transfer_func.type == TF_TYPE_DISTRIBUTED_POINTS && cm_helper_translate_curve_to_hw_format(dc->ctx, &stream->out_transfer_func, &mpc->blender_params, false)) params = &mpc->blender_params; if (stream->out_transfer_func.type == TF_TYPE_PREDEFINED) BREAK_TO_DEBUGGER(); } <S2SV_StartVul> mpc->funcs->set_output_gamma(mpc, mpcc_id, params); <S2SV_EndVul> return true; }","- mpc->funcs->set_output_gamma(mpc, mpcc_id, params);
+ if (mpc->funcs->set_output_gamma)
+ mpc->funcs->set_output_gamma(mpc, mpcc_id, params);","bool dcn20_set_output_transfer_func(struct dc *dc, struct pipe_ctx *pipe_ctx, const struct dc_stream_state *stream) { int mpcc_id = pipe_ctx->plane_res.hubp->inst; struct mpc *mpc = pipe_ctx->stream_res.opp->ctx->dc->res_pool->mpc; const struct pwl_params *params = NULL; if (mpc->funcs->power_on_mpc_mem_pwr) mpc->funcs->power_on_mpc_mem_pwr(mpc, mpcc_id, true); if (pipe_ctx->top_pipe == NULL && mpc->funcs->set_output_gamma) { if (stream->out_transfer_func.type == TF_TYPE_HWPWL) params = &stream->out_transfer_func.pwl; else if (pipe_ctx->stream->out_transfer_func.type == TF_TYPE_DISTRIBUTED_POINTS && cm_helper_translate_curve_to_hw_format(dc->ctx, &stream->out_transfer_func, &mpc->blender_params, false)) params = &mpc->blender_params; if (stream->out_transfer_func.type == TF_TYPE_PREDEFINED) BREAK_TO_DEBUGGER(); } if (mpc->funcs->set_output_gamma) mpc->funcs->set_output_gamma(mpc, mpcc_id, params); return true; }"
1281----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56578/bad/mxc-jpeg.c----mxc_jpeg_probe,"static int mxc_jpeg_probe(struct platform_device *pdev) { struct mxc_jpeg_dev *jpeg; struct device *dev = &pdev->dev; int dec_irq; int ret; int mode; const struct of_device_id *of_id; of_id = of_match_node(mxc_jpeg_match, dev->of_node); if (!of_id) return -ENODEV; mode = *(const int *)of_id->data; jpeg = devm_kzalloc(dev, sizeof(struct mxc_jpeg_dev), GFP_KERNEL); if (!jpeg) return -ENOMEM; mutex_init(&jpeg->lock); spin_lock_init(&jpeg->hw_lock); ret = dma_set_mask_and_coherent(dev, DMA_BIT_MASK(32)); if (ret) { dev_err(&pdev->dev, ""No suitable DMA available.\n""); goto err_irq; } jpeg->base_reg = devm_platform_ioremap_resource(pdev, 0); if (IS_ERR(jpeg->base_reg)) return PTR_ERR(jpeg->base_reg); ret = of_property_read_u32_index(pdev->dev.of_node, ""slot"", 0, &jpeg->slot_data.slot); if (ret) jpeg->slot_data.slot = 0; dev_info(&pdev->dev, ""choose slot %d\n"", jpeg->slot_data.slot); dec_irq = platform_get_irq(pdev, 0); if (dec_irq < 0) { ret = dec_irq; goto err_irq; } ret = devm_request_irq(&pdev->dev, dec_irq, mxc_jpeg_dec_irq, 0, pdev->name, jpeg); if (ret) { dev_err(&pdev->dev, ""Failed to request irq %d (%d)\n"", dec_irq, ret); goto err_irq; } jpeg->pdev = pdev; jpeg->dev = dev; jpeg->mode = mode; ret = devm_clk_bulk_get_all(&pdev->dev, &jpeg->clks); if (ret < 0) { dev_err(dev, ""failed to get clock\n""); goto err_clk; } jpeg->num_clks = ret; ret = mxc_jpeg_attach_pm_domains(jpeg); if (ret < 0) { dev_err(dev, ""failed to attach power domains %d\n"", ret); goto err_clk; } ret = v4l2_device_register(dev, &jpeg->v4l2_dev); if (ret) { dev_err(dev, ""failed to register v4l2 device\n""); goto err_register; } jpeg->m2m_dev = v4l2_m2m_init(&mxc_jpeg_m2m_ops); if (IS_ERR(jpeg->m2m_dev)) { dev_err(dev, ""failed to register v4l2 device\n""); ret = PTR_ERR(jpeg->m2m_dev); goto err_m2m; } jpeg->dec_vdev = video_device_alloc(); if (!jpeg->dec_vdev) { dev_err(dev, ""failed to register v4l2 device\n""); ret = -ENOMEM; goto err_vdev_alloc; } if (mode == MXC_JPEG_ENCODE) snprintf(jpeg->dec_vdev->name, sizeof(jpeg->dec_vdev->name), ""%s-enc"", MXC_JPEG_NAME); else snprintf(jpeg->dec_vdev->name, sizeof(jpeg->dec_vdev->name), ""%s-dec"", MXC_JPEG_NAME); jpeg->dec_vdev->fops = &mxc_jpeg_fops; jpeg->dec_vdev->ioctl_ops = &mxc_jpeg_ioctl_ops; jpeg->dec_vdev->minor = -1; jpeg->dec_vdev->release = video_device_release; jpeg->dec_vdev->lock = &jpeg->lock; jpeg->dec_vdev->v4l2_dev = &jpeg->v4l2_dev; jpeg->dec_vdev->vfl_dir = VFL_DIR_M2M; jpeg->dec_vdev->device_caps = V4L2_CAP_STREAMING | V4L2_CAP_VIDEO_M2M_MPLANE; if (mode == MXC_JPEG_ENCODE) { v4l2_disable_ioctl(jpeg->dec_vdev, VIDIOC_DECODER_CMD); v4l2_disable_ioctl(jpeg->dec_vdev, VIDIOC_TRY_DECODER_CMD); } else { v4l2_disable_ioctl(jpeg->dec_vdev, VIDIOC_ENCODER_CMD); v4l2_disable_ioctl(jpeg->dec_vdev, VIDIOC_TRY_ENCODER_CMD); } ret = video_register_device(jpeg->dec_vdev, VFL_TYPE_VIDEO, -1); if (ret) { dev_err(dev, ""failed to register video device\n""); goto err_vdev_register; } <S2SV_StartVul> video_set_drvdata(jpeg->dec_vdev, jpeg); <S2SV_EndVul> if (mode == MXC_JPEG_ENCODE) v4l2_info(&jpeg->v4l2_dev, ""encoder device registered as /dev/video%d (%d,%d)\n"", jpeg->dec_vdev->num, VIDEO_MAJOR, jpeg->dec_vdev->minor); else v4l2_info(&jpeg->v4l2_dev, ""decoder device registered as /dev/video%d (%d,%d)\n"", jpeg->dec_vdev->num, VIDEO_MAJOR, jpeg->dec_vdev->minor); platform_set_drvdata(pdev, jpeg); pm_runtime_enable(dev); return 0; err_vdev_register: video_device_release(jpeg->dec_vdev); err_vdev_alloc: v4l2_m2m_release(jpeg->m2m_dev); err_m2m: v4l2_device_unregister(&jpeg->v4l2_dev); err_register: mxc_jpeg_detach_pm_domains(jpeg); err_irq: err_clk: return ret; }","- video_set_drvdata(jpeg->dec_vdev, jpeg);
+ video_set_drvdata(jpeg->dec_vdev, jpeg);","static int mxc_jpeg_probe(struct platform_device *pdev) { struct mxc_jpeg_dev *jpeg; struct device *dev = &pdev->dev; int dec_irq; int ret; int mode; const struct of_device_id *of_id; of_id = of_match_node(mxc_jpeg_match, dev->of_node); if (!of_id) return -ENODEV; mode = *(const int *)of_id->data; jpeg = devm_kzalloc(dev, sizeof(struct mxc_jpeg_dev), GFP_KERNEL); if (!jpeg) return -ENOMEM; mutex_init(&jpeg->lock); spin_lock_init(&jpeg->hw_lock); ret = dma_set_mask_and_coherent(dev, DMA_BIT_MASK(32)); if (ret) { dev_err(&pdev->dev, ""No suitable DMA available.\n""); goto err_irq; } jpeg->base_reg = devm_platform_ioremap_resource(pdev, 0); if (IS_ERR(jpeg->base_reg)) return PTR_ERR(jpeg->base_reg); ret = of_property_read_u32_index(pdev->dev.of_node, ""slot"", 0, &jpeg->slot_data.slot); if (ret) jpeg->slot_data.slot = 0; dev_info(&pdev->dev, ""choose slot %d\n"", jpeg->slot_data.slot); dec_irq = platform_get_irq(pdev, 0); if (dec_irq < 0) { ret = dec_irq; goto err_irq; } ret = devm_request_irq(&pdev->dev, dec_irq, mxc_jpeg_dec_irq, 0, pdev->name, jpeg); if (ret) { dev_err(&pdev->dev, ""Failed to request irq %d (%d)\n"", dec_irq, ret); goto err_irq; } jpeg->pdev = pdev; jpeg->dev = dev; jpeg->mode = mode; ret = devm_clk_bulk_get_all(&pdev->dev, &jpeg->clks); if (ret < 0) { dev_err(dev, ""failed to get clock\n""); goto err_clk; } jpeg->num_clks = ret; ret = mxc_jpeg_attach_pm_domains(jpeg); if (ret < 0) { dev_err(dev, ""failed to attach power domains %d\n"", ret); goto err_clk; } ret = v4l2_device_register(dev, &jpeg->v4l2_dev); if (ret) { dev_err(dev, ""failed to register v4l2 device\n""); goto err_register; } jpeg->m2m_dev = v4l2_m2m_init(&mxc_jpeg_m2m_ops); if (IS_ERR(jpeg->m2m_dev)) { dev_err(dev, ""failed to register v4l2 device\n""); ret = PTR_ERR(jpeg->m2m_dev); goto err_m2m; } jpeg->dec_vdev = video_device_alloc(); if (!jpeg->dec_vdev) { dev_err(dev, ""failed to register v4l2 device\n""); ret = -ENOMEM; goto err_vdev_alloc; } if (mode == MXC_JPEG_ENCODE) snprintf(jpeg->dec_vdev->name, sizeof(jpeg->dec_vdev->name), ""%s-enc"", MXC_JPEG_NAME); else snprintf(jpeg->dec_vdev->name, sizeof(jpeg->dec_vdev->name), ""%s-dec"", MXC_JPEG_NAME); jpeg->dec_vdev->fops = &mxc_jpeg_fops; jpeg->dec_vdev->ioctl_ops = &mxc_jpeg_ioctl_ops; jpeg->dec_vdev->minor = -1; jpeg->dec_vdev->release = video_device_release; jpeg->dec_vdev->lock = &jpeg->lock; jpeg->dec_vdev->v4l2_dev = &jpeg->v4l2_dev; jpeg->dec_vdev->vfl_dir = VFL_DIR_M2M; jpeg->dec_vdev->device_caps = V4L2_CAP_STREAMING | V4L2_CAP_VIDEO_M2M_MPLANE; video_set_drvdata(jpeg->dec_vdev, jpeg); if (mode == MXC_JPEG_ENCODE) { v4l2_disable_ioctl(jpeg->dec_vdev, VIDIOC_DECODER_CMD); v4l2_disable_ioctl(jpeg->dec_vdev, VIDIOC_TRY_DECODER_CMD); } else { v4l2_disable_ioctl(jpeg->dec_vdev, VIDIOC_ENCODER_CMD); v4l2_disable_ioctl(jpeg->dec_vdev, VIDIOC_TRY_ENCODER_CMD); } ret = video_register_device(jpeg->dec_vdev, VFL_TYPE_VIDEO, -1); if (ret) { dev_err(dev, ""failed to register video device\n""); goto err_vdev_register; } if (mode == MXC_JPEG_ENCODE) v4l2_info(&jpeg->v4l2_dev, ""encoder device registered as /dev/video%d (%d,%d)\n"", jpeg->dec_vdev->num, VIDEO_MAJOR, jpeg->dec_vdev->minor); else v4l2_info(&jpeg->v4l2_dev, ""decoder device registered as /dev/video%d (%d,%d)\n"", jpeg->dec_vdev->num, VIDEO_MAJOR, jpeg->dec_vdev->minor); platform_set_drvdata(pdev, jpeg); pm_runtime_enable(dev); return 0; err_vdev_register: video_device_release(jpeg->dec_vdev); err_vdev_alloc: v4l2_m2m_release(jpeg->m2m_dev); err_m2m: v4l2_device_unregister(&jpeg->v4l2_dev); err_register: mxc_jpeg_detach_pm_domains(jpeg); err_irq: err_clk: return ret; }"
1501----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-57952/bad/libfs.c----offset_dir_llseek,"static loff_t offset_dir_llseek(struct file *file, loff_t offset, int whence) { struct inode *inode = file->f_inode; struct offset_ctx *ctx = inode->i_op->get_offset_ctx(inode); switch (whence) { case SEEK_CUR: offset += file->f_pos; fallthrough; case SEEK_SET: if (offset >= 0) break; fallthrough; default: return -EINVAL; } <S2SV_StartVul> if (!offset) <S2SV_EndVul> <S2SV_StartVul> file->private_data = (void *)ctx->next_offset; <S2SV_EndVul> return vfs_setpos(file, offset, LONG_MAX); <S2SV_StartVul> } <S2SV_EndVul>","- if (!offset)
- file->private_data = (void *)ctx->next_offset;
- }
+ file->private_data = NULL;","static loff_t offset_dir_llseek(struct file *file, loff_t offset, int whence) { switch (whence) { case SEEK_CUR: offset += file->f_pos; fallthrough; case SEEK_SET: if (offset >= 0) break; fallthrough; default: return -EINVAL; } file->private_data = NULL; return vfs_setpos(file, offset, LONG_MAX); }"
1215----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53224/bad/main.c----mlx5_ib_dev_res_init,"static int mlx5_ib_dev_res_init(struct mlx5_ib_dev *dev) { struct mlx5_ib_resources *devr = &dev->devr; <S2SV_StartVul> int port; <S2SV_EndVul> int ret; if (!MLX5_CAP_GEN(dev->mdev, xrc)) return -EOPNOTSUPP; ret = mlx5_cmd_xrcd_alloc(dev->mdev, &devr->xrcdn0, 0); if (ret) return ret; ret = mlx5_cmd_xrcd_alloc(dev->mdev, &devr->xrcdn1, 0); if (ret) { mlx5_cmd_xrcd_dealloc(dev->mdev, devr->xrcdn0, 0); return ret; } <S2SV_StartVul> for (port = 0; port < ARRAY_SIZE(devr->ports); ++port) <S2SV_EndVul> <S2SV_StartVul> INIT_WORK(&devr->ports[port].pkey_change_work, <S2SV_EndVul> <S2SV_StartVul> pkey_change_handler); <S2SV_EndVul> mutex_init(&devr->cq_lock); mutex_init(&devr->srq_lock); return 0; }","- int port;
- for (port = 0; port < ARRAY_SIZE(devr->ports); ++port)
- INIT_WORK(&devr->ports[port].pkey_change_work,
- pkey_change_handler);
+ struct mlx5_ib_resources *devr = &dev->devr;","static int mlx5_ib_dev_res_init(struct mlx5_ib_dev *dev) { struct mlx5_ib_resources *devr = &dev->devr; int ret; if (!MLX5_CAP_GEN(dev->mdev, xrc)) return -EOPNOTSUPP; ret = mlx5_cmd_xrcd_alloc(dev->mdev, &devr->xrcdn0, 0); if (ret) return ret; ret = mlx5_cmd_xrcd_alloc(dev->mdev, &devr->xrcdn1, 0); if (ret) { mlx5_cmd_xrcd_dealloc(dev->mdev, devr->xrcdn0, 0); return ret; } mutex_init(&devr->cq_lock); mutex_init(&devr->srq_lock); return 0; }"
886----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50138/bad/ringbuf.c----*__bpf_ringbuf_reserve,"static void *__bpf_ringbuf_reserve(struct bpf_ringbuf *rb, u64 size) { unsigned long cons_pos, prod_pos, new_prod_pos, pend_pos, flags; struct bpf_ringbuf_hdr *hdr; u32 len, pg_off, tmp_size, hdr_len; if (unlikely(size > RINGBUF_MAX_RECORD_SZ)) return NULL; len = round_up(size + BPF_RINGBUF_HDR_SZ, 8); if (len > ringbuf_total_data_sz(rb)) return NULL; cons_pos = smp_load_acquire(&rb->consumer_pos); if (in_nmi()) { <S2SV_StartVul> if (!spin_trylock_irqsave(&rb->spinlock, flags)) <S2SV_EndVul> return NULL; } else { <S2SV_StartVul> spin_lock_irqsave(&rb->spinlock, flags); <S2SV_EndVul> } pend_pos = rb->pending_pos; prod_pos = rb->producer_pos; new_prod_pos = prod_pos + len; while (pend_pos < prod_pos) { hdr = (void *)rb->data + (pend_pos & rb->mask); hdr_len = READ_ONCE(hdr->len); if (hdr_len & BPF_RINGBUF_BUSY_BIT) break; tmp_size = hdr_len & ~BPF_RINGBUF_DISCARD_BIT; tmp_size = round_up(tmp_size + BPF_RINGBUF_HDR_SZ, 8); pend_pos += tmp_size; } rb->pending_pos = pend_pos; if (new_prod_pos - cons_pos > rb->mask || new_prod_pos - pend_pos > rb->mask) { <S2SV_StartVul> spin_unlock_irqrestore(&rb->spinlock, flags); <S2SV_EndVul> return NULL; } hdr = (void *)rb->data + (prod_pos & rb->mask); pg_off = bpf_ringbuf_rec_pg_off(rb, hdr); hdr->len = size | BPF_RINGBUF_BUSY_BIT; hdr->pg_off = pg_off; smp_store_release(&rb->producer_pos, new_prod_pos); <S2SV_StartVul> spin_unlock_irqrestore(&rb->spinlock, flags); <S2SV_EndVul> return (void *)hdr + BPF_RINGBUF_HDR_SZ; }","- if (!spin_trylock_irqsave(&rb->spinlock, flags))
- spin_lock_irqsave(&rb->spinlock, flags);
- spin_unlock_irqrestore(&rb->spinlock, flags);
- spin_unlock_irqrestore(&rb->spinlock, flags);
+ if (!raw_spin_trylock_irqsave(&rb->spinlock, flags))
+ raw_spin_lock_irqsave(&rb->spinlock, flags);
+ raw_spin_unlock_irqrestore(&rb->spinlock, flags);
+ raw_spin_unlock_irqrestore(&rb->spinlock, flags);","static void *__bpf_ringbuf_reserve(struct bpf_ringbuf *rb, u64 size) { unsigned long cons_pos, prod_pos, new_prod_pos, pend_pos, flags; struct bpf_ringbuf_hdr *hdr; u32 len, pg_off, tmp_size, hdr_len; if (unlikely(size > RINGBUF_MAX_RECORD_SZ)) return NULL; len = round_up(size + BPF_RINGBUF_HDR_SZ, 8); if (len > ringbuf_total_data_sz(rb)) return NULL; cons_pos = smp_load_acquire(&rb->consumer_pos); if (in_nmi()) { if (!raw_spin_trylock_irqsave(&rb->spinlock, flags)) return NULL; } else { raw_spin_lock_irqsave(&rb->spinlock, flags); } pend_pos = rb->pending_pos; prod_pos = rb->producer_pos; new_prod_pos = prod_pos + len; while (pend_pos < prod_pos) { hdr = (void *)rb->data + (pend_pos & rb->mask); hdr_len = READ_ONCE(hdr->len); if (hdr_len & BPF_RINGBUF_BUSY_BIT) break; tmp_size = hdr_len & ~BPF_RINGBUF_DISCARD_BIT; tmp_size = round_up(tmp_size + BPF_RINGBUF_HDR_SZ, 8); pend_pos += tmp_size; } rb->pending_pos = pend_pos; if (new_prod_pos - cons_pos > rb->mask || new_prod_pos - pend_pos > rb->mask) { raw_spin_unlock_irqrestore(&rb->spinlock, flags); return NULL; } hdr = (void *)rb->data + (prod_pos & rb->mask); pg_off = bpf_ringbuf_rec_pg_off(rb, hdr); hdr->len = size | BPF_RINGBUF_BUSY_BIT; hdr->pg_off = pg_off; smp_store_release(&rb->producer_pos, new_prod_pos); raw_spin_unlock_irqrestore(&rb->spinlock, flags); return (void *)hdr + BPF_RINGBUF_HDR_SZ; }"
1239----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-54191/bad/iso.c----iso_sock_recvmsg,"static int iso_sock_recvmsg(struct socket *sock, struct msghdr *msg, size_t len, int flags) { struct sock *sk = sock->sk; struct iso_pinfo *pi = iso_pi(sk); BT_DBG(""sk %p"", sk); if (test_and_clear_bit(BT_SK_DEFER_SETUP, &bt_sk(sk)->flags)) { lock_sock(sk); switch (sk->sk_state) { case BT_CONNECT2: if (test_bit(BT_SK_PA_SYNC, &pi->flags)) { iso_conn_big_sync(sk); sk->sk_state = BT_LISTEN; } else { iso_conn_defer_accept(pi->conn->hcon); sk->sk_state = BT_CONFIG; } <S2SV_StartVul> release_sock(sk); <S2SV_EndVul> <S2SV_StartVul> return 0; <S2SV_EndVul> case BT_CONNECTED: if (test_bit(BT_SK_PA_SYNC, &iso_pi(sk)->flags)) { iso_conn_big_sync(sk); sk->sk_state = BT_LISTEN; <S2SV_StartVul> release_sock(sk); <S2SV_EndVul> <S2SV_StartVul> return 0; <S2SV_EndVul> } <S2SV_StartVul> release_sock(sk); <S2SV_EndVul> break; case BT_CONNECT: <S2SV_StartVul> release_sock(sk); <S2SV_EndVul> <S2SV_StartVul> return iso_connect_cis(sk); <S2SV_EndVul> default: <S2SV_StartVul> release_sock(sk); <S2SV_EndVul> break; } } return bt_sock_recvmsg(sock, msg, len, flags); }","- release_sock(sk);
- return 0;
- release_sock(sk);
- return 0;
- release_sock(sk);
- release_sock(sk);
- return iso_connect_cis(sk);
- release_sock(sk);
+ bool early_ret = false;
+ int err = 0;
+ sock_hold(sk);
+ lock_sock(sk);
+ release_sock(sk);
+ lock_sock(sk);
+ early_ret = true;
+ break;
+ release_sock(sk);
+ lock_sock(sk);
+ early_ret = true;
+ break;
+ release_sock(sk);
+ err = iso_connect_cis(sk);
+ lock_sock(sk);
+ early_ret = true;
+ break;
+ break;
+ release_sock(sk);
+ sock_put(sk);
+ if (early_ret)
+ return err;","static int iso_sock_recvmsg(struct socket *sock, struct msghdr *msg, size_t len, int flags) { struct sock *sk = sock->sk; struct iso_pinfo *pi = iso_pi(sk); bool early_ret = false; int err = 0; BT_DBG(""sk %p"", sk); if (test_and_clear_bit(BT_SK_DEFER_SETUP, &bt_sk(sk)->flags)) { sock_hold(sk); lock_sock(sk); switch (sk->sk_state) { case BT_CONNECT2: if (test_bit(BT_SK_PA_SYNC, &pi->flags)) { release_sock(sk); iso_conn_big_sync(sk); lock_sock(sk); sk->sk_state = BT_LISTEN; } else { iso_conn_defer_accept(pi->conn->hcon); sk->sk_state = BT_CONFIG; } early_ret = true; break; case BT_CONNECTED: if (test_bit(BT_SK_PA_SYNC, &iso_pi(sk)->flags)) { release_sock(sk); iso_conn_big_sync(sk); lock_sock(sk); sk->sk_state = BT_LISTEN; early_ret = true; } break; case BT_CONNECT: release_sock(sk); err = iso_connect_cis(sk); lock_sock(sk); early_ret = true; break; default: break; } release_sock(sk); sock_put(sk); if (early_ret) return err; } return bt_sock_recvmsg(sock, msg, len, flags); }"
1488----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-57914/bad/tcpci.c----tcpci_probe,"static int tcpci_probe(struct i2c_client *client) { struct tcpci_chip *chip; int err; u16 val = 0; chip = devm_kzalloc(&client->dev, sizeof(*chip), GFP_KERNEL); if (!chip) return -ENOMEM; chip->data.regmap = devm_regmap_init_i2c(client, &tcpci_regmap_config); if (IS_ERR(chip->data.regmap)) return PTR_ERR(chip->data.regmap); i2c_set_clientdata(client, chip); err = regmap_raw_write(chip->data.regmap, TCPC_ALERT_MASK, &val, sizeof(u16)); if (err < 0) return err; err = tcpci_check_std_output_cap(chip->data.regmap, TCPC_STD_OUTPUT_CAP_ORIENTATION); if (err < 0) return err; chip->data.set_orientation = err; err = devm_request_threaded_irq(&client->dev, client->irq, NULL, _tcpci_irq, IRQF_SHARED | IRQF_ONESHOT, dev_name(&client->dev), chip); if (err < 0) <S2SV_StartVul> return err; <S2SV_EndVul> <S2SV_StartVul> disable_irq(client->irq); <S2SV_EndVul> <S2SV_StartVul> chip->tcpci = tcpci_register_port(&client->dev, &chip->data); <S2SV_EndVul> <S2SV_StartVul> enable_irq(client->irq); <S2SV_EndVul> <S2SV_StartVul> return PTR_ERR_OR_ZERO(chip->tcpci); <S2SV_EndVul> }","- return err;
- disable_irq(client->irq);
- chip->tcpci = tcpci_register_port(&client->dev, &chip->data);
- enable_irq(client->irq);
- return PTR_ERR_OR_ZERO(chip->tcpci);
+ chip->tcpci = tcpci_register_port(&client->dev, &chip->data);
+ if (IS_ERR(chip->tcpci))
+ return PTR_ERR(chip->tcpci);
+ if (err < 0)
+ goto unregister_port;
+ err = tcpci_write16(chip->tcpci, TCPC_ALERT_MASK, chip->tcpci->alert_mask);
+ if (err < 0)
+ goto unregister_port;
+ return 0;
+ unregister_port:
+ tcpci_unregister_port(chip->tcpci);
+ return err;","static int tcpci_probe(struct i2c_client *client) { struct tcpci_chip *chip; int err; u16 val = 0; chip = devm_kzalloc(&client->dev, sizeof(*chip), GFP_KERNEL); if (!chip) return -ENOMEM; chip->data.regmap = devm_regmap_init_i2c(client, &tcpci_regmap_config); if (IS_ERR(chip->data.regmap)) return PTR_ERR(chip->data.regmap); i2c_set_clientdata(client, chip); err = regmap_raw_write(chip->data.regmap, TCPC_ALERT_MASK, &val, sizeof(u16)); if (err < 0) return err; err = tcpci_check_std_output_cap(chip->data.regmap, TCPC_STD_OUTPUT_CAP_ORIENTATION); if (err < 0) return err; chip->data.set_orientation = err; chip->tcpci = tcpci_register_port(&client->dev, &chip->data); if (IS_ERR(chip->tcpci)) return PTR_ERR(chip->tcpci); err = devm_request_threaded_irq(&client->dev, client->irq, NULL, _tcpci_irq, IRQF_SHARED | IRQF_ONESHOT, dev_name(&client->dev), chip); if (err < 0) goto unregister_port; err = tcpci_write16(chip->tcpci, TCPC_ALERT_MASK, chip->tcpci->alert_mask); if (err < 0) goto unregister_port; return 0; unregister_port: tcpci_unregister_port(chip->tcpci); return err; }"
321----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46788/bad/trace_osnoise.c----start_per_cpu_kthreads,"static int start_per_cpu_kthreads(void) { struct cpumask *current_mask = &save_cpumask; int retval = 0; int cpu; if (!test_bit(OSN_WORKLOAD, &osnoise_options)) { if (timerlat_enabled()) return 0; } cpus_read_lock(); cpumask_and(current_mask, cpu_online_mask, &osnoise_cpumask); <S2SV_StartVul> for_each_possible_cpu(cpu) <S2SV_EndVul> per_cpu(per_cpu_osnoise_var, cpu).kthread = NULL; for_each_cpu(cpu, current_mask) { retval = start_kthread(cpu); if (retval) { cpus_read_unlock(); stop_per_cpu_kthreads(); return retval; } } cpus_read_unlock(); return retval; }","- for_each_possible_cpu(cpu)
+ for_each_possible_cpu(cpu) {
+ if (cpumask_test_and_clear_cpu(cpu, &kthread_cpumask)) {
+ struct task_struct *kthread;
+ kthread = per_cpu(per_cpu_osnoise_var, cpu).kthread;
+ if (!WARN_ON(!kthread))
+ kthread_stop(kthread);
+ }
+ }","static int start_per_cpu_kthreads(void) { struct cpumask *current_mask = &save_cpumask; int retval = 0; int cpu; if (!test_bit(OSN_WORKLOAD, &osnoise_options)) { if (timerlat_enabled()) return 0; } cpus_read_lock(); cpumask_and(current_mask, cpu_online_mask, &osnoise_cpumask); for_each_possible_cpu(cpu) { if (cpumask_test_and_clear_cpu(cpu, &kthread_cpumask)) { struct task_struct *kthread; kthread = per_cpu(per_cpu_osnoise_var, cpu).kthread; if (!WARN_ON(!kthread)) kthread_stop(kthread); } per_cpu(per_cpu_osnoise_var, cpu).kthread = NULL; } for_each_cpu(cpu, current_mask) { retval = start_kthread(cpu); if (retval) { cpus_read_unlock(); stop_per_cpu_kthreads(); return retval; } } cpus_read_unlock(); return retval; }"
1062----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50304/bad/ip_tunnel.c----*ip_tunnel_find,"static struct ip_tunnel *ip_tunnel_find(struct ip_tunnel_net *itn, struct ip_tunnel_parm *parms, int type) { __be32 remote = parms->iph.daddr; __be32 local = parms->iph.saddr; __be32 key = parms->i_key; __be16 flags = parms->i_flags; int link = parms->link; struct ip_tunnel *t = NULL; struct hlist_head *head = ip_bucket(itn, parms); <S2SV_StartVul> hlist_for_each_entry_rcu(t, head, hash_node) { <S2SV_EndVul> if (local == t->parms.iph.saddr && remote == t->parms.iph.daddr && link == READ_ONCE(t->parms.link) && type == t->dev->type && ip_tunnel_key_match(&t->parms, flags, key)) break; } return t; }","- hlist_for_each_entry_rcu(t, head, hash_node) {
+ hlist_for_each_entry_rcu(t, head, hash_node, lockdep_rtnl_is_held()) {","static struct ip_tunnel *ip_tunnel_find(struct ip_tunnel_net *itn, struct ip_tunnel_parm *parms, int type) { __be32 remote = parms->iph.daddr; __be32 local = parms->iph.saddr; __be32 key = parms->i_key; __be16 flags = parms->i_flags; int link = parms->link; struct ip_tunnel *t = NULL; struct hlist_head *head = ip_bucket(itn, parms); hlist_for_each_entry_rcu(t, head, hash_node, lockdep_rtnl_is_held()) { if (local == t->parms.iph.saddr && remote == t->parms.iph.daddr && link == READ_ONCE(t->parms.link) && type == t->dev->type && ip_tunnel_key_match(&t->parms, flags, key)) break; } return t; }"
654----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49955/bad/battery.c----battery_hook_unregister_unlocked,"static void battery_hook_unregister_unlocked(struct acpi_battery_hook *hook) { struct acpi_battery *battery; list_for_each_entry(battery, &acpi_battery_list, list) { if (!hook->remove_battery(battery->bat, hook)) power_supply_changed(battery->bat); } <S2SV_StartVul> list_del(&hook->list); <S2SV_EndVul> pr_info(""extension unregistered: %s\n"", hook->name); }","- list_del(&hook->list);
+ list_del_init(&hook->list);","static void battery_hook_unregister_unlocked(struct acpi_battery_hook *hook) { struct acpi_battery *battery; list_for_each_entry(battery, &acpi_battery_list, list) { if (!hook->remove_battery(battery->bat, hook)) power_supply_changed(battery->bat); } list_del_init(&hook->list); pr_info(""extension unregistered: %s\n"", hook->name); }"
294----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46766/bad/ice_main.c----ice_suspend,"static int ice_suspend(struct device *dev) { struct pci_dev *pdev = to_pci_dev(dev); struct ice_pf *pf; int disabled, v; pf = pci_get_drvdata(pdev); if (!ice_pf_state_is_nominal(pf)) { dev_err(dev, ""Device is not ready, no need to suspend it\n""); return -EBUSY; } disabled = ice_service_task_stop(pf); ice_deinit_rdma(pf); if (test_and_set_bit(ICE_SUSPENDED, pf->state)) { if (!disabled) ice_service_task_restart(pf); return 0; } if (test_bit(ICE_DOWN, pf->state) || ice_is_reset_in_progress(pf->state)) { dev_err(dev, ""can't suspend device in reset or already down\n""); if (!disabled) ice_service_task_restart(pf); return 0; } ice_setup_mc_magic_wake(pf); ice_prepare_for_shutdown(pf); ice_set_wake(pf); ice_free_irq_msix_misc(pf); ice_for_each_vsi(pf, v) { if (!pf->vsi[v]) continue; ice_vsi_free_q_vectors(pf->vsi[v]); <S2SV_StartVul> } <S2SV_EndVul> ice_clear_interrupt_scheme(pf); pci_save_state(pdev); pci_wake_from_d3(pdev, pf->wol_ena); pci_set_power_state(pdev, PCI_D3hot); return 0; }","- }
+ rtnl_lock();
+ ice_vsi_clear_napi_queues(pf->vsi[v]);
+ rtnl_unlock();
+ }","static int ice_suspend(struct device *dev) { struct pci_dev *pdev = to_pci_dev(dev); struct ice_pf *pf; int disabled, v; pf = pci_get_drvdata(pdev); if (!ice_pf_state_is_nominal(pf)) { dev_err(dev, ""Device is not ready, no need to suspend it\n""); return -EBUSY; } disabled = ice_service_task_stop(pf); ice_deinit_rdma(pf); if (test_and_set_bit(ICE_SUSPENDED, pf->state)) { if (!disabled) ice_service_task_restart(pf); return 0; } if (test_bit(ICE_DOWN, pf->state) || ice_is_reset_in_progress(pf->state)) { dev_err(dev, ""can't suspend device in reset or already down\n""); if (!disabled) ice_service_task_restart(pf); return 0; } ice_setup_mc_magic_wake(pf); ice_prepare_for_shutdown(pf); ice_set_wake(pf); ice_free_irq_msix_misc(pf); ice_for_each_vsi(pf, v) { if (!pf->vsi[v]) continue; rtnl_lock(); ice_vsi_clear_napi_queues(pf->vsi[v]); rtnl_unlock(); ice_vsi_free_q_vectors(pf->vsi[v]); } ice_clear_interrupt_scheme(pf); pci_save_state(pdev); pci_wake_from_d3(pdev, pf->wol_ena); pci_set_power_state(pdev, PCI_D3hot); return 0; }"
429----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47669/bad/segment.c----nilfs_segctor_do_construct,"static int nilfs_segctor_do_construct(struct nilfs_sc_info *sci, int mode) { struct the_nilfs *nilfs = sci->sc_super->s_fs_info; int err; if (sb_rdonly(sci->sc_super)) return -EROFS; nilfs_sc_cstage_set(sci, NILFS_ST_INIT); sci->sc_cno = nilfs->ns_cno; err = nilfs_segctor_collect_dirty_files(sci, nilfs); if (unlikely(err)) goto out; if (nilfs_test_metadata_dirty(nilfs, sci->sc_root)) set_bit(NILFS_SC_DIRTY, &sci->sc_flags); if (nilfs_segctor_clean(sci)) goto out; do { sci->sc_stage.flags &= ~NILFS_CF_HISTORY_MASK; err = nilfs_segctor_begin_construction(sci, nilfs); if (unlikely(err)) <S2SV_StartVul> goto out; <S2SV_EndVul> sci->sc_seg_ctime = ktime_get_real_seconds(); err = nilfs_segctor_collect(sci, nilfs, mode); if (unlikely(err)) goto failed; if (nilfs_sc_cstage_get(sci) == NILFS_ST_DONE && nilfs_segbuf_empty(sci->sc_curseg)) { nilfs_segctor_abort_construction(sci, nilfs, 1); goto out; } err = nilfs_segctor_assign(sci, mode); if (unlikely(err)) goto failed; if (sci->sc_stage.flags & NILFS_CF_IFILE_STARTED) nilfs_segctor_fill_in_file_bmap(sci); if (mode == SC_LSEG_SR && nilfs_sc_cstage_get(sci) >= NILFS_ST_CPFILE) { err = nilfs_segctor_fill_in_checkpoint(sci); if (unlikely(err)) goto failed_to_write; nilfs_segctor_fill_in_super_root(sci, nilfs); } nilfs_segctor_update_segusage(sci, nilfs->ns_sufile); nilfs_segctor_prepare_write(sci); nilfs_add_checksums_on_logs(&sci->sc_segbufs, nilfs->ns_crc_seed); err = nilfs_segctor_write(sci, nilfs); if (unlikely(err)) goto failed_to_write; if (nilfs_sc_cstage_get(sci) == NILFS_ST_DONE || nilfs->ns_blocksize_bits != PAGE_SHIFT) { err = nilfs_segctor_wait(sci); if (err) goto failed_to_write; } } while (nilfs_sc_cstage_get(sci) != NILFS_ST_DONE); out: nilfs_segctor_drop_written_files(sci, nilfs); return err; failed_to_write: <S2SV_StartVul> if (sci->sc_stage.flags & NILFS_CF_IFILE_STARTED) <S2SV_EndVul> <S2SV_StartVul> nilfs_redirty_inodes(&sci->sc_dirty_files); <S2SV_EndVul> failed: if (nilfs_doing_gc()) nilfs_redirty_inodes(&sci->sc_gc_inodes); nilfs_segctor_abort_construction(sci, nilfs, err); goto out; }","- goto out;
- if (sci->sc_stage.flags & NILFS_CF_IFILE_STARTED)
- nilfs_redirty_inodes(&sci->sc_dirty_files);
+ goto failed;
+ if (mode == SC_LSEG_SR && nilfs_sc_cstage_get(sci) >= NILFS_ST_IFILE)
+ nilfs_redirty_inodes(&sci->sc_dirty_files);","static int nilfs_segctor_do_construct(struct nilfs_sc_info *sci, int mode) { struct the_nilfs *nilfs = sci->sc_super->s_fs_info; int err; if (sb_rdonly(sci->sc_super)) return -EROFS; nilfs_sc_cstage_set(sci, NILFS_ST_INIT); sci->sc_cno = nilfs->ns_cno; err = nilfs_segctor_collect_dirty_files(sci, nilfs); if (unlikely(err)) goto out; if (nilfs_test_metadata_dirty(nilfs, sci->sc_root)) set_bit(NILFS_SC_DIRTY, &sci->sc_flags); if (nilfs_segctor_clean(sci)) goto out; do { sci->sc_stage.flags &= ~NILFS_CF_HISTORY_MASK; err = nilfs_segctor_begin_construction(sci, nilfs); if (unlikely(err)) goto failed; sci->sc_seg_ctime = ktime_get_real_seconds(); err = nilfs_segctor_collect(sci, nilfs, mode); if (unlikely(err)) goto failed; if (nilfs_sc_cstage_get(sci) == NILFS_ST_DONE && nilfs_segbuf_empty(sci->sc_curseg)) { nilfs_segctor_abort_construction(sci, nilfs, 1); goto out; } err = nilfs_segctor_assign(sci, mode); if (unlikely(err)) goto failed; if (sci->sc_stage.flags & NILFS_CF_IFILE_STARTED) nilfs_segctor_fill_in_file_bmap(sci); if (mode == SC_LSEG_SR && nilfs_sc_cstage_get(sci) >= NILFS_ST_CPFILE) { err = nilfs_segctor_fill_in_checkpoint(sci); if (unlikely(err)) goto failed_to_write; nilfs_segctor_fill_in_super_root(sci, nilfs); } nilfs_segctor_update_segusage(sci, nilfs->ns_sufile); nilfs_segctor_prepare_write(sci); nilfs_add_checksums_on_logs(&sci->sc_segbufs, nilfs->ns_crc_seed); err = nilfs_segctor_write(sci, nilfs); if (unlikely(err)) goto failed_to_write; if (nilfs_sc_cstage_get(sci) == NILFS_ST_DONE || nilfs->ns_blocksize_bits != PAGE_SHIFT) { err = nilfs_segctor_wait(sci); if (err) goto failed_to_write; } } while (nilfs_sc_cstage_get(sci) != NILFS_ST_DONE); out: nilfs_segctor_drop_written_files(sci, nilfs); return err; failed_to_write: failed: if (mode == SC_LSEG_SR && nilfs_sc_cstage_get(sci) >= NILFS_ST_IFILE) nilfs_redirty_inodes(&sci->sc_dirty_files); if (nilfs_doing_gc()) nilfs_redirty_inodes(&sci->sc_gc_inodes); nilfs_segctor_abort_construction(sci, nilfs, err); goto out; }"
281----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46760/bad/usb.c----rtw_usb_init_rx,"static int rtw_usb_init_rx(struct rtw_dev *rtwdev) { struct rtw_usb *rtwusb = rtw_get_usb_priv(rtwdev); <S2SV_StartVul> int i; <S2SV_EndVul> rtwusb->rxwq = create_singlethread_workqueue(""rtw88_usb: rx wq""); if (!rtwusb->rxwq) { rtw_err(rtwdev, ""failed to create RX work queue\n""); return -ENOMEM; } skb_queue_head_init(&rtwusb->rx_queue); INIT_WORK(&rtwusb->rx_work, rtw_usb_rx_handler); for (i = 0; i < RTW_USB_RXCB_NUM; i++) { struct rx_usb_ctrl_block *rxcb = &rtwusb->rx_cb[i]; rtw_usb_rx_resubmit(rtwusb, rxcb); } <S2SV_StartVul> return 0; <S2SV_EndVul> }","- int i;
- return 0;
+ {
+ struct rtw_usb *rtwusb = rtw_get_usb_priv(rtwdev);
+ return 0;
+ }","static int rtw_usb_init_rx(struct rtw_dev *rtwdev) { struct rtw_usb *rtwusb = rtw_get_usb_priv(rtwdev); rtwusb->rxwq = create_singlethread_workqueue(""rtw88_usb: rx wq""); if (!rtwusb->rxwq) { rtw_err(rtwdev, ""failed to create RX work queue\n""); return -ENOMEM; } skb_queue_head_init(&rtwusb->rx_queue); INIT_WORK(&rtwusb->rx_work, rtw_usb_rx_handler); return 0; }"
644----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49946/bad/ppp_generic.c----ppp_channel_bridge_input,"static bool ppp_channel_bridge_input(struct channel *pch, struct sk_buff *skb) { struct channel *pchb; rcu_read_lock(); pchb = rcu_dereference(pch->bridge); if (!pchb) goto out_rcu; <S2SV_StartVul> spin_lock(&pchb->downl); <S2SV_EndVul> if (!pchb->chan) { kfree_skb(skb); goto outl; } skb_scrub_packet(skb, !net_eq(pch->chan_net, pchb->chan_net)); if (!pchb->chan->ops->start_xmit(pchb->chan, skb)) kfree_skb(skb); outl: <S2SV_StartVul> spin_unlock(&pchb->downl); <S2SV_EndVul> out_rcu: rcu_read_unlock(); return !!pchb; }","- spin_lock(&pchb->downl);
- spin_unlock(&pchb->downl);
+ spin_lock_bh(&pchb->downl);
+ spin_unlock_bh(&pchb->downl);","static bool ppp_channel_bridge_input(struct channel *pch, struct sk_buff *skb) { struct channel *pchb; rcu_read_lock(); pchb = rcu_dereference(pch->bridge); if (!pchb) goto out_rcu; spin_lock_bh(&pchb->downl); if (!pchb->chan) { kfree_skb(skb); goto outl; } skb_scrub_packet(skb, !net_eq(pch->chan_net, pchb->chan_net)); if (!pchb->chan->ops->start_xmit(pchb->chan, skb)) kfree_skb(skb); outl: spin_unlock_bh(&pchb->downl); out_rcu: rcu_read_unlock(); return !!pchb; }"
350----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46812/bad/display_mode_vba.c----ModeSupportAndSystemConfiguration,"void ModeSupportAndSystemConfiguration(struct display_mode_lib *mode_lib) { soc_bounding_box_st *soc = &mode_lib->vba.soc; unsigned int k; unsigned int total_pipes = 0; unsigned int pipe_idx = 0; mode_lib->vba.VoltageLevel = mode_lib->vba.cache_pipes[0].clks_cfg.voltage; mode_lib->vba.ReturnBW = mode_lib->vba.ReturnBWPerState[mode_lib->vba.VoltageLevel][mode_lib->vba.maxMpcComb]; if (mode_lib->vba.ReturnBW == 0) mode_lib->vba.ReturnBW = mode_lib->vba.ReturnBWPerState[mode_lib->vba.VoltageLevel][0]; mode_lib->vba.FabricAndDRAMBandwidth = mode_lib->vba.FabricAndDRAMBandwidthPerState[mode_lib->vba.VoltageLevel]; fetch_socbb_params(mode_lib); fetch_ip_params(mode_lib); fetch_pipe_params(mode_lib); mode_lib->vba.DCFCLK = mode_lib->vba.cache_pipes[0].clks_cfg.dcfclk_mhz; mode_lib->vba.SOCCLK = mode_lib->vba.cache_pipes[0].clks_cfg.socclk_mhz; if (mode_lib->vba.cache_pipes[0].clks_cfg.dispclk_mhz > 0.0) mode_lib->vba.DISPCLK = mode_lib->vba.cache_pipes[0].clks_cfg.dispclk_mhz; else mode_lib->vba.DISPCLK = soc->clock_limits[mode_lib->vba.VoltageLevel].dispclk_mhz; for (k = 0; k < mode_lib->vba.NumberOfActivePlanes; ++k) { <S2SV_StartVul> total_pipes += mode_lib->vba.DPPPerPlane[k]; <S2SV_EndVul> pipe_idx = get_pipe_idx(mode_lib, k); if (mode_lib->vba.cache_pipes[pipe_idx].clks_cfg.dppclk_mhz > 0.0) mode_lib->vba.DPPCLK[k] = mode_lib->vba.cache_pipes[pipe_idx].clks_cfg.dppclk_mhz; else mode_lib->vba.DPPCLK[k] = soc->clock_limits[mode_lib->vba.VoltageLevel].dppclk_mhz; } ASSERT(total_pipes <= DC__NUM_DPP__MAX); }","- total_pipes += mode_lib->vba.DPPPerPlane[k];
+ if (pipe_idx == -1) {
+ ASSERT(0);
+ continue; // skip inactive planes
+ }
+ total_pipes += mode_lib->vba.DPPPerPlane[k];","void ModeSupportAndSystemConfiguration(struct display_mode_lib *mode_lib) { soc_bounding_box_st *soc = &mode_lib->vba.soc; unsigned int k; unsigned int total_pipes = 0; unsigned int pipe_idx = 0; mode_lib->vba.VoltageLevel = mode_lib->vba.cache_pipes[0].clks_cfg.voltage; mode_lib->vba.ReturnBW = mode_lib->vba.ReturnBWPerState[mode_lib->vba.VoltageLevel][mode_lib->vba.maxMpcComb]; if (mode_lib->vba.ReturnBW == 0) mode_lib->vba.ReturnBW = mode_lib->vba.ReturnBWPerState[mode_lib->vba.VoltageLevel][0]; mode_lib->vba.FabricAndDRAMBandwidth = mode_lib->vba.FabricAndDRAMBandwidthPerState[mode_lib->vba.VoltageLevel]; fetch_socbb_params(mode_lib); fetch_ip_params(mode_lib); fetch_pipe_params(mode_lib); mode_lib->vba.DCFCLK = mode_lib->vba.cache_pipes[0].clks_cfg.dcfclk_mhz; mode_lib->vba.SOCCLK = mode_lib->vba.cache_pipes[0].clks_cfg.socclk_mhz; if (mode_lib->vba.cache_pipes[0].clks_cfg.dispclk_mhz > 0.0) mode_lib->vba.DISPCLK = mode_lib->vba.cache_pipes[0].clks_cfg.dispclk_mhz; else mode_lib->vba.DISPCLK = soc->clock_limits[mode_lib->vba.VoltageLevel].dispclk_mhz; for (k = 0; k < mode_lib->vba.NumberOfActivePlanes; ++k) { pipe_idx = get_pipe_idx(mode_lib, k); if (pipe_idx == -1) { ASSERT(0); continue; } total_pipes += mode_lib->vba.DPPPerPlane[k]; if (mode_lib->vba.cache_pipes[pipe_idx].clks_cfg.dppclk_mhz > 0.0) mode_lib->vba.DPPCLK[k] = mode_lib->vba.cache_pipes[pipe_idx].clks_cfg.dppclk_mhz; else mode_lib->vba.DPPCLK[k] = soc->clock_limits[mode_lib->vba.VoltageLevel].dppclk_mhz; } ASSERT(total_pipes <= DC__NUM_DPP__MAX); }"
433----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47671/bad/usbtmc.c----*usbtmc_create_urb,"static struct urb *usbtmc_create_urb(void) { const size_t bufsize = USBTMC_BUFSIZE; u8 *dmabuf = NULL; struct urb *urb = usb_alloc_urb(0, GFP_KERNEL); if (!urb) return NULL; <S2SV_StartVul> dmabuf = kmalloc(bufsize, GFP_KERNEL); <S2SV_EndVul> if (!dmabuf) { usb_free_urb(urb); return NULL; } urb->transfer_buffer = dmabuf; urb->transfer_buffer_length = bufsize; urb->transfer_flags |= URB_FREE_BUFFER; return urb; }","- dmabuf = kmalloc(bufsize, GFP_KERNEL);
+ dmabuf = kzalloc(bufsize, GFP_KERNEL);","static struct urb *usbtmc_create_urb(void) { const size_t bufsize = USBTMC_BUFSIZE; u8 *dmabuf = NULL; struct urb *urb = usb_alloc_urb(0, GFP_KERNEL); if (!urb) return NULL; dmabuf = kzalloc(bufsize, GFP_KERNEL); if (!dmabuf) { usb_free_urb(urb); return NULL; } urb->transfer_buffer = dmabuf; urb->transfer_buffer_length = bufsize; urb->transfer_flags |= URB_FREE_BUFFER; return urb; }"
820----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50089/bad/mkutf8data.c----main,"int main(int argc, char *argv[]) { unsigned int unichar; int opt; argv0 = argv[0]; while ((opt = getopt(argc, argv, ""a:c:d:f:hn:o:p:t:v"")) != -1) { switch (opt) { case 'a': age_name = optarg; break; case 'c': ccc_name = optarg; break; case 'd': data_name = optarg; break; case 'f': fold_name = optarg; break; case 'n': norm_name = optarg; break; case 'o': utf8_name = optarg; break; case 'p': prop_name = optarg; break; case 't': test_name = optarg; break; case 'v': verbose++; break; case 'h': help(); exit(0); default: usage(); } } if (verbose > 1) help(); for (unichar = 0; unichar != 0x110000; unichar++) unicode_data[unichar].code = unichar; age_init(); ccc_init(); nfdi_init(); nfdicf_init(); <S2SV_StartVul> ignore_init(); <S2SV_EndVul> corrections_init(); hangul_decompose(); nfdi_decompose(); nfdicf_decompose(); utf8_init(); trees_init(); trees_populate(); trees_reduce(); trees_verify(); (void)lookup(nfdi_tree, "" ""); if (verbose > 2) tree_walk(nfdi_tree); if (verbose > 2) tree_walk(nfdicf_tree); normalization_test(); write_file(); return 0; }",- ignore_init();,"int main(int argc, char *argv[]) { unsigned int unichar; int opt; argv0 = argv[0]; while ((opt = getopt(argc, argv, ""a:c:d:f:hn:o:p:t:v"")) != -1) { switch (opt) { case 'a': age_name = optarg; break; case 'c': ccc_name = optarg; break; case 'd': data_name = optarg; break; case 'f': fold_name = optarg; break; case 'n': norm_name = optarg; break; case 'o': utf8_name = optarg; break; case 'p': prop_name = optarg; break; case 't': test_name = optarg; break; case 'v': verbose++; break; case 'h': help(); exit(0); default: usage(); } } if (verbose > 1) help(); for (unichar = 0; unichar != 0x110000; unichar++) unicode_data[unichar].code = unichar; age_init(); ccc_init(); nfdi_init(); nfdicf_init(); corrections_init(); hangul_decompose(); nfdi_decompose(); nfdicf_decompose(); utf8_init(); trees_init(); trees_populate(); trees_reduce(); trees_verify(); (void)lookup(nfdi_tree, "" ""); if (verbose > 2) tree_walk(nfdi_tree); if (verbose > 2) tree_walk(nfdicf_tree); normalization_test(); write_file(); return 0; }"
1194----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53191/bad/dp.c----ath12k_dp_free,"void ath12k_dp_free(struct ath12k_base *ab) { struct ath12k_dp *dp = &ab->dp; int i; ath12k_dp_link_desc_cleanup(ab, dp->link_desc_banks, HAL_WBM_IDLE_LINK, &dp->wbm_idle_ring); ath12k_dp_cc_cleanup(ab); ath12k_dp_reoq_lut_cleanup(ab); ath12k_dp_deinit_bank_profiles(ab); ath12k_dp_srng_common_cleanup(ab); ath12k_dp_rx_reo_cmd_list_cleanup(ab); <S2SV_StartVul> for (i = 0; i < ab->hw_params->max_tx_ring; i++) <S2SV_EndVul> kfree(dp->tx_ring[i].tx_status); ath12k_dp_rx_free(ab); }","- for (i = 0; i < ab->hw_params->max_tx_ring; i++)
+ for (i = 0; i < ab->hw_params->max_tx_ring; i++) {
+ dp->tx_ring[i].tx_status = NULL;
+ }","void ath12k_dp_free(struct ath12k_base *ab) { struct ath12k_dp *dp = &ab->dp; int i; ath12k_dp_link_desc_cleanup(ab, dp->link_desc_banks, HAL_WBM_IDLE_LINK, &dp->wbm_idle_ring); ath12k_dp_cc_cleanup(ab); ath12k_dp_reoq_lut_cleanup(ab); ath12k_dp_deinit_bank_profiles(ab); ath12k_dp_srng_common_cleanup(ab); ath12k_dp_rx_reo_cmd_list_cleanup(ab); for (i = 0; i < ab->hw_params->max_tx_ring; i++) { kfree(dp->tx_ring[i].tx_status); dp->tx_ring[i].tx_status = NULL; } ath12k_dp_rx_free(ab); }"
460----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47702/bad/filter.c----flow_dissector_is_valid_access,"static bool flow_dissector_is_valid_access(int off, int size, enum bpf_access_type type, const struct bpf_prog *prog, struct bpf_insn_access_aux *info) { const int size_default = sizeof(__u32); if (off < 0 || off >= sizeof(struct __sk_buff)) return false; if (type == BPF_WRITE) return false; switch (off) { <S2SV_StartVul> case bpf_ctx_range(struct __sk_buff, data): <S2SV_EndVul> <S2SV_StartVul> if (size != size_default) <S2SV_EndVul> return false; info->reg_type = PTR_TO_PACKET; return true; <S2SV_StartVul> case bpf_ctx_range(struct __sk_buff, data_end): <S2SV_EndVul> <S2SV_StartVul> if (size != size_default) <S2SV_EndVul> return false; info->reg_type = PTR_TO_PACKET_END; return true; case bpf_ctx_range_ptr(struct __sk_buff, flow_keys): if (size != sizeof(__u64)) return false; info->reg_type = PTR_TO_FLOW_KEYS; return true; default: return false; } }","- case bpf_ctx_range(struct __sk_buff, data):
- if (size != size_default)
- case bpf_ctx_range(struct __sk_buff, data_end):
- if (size != size_default)
+ switch (off) {
+ case bpf_ctx_range(struct __sk_buff, data):
+ if (info->is_ldsx || size != size_default)
+ return false;
+ case bpf_ctx_range(struct __sk_buff, data_end):
+ if (info->is_ldsx || size != size_default)
+ return false;","static bool flow_dissector_is_valid_access(int off, int size, enum bpf_access_type type, const struct bpf_prog *prog, struct bpf_insn_access_aux *info) { const int size_default = sizeof(__u32); if (off < 0 || off >= sizeof(struct __sk_buff)) return false; if (type == BPF_WRITE) return false; switch (off) { case bpf_ctx_range(struct __sk_buff, data): if (info->is_ldsx || size != size_default) return false; info->reg_type = PTR_TO_PACKET; return true; case bpf_ctx_range(struct __sk_buff, data_end): if (info->is_ldsx || size != size_default) return false; info->reg_type = PTR_TO_PACKET_END; return true; case bpf_ctx_range_ptr(struct __sk_buff, flow_keys): if (size != sizeof(__u64)) return false; info->reg_type = PTR_TO_FLOW_KEYS; return true; default: return false; } }"
691----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49985/bad/i2c-stm32f7.c----stm32f7_i2c_runtime_resume,"static int __maybe_unused stm32f7_i2c_runtime_resume(struct device *dev) { struct stm32f7_i2c_dev *i2c_dev = dev_get_drvdata(dev); int ret; if (!stm32f7_i2c_is_slave_registered(i2c_dev)) { <S2SV_StartVul> ret = clk_prepare_enable(i2c_dev->clk); <S2SV_EndVul> if (ret) { <S2SV_StartVul> dev_err(dev, ""failed to prepare_enable clock\n""); <S2SV_EndVul> return ret; } } return 0; }","- ret = clk_prepare_enable(i2c_dev->clk);
- dev_err(dev, ""failed to prepare_enable clock\n"");
+ ret = clk_enable(i2c_dev->clk);
+ dev_err(dev, ""failed to enable clock\n"");","static int __maybe_unused stm32f7_i2c_runtime_resume(struct device *dev) { struct stm32f7_i2c_dev *i2c_dev = dev_get_drvdata(dev); int ret; if (!stm32f7_i2c_is_slave_registered(i2c_dev)) { ret = clk_enable(i2c_dev->clk); if (ret) { dev_err(dev, ""failed to enable clock\n""); return ret; } } return 0; }"
1507----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2025-21652/bad/link_watch.c----default_operstate,"static unsigned int default_operstate(const struct net_device *dev) { if (netif_testing(dev)) return IF_OPER_TESTING; if (!netif_carrier_ok(dev)) { <S2SV_StartVul> int iflink = dev_get_iflink(dev); <S2SV_EndVul> struct net_device *peer; <S2SV_StartVul> if (dev->reg_state == NETREG_UNREGISTERED || <S2SV_EndVul> <S2SV_StartVul> iflink == dev->ifindex) <S2SV_EndVul> return IF_OPER_DOWN; ASSERT_RTNL(); peer = __dev_get_by_index(dev_net(dev), iflink); if (!peer) return IF_OPER_DOWN; return netif_carrier_ok(peer) ? IF_OPER_DOWN : IF_OPER_LOWERLAYERDOWN; } if (netif_dormant(dev)) return IF_OPER_DORMANT; return IF_OPER_UP; }","- int iflink = dev_get_iflink(dev);
- if (dev->reg_state == NETREG_UNREGISTERED ||
- iflink == dev->ifindex)
+ int iflink;
+ if (dev->reg_state <= NETREG_REGISTERED)
+ iflink = dev_get_iflink(dev);
+ else
+ iflink = dev->ifindex;
+ if (iflink == dev->ifindex)","static unsigned int default_operstate(const struct net_device *dev) { if (netif_testing(dev)) return IF_OPER_TESTING; if (!netif_carrier_ok(dev)) { struct net_device *peer; int iflink; if (dev->reg_state <= NETREG_REGISTERED) iflink = dev_get_iflink(dev); else iflink = dev->ifindex; if (iflink == dev->ifindex) return IF_OPER_DOWN; ASSERT_RTNL(); peer = __dev_get_by_index(dev_net(dev), iflink); if (!peer) return IF_OPER_DOWN; return netif_carrier_ok(peer) ? IF_OPER_DOWN : IF_OPER_LOWERLAYERDOWN; } if (netif_dormant(dev)) return IF_OPER_DORMANT; return IF_OPER_UP; }"
989----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50226/bad/region.c----cxl_region_detach,"static int cxl_region_detach(struct cxl_endpoint_decoder *cxled) { struct cxl_port *iter, *ep_port = cxled_to_port(cxled); struct cxl_region *cxlr = cxled->cxld.region; struct cxl_region_params *p; int rc = 0; lockdep_assert_held_write(&cxl_region_rwsem); if (!cxlr) return 0; p = &cxlr->params; get_device(&cxlr->dev); if (p->state > CXL_CONFIG_ACTIVE) { <S2SV_StartVul> rc = cxl_region_decode_reset(cxlr, p->interleave_ways); <S2SV_EndVul> <S2SV_StartVul> if (rc) <S2SV_EndVul> <S2SV_StartVul> goto out; <S2SV_EndVul> <S2SV_StartVul> p->state = CXL_CONFIG_ACTIVE; <S2SV_EndVul> } for (iter = ep_port; !is_cxl_root(iter); iter = to_cxl_port(iter->dev.parent)) cxl_port_detach_region(iter, cxlr, cxled); if (cxled->pos < 0 || cxled->pos >= p->interleave_ways || p->targets[cxled->pos] != cxled) { struct cxl_memdev *cxlmd = cxled_to_memdev(cxled); dev_WARN_ONCE(&cxlr->dev, 1, ""expected %s:%s at position %d\n"", dev_name(&cxlmd->dev), dev_name(&cxled->cxld.dev), cxled->pos); goto out; } if (p->state == CXL_CONFIG_ACTIVE) { p->state = CXL_CONFIG_INTERLEAVE_ACTIVE; cxl_region_teardown_targets(cxlr); } p->targets[cxled->pos] = NULL; p->nr_targets--; cxled->cxld.hpa_range = (struct range) { .start = 0, .end = -1, }; up_write(&cxl_region_rwsem); device_release_driver(&cxlr->dev); down_write(&cxl_region_rwsem); out: put_device(&cxlr->dev); return rc; }","- rc = cxl_region_decode_reset(cxlr, p->interleave_ways);
- if (rc)
- goto out;
- p->state = CXL_CONFIG_ACTIVE;
+ cxl_region_decode_reset(cxlr, p->interleave_ways);
+ p->state = CXL_CONFIG_ACTIVE;","static int cxl_region_detach(struct cxl_endpoint_decoder *cxled) { struct cxl_port *iter, *ep_port = cxled_to_port(cxled); struct cxl_region *cxlr = cxled->cxld.region; struct cxl_region_params *p; int rc = 0; lockdep_assert_held_write(&cxl_region_rwsem); if (!cxlr) return 0; p = &cxlr->params; get_device(&cxlr->dev); if (p->state > CXL_CONFIG_ACTIVE) { cxl_region_decode_reset(cxlr, p->interleave_ways); p->state = CXL_CONFIG_ACTIVE; } for (iter = ep_port; !is_cxl_root(iter); iter = to_cxl_port(iter->dev.parent)) cxl_port_detach_region(iter, cxlr, cxled); if (cxled->pos < 0 || cxled->pos >= p->interleave_ways || p->targets[cxled->pos] != cxled) { struct cxl_memdev *cxlmd = cxled_to_memdev(cxled); dev_WARN_ONCE(&cxlr->dev, 1, ""expected %s:%s at position %d\n"", dev_name(&cxlmd->dev), dev_name(&cxled->cxld.dev), cxled->pos); goto out; } if (p->state == CXL_CONFIG_ACTIVE) { p->state = CXL_CONFIG_INTERLEAVE_ACTIVE; cxl_region_teardown_targets(cxlr); } p->targets[cxled->pos] = NULL; p->nr_targets--; cxled->cxld.hpa_range = (struct range) { .start = 0, .end = -1, }; up_write(&cxl_region_rwsem); device_release_driver(&cxlr->dev); down_write(&cxl_region_rwsem); out: put_device(&cxlr->dev); return rc; }"
760----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50037/bad/drm_fbdev_dma.c----drm_fbdev_dma_fb_destroy,static void drm_fbdev_dma_fb_destroy(struct fb_info *info) { struct drm_fb_helper *fb_helper = info->par; if (!fb_helper->dev) return; <S2SV_StartVul> fb_deferred_io_cleanup(info); <S2SV_EndVul> drm_fb_helper_fini(fb_helper); drm_client_buffer_vunmap(fb_helper->buffer); drm_client_framebuffer_delete(fb_helper->buffer); drm_client_release(&fb_helper->client); drm_fb_helper_unprepare(fb_helper); kfree(fb_helper); },"- fb_deferred_io_cleanup(info);
+ if (info->fbdefio)
+ fb_deferred_io_cleanup(info);",static void drm_fbdev_dma_fb_destroy(struct fb_info *info) { struct drm_fb_helper *fb_helper = info->par; if (!fb_helper->dev) return; if (info->fbdefio) fb_deferred_io_cleanup(info); drm_fb_helper_fini(fb_helper); drm_client_buffer_vunmap(fb_helper->buffer); drm_client_framebuffer_delete(fb_helper->buffer); drm_client_release(&fb_helper->client); drm_fb_helper_unprepare(fb_helper); kfree(fb_helper); }
381----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46842/bad/lpfc_els.c----lpfc_get_sfp_info_wait,"int lpfc_get_sfp_info_wait(struct lpfc_hba *phba, struct lpfc_rdp_context *rdp_context) { LPFC_MBOXQ_t *mbox = NULL; int rc; struct lpfc_dmabuf *mp; struct lpfc_dmabuf *mpsave; void *virt; MAILBOX_t *mb; mbox = mempool_alloc(phba->mbox_mem_pool, GFP_KERNEL); if (!mbox) { lpfc_printf_log(phba, KERN_WARNING, LOG_MBOX | LOG_ELS, ""7205 failed to allocate mailbox memory""); return 1; } if (lpfc_sli4_dump_page_a0(phba, mbox)) goto sfp_fail; mp = mbox->ctx_buf; mpsave = mp; virt = mp->virt; if (phba->sli_rev < LPFC_SLI_REV4) { mb = &mbox->u.mb; mb->un.varDmp.cv = 1; mb->un.varDmp.co = 1; mb->un.varWords[2] = 0; mb->un.varWords[3] = DMP_SFF_PAGE_A0_SIZE / 4; mb->un.varWords[4] = 0; mb->un.varWords[5] = 0; mb->un.varWords[6] = 0; mb->un.varWords[7] = 0; mb->un.varWords[8] = 0; mb->un.varWords[9] = 0; mb->un.varWords[10] = 0; mbox->in_ext_byte_len = DMP_SFF_PAGE_A0_SIZE; mbox->out_ext_byte_len = DMP_SFF_PAGE_A0_SIZE; mbox->mbox_offset_word = 5; mbox->ext_buf = virt; } else { bf_set(lpfc_mbx_memory_dump_type3_length, &mbox->u.mqe.un.mem_dump_type3, DMP_SFF_PAGE_A0_SIZE); mbox->u.mqe.un.mem_dump_type3.addr_lo = putPaddrLow(mp->phys); mbox->u.mqe.un.mem_dump_type3.addr_hi = putPaddrHigh(mp->phys); } mbox->vport = phba->pport; <S2SV_StartVul> rc = lpfc_sli_issue_mbox_wait(phba, mbox, 30); <S2SV_EndVul> if (rc == MBX_NOT_FINISHED) { rc = 1; goto error; } if (phba->sli_rev == LPFC_SLI_REV4) mp = mbox->ctx_buf; else mp = mpsave; if (bf_get(lpfc_mqe_status, &mbox->u.mqe)) { rc = 1; goto error; } lpfc_sli_bemem_bcopy(mp->virt, &rdp_context->page_a0, DMP_SFF_PAGE_A0_SIZE); memset(mbox, 0, sizeof(*mbox)); memset(mp->virt, 0, DMP_SFF_PAGE_A2_SIZE); INIT_LIST_HEAD(&mp->list); mbox->ctx_buf = mp; mbox->vport = phba->pport; bf_set(lpfc_mqe_command, &mbox->u.mqe, MBX_DUMP_MEMORY); bf_set(lpfc_mbx_memory_dump_type3_type, &mbox->u.mqe.un.mem_dump_type3, DMP_LMSD); bf_set(lpfc_mbx_memory_dump_type3_link, &mbox->u.mqe.un.mem_dump_type3, phba->sli4_hba.physical_port); bf_set(lpfc_mbx_memory_dump_type3_page_no, &mbox->u.mqe.un.mem_dump_type3, DMP_PAGE_A2); if (phba->sli_rev < LPFC_SLI_REV4) { mb = &mbox->u.mb; mb->un.varDmp.cv = 1; mb->un.varDmp.co = 1; mb->un.varWords[2] = 0; mb->un.varWords[3] = DMP_SFF_PAGE_A2_SIZE / 4; mb->un.varWords[4] = 0; mb->un.varWords[5] = 0; mb->un.varWords[6] = 0; mb->un.varWords[7] = 0; mb->un.varWords[8] = 0; mb->un.varWords[9] = 0; mb->un.varWords[10] = 0; mbox->in_ext_byte_len = DMP_SFF_PAGE_A2_SIZE; mbox->out_ext_byte_len = DMP_SFF_PAGE_A2_SIZE; mbox->mbox_offset_word = 5; mbox->ext_buf = virt; } else { bf_set(lpfc_mbx_memory_dump_type3_length, &mbox->u.mqe.un.mem_dump_type3, DMP_SFF_PAGE_A2_SIZE); mbox->u.mqe.un.mem_dump_type3.addr_lo = putPaddrLow(mp->phys); mbox->u.mqe.un.mem_dump_type3.addr_hi = putPaddrHigh(mp->phys); } <S2SV_StartVul> rc = lpfc_sli_issue_mbox_wait(phba, mbox, 30); <S2SV_EndVul> if (bf_get(lpfc_mqe_status, &mbox->u.mqe)) { rc = 1; goto error; } rc = 0; lpfc_sli_bemem_bcopy(mp->virt, &rdp_context->page_a2, DMP_SFF_PAGE_A2_SIZE); error: <S2SV_StartVul> mbox->ctx_buf = mpsave; <S2SV_EndVul> <S2SV_StartVul> lpfc_mbox_rsrc_cleanup(phba, mbox, MBOX_THD_UNLOCKED); <S2SV_EndVul> return rc; sfp_fail: mempool_free(mbox, phba->mbox_mem_pool); return 1; }","- rc = lpfc_sli_issue_mbox_wait(phba, mbox, 30);
- rc = lpfc_sli_issue_mbox_wait(phba, mbox, 30);
- mbox->ctx_buf = mpsave;
- lpfc_mbox_rsrc_cleanup(phba, mbox, MBOX_THD_UNLOCKED);
+ }
+ rc = lpfc_sli_issue_mbox_wait(phba, mbox, LPFC_MBOX_SLI4_CONFIG_TMO);
+ goto error;
+ }
+ if (rc == MBX_TIMEOUT)
+ goto error;
+ }
+ rc = lpfc_sli_issue_mbox_wait(phba, mbox, LPFC_MBOX_SLI4_CONFIG_TMO);
+ if (rc == MBX_TIMEOUT)
+ goto error;
+ goto error;
+ if (mbox->mbox_flag & LPFC_MBX_WAKE) {
+ mbox->ctx_buf = mpsave;
+ lpfc_mbox_rsrc_cleanup(phba, mbox, MBOX_THD_UNLOCKED);
+ }","int lpfc_get_sfp_info_wait(struct lpfc_hba *phba, struct lpfc_rdp_context *rdp_context) { LPFC_MBOXQ_t *mbox = NULL; int rc; struct lpfc_dmabuf *mp; struct lpfc_dmabuf *mpsave; void *virt; MAILBOX_t *mb; mbox = mempool_alloc(phba->mbox_mem_pool, GFP_KERNEL); if (!mbox) { lpfc_printf_log(phba, KERN_WARNING, LOG_MBOX | LOG_ELS, ""7205 failed to allocate mailbox memory""); return 1; } if (lpfc_sli4_dump_page_a0(phba, mbox)) goto sfp_fail; mp = mbox->ctx_buf; mpsave = mp; virt = mp->virt; if (phba->sli_rev < LPFC_SLI_REV4) { mb = &mbox->u.mb; mb->un.varDmp.cv = 1; mb->un.varDmp.co = 1; mb->un.varWords[2] = 0; mb->un.varWords[3] = DMP_SFF_PAGE_A0_SIZE / 4; mb->un.varWords[4] = 0; mb->un.varWords[5] = 0; mb->un.varWords[6] = 0; mb->un.varWords[7] = 0; mb->un.varWords[8] = 0; mb->un.varWords[9] = 0; mb->un.varWords[10] = 0; mbox->in_ext_byte_len = DMP_SFF_PAGE_A0_SIZE; mbox->out_ext_byte_len = DMP_SFF_PAGE_A0_SIZE; mbox->mbox_offset_word = 5; mbox->ext_buf = virt; } else { bf_set(lpfc_mbx_memory_dump_type3_length, &mbox->u.mqe.un.mem_dump_type3, DMP_SFF_PAGE_A0_SIZE); mbox->u.mqe.un.mem_dump_type3.addr_lo = putPaddrLow(mp->phys); mbox->u.mqe.un.mem_dump_type3.addr_hi = putPaddrHigh(mp->phys); } mbox->vport = phba->pport; rc = lpfc_sli_issue_mbox_wait(phba, mbox, LPFC_MBOX_SLI4_CONFIG_TMO); if (rc == MBX_NOT_FINISHED) { rc = 1; goto error; } if (rc == MBX_TIMEOUT) goto error; if (phba->sli_rev == LPFC_SLI_REV4) mp = mbox->ctx_buf; else mp = mpsave; if (bf_get(lpfc_mqe_status, &mbox->u.mqe)) { rc = 1; goto error; } lpfc_sli_bemem_bcopy(mp->virt, &rdp_context->page_a0, DMP_SFF_PAGE_A0_SIZE); memset(mbox, 0, sizeof(*mbox)); memset(mp->virt, 0, DMP_SFF_PAGE_A2_SIZE); INIT_LIST_HEAD(&mp->list); mbox->ctx_buf = mp; mbox->vport = phba->pport; bf_set(lpfc_mqe_command, &mbox->u.mqe, MBX_DUMP_MEMORY); bf_set(lpfc_mbx_memory_dump_type3_type, &mbox->u.mqe.un.mem_dump_type3, DMP_LMSD); bf_set(lpfc_mbx_memory_dump_type3_link, &mbox->u.mqe.un.mem_dump_type3, phba->sli4_hba.physical_port); bf_set(lpfc_mbx_memory_dump_type3_page_no, &mbox->u.mqe.un.mem_dump_type3, DMP_PAGE_A2); if (phba->sli_rev < LPFC_SLI_REV4) { mb = &mbox->u.mb; mb->un.varDmp.cv = 1; mb->un.varDmp.co = 1; mb->un.varWords[2] = 0; mb->un.varWords[3] = DMP_SFF_PAGE_A2_SIZE / 4; mb->un.varWords[4] = 0; mb->un.varWords[5] = 0; mb->un.varWords[6] = 0; mb->un.varWords[7] = 0; mb->un.varWords[8] = 0; mb->un.varWords[9] = 0; mb->un.varWords[10] = 0; mbox->in_ext_byte_len = DMP_SFF_PAGE_A2_SIZE; mbox->out_ext_byte_len = DMP_SFF_PAGE_A2_SIZE; mbox->mbox_offset_word = 5; mbox->ext_buf = virt; } else { bf_set(lpfc_mbx_memory_dump_type3_length, &mbox->u.mqe.un.mem_dump_type3, DMP_SFF_PAGE_A2_SIZE); mbox->u.mqe.un.mem_dump_type3.addr_lo = putPaddrLow(mp->phys); mbox->u.mqe.un.mem_dump_type3.addr_hi = putPaddrHigh(mp->phys); } rc = lpfc_sli_issue_mbox_wait(phba, mbox, LPFC_MBOX_SLI4_CONFIG_TMO); if (rc == MBX_TIMEOUT) goto error; if (bf_get(lpfc_mqe_status, &mbox->u.mqe)) { rc = 1; goto error; } rc = 0; lpfc_sli_bemem_bcopy(mp->virt, &rdp_context->page_a2, DMP_SFF_PAGE_A2_SIZE); error: if (mbox->mbox_flag & LPFC_MBX_WAKE) { mbox->ctx_buf = mpsave; lpfc_mbox_rsrc_cleanup(phba, mbox, MBOX_THD_UNLOCKED); } return rc; sfp_fail: mempool_free(mbox, phba->mbox_mem_pool); return 1; }"
120----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44979/bad/xe_gt_pagefault.c----xe_gt_pagefault_init,"int xe_gt_pagefault_init(struct xe_gt *gt) { struct xe_device *xe = gt_to_xe(gt); int i; if (!xe->info.has_usm) return 0; for (i = 0; i < NUM_PF_QUEUE; ++i) { gt->usm.pf_queue[i].gt = gt; spin_lock_init(&gt->usm.pf_queue[i].lock); INIT_WORK(&gt->usm.pf_queue[i].worker, pf_queue_work_func); } for (i = 0; i < NUM_ACC_QUEUE; ++i) { gt->usm.acc_queue[i].gt = gt; spin_lock_init(&gt->usm.acc_queue[i].lock); INIT_WORK(&gt->usm.acc_queue[i].worker, acc_queue_work_func); } gt->usm.pf_wq = alloc_workqueue(""xe_gt_page_fault_work_queue"", WQ_UNBOUND | WQ_HIGHPRI, NUM_PF_QUEUE); if (!gt->usm.pf_wq) return -ENOMEM; gt->usm.acc_wq = alloc_workqueue(""xe_gt_access_counter_work_queue"", WQ_UNBOUND | WQ_HIGHPRI, NUM_ACC_QUEUE); <S2SV_StartVul> if (!gt->usm.acc_wq) <S2SV_EndVul> return -ENOMEM; <S2SV_StartVul> return 0; <S2SV_EndVul> }","- if (!gt->usm.acc_wq)
- return 0;
+ if (!gt->usm.acc_wq) {
+ destroy_workqueue(gt->usm.pf_wq);
+ }
+ return devm_add_action_or_reset(xe->drm.dev, pagefault_fini, gt);
+ }","int xe_gt_pagefault_init(struct xe_gt *gt) { struct xe_device *xe = gt_to_xe(gt); int i; if (!xe->info.has_usm) return 0; for (i = 0; i < NUM_PF_QUEUE; ++i) { gt->usm.pf_queue[i].gt = gt; spin_lock_init(&gt->usm.pf_queue[i].lock); INIT_WORK(&gt->usm.pf_queue[i].worker, pf_queue_work_func); } for (i = 0; i < NUM_ACC_QUEUE; ++i) { gt->usm.acc_queue[i].gt = gt; spin_lock_init(&gt->usm.acc_queue[i].lock); INIT_WORK(&gt->usm.acc_queue[i].worker, acc_queue_work_func); } gt->usm.pf_wq = alloc_workqueue(""xe_gt_page_fault_work_queue"", WQ_UNBOUND | WQ_HIGHPRI, NUM_PF_QUEUE); if (!gt->usm.pf_wq) return -ENOMEM; gt->usm.acc_wq = alloc_workqueue(""xe_gt_access_counter_work_queue"", WQ_UNBOUND | WQ_HIGHPRI, NUM_ACC_QUEUE); if (!gt->usm.acc_wq) { destroy_workqueue(gt->usm.pf_wq); return -ENOMEM; } return devm_add_action_or_reset(xe->drm.dev, pagefault_fini, gt); }"
1314----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56617/bad/cacheinfo.c----init_level_allocate_ci,static inline int init_level_allocate_ci(unsigned int cpu) { unsigned int early_leaves = cache_leaves(cpu); if (per_cpu_cacheinfo(cpu) && !ci_cacheinfo(cpu)->early_ci_levels) return 0; if (init_cache_level(cpu) || !cache_leaves(cpu)) return -ENOENT; ci_cacheinfo(cpu)->early_ci_levels = false; <S2SV_StartVul> if (cache_leaves(cpu) <= early_leaves) <S2SV_EndVul> return 0; kfree(per_cpu_cacheinfo(cpu)); return allocate_cache_info(cpu); },"- if (cache_leaves(cpu) <= early_leaves)
+ if (cache_leaves(cpu) <= early_leaves && per_cpu_cacheinfo(cpu))",static inline int init_level_allocate_ci(unsigned int cpu) { unsigned int early_leaves = cache_leaves(cpu); if (per_cpu_cacheinfo(cpu) && !ci_cacheinfo(cpu)->early_ci_levels) return 0; if (init_cache_level(cpu) || !cache_leaves(cpu)) return -ENOENT; ci_cacheinfo(cpu)->early_ci_levels = false; if (cache_leaves(cpu) <= early_leaves && per_cpu_cacheinfo(cpu)) return 0; kfree(per_cpu_cacheinfo(cpu)); return allocate_cache_info(cpu); }
756----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50035/bad/ppp_async.c----ppp_async_encode,"ppp_async_encode(struct asyncppp *ap) { int fcs, i, count, c, proto; unsigned char *buf, *buflim; unsigned char *data; int islcp; buf = ap->obuf; ap->olim = buf; ap->optr = buf; i = ap->tpkt_pos; data = ap->tpkt->data; count = ap->tpkt->len; fcs = ap->tfcs; proto = get_unaligned_be16(data); <S2SV_StartVul> islcp = proto == PPP_LCP && 1 <= data[2] && data[2] <= 7; <S2SV_EndVul> if (i == 0) { if (islcp) async_lcp_peek(ap, data, count, 0); if (islcp || flag_time == 0 || time_after_eq(jiffies, ap->last_xmit + flag_time)) *buf++ = PPP_FLAG; ap->last_xmit = jiffies; fcs = PPP_INITFCS; if ((ap->flags & SC_COMP_AC) == 0 || islcp) { PUT_BYTE(ap, buf, 0xff, islcp); fcs = PPP_FCS(fcs, 0xff); PUT_BYTE(ap, buf, 0x03, islcp); fcs = PPP_FCS(fcs, 0x03); } } buflim = ap->obuf + OBUFSIZE - 6; while (i < count && buf < buflim) { c = data[i++]; if (i == 1 && c == 0 && (ap->flags & SC_COMP_PROT)) continue; fcs = PPP_FCS(fcs, c); PUT_BYTE(ap, buf, c, islcp); } if (i < count) { ap->olim = buf; ap->tpkt_pos = i; ap->tfcs = fcs; return 0; } fcs = ~fcs; c = fcs & 0xff; PUT_BYTE(ap, buf, c, islcp); c = (fcs >> 8) & 0xff; PUT_BYTE(ap, buf, c, islcp); *buf++ = PPP_FLAG; ap->olim = buf; consume_skb(ap->tpkt); ap->tpkt = NULL; return 1; }","- islcp = proto == PPP_LCP && 1 <= data[2] && data[2] <= 7;
+ islcp = proto == PPP_LCP && count >= 3 && 1 <= data[2] && data[2] <= 7;","ppp_async_encode(struct asyncppp *ap) { int fcs, i, count, c, proto; unsigned char *buf, *buflim; unsigned char *data; int islcp; buf = ap->obuf; ap->olim = buf; ap->optr = buf; i = ap->tpkt_pos; data = ap->tpkt->data; count = ap->tpkt->len; fcs = ap->tfcs; proto = get_unaligned_be16(data); islcp = proto == PPP_LCP && count >= 3 && 1 <= data[2] && data[2] <= 7; if (i == 0) { if (islcp) async_lcp_peek(ap, data, count, 0); if (islcp || flag_time == 0 || time_after_eq(jiffies, ap->last_xmit + flag_time)) *buf++ = PPP_FLAG; ap->last_xmit = jiffies; fcs = PPP_INITFCS; if ((ap->flags & SC_COMP_AC) == 0 || islcp) { PUT_BYTE(ap, buf, 0xff, islcp); fcs = PPP_FCS(fcs, 0xff); PUT_BYTE(ap, buf, 0x03, islcp); fcs = PPP_FCS(fcs, 0x03); } } buflim = ap->obuf + OBUFSIZE - 6; while (i < count && buf < buflim) { c = data[i++]; if (i == 1 && c == 0 && (ap->flags & SC_COMP_PROT)) continue; fcs = PPP_FCS(fcs, c); PUT_BYTE(ap, buf, c, islcp); } if (i < count) { ap->olim = buf; ap->tpkt_pos = i; ap->tfcs = fcs; return 0; } fcs = ~fcs; c = fcs & 0xff; PUT_BYTE(ap, buf, c, islcp); c = (fcs >> 8) & 0xff; PUT_BYTE(ap, buf, c, islcp); *buf++ = PPP_FLAG; ap->olim = buf; consume_skb(ap->tpkt); ap->tpkt = NULL; return 1; }"
1517----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2025-21673/bad/connect.c----cifs_put_tcp_session,"cifs_put_tcp_session(struct TCP_Server_Info *server, int from_reconnect) { struct task_struct *task; spin_lock(&cifs_tcp_ses_lock); if (--server->srv_count > 0) { spin_unlock(&cifs_tcp_ses_lock); return; } WARN_ON(server->srv_count < 0); list_del_init(&server->tcp_ses_list); spin_unlock(&cifs_tcp_ses_lock); cancel_delayed_work_sync(&server->echo); if (from_reconnect) cancel_delayed_work(&server->reconnect); else cancel_delayed_work_sync(&server->reconnect); if (SERVER_IS_CHAN(server)) cifs_put_tcp_session(server->primary_server, from_reconnect); spin_lock(&server->srv_lock); server->tcpStatus = CifsExiting; spin_unlock(&server->srv_lock); cifs_crypto_secmech_release(server); kfree_sensitive(server->session_key.response); server->session_key.response = NULL; server->session_key.len = 0; <S2SV_StartVul> kfree(server->hostname); <S2SV_EndVul> <S2SV_StartVul> server->hostname = NULL; <S2SV_EndVul> task = xchg(&server->tsk, NULL); if (task) send_sig(SIGKILL, task, 1); }","- kfree(server->hostname);
- server->hostname = NULL;","cifs_put_tcp_session(struct TCP_Server_Info *server, int from_reconnect) { struct task_struct *task; spin_lock(&cifs_tcp_ses_lock); if (--server->srv_count > 0) { spin_unlock(&cifs_tcp_ses_lock); return; } WARN_ON(server->srv_count < 0); list_del_init(&server->tcp_ses_list); spin_unlock(&cifs_tcp_ses_lock); cancel_delayed_work_sync(&server->echo); if (from_reconnect) cancel_delayed_work(&server->reconnect); else cancel_delayed_work_sync(&server->reconnect); if (SERVER_IS_CHAN(server)) cifs_put_tcp_session(server->primary_server, from_reconnect); spin_lock(&server->srv_lock); server->tcpStatus = CifsExiting; spin_unlock(&server->srv_lock); cifs_crypto_secmech_release(server); kfree_sensitive(server->session_key.response); server->session_key.response = NULL; server->session_key.len = 0; task = xchg(&server->tsk, NULL); if (task) send_sig(SIGKILL, task, 1); }"
285----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46762/bad/privcmd.c----privcmd_irqfd_assign,"static int privcmd_irqfd_assign(struct privcmd_irqfd *irqfd) { struct privcmd_kernel_irqfd *kirqfd, *tmp; unsigned long flags; __poll_t events; struct fd f; void *dm_op; <S2SV_StartVul> int ret; <S2SV_EndVul> kirqfd = kzalloc(sizeof(*kirqfd) + irqfd->size, GFP_KERNEL); if (!kirqfd) return -ENOMEM; dm_op = kirqfd + 1; if (copy_from_user(dm_op, u64_to_user_ptr(irqfd->dm_op), irqfd->size)) { ret = -EFAULT; goto error_kfree; } kirqfd->xbufs.size = irqfd->size; set_xen_guest_handle(kirqfd->xbufs.h, dm_op); kirqfd->dom = irqfd->dom; INIT_WORK(&kirqfd->shutdown, irqfd_shutdown); f = fdget(irqfd->fd); if (!f.file) { ret = -EBADF; goto error_kfree; } kirqfd->eventfd = eventfd_ctx_fileget(f.file); if (IS_ERR(kirqfd->eventfd)) { ret = PTR_ERR(kirqfd->eventfd); goto error_fd_put; } init_waitqueue_func_entry(&kirqfd->wait, irqfd_wakeup); init_poll_funcptr(&kirqfd->pt, irqfd_poll_func); spin_lock_irqsave(&irqfds_lock, flags); list_for_each_entry(tmp, &irqfds_list, list) { if (kirqfd->eventfd == tmp->eventfd) { ret = -EBUSY; spin_unlock_irqrestore(&irqfds_lock, flags); goto error_eventfd; } } list_add_tail(&kirqfd->list, &irqfds_list); spin_unlock_irqrestore(&irqfds_lock, flags); events = vfs_poll(f.file, &kirqfd->pt); if (events & EPOLLIN) irqfd_inject(kirqfd); fdput(f); return 0; error_eventfd: eventfd_ctx_put(kirqfd->eventfd); error_fd_put: fdput(f); error_kfree: kfree(kirqfd); return ret; }","- int ret;
+ int ret, idx;
+ idx = srcu_read_lock(&irqfds_srcu);
+ srcu_read_unlock(&irqfds_srcu, idx);","static int privcmd_irqfd_assign(struct privcmd_irqfd *irqfd) { struct privcmd_kernel_irqfd *kirqfd, *tmp; unsigned long flags; __poll_t events; struct fd f; void *dm_op; int ret, idx; kirqfd = kzalloc(sizeof(*kirqfd) + irqfd->size, GFP_KERNEL); if (!kirqfd) return -ENOMEM; dm_op = kirqfd + 1; if (copy_from_user(dm_op, u64_to_user_ptr(irqfd->dm_op), irqfd->size)) { ret = -EFAULT; goto error_kfree; } kirqfd->xbufs.size = irqfd->size; set_xen_guest_handle(kirqfd->xbufs.h, dm_op); kirqfd->dom = irqfd->dom; INIT_WORK(&kirqfd->shutdown, irqfd_shutdown); f = fdget(irqfd->fd); if (!f.file) { ret = -EBADF; goto error_kfree; } kirqfd->eventfd = eventfd_ctx_fileget(f.file); if (IS_ERR(kirqfd->eventfd)) { ret = PTR_ERR(kirqfd->eventfd); goto error_fd_put; } init_waitqueue_func_entry(&kirqfd->wait, irqfd_wakeup); init_poll_funcptr(&kirqfd->pt, irqfd_poll_func); spin_lock_irqsave(&irqfds_lock, flags); list_for_each_entry(tmp, &irqfds_list, list) { if (kirqfd->eventfd == tmp->eventfd) { ret = -EBUSY; spin_unlock_irqrestore(&irqfds_lock, flags); goto error_eventfd; } } idx = srcu_read_lock(&irqfds_srcu); list_add_tail(&kirqfd->list, &irqfds_list); spin_unlock_irqrestore(&irqfds_lock, flags); events = vfs_poll(f.file, &kirqfd->pt); if (events & EPOLLIN) irqfd_inject(kirqfd); srcu_read_unlock(&irqfds_srcu, idx); fdput(f); return 0; error_eventfd: eventfd_ctx_put(kirqfd->eventfd); error_fd_put: fdput(f); error_kfree: kfree(kirqfd); return ret; }"
732----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50007/bad/hpimsgx.c----HPIMSGX__init,"static u16 HPIMSGX__init(struct hpi_message *phm, struct hpi_response *phr ) { hpi_handler_func *entry_point_func; struct hpi_response hr; hpi_init_response(&hr, phm->object, phm->function, HPI_ERROR_INVALID_OBJ); entry_point_func = hpi_lookup_entry_point_function(phm->u.s.resource.r.pci); if (entry_point_func) { HPI_DEBUG_MESSAGE(DEBUG, phm); entry_point_func(phm, &hr); } else { phr->error = HPI_ERROR_PROCESSING_MESSAGE; return phr->error; } <S2SV_StartVul> if (hr.error == 0) { <S2SV_EndVul> hpi_entry_points[hr.u.s.adapter_index] = entry_point_func; HPI_DEBUG_LOG(DEBUG, ""HPI_SUBSYS_CREATE_ADAPTER successful,"" "" preparing adapter\n""); adapter_prepare(hr.u.s.adapter_index); } memcpy(phr, &hr, hr.size); return phr->error; }","- if (hr.error == 0) {
+ if (hr.error == 0 && hr.u.s.adapter_index < HPI_MAX_ADAPTERS) {","static u16 HPIMSGX__init(struct hpi_message *phm, struct hpi_response *phr ) { hpi_handler_func *entry_point_func; struct hpi_response hr; hpi_init_response(&hr, phm->object, phm->function, HPI_ERROR_INVALID_OBJ); entry_point_func = hpi_lookup_entry_point_function(phm->u.s.resource.r.pci); if (entry_point_func) { HPI_DEBUG_MESSAGE(DEBUG, phm); entry_point_func(phm, &hr); } else { phr->error = HPI_ERROR_PROCESSING_MESSAGE; return phr->error; } if (hr.error == 0 && hr.u.s.adapter_index < HPI_MAX_ADAPTERS) { hpi_entry_points[hr.u.s.adapter_index] = entry_point_func; HPI_DEBUG_LOG(DEBUG, ""HPI_SUBSYS_CREATE_ADAPTER successful,"" "" preparing adapter\n""); adapter_prepare(hr.u.s.adapter_index); } memcpy(phr, &hr, hr.size); return phr->error; }"
1539----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2025-21690/bad/storvsc_drv.c----storvsc_on_io_completion,"static void storvsc_on_io_completion(struct storvsc_device *stor_device, struct vstor_packet *vstor_packet, struct storvsc_cmd_request *request) { struct vstor_packet *stor_pkt; struct hv_device *device = stor_device->device; stor_pkt = &request->vstor_packet; if ((stor_pkt->vm_srb.cdb[0] == INQUIRY) || (stor_pkt->vm_srb.cdb[0] == MODE_SENSE) || (stor_pkt->vm_srb.cdb[0] == MAINTENANCE_IN && hv_dev_is_fc(device))) { vstor_packet->vm_srb.scsi_status = 0; vstor_packet->vm_srb.srb_status = SRB_STATUS_SUCCESS; } stor_pkt->vm_srb.scsi_status = vstor_packet->vm_srb.scsi_status; stor_pkt->vm_srb.srb_status = vstor_packet->vm_srb.srb_status; stor_pkt->vm_srb.sense_info_length = min_t(u8, sense_buffer_size, vstor_packet->vm_srb.sense_info_length); if (vstor_packet->vm_srb.scsi_status != 0 || vstor_packet->vm_srb.srb_status != SRB_STATUS_SUCCESS) { int loglevel = (stor_pkt->vm_srb.cdb[0] == TEST_UNIT_READY) ? STORVSC_LOGGING_WARN : STORVSC_LOGGING_ERROR; <S2SV_StartVul> storvsc_log(device, loglevel, <S2SV_EndVul> ""tag#%d cmd 0x%x status: scsi 0x%x srb 0x%x hv 0x%x\n"", scsi_cmd_to_rq(request->cmd)->tag, stor_pkt->vm_srb.cdb[0], vstor_packet->vm_srb.scsi_status, vstor_packet->vm_srb.srb_status, vstor_packet->status); } if (vstor_packet->vm_srb.scsi_status == SAM_STAT_CHECK_CONDITION && (vstor_packet->vm_srb.srb_status & SRB_STATUS_AUTOSENSE_VALID)) memcpy(request->cmd->sense_buffer, vstor_packet->vm_srb.sense_data, stor_pkt->vm_srb.sense_info_length); stor_pkt->vm_srb.data_transfer_length = vstor_packet->vm_srb.data_transfer_length; storvsc_command_completion(request, stor_device); if (atomic_dec_and_test(&stor_device->num_outstanding_req) && stor_device->drain_notify) wake_up(&stor_device->waiting_to_drain); }","- storvsc_log(device, loglevel,
+ storvsc_log_ratelimited(device, loglevel,","static void storvsc_on_io_completion(struct storvsc_device *stor_device, struct vstor_packet *vstor_packet, struct storvsc_cmd_request *request) { struct vstor_packet *stor_pkt; struct hv_device *device = stor_device->device; stor_pkt = &request->vstor_packet; if ((stor_pkt->vm_srb.cdb[0] == INQUIRY) || (stor_pkt->vm_srb.cdb[0] == MODE_SENSE) || (stor_pkt->vm_srb.cdb[0] == MAINTENANCE_IN && hv_dev_is_fc(device))) { vstor_packet->vm_srb.scsi_status = 0; vstor_packet->vm_srb.srb_status = SRB_STATUS_SUCCESS; } stor_pkt->vm_srb.scsi_status = vstor_packet->vm_srb.scsi_status; stor_pkt->vm_srb.srb_status = vstor_packet->vm_srb.srb_status; stor_pkt->vm_srb.sense_info_length = min_t(u8, sense_buffer_size, vstor_packet->vm_srb.sense_info_length); if (vstor_packet->vm_srb.scsi_status != 0 || vstor_packet->vm_srb.srb_status != SRB_STATUS_SUCCESS) { int loglevel = (stor_pkt->vm_srb.cdb[0] == TEST_UNIT_READY) ? STORVSC_LOGGING_WARN : STORVSC_LOGGING_ERROR; storvsc_log_ratelimited(device, loglevel, ""tag#%d cmd 0x%x status: scsi 0x%x srb 0x%x hv 0x%x\n"", scsi_cmd_to_rq(request->cmd)->tag, stor_pkt->vm_srb.cdb[0], vstor_packet->vm_srb.scsi_status, vstor_packet->vm_srb.srb_status, vstor_packet->status); } if (vstor_packet->vm_srb.scsi_status == SAM_STAT_CHECK_CONDITION && (vstor_packet->vm_srb.srb_status & SRB_STATUS_AUTOSENSE_VALID)) memcpy(request->cmd->sense_buffer, vstor_packet->vm_srb.sense_data, stor_pkt->vm_srb.sense_info_length); stor_pkt->vm_srb.data_transfer_length = vstor_packet->vm_srb.data_transfer_length; storvsc_command_completion(request, stor_device); if (atomic_dec_and_test(&stor_device->num_outstanding_req) && stor_device->drain_notify) wake_up(&stor_device->waiting_to_drain); }"
1287----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56587/bad/led-class.c----brightness_show,"static ssize_t brightness_show(struct device *dev, struct device_attribute *attr, char *buf) { struct led_classdev *led_cdev = dev_get_drvdata(dev); led_update_brightness(led_cdev); <S2SV_StartVul> return sprintf(buf, ""%u\n"", led_cdev->brightness); <S2SV_EndVul> }","- return sprintf(buf, ""%u\n"", led_cdev->brightness);
+ unsigned int brightness;
+ mutex_lock(&led_cdev->led_access);
+ brightness = led_cdev->brightness;
+ mutex_unlock(&led_cdev->led_access);
+ return sprintf(buf, ""%u\n"", brightness);","static ssize_t brightness_show(struct device *dev, struct device_attribute *attr, char *buf) { struct led_classdev *led_cdev = dev_get_drvdata(dev); unsigned int brightness; mutex_lock(&led_cdev->led_access); led_update_brightness(led_cdev); brightness = led_cdev->brightness; mutex_unlock(&led_cdev->led_access); return sprintf(buf, ""%u\n"", brightness); }"
949----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50192/bad/irq-gic-v3-its.c----*its_build_vmapp_cmd,"static struct its_vpe *its_build_vmapp_cmd(struct its_node *its, struct its_cmd_block *cmd, struct its_cmd_desc *desc) { struct its_vpe *vpe = valid_vpe(its, desc->its_vmapp_cmd.vpe); unsigned long vpt_addr, vconf_addr; u64 target; bool alloc; its_encode_cmd(cmd, GITS_CMD_VMAPP); its_encode_vpeid(cmd, desc->its_vmapp_cmd.vpe->vpe_id); its_encode_valid(cmd, desc->its_vmapp_cmd.valid); if (!desc->its_vmapp_cmd.valid) { if (is_v4_1(its)) { <S2SV_StartVul> alloc = !atomic_dec_return(&desc->its_vmapp_cmd.vpe->vmapp_count); <S2SV_EndVul> its_encode_alloc(cmd, alloc); vpe = NULL; } goto out; } vpt_addr = virt_to_phys(page_address(desc->its_vmapp_cmd.vpe->vpt_page)); target = desc->its_vmapp_cmd.col->target_address + its->vlpi_redist_offset; its_encode_target(cmd, target); its_encode_vpt_addr(cmd, vpt_addr); its_encode_vpt_size(cmd, LPI_NRBITS - 1); if (!is_v4_1(its)) goto out; vconf_addr = virt_to_phys(page_address(desc->its_vmapp_cmd.vpe->its_vm->vprop_page)); <S2SV_StartVul> alloc = !atomic_fetch_inc(&desc->its_vmapp_cmd.vpe->vmapp_count); <S2SV_EndVul> its_encode_alloc(cmd, alloc); its_encode_ptz(cmd, false); its_encode_vconf_addr(cmd, vconf_addr); its_encode_vmapp_default_db(cmd, desc->its_vmapp_cmd.vpe->vpe_db_lpi); out: its_fixup_cmd(cmd); return vpe; }","- alloc = !atomic_dec_return(&desc->its_vmapp_cmd.vpe->vmapp_count);
- alloc = !atomic_fetch_inc(&desc->its_vmapp_cmd.vpe->vmapp_count);
+ alloc = !atomic_dec_return(&desc->its_vmapp_cmd.vpe->vmapp_count);
+ alloc = !atomic_fetch_inc(&desc->its_vmapp_cmd.vpe->vmapp_count);","static struct its_vpe *its_build_vmapp_cmd(struct its_node *its, struct its_cmd_block *cmd, struct its_cmd_desc *desc) { struct its_vpe *vpe = valid_vpe(its, desc->its_vmapp_cmd.vpe); unsigned long vpt_addr, vconf_addr; u64 target; bool alloc; its_encode_cmd(cmd, GITS_CMD_VMAPP); its_encode_vpeid(cmd, desc->its_vmapp_cmd.vpe->vpe_id); its_encode_valid(cmd, desc->its_vmapp_cmd.valid); if (!desc->its_vmapp_cmd.valid) { alloc = !atomic_dec_return(&desc->its_vmapp_cmd.vpe->vmapp_count); if (is_v4_1(its)) { its_encode_alloc(cmd, alloc); vpe = NULL; } goto out; } vpt_addr = virt_to_phys(page_address(desc->its_vmapp_cmd.vpe->vpt_page)); target = desc->its_vmapp_cmd.col->target_address + its->vlpi_redist_offset; its_encode_target(cmd, target); its_encode_vpt_addr(cmd, vpt_addr); its_encode_vpt_size(cmd, LPI_NRBITS - 1); alloc = !atomic_fetch_inc(&desc->its_vmapp_cmd.vpe->vmapp_count); if (!is_v4_1(its)) goto out; vconf_addr = virt_to_phys(page_address(desc->its_vmapp_cmd.vpe->its_vm->vprop_page)); its_encode_alloc(cmd, alloc); its_encode_ptz(cmd, false); its_encode_vconf_addr(cmd, vconf_addr); its_encode_vmapp_default_db(cmd, desc->its_vmapp_cmd.vpe->vpe_db_lpi); out: its_fixup_cmd(cmd); return vpe; }"
592----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49914/bad/dcn20_hwseq.c----dcn20_program_pipe,"static void dcn20_program_pipe( struct dc *dc, struct pipe_ctx *pipe_ctx, struct dc_state *context) { struct dce_hwseq *hws = dc->hwseq; if (resource_is_pipe_type(pipe_ctx, OTG_MASTER)) { if (pipe_ctx->update_flags.bits.enable || pipe_ctx->update_flags.bits.odm || pipe_ctx->stream->update_flags.bits.abm_level) hws->funcs.blank_pixel_data(dc, pipe_ctx, !pipe_ctx->plane_state || !pipe_ctx->plane_state->visible); } if (pipe_ctx->update_flags.bits.global_sync && !pipe_ctx->top_pipe && !pipe_ctx->prev_odm_pipe) { pipe_ctx->stream_res.tg->funcs->program_global_sync( pipe_ctx->stream_res.tg, calculate_vready_offset_for_group(pipe_ctx), pipe_ctx->pipe_dlg_param.vstartup_start, pipe_ctx->pipe_dlg_param.vupdate_offset, pipe_ctx->pipe_dlg_param.vupdate_width); if (pipe_ctx->stream->mall_stream_config.type != SUBVP_PHANTOM) pipe_ctx->stream_res.tg->funcs->wait_for_state(pipe_ctx->stream_res.tg, CRTC_STATE_VACTIVE); pipe_ctx->stream_res.tg->funcs->set_vtg_params( pipe_ctx->stream_res.tg, &pipe_ctx->stream->timing, true); if (hws->funcs.setup_vupdate_interrupt) hws->funcs.setup_vupdate_interrupt(dc, pipe_ctx); } if (pipe_ctx->update_flags.bits.odm) hws->funcs.update_odm(dc, context, pipe_ctx); if (pipe_ctx->update_flags.bits.enable) { dcn20_enable_plane(dc, pipe_ctx, context); if (dc->res_pool->hubbub->funcs->force_wm_propagate_to_pipes) dc->res_pool->hubbub->funcs->force_wm_propagate_to_pipes(dc->res_pool->hubbub); } if (dc->res_pool->hubbub->funcs->program_det_size && pipe_ctx->update_flags.bits.det_size) dc->res_pool->hubbub->funcs->program_det_size( dc->res_pool->hubbub, pipe_ctx->plane_res.hubp->inst, pipe_ctx->det_buffer_size_kb); <S2SV_StartVul> if (pipe_ctx->update_flags.raw || pipe_ctx->plane_state->update_flags.raw || pipe_ctx->stream->update_flags.raw) <S2SV_EndVul> dcn20_update_dchubp_dpp(dc, pipe_ctx, context); <S2SV_StartVul> if (pipe_ctx->update_flags.bits.enable <S2SV_EndVul> <S2SV_StartVul> || pipe_ctx->plane_state->update_flags.bits.hdr_mult) <S2SV_EndVul> hws->funcs.set_hdr_multiplier(pipe_ctx); if (pipe_ctx->update_flags.bits.enable || <S2SV_StartVul> pipe_ctx->plane_state->update_flags.bits.in_transfer_func_change || <S2SV_EndVul> <S2SV_StartVul> pipe_ctx->plane_state->update_flags.bits.gamma_change || <S2SV_EndVul> <S2SV_StartVul> pipe_ctx->plane_state->update_flags.bits.lut_3d) <S2SV_EndVul> hws->funcs.set_input_transfer_func(dc, pipe_ctx, pipe_ctx->plane_state); if (pipe_ctx->update_flags.bits.enable || pipe_ctx->update_flags.bits.plane_changed || pipe_ctx->stream->update_flags.bits.out_tf || <S2SV_StartVul> pipe_ctx->plane_state->update_flags.bits.output_tf_change) <S2SV_EndVul> hws->funcs.set_output_transfer_func(dc, pipe_ctx, pipe_ctx->stream); if (pipe_ctx->update_flags.bits.enable || pipe_ctx->update_flags.bits.opp_changed) { pipe_ctx->stream_res.opp->funcs->opp_set_dyn_expansion( pipe_ctx->stream_res.opp, COLOR_SPACE_YCBCR601, pipe_ctx->stream->timing.display_color_depth, pipe_ctx->stream->signal); pipe_ctx->stream_res.opp->funcs->opp_program_fmt( pipe_ctx->stream_res.opp, &pipe_ctx->stream->bit_depth_params, &pipe_ctx->stream->clamping); } <S2SV_StartVul> if (pipe_ctx->plane_state->visible) { <S2SV_EndVul> if (pipe_ctx->stream_res.abm) { dc->hwss.set_pipe(pipe_ctx); pipe_ctx->stream_res.abm->funcs->set_abm_level(pipe_ctx->stream_res.abm, pipe_ctx->stream->abm_level); } } }","- if (pipe_ctx->update_flags.raw || pipe_ctx->plane_state->update_flags.raw || pipe_ctx->stream->update_flags.raw)
- if (pipe_ctx->update_flags.bits.enable
- || pipe_ctx->plane_state->update_flags.bits.hdr_mult)
- pipe_ctx->plane_state->update_flags.bits.in_transfer_func_change ||
- pipe_ctx->plane_state->update_flags.bits.gamma_change ||
- pipe_ctx->plane_state->update_flags.bits.lut_3d)
- pipe_ctx->plane_state->update_flags.bits.output_tf_change)
- if (pipe_ctx->plane_state->visible) {
+ if (pipe_ctx->update_flags.raw ||
+ (pipe_ctx->plane_state && pipe_ctx->plane_state->update_flags.raw) ||
+ pipe_ctx->stream->update_flags.raw)
+ if (pipe_ctx->update_flags.bits.enable ||
+ (pipe_ctx->plane_state && pipe_ctx->plane_state->update_flags.bits.hdr_mult))
+ if (pipe_ctx->update_flags.bits.enable ||
+ (pipe_ctx->plane_state &&
+ pipe_ctx->plane_state->update_flags.bits.in_transfer_func_change) ||
+ (pipe_ctx->plane_state &&
+ pipe_ctx->plane_state->update_flags.bits.gamma_change) ||
+ (pipe_ctx->plane_state &&
+ pipe_ctx->plane_state->update_flags.bits.lut_3d))
+ (pipe_ctx->plane_state &&
+ pipe_ctx->plane_state->update_flags.bits.output_tf_change))
+ if ((pipe_ctx->plane_state && pipe_ctx->plane_state->visible)) {","static void dcn20_program_pipe( struct dc *dc, struct pipe_ctx *pipe_ctx, struct dc_state *context) { struct dce_hwseq *hws = dc->hwseq; if (resource_is_pipe_type(pipe_ctx, OTG_MASTER)) { if (pipe_ctx->update_flags.bits.enable || pipe_ctx->update_flags.bits.odm || pipe_ctx->stream->update_flags.bits.abm_level) hws->funcs.blank_pixel_data(dc, pipe_ctx, !pipe_ctx->plane_state || !pipe_ctx->plane_state->visible); } if (pipe_ctx->update_flags.bits.global_sync && !pipe_ctx->top_pipe && !pipe_ctx->prev_odm_pipe) { pipe_ctx->stream_res.tg->funcs->program_global_sync( pipe_ctx->stream_res.tg, calculate_vready_offset_for_group(pipe_ctx), pipe_ctx->pipe_dlg_param.vstartup_start, pipe_ctx->pipe_dlg_param.vupdate_offset, pipe_ctx->pipe_dlg_param.vupdate_width); if (pipe_ctx->stream->mall_stream_config.type != SUBVP_PHANTOM) pipe_ctx->stream_res.tg->funcs->wait_for_state(pipe_ctx->stream_res.tg, CRTC_STATE_VACTIVE); pipe_ctx->stream_res.tg->funcs->set_vtg_params( pipe_ctx->stream_res.tg, &pipe_ctx->stream->timing, true); if (hws->funcs.setup_vupdate_interrupt) hws->funcs.setup_vupdate_interrupt(dc, pipe_ctx); } if (pipe_ctx->update_flags.bits.odm) hws->funcs.update_odm(dc, context, pipe_ctx); if (pipe_ctx->update_flags.bits.enable) { dcn20_enable_plane(dc, pipe_ctx, context); if (dc->res_pool->hubbub->funcs->force_wm_propagate_to_pipes) dc->res_pool->hubbub->funcs->force_wm_propagate_to_pipes(dc->res_pool->hubbub); } if (dc->res_pool->hubbub->funcs->program_det_size && pipe_ctx->update_flags.bits.det_size) dc->res_pool->hubbub->funcs->program_det_size( dc->res_pool->hubbub, pipe_ctx->plane_res.hubp->inst, pipe_ctx->det_buffer_size_kb); if (pipe_ctx->update_flags.raw || (pipe_ctx->plane_state && pipe_ctx->plane_state->update_flags.raw) || pipe_ctx->stream->update_flags.raw) dcn20_update_dchubp_dpp(dc, pipe_ctx, context); if (pipe_ctx->update_flags.bits.enable || (pipe_ctx->plane_state && pipe_ctx->plane_state->update_flags.bits.hdr_mult)) hws->funcs.set_hdr_multiplier(pipe_ctx); if (pipe_ctx->update_flags.bits.enable || (pipe_ctx->plane_state && pipe_ctx->plane_state->update_flags.bits.in_transfer_func_change) || (pipe_ctx->plane_state && pipe_ctx->plane_state->update_flags.bits.gamma_change) || (pipe_ctx->plane_state && pipe_ctx->plane_state->update_flags.bits.lut_3d)) hws->funcs.set_input_transfer_func(dc, pipe_ctx, pipe_ctx->plane_state); if (pipe_ctx->update_flags.bits.enable || pipe_ctx->update_flags.bits.plane_changed || pipe_ctx->stream->update_flags.bits.out_tf || (pipe_ctx->plane_state && pipe_ctx->plane_state->update_flags.bits.output_tf_change)) hws->funcs.set_output_transfer_func(dc, pipe_ctx, pipe_ctx->stream); if (pipe_ctx->update_flags.bits.enable || pipe_ctx->update_flags.bits.opp_changed) { pipe_ctx->stream_res.opp->funcs->opp_set_dyn_expansion( pipe_ctx->stream_res.opp, COLOR_SPACE_YCBCR601, pipe_ctx->stream->timing.display_color_depth, pipe_ctx->stream->signal); pipe_ctx->stream_res.opp->funcs->opp_program_fmt( pipe_ctx->stream_res.opp, &pipe_ctx->stream->bit_depth_params, &pipe_ctx->stream->clamping); } if ((pipe_ctx->plane_state && pipe_ctx->plane_state->visible)) { if (pipe_ctx->stream_res.abm) { dc->hwss.set_pipe(pipe_ctx); pipe_ctx->stream_res.abm->funcs->set_abm_level(pipe_ctx->stream_res.abm, pipe_ctx->stream->abm_level); } } }"
1282----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56579/bad/vpu_v4l2.c----vpu_add_func,"int vpu_add_func(struct vpu_dev *vpu, struct vpu_func *func) { struct video_device *vfd; int ret; if (!vpu || !func) return -EINVAL; if (func->vfd) return 0; func->m2m_dev = v4l2_m2m_init(&vpu_m2m_ops); if (IS_ERR(func->m2m_dev)) { dev_err(vpu->dev, ""v4l2_m2m_init fail\n""); func->vfd = NULL; return PTR_ERR(func->m2m_dev); } vfd = video_device_alloc(); if (!vfd) { v4l2_m2m_release(func->m2m_dev); dev_err(vpu->dev, ""alloc vpu decoder video device fail\n""); return -ENOMEM; } vfd->release = video_device_release; vfd->vfl_dir = VFL_DIR_M2M; vfd->v4l2_dev = &vpu->v4l2_dev; vfd->device_caps = V4L2_CAP_VIDEO_M2M_MPLANE | V4L2_CAP_STREAMING; if (func->type == VPU_CORE_TYPE_ENC) { strscpy(vfd->name, ""amphion-vpu-encoder"", sizeof(vfd->name)); vfd->fops = venc_get_fops(); vfd->ioctl_ops = venc_get_ioctl_ops(); } else { strscpy(vfd->name, ""amphion-vpu-decoder"", sizeof(vfd->name)); vfd->fops = vdec_get_fops(); vfd->ioctl_ops = vdec_get_ioctl_ops(); } ret = video_register_device(vfd, VFL_TYPE_VIDEO, -1); if (ret) { video_device_release(vfd); v4l2_m2m_release(func->m2m_dev); return ret; } <S2SV_StartVul> video_set_drvdata(vfd, vpu); <S2SV_EndVul> func->vfd = vfd; ret = v4l2_m2m_register_media_controller(func->m2m_dev, func->vfd, func->function); if (ret) { v4l2_m2m_release(func->m2m_dev); func->m2m_dev = NULL; video_unregister_device(func->vfd); func->vfd = NULL; return ret; } return 0; }","- video_set_drvdata(vfd, vpu);
+ video_set_drvdata(vfd, vpu);","int vpu_add_func(struct vpu_dev *vpu, struct vpu_func *func) { struct video_device *vfd; int ret; if (!vpu || !func) return -EINVAL; if (func->vfd) return 0; func->m2m_dev = v4l2_m2m_init(&vpu_m2m_ops); if (IS_ERR(func->m2m_dev)) { dev_err(vpu->dev, ""v4l2_m2m_init fail\n""); func->vfd = NULL; return PTR_ERR(func->m2m_dev); } vfd = video_device_alloc(); if (!vfd) { v4l2_m2m_release(func->m2m_dev); dev_err(vpu->dev, ""alloc vpu decoder video device fail\n""); return -ENOMEM; } vfd->release = video_device_release; vfd->vfl_dir = VFL_DIR_M2M; vfd->v4l2_dev = &vpu->v4l2_dev; vfd->device_caps = V4L2_CAP_VIDEO_M2M_MPLANE | V4L2_CAP_STREAMING; if (func->type == VPU_CORE_TYPE_ENC) { strscpy(vfd->name, ""amphion-vpu-encoder"", sizeof(vfd->name)); vfd->fops = venc_get_fops(); vfd->ioctl_ops = venc_get_ioctl_ops(); } else { strscpy(vfd->name, ""amphion-vpu-decoder"", sizeof(vfd->name)); vfd->fops = vdec_get_fops(); vfd->ioctl_ops = vdec_get_ioctl_ops(); } video_set_drvdata(vfd, vpu); ret = video_register_device(vfd, VFL_TYPE_VIDEO, -1); if (ret) { video_device_release(vfd); v4l2_m2m_release(func->m2m_dev); return ret; } func->vfd = vfd; ret = v4l2_m2m_register_media_controller(func->m2m_dev, func->vfd, func->function); if (ret) { v4l2_m2m_release(func->m2m_dev); func->m2m_dev = NULL; video_unregister_device(func->vfd); func->vfd = NULL; return ret; } return 0; }"
829----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50096/bad/nouveau_dmem.c----nouveau_dmem_fault_copy_one,"static vm_fault_t nouveau_dmem_fault_copy_one(struct nouveau_drm *drm, struct vm_fault *vmf, struct migrate_vma *args, dma_addr_t *dma_addr) { struct device *dev = drm->dev->dev; struct page *dpage, *spage; struct nouveau_svmm *svmm; spage = migrate_pfn_to_page(args->src[0]); if (!spage || !(args->src[0] & MIGRATE_PFN_MIGRATE)) return 0; <S2SV_StartVul> dpage = alloc_page_vma(GFP_HIGHUSER, vmf->vma, vmf->address); <S2SV_EndVul> if (!dpage) return VM_FAULT_SIGBUS; lock_page(dpage); *dma_addr = dma_map_page(dev, dpage, 0, PAGE_SIZE, DMA_BIDIRECTIONAL); if (dma_mapping_error(dev, *dma_addr)) goto error_free_page; svmm = spage->zone_device_data; mutex_lock(&svmm->mutex); nouveau_svmm_invalidate(svmm, args->start, args->end); if (drm->dmem->migrate.copy_func(drm, 1, NOUVEAU_APER_HOST, *dma_addr, NOUVEAU_APER_VRAM, nouveau_dmem_page_addr(spage))) goto error_dma_unmap; mutex_unlock(&svmm->mutex); args->dst[0] = migrate_pfn(page_to_pfn(dpage)) | MIGRATE_PFN_LOCKED; return 0; error_dma_unmap: mutex_unlock(&svmm->mutex); dma_unmap_page(dev, *dma_addr, PAGE_SIZE, DMA_BIDIRECTIONAL); error_free_page: __free_page(dpage); return VM_FAULT_SIGBUS; }","- dpage = alloc_page_vma(GFP_HIGHUSER, vmf->vma, vmf->address);
+ dpage = alloc_page_vma(GFP_HIGHUSER | __GFP_ZERO, vmf->vma, vmf->address);","static vm_fault_t nouveau_dmem_fault_copy_one(struct nouveau_drm *drm, struct vm_fault *vmf, struct migrate_vma *args, dma_addr_t *dma_addr) { struct device *dev = drm->dev->dev; struct page *dpage, *spage; struct nouveau_svmm *svmm; spage = migrate_pfn_to_page(args->src[0]); if (!spage || !(args->src[0] & MIGRATE_PFN_MIGRATE)) return 0; dpage = alloc_page_vma(GFP_HIGHUSER | __GFP_ZERO, vmf->vma, vmf->address); if (!dpage) return VM_FAULT_SIGBUS; lock_page(dpage); *dma_addr = dma_map_page(dev, dpage, 0, PAGE_SIZE, DMA_BIDIRECTIONAL); if (dma_mapping_error(dev, *dma_addr)) goto error_free_page; svmm = spage->zone_device_data; mutex_lock(&svmm->mutex); nouveau_svmm_invalidate(svmm, args->start, args->end); if (drm->dmem->migrate.copy_func(drm, 1, NOUVEAU_APER_HOST, *dma_addr, NOUVEAU_APER_VRAM, nouveau_dmem_page_addr(spage))) goto error_dma_unmap; mutex_unlock(&svmm->mutex); args->dst[0] = migrate_pfn(page_to_pfn(dpage)) | MIGRATE_PFN_LOCKED; return 0; error_dma_unmap: mutex_unlock(&svmm->mutex); dma_unmap_page(dev, *dma_addr, PAGE_SIZE, DMA_BIDIRECTIONAL); error_free_page: __free_page(dpage); return VM_FAULT_SIGBUS; }"
572----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49893/bad/dc.c----commit_planes_for_stream_fast,"static void commit_planes_for_stream_fast(struct dc *dc, struct dc_surface_update *srf_updates, int surface_count, struct dc_stream_state *stream, struct dc_stream_update *stream_update, enum surface_update_type update_type, struct dc_state *context) { int i, j; struct pipe_ctx *top_pipe_to_program = NULL; struct dc_stream_status *stream_status = NULL; bool should_offload_fams2_flip = false; if (dc->debug.fams2_config.bits.enable && dc->debug.fams2_config.bits.enable_offload_flip && dc_state_is_fams2_in_use(dc, context)) { should_offload_fams2_flip = true; for (i = 0; i < surface_count; i++) { if (srf_updates[i].surface && srf_updates[i].surface->update_flags.raw && !check_address_only_update(srf_updates[i].surface->update_flags)) { should_offload_fams2_flip = false; break; } } if (stream_update) { should_offload_fams2_flip = false; } } dc_exit_ips_for_hw_access(dc); dc_z10_restore(dc); top_pipe_to_program = resource_get_otg_master_for_stream( &context->res_ctx, stream); if (!top_pipe_to_program) return; for (i = 0; i < dc->res_pool->pipe_count; i++) { struct pipe_ctx *pipe = &context->res_ctx.pipe_ctx[i]; if (pipe->stream && pipe->plane_state) { if (!dc->debug.using_dml2) set_p_state_switch_method(dc, context, pipe); if (dc->debug.visual_confirm) dc_update_visual_confirm_color(dc, context, pipe); } } for (i = 0; i < surface_count; i++) { struct dc_plane_state *plane_state = srf_updates[i].surface; for (j = 0; j < dc->res_pool->pipe_count; j++) { struct pipe_ctx *pipe_ctx = &context->res_ctx.pipe_ctx[j]; if (!pipe_ctx->plane_state) continue; if (should_update_pipe_for_plane(context, pipe_ctx, plane_state)) continue; pipe_ctx->plane_state->triplebuffer_flips = false; if (update_type == UPDATE_TYPE_FAST && dc->hwss.program_triplebuffer != NULL && !pipe_ctx->plane_state->flip_immediate && dc->debug.enable_tri_buf) { pipe_ctx->plane_state->triplebuffer_flips = true; } } } stream_status = dc_state_get_stream_status(context, stream); if (should_offload_fams2_flip) { commit_plane_for_stream_offload_fams2_flip(dc, srf_updates, surface_count, stream, context); <S2SV_StartVul> } else { <S2SV_EndVul> build_dmub_cmd_list(dc, srf_updates, surface_count, stream, context, context->dc_dmub_cmd, &(context->dmub_cmd_count)); hwss_build_fast_sequence(dc, context->dc_dmub_cmd, context->dmub_cmd_count, context->block_sequence, &(context->block_sequence_steps), top_pipe_to_program, stream_status, context); hwss_execute_sequence(dc, context->block_sequence, context->block_sequence_steps); } if (top_pipe_to_program->stream) top_pipe_to_program->stream->update_flags.raw = 0; }","- } else {
+ } else if (stream_status) {","static void commit_planes_for_stream_fast(struct dc *dc, struct dc_surface_update *srf_updates, int surface_count, struct dc_stream_state *stream, struct dc_stream_update *stream_update, enum surface_update_type update_type, struct dc_state *context) { int i, j; struct pipe_ctx *top_pipe_to_program = NULL; struct dc_stream_status *stream_status = NULL; bool should_offload_fams2_flip = false; if (dc->debug.fams2_config.bits.enable && dc->debug.fams2_config.bits.enable_offload_flip && dc_state_is_fams2_in_use(dc, context)) { should_offload_fams2_flip = true; for (i = 0; i < surface_count; i++) { if (srf_updates[i].surface && srf_updates[i].surface->update_flags.raw && !check_address_only_update(srf_updates[i].surface->update_flags)) { should_offload_fams2_flip = false; break; } } if (stream_update) { should_offload_fams2_flip = false; } } dc_exit_ips_for_hw_access(dc); dc_z10_restore(dc); top_pipe_to_program = resource_get_otg_master_for_stream( &context->res_ctx, stream); if (!top_pipe_to_program) return; for (i = 0; i < dc->res_pool->pipe_count; i++) { struct pipe_ctx *pipe = &context->res_ctx.pipe_ctx[i]; if (pipe->stream && pipe->plane_state) { if (!dc->debug.using_dml2) set_p_state_switch_method(dc, context, pipe); if (dc->debug.visual_confirm) dc_update_visual_confirm_color(dc, context, pipe); } } for (i = 0; i < surface_count; i++) { struct dc_plane_state *plane_state = srf_updates[i].surface; for (j = 0; j < dc->res_pool->pipe_count; j++) { struct pipe_ctx *pipe_ctx = &context->res_ctx.pipe_ctx[j]; if (!pipe_ctx->plane_state) continue; if (should_update_pipe_for_plane(context, pipe_ctx, plane_state)) continue; pipe_ctx->plane_state->triplebuffer_flips = false; if (update_type == UPDATE_TYPE_FAST && dc->hwss.program_triplebuffer != NULL && !pipe_ctx->plane_state->flip_immediate && dc->debug.enable_tri_buf) { pipe_ctx->plane_state->triplebuffer_flips = true; } } } stream_status = dc_state_get_stream_status(context, stream); if (should_offload_fams2_flip) { commit_plane_for_stream_offload_fams2_flip(dc, srf_updates, surface_count, stream, context); } else if (stream_status) { build_dmub_cmd_list(dc, srf_updates, surface_count, stream, context, context->dc_dmub_cmd, &(context->dmub_cmd_count)); hwss_build_fast_sequence(dc, context->dc_dmub_cmd, context->dmub_cmd_count, context->block_sequence, &(context->block_sequence_steps), top_pipe_to_program, stream_status, context); hwss_execute_sequence(dc, context->block_sequence, context->block_sequence_steps); } if (top_pipe_to_program->stream) top_pipe_to_program->stream->update_flags.raw = 0; }"
342----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46803/bad/kfd_debug.c----debug_event_write_work_handler,"void debug_event_write_work_handler(struct work_struct *work) { struct kfd_process *process; static const char write_data = '.'; loff_t pos = 0; process = container_of(work, struct kfd_process, debug_event_workarea); <S2SV_StartVul> kernel_write(process->dbg_ev_file, &write_data, 1, &pos); <S2SV_EndVul> }","- kernel_write(process->dbg_ev_file, &write_data, 1, &pos);
+ if (process->debug_trap_enabled && process->dbg_ev_file)
+ kernel_write(process->dbg_ev_file, &write_data, 1, &pos);","void debug_event_write_work_handler(struct work_struct *work) { struct kfd_process *process; static const char write_data = '.'; loff_t pos = 0; process = container_of(work, struct kfd_process, debug_event_workarea); if (process->debug_trap_enabled && process->dbg_ev_file) kernel_write(process->dbg_ev_file, &write_data, 1, &pos); }"
1461----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-57801/bad/ipsec_fs.c----mlx5_esw_ipsec_restore_dest_uplink,"void mlx5_esw_ipsec_restore_dest_uplink(struct mlx5_core_dev *mdev) { #if IS_ENABLED(CONFIG_MLX5_CLS_ACT) struct mlx5_eswitch *esw = mdev->priv.eswitch; struct mlx5_eswitch_rep *rep; struct mlx5e_rep_priv *rpriv; struct rhashtable_iter iter; struct mlx5e_tc_flow *flow; unsigned long i; int err; <S2SV_StartVul> xa_for_each(&esw->offloads.vport_reps, i, rep) { <S2SV_EndVul> <S2SV_StartVul> rpriv = rep->rep_data[REP_ETH].priv; <S2SV_EndVul> <S2SV_StartVul> if (!rpriv || !rpriv->netdev) <S2SV_EndVul> continue; rhashtable_walk_enter(&rpriv->tc_ht, &iter); rhashtable_walk_start(&iter); while ((flow = rhashtable_walk_next(&iter)) != NULL) { if (IS_ERR(flow)) continue; err = mlx5_esw_ipsec_modify_flow_dests(esw, flow); if (err) mlx5_core_warn_once(mdev, ""Failed to modify flow dests for IPsec""); } rhashtable_walk_stop(&iter); rhashtable_walk_exit(&iter); } #endif }","- xa_for_each(&esw->offloads.vport_reps, i, rep) {
- rpriv = rep->rep_data[REP_ETH].priv;
- if (!rpriv || !rpriv->netdev)
+ mlx5_esw_for_each_rep(esw, i, rep) {
+ if (atomic_read(&rep->rep_data[REP_ETH].state) != REP_LOADED)
+ rpriv = rep->rep_data[REP_ETH].priv;","void mlx5_esw_ipsec_restore_dest_uplink(struct mlx5_core_dev *mdev) { #if IS_ENABLED(CONFIG_MLX5_CLS_ACT) struct mlx5_eswitch *esw = mdev->priv.eswitch; struct mlx5_eswitch_rep *rep; struct mlx5e_rep_priv *rpriv; struct rhashtable_iter iter; struct mlx5e_tc_flow *flow; unsigned long i; int err; mlx5_esw_for_each_rep(esw, i, rep) { if (atomic_read(&rep->rep_data[REP_ETH].state) != REP_LOADED) continue; rpriv = rep->rep_data[REP_ETH].priv; rhashtable_walk_enter(&rpriv->tc_ht, &iter); rhashtable_walk_start(&iter); while ((flow = rhashtable_walk_next(&iter)) != NULL) { if (IS_ERR(flow)) continue; err = mlx5_esw_ipsec_modify_flow_dests(esw, flow); if (err) mlx5_core_warn_once(mdev, ""Failed to modify flow dests for IPsec""); } rhashtable_walk_stop(&iter); rhashtable_walk_exit(&iter); } #endif }"
734----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50009/bad/amd-pstate.c----amd_pstate_adjust_perf,"static void amd_pstate_adjust_perf(unsigned int cpu, unsigned long _min_perf, unsigned long target_perf, unsigned long capacity) { unsigned long max_perf, min_perf, des_perf, cap_perf, lowest_nonlinear_perf; struct cpufreq_policy *policy = cpufreq_cpu_get(cpu); <S2SV_StartVul> struct amd_cpudata *cpudata = policy->driver_data; <S2SV_EndVul> if (policy->min != cpudata->min_limit_freq || policy->max != cpudata->max_limit_freq) amd_pstate_update_min_max_limit(policy); cap_perf = READ_ONCE(cpudata->highest_perf); lowest_nonlinear_perf = READ_ONCE(cpudata->lowest_nonlinear_perf); des_perf = cap_perf; if (target_perf < capacity) des_perf = DIV_ROUND_UP(cap_perf * target_perf, capacity); min_perf = READ_ONCE(cpudata->lowest_perf); if (_min_perf < capacity) min_perf = DIV_ROUND_UP(cap_perf * _min_perf, capacity); if (min_perf < lowest_nonlinear_perf) min_perf = lowest_nonlinear_perf; max_perf = cap_perf; if (max_perf < min_perf) max_perf = min_perf; des_perf = clamp_t(unsigned long, des_perf, min_perf, max_perf); amd_pstate_update(cpudata, min_perf, des_perf, max_perf, true, policy->governor->flags); cpufreq_cpu_put(policy); }","- struct amd_cpudata *cpudata = policy->driver_data;
+ struct amd_cpudata *cpudata;
+ if (!policy)
+ return;
+ cpudata = policy->driver_data;","static void amd_pstate_adjust_perf(unsigned int cpu, unsigned long _min_perf, unsigned long target_perf, unsigned long capacity) { unsigned long max_perf, min_perf, des_perf, cap_perf, lowest_nonlinear_perf; struct cpufreq_policy *policy = cpufreq_cpu_get(cpu); struct amd_cpudata *cpudata; if (!policy) return; cpudata = policy->driver_data; if (policy->min != cpudata->min_limit_freq || policy->max != cpudata->max_limit_freq) amd_pstate_update_min_max_limit(policy); cap_perf = READ_ONCE(cpudata->highest_perf); lowest_nonlinear_perf = READ_ONCE(cpudata->lowest_nonlinear_perf); des_perf = cap_perf; if (target_perf < capacity) des_perf = DIV_ROUND_UP(cap_perf * target_perf, capacity); min_perf = READ_ONCE(cpudata->lowest_perf); if (_min_perf < capacity) min_perf = DIV_ROUND_UP(cap_perf * _min_perf, capacity); if (min_perf < lowest_nonlinear_perf) min_perf = lowest_nonlinear_perf; max_perf = cap_perf; if (max_perf < min_perf) max_perf = min_perf; des_perf = clamp_t(unsigned long, des_perf, min_perf, max_perf); amd_pstate_update(cpudata, min_perf, des_perf, max_perf, true, policy->governor->flags); cpufreq_cpu_put(policy); }"
1067----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53049/bad/slub_kunit.c----test_kmalloc_redzone_access,"static void test_kmalloc_redzone_access(struct kunit *test) { struct kmem_cache *s = test_kmem_cache_create(""TestSlub_RZ_kmalloc"", 32, SLAB_KMALLOC|SLAB_STORE_USER|SLAB_RED_ZONE); <S2SV_StartVul> u8 *p = __kmalloc_cache_noprof(s, GFP_KERNEL, 18); <S2SV_EndVul> kasan_disable_current(); OPTIMIZER_HIDE_VAR(p); p[18] = 0xab; p[19] = 0xab; validate_slab_cache(s); KUNIT_EXPECT_EQ(test, 2, slab_errors); kasan_enable_current(); kmem_cache_free(s, p); kmem_cache_destroy(s); }","- u8 *p = __kmalloc_cache_noprof(s, GFP_KERNEL, 18);
+ u8 *p = alloc_hooks(__kmalloc_cache_noprof(s, GFP_KERNEL, 18));","static void test_kmalloc_redzone_access(struct kunit *test) { struct kmem_cache *s = test_kmem_cache_create(""TestSlub_RZ_kmalloc"", 32, SLAB_KMALLOC|SLAB_STORE_USER|SLAB_RED_ZONE); u8 *p = alloc_hooks(__kmalloc_cache_noprof(s, GFP_KERNEL, 18)); kasan_disable_current(); OPTIMIZER_HIDE_VAR(p); p[18] = 0xab; p[19] = 0xab; validate_slab_cache(s); KUNIT_EXPECT_EQ(test, 2, slab_errors); kasan_enable_current(); kmem_cache_free(s, p); kmem_cache_destroy(s); }"
797----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50074/bad/procfs.c----do_hardware_dma,"static int do_hardware_dma(const struct ctl_table *table, int write, void *result, size_t *lenp, loff_t *ppos) { struct parport *port = (struct parport *)table->extra1; char buffer[20]; int len = 0; if (*ppos) { *lenp = 0; return 0; } if (write) return -EACCES; <S2SV_StartVul> len += snprintf (buffer, sizeof(buffer), ""%d\n"", port->dma); <S2SV_EndVul> if (len > *lenp) len = *lenp; else *lenp = len; *ppos += len; memcpy(result, buffer, len); return 0; }","- len += snprintf (buffer, sizeof(buffer), ""%d\n"", port->dma);
+ len += scnprintf (buffer, sizeof(buffer), ""%d\n"", port->dma);","static int do_hardware_dma(const struct ctl_table *table, int write, void *result, size_t *lenp, loff_t *ppos) { struct parport *port = (struct parport *)table->extra1; char buffer[20]; int len = 0; if (*ppos) { *lenp = 0; return 0; } if (write) return -EACCES; len += scnprintf (buffer, sizeof(buffer), ""%d\n"", port->dma); if (len > *lenp) len = *lenp; else *lenp = len; *ppos += len; memcpy(result, buffer, len); return 0; }"
367----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46831/bad/vcap_api_kunit.c----vcap_api_encode_rule_test,"static void vcap_api_encode_rule_test(struct kunit *test) { static u32 keydata[32] = {}; static u32 mskdata[32] = {}; static u32 actdata[32] = {}; struct vcap_admin is2_admin = { .vtype = VCAP_TYPE_IS2, .first_cid = 8000000, .last_cid = 8099999, .lookups = 4, .last_valid_addr = 3071, .first_valid_addr = 0, .last_used_addr = 800, .cache = { .keystream = keydata, .maskstream = mskdata, .actionstream = actdata, }, }; struct vcap_rule *rule; struct vcap_rule_internal *ri; int vcap_chain_id = 8000000; enum vcap_user user = VCAP_USER_VCAP_UTIL; u16 priority = 10; int id = 100; int ret; struct vcap_u48_key smac = { .value = { 0x88, 0x75, 0x32, 0x34, 0x9e, 0xb1 }, .mask = { 0xff, 0xff, 0xff, 0xff, 0xff, 0xff } }; struct vcap_u48_key dmac = { .value = { 0x06, 0x05, 0x04, 0x03, 0x02, 0x01 }, .mask = { 0xff, 0xff, 0xff, 0xff, 0xff, 0xff } }; u32 port_mask_rng_value = 0x05; u32 port_mask_rng_mask = 0x0f; u32 igr_port_mask_value = 0xffabcd01; u32 igr_port_mask_mask = ~0; u32 expwriteaddr[] = {792, 792, 793, 794, 795, 796, 797}; int idx; vcap_test_api_init(&is2_admin); rule = vcap_alloc_rule(&test_vctrl, &test_netdev, vcap_chain_id, user, priority, id); KUNIT_EXPECT_PTR_NE(test, NULL, rule); ri = (struct vcap_rule_internal *)rule; ret = vcap_rule_add_key_u48(rule, VCAP_KF_L2_DMAC, &dmac); KUNIT_EXPECT_EQ(test, 0, ret); ret = vcap_rule_add_key_u48(rule, VCAP_KF_L2_SMAC, &smac); KUNIT_EXPECT_EQ(test, 0, ret); ret = vcap_rule_add_key_bit(rule, VCAP_KF_ETYPE_LEN_IS, VCAP_BIT_1); KUNIT_EXPECT_EQ(test, 0, ret); ret = vcap_rule_add_key_bit(rule, VCAP_KF_ETYPE_LEN_IS, VCAP_BIT_1); KUNIT_EXPECT_EQ(test, -EINVAL, ret); ret = vcap_rule_add_key_bit(rule, VCAP_KF_IF_IGR_PORT_MASK_L3, VCAP_BIT_ANY); KUNIT_EXPECT_EQ(test, 0, ret); ret = vcap_rule_add_key_u32(rule, VCAP_KF_IF_IGR_PORT_MASK_RNG, port_mask_rng_value, port_mask_rng_mask); KUNIT_EXPECT_EQ(test, 0, ret); ret = vcap_rule_add_key_u32(rule, VCAP_KF_IF_IGR_PORT_MASK, igr_port_mask_value, igr_port_mask_mask); KUNIT_EXPECT_EQ(test, 0, ret); ret = vcap_rule_add_action_bit(rule, VCAP_AF_POLICE_ENA, VCAP_BIT_1); KUNIT_EXPECT_EQ(test, 0, ret); ret = vcap_rule_add_action_u32(rule, VCAP_AF_CNT_ID, id); KUNIT_EXPECT_EQ(test, 0, ret); ret = vcap_rule_add_action_u32(rule, VCAP_AF_MATCH_ID, 1); KUNIT_EXPECT_EQ(test, 0, ret); ret = vcap_rule_add_action_u32(rule, VCAP_AF_MATCH_ID_MASK, 1); KUNIT_EXPECT_EQ(test, 0, ret); ret = vcap_set_rule_set_actionset(rule, VCAP_AFS_BASE_TYPE); KUNIT_EXPECT_EQ(test, 0, ret); ret = vcap_val_rule(rule, ETH_P_ALL); KUNIT_EXPECT_EQ(test, 0, ret); KUNIT_EXPECT_EQ(test, VCAP_KFS_MAC_ETYPE, rule->keyset); KUNIT_EXPECT_EQ(test, VCAP_AFS_BASE_TYPE, rule->actionset); KUNIT_EXPECT_EQ(test, 6, ri->size); KUNIT_EXPECT_EQ(test, 2, ri->keyset_sw_regs); KUNIT_EXPECT_EQ(test, 4, ri->actionset_sw_regs); ret = vcap_enable_lookups(&test_vctrl, &test_netdev, 0, rule->vcap_chain_id, rule->cookie, true); KUNIT_EXPECT_EQ(test, 0, ret); ret = vcap_add_rule(rule); KUNIT_EXPECT_EQ(test, 0, ret); KUNIT_EXPECT_EQ(test, 792, is2_admin.last_used_addr); for (idx = 0; idx < ARRAY_SIZE(expwriteaddr); ++idx) KUNIT_EXPECT_EQ(test, expwriteaddr[idx], test_updateaddr[idx]); ret = list_empty(&is2_admin.rules); KUNIT_EXPECT_EQ(test, false, ret); KUNIT_EXPECT_EQ(test, 0, ret); vcap_enable_lookups(&test_vctrl, &test_netdev, 0, 0, rule->cookie, false); <S2SV_StartVul> vcap_free_rule(rule); <S2SV_EndVul> <S2SV_StartVul> KUNIT_EXPECT_PTR_NE(test, NULL, rule); <S2SV_EndVul> <S2SV_StartVul> ret = list_empty(&rule->keyfields); <S2SV_EndVul> <S2SV_StartVul> KUNIT_EXPECT_EQ(test, true, ret); <S2SV_EndVul> <S2SV_StartVul> ret = list_empty(&rule->actionfields); <S2SV_EndVul> <S2SV_StartVul> KUNIT_EXPECT_EQ(test, true, ret); <S2SV_EndVul> <S2SV_StartVul> vcap_del_rule(&test_vctrl, &test_netdev, id); <S2SV_EndVul> }","- vcap_free_rule(rule);
- KUNIT_EXPECT_PTR_NE(test, NULL, rule);
- ret = list_empty(&rule->keyfields);
- KUNIT_EXPECT_EQ(test, true, ret);
- ret = list_empty(&rule->actionfields);
- KUNIT_EXPECT_EQ(test, true, ret);
- vcap_del_rule(&test_vctrl, &test_netdev, id);
+ ret = vcap_del_rule(&test_vctrl, &test_netdev, id);
+ KUNIT_EXPECT_EQ(test, 0, ret);","static void vcap_api_encode_rule_test(struct kunit *test) { static u32 keydata[32] = {}; static u32 mskdata[32] = {}; static u32 actdata[32] = {}; struct vcap_admin is2_admin = { .vtype = VCAP_TYPE_IS2, .first_cid = 8000000, .last_cid = 8099999, .lookups = 4, .last_valid_addr = 3071, .first_valid_addr = 0, .last_used_addr = 800, .cache = { .keystream = keydata, .maskstream = mskdata, .actionstream = actdata, }, }; struct vcap_rule *rule; struct vcap_rule_internal *ri; int vcap_chain_id = 8000000; enum vcap_user user = VCAP_USER_VCAP_UTIL; u16 priority = 10; int id = 100; int ret; struct vcap_u48_key smac = { .value = { 0x88, 0x75, 0x32, 0x34, 0x9e, 0xb1 }, .mask = { 0xff, 0xff, 0xff, 0xff, 0xff, 0xff } }; struct vcap_u48_key dmac = { .value = { 0x06, 0x05, 0x04, 0x03, 0x02, 0x01 }, .mask = { 0xff, 0xff, 0xff, 0xff, 0xff, 0xff } }; u32 port_mask_rng_value = 0x05; u32 port_mask_rng_mask = 0x0f; u32 igr_port_mask_value = 0xffabcd01; u32 igr_port_mask_mask = ~0; u32 expwriteaddr[] = {792, 792, 793, 794, 795, 796, 797}; int idx; vcap_test_api_init(&is2_admin); rule = vcap_alloc_rule(&test_vctrl, &test_netdev, vcap_chain_id, user, priority, id); KUNIT_EXPECT_PTR_NE(test, NULL, rule); ri = (struct vcap_rule_internal *)rule; ret = vcap_rule_add_key_u48(rule, VCAP_KF_L2_DMAC, &dmac); KUNIT_EXPECT_EQ(test, 0, ret); ret = vcap_rule_add_key_u48(rule, VCAP_KF_L2_SMAC, &smac); KUNIT_EXPECT_EQ(test, 0, ret); ret = vcap_rule_add_key_bit(rule, VCAP_KF_ETYPE_LEN_IS, VCAP_BIT_1); KUNIT_EXPECT_EQ(test, 0, ret); ret = vcap_rule_add_key_bit(rule, VCAP_KF_ETYPE_LEN_IS, VCAP_BIT_1); KUNIT_EXPECT_EQ(test, -EINVAL, ret); ret = vcap_rule_add_key_bit(rule, VCAP_KF_IF_IGR_PORT_MASK_L3, VCAP_BIT_ANY); KUNIT_EXPECT_EQ(test, 0, ret); ret = vcap_rule_add_key_u32(rule, VCAP_KF_IF_IGR_PORT_MASK_RNG, port_mask_rng_value, port_mask_rng_mask); KUNIT_EXPECT_EQ(test, 0, ret); ret = vcap_rule_add_key_u32(rule, VCAP_KF_IF_IGR_PORT_MASK, igr_port_mask_value, igr_port_mask_mask); KUNIT_EXPECT_EQ(test, 0, ret); ret = vcap_rule_add_action_bit(rule, VCAP_AF_POLICE_ENA, VCAP_BIT_1); KUNIT_EXPECT_EQ(test, 0, ret); ret = vcap_rule_add_action_u32(rule, VCAP_AF_CNT_ID, id); KUNIT_EXPECT_EQ(test, 0, ret); ret = vcap_rule_add_action_u32(rule, VCAP_AF_MATCH_ID, 1); KUNIT_EXPECT_EQ(test, 0, ret); ret = vcap_rule_add_action_u32(rule, VCAP_AF_MATCH_ID_MASK, 1); KUNIT_EXPECT_EQ(test, 0, ret); ret = vcap_set_rule_set_actionset(rule, VCAP_AFS_BASE_TYPE); KUNIT_EXPECT_EQ(test, 0, ret); ret = vcap_val_rule(rule, ETH_P_ALL); KUNIT_EXPECT_EQ(test, 0, ret); KUNIT_EXPECT_EQ(test, VCAP_KFS_MAC_ETYPE, rule->keyset); KUNIT_EXPECT_EQ(test, VCAP_AFS_BASE_TYPE, rule->actionset); KUNIT_EXPECT_EQ(test, 6, ri->size); KUNIT_EXPECT_EQ(test, 2, ri->keyset_sw_regs); KUNIT_EXPECT_EQ(test, 4, ri->actionset_sw_regs); ret = vcap_enable_lookups(&test_vctrl, &test_netdev, 0, rule->vcap_chain_id, rule->cookie, true); KUNIT_EXPECT_EQ(test, 0, ret); ret = vcap_add_rule(rule); KUNIT_EXPECT_EQ(test, 0, ret); KUNIT_EXPECT_EQ(test, 792, is2_admin.last_used_addr); for (idx = 0; idx < ARRAY_SIZE(expwriteaddr); ++idx) KUNIT_EXPECT_EQ(test, expwriteaddr[idx], test_updateaddr[idx]); ret = list_empty(&is2_admin.rules); KUNIT_EXPECT_EQ(test, false, ret); KUNIT_EXPECT_EQ(test, 0, ret); vcap_enable_lookups(&test_vctrl, &test_netdev, 0, 0, rule->cookie, false); ret = vcap_del_rule(&test_vctrl, &test_netdev, id); KUNIT_EXPECT_EQ(test, 0, ret); }"
177----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-45026/bad/dasd_eckd.c----*dasd_eckd_build_cp_cmd_single,"static struct dasd_ccw_req *dasd_eckd_build_cp_cmd_single( struct dasd_device *startdev, struct dasd_block *block, struct request *req, sector_t first_rec, sector_t last_rec, sector_t first_trk, sector_t last_trk, unsigned int first_offs, unsigned int last_offs, unsigned int blk_per_trk, unsigned int blksize) { struct dasd_eckd_private *private; dma64_t *idaws; struct LO_eckd_data *LO_data; struct dasd_ccw_req *cqr; struct ccw1 *ccw; struct req_iterator iter; struct bio_vec bv; char *dst; unsigned int off; int count, cidaw, cplength, datasize; sector_t recid; unsigned char cmd, rcmd; int use_prefix; struct dasd_device *basedev; basedev = block->base; private = basedev->private; if (rq_data_dir(req) == READ) cmd = DASD_ECKD_CCW_READ_MT; else if (rq_data_dir(req) == WRITE) cmd = DASD_ECKD_CCW_WRITE_MT; else return ERR_PTR(-EINVAL); count = 0; cidaw = 0; rq_for_each_segment(bv, req, iter) { if (bv.bv_len & (blksize - 1)) return ERR_PTR(-EINVAL); count += bv.bv_len >> (block->s2b_shift + 9); if (idal_is_needed (page_address(bv.bv_page), bv.bv_len)) cidaw += bv.bv_len >> (block->s2b_shift + 9); } if (count != last_rec - first_rec + 1) return ERR_PTR(-EINVAL); use_prefix = private->features.feature[8] & 0x01; if (use_prefix) { cplength = 2 + count; datasize = sizeof(struct PFX_eckd_data) + sizeof(struct LO_eckd_data) + cidaw * sizeof(unsigned long); } else { cplength = 2 + count; datasize = sizeof(struct DE_eckd_data) + sizeof(struct LO_eckd_data) + cidaw * sizeof(unsigned long); } if (private->uses_cdl && first_rec < 2*blk_per_trk) { if (last_rec >= 2*blk_per_trk) count = 2*blk_per_trk - first_rec; cplength += count; datasize += count*sizeof(struct LO_eckd_data); } cqr = dasd_smalloc_request(DASD_ECKD_MAGIC, cplength, datasize, startdev, blk_mq_rq_to_pdu(req)); if (IS_ERR(cqr)) return cqr; ccw = cqr->cpaddr; if (use_prefix) { if (prefix(ccw++, cqr->data, first_trk, last_trk, cmd, basedev, startdev) == -EAGAIN) { dasd_sfree_request(cqr, startdev); return ERR_PTR(-EAGAIN); } idaws = (dma64_t *)(cqr->data + sizeof(struct PFX_eckd_data)); } else { if (define_extent(ccw++, cqr->data, first_trk, last_trk, cmd, basedev, 0) == -EAGAIN) { dasd_sfree_request(cqr, startdev); return ERR_PTR(-EAGAIN); } idaws = (dma64_t *)(cqr->data + sizeof(struct DE_eckd_data)); } LO_data = (struct LO_eckd_data *) (idaws + cidaw); recid = first_rec; if (private->uses_cdl == 0 || recid > 2*blk_per_trk) { ccw[-1].flags |= CCW_FLAG_CC; locate_record(ccw++, LO_data++, first_trk, first_offs + 1, last_rec - recid + 1, cmd, basedev, blksize); } rq_for_each_segment(bv, req, iter) { dst = bvec_virt(&bv); if (dasd_page_cache) { char *copy = kmem_cache_alloc(dasd_page_cache, GFP_DMA | __GFP_NOWARN); if (copy && rq_data_dir(req) == WRITE) memcpy(copy + bv.bv_offset, dst, bv.bv_len); if (copy) dst = copy + bv.bv_offset; } for (off = 0; off < bv.bv_len; off += blksize) { sector_t trkid = recid; unsigned int recoffs = sector_div(trkid, blk_per_trk); rcmd = cmd; count = blksize; if (private->uses_cdl && recid < 2*blk_per_trk) { if (dasd_eckd_cdl_special(blk_per_trk, recid)){ rcmd |= 0x8; count = dasd_eckd_cdl_reclen(recid); if (count < blksize && rq_data_dir(req) == READ) memset(dst + count, 0xe5, blksize - count); } ccw[-1].flags |= CCW_FLAG_CC; locate_record(ccw++, LO_data++, trkid, recoffs + 1, 1, rcmd, basedev, count); } if (private->uses_cdl && recid == 2*blk_per_trk) { ccw[-1].flags |= CCW_FLAG_CC; locate_record(ccw++, LO_data++, trkid, recoffs + 1, last_rec - recid + 1, cmd, basedev, count); } ccw[-1].flags |= CCW_FLAG_CC; ccw->cmd_code = rcmd; ccw->count = count; if (idal_is_needed(dst, blksize)) { ccw->cda = virt_to_dma32(idaws); ccw->flags = CCW_FLAG_IDA; idaws = idal_create_words(idaws, dst, blksize); } else { ccw->cda = virt_to_dma32(dst); ccw->flags = 0; } ccw++; dst += blksize; recid++; } } if (blk_noretry_request(req) || block->base->features & DASD_FEATURE_FAILFAST) set_bit(DASD_CQR_FLAGS_FAILFAST, &cqr->flags); cqr->startdev = startdev; cqr->memdev = startdev; cqr->block = block; cqr->expires = startdev->default_expires * HZ; cqr->lpm = dasd_path_get_ppm(startdev); cqr->retries = startdev->default_retries; cqr->buildclk = get_tod_clock(); cqr->status = DASD_CQR_FILLED; if (dasd_eckd_is_ese(basedev)) { <S2SV_StartVul> set_bit(DASD_CQR_SUPPRESS_FP, &cqr->flags); <S2SV_EndVul> <S2SV_StartVul> set_bit(DASD_CQR_SUPPRESS_IL, &cqr->flags); <S2SV_EndVul> set_bit(DASD_CQR_SUPPRESS_NRF, &cqr->flags); <S2SV_StartVul> } <S2SV_EndVul> return cqr; }","- set_bit(DASD_CQR_SUPPRESS_FP, &cqr->flags);
- set_bit(DASD_CQR_SUPPRESS_IL, &cqr->flags);
- }","static struct dasd_ccw_req *dasd_eckd_build_cp_cmd_single( struct dasd_device *startdev, struct dasd_block *block, struct request *req, sector_t first_rec, sector_t last_rec, sector_t first_trk, sector_t last_trk, unsigned int first_offs, unsigned int last_offs, unsigned int blk_per_trk, unsigned int blksize) { struct dasd_eckd_private *private; dma64_t *idaws; struct LO_eckd_data *LO_data; struct dasd_ccw_req *cqr; struct ccw1 *ccw; struct req_iterator iter; struct bio_vec bv; char *dst; unsigned int off; int count, cidaw, cplength, datasize; sector_t recid; unsigned char cmd, rcmd; int use_prefix; struct dasd_device *basedev; basedev = block->base; private = basedev->private; if (rq_data_dir(req) == READ) cmd = DASD_ECKD_CCW_READ_MT; else if (rq_data_dir(req) == WRITE) cmd = DASD_ECKD_CCW_WRITE_MT; else return ERR_PTR(-EINVAL); count = 0; cidaw = 0; rq_for_each_segment(bv, req, iter) { if (bv.bv_len & (blksize - 1)) return ERR_PTR(-EINVAL); count += bv.bv_len >> (block->s2b_shift + 9); if (idal_is_needed (page_address(bv.bv_page), bv.bv_len)) cidaw += bv.bv_len >> (block->s2b_shift + 9); } if (count != last_rec - first_rec + 1) return ERR_PTR(-EINVAL); use_prefix = private->features.feature[8] & 0x01; if (use_prefix) { cplength = 2 + count; datasize = sizeof(struct PFX_eckd_data) + sizeof(struct LO_eckd_data) + cidaw * sizeof(unsigned long); } else { cplength = 2 + count; datasize = sizeof(struct DE_eckd_data) + sizeof(struct LO_eckd_data) + cidaw * sizeof(unsigned long); } if (private->uses_cdl && first_rec < 2*blk_per_trk) { if (last_rec >= 2*blk_per_trk) count = 2*blk_per_trk - first_rec; cplength += count; datasize += count*sizeof(struct LO_eckd_data); } cqr = dasd_smalloc_request(DASD_ECKD_MAGIC, cplength, datasize, startdev, blk_mq_rq_to_pdu(req)); if (IS_ERR(cqr)) return cqr; ccw = cqr->cpaddr; if (use_prefix) { if (prefix(ccw++, cqr->data, first_trk, last_trk, cmd, basedev, startdev) == -EAGAIN) { dasd_sfree_request(cqr, startdev); return ERR_PTR(-EAGAIN); } idaws = (dma64_t *)(cqr->data + sizeof(struct PFX_eckd_data)); } else { if (define_extent(ccw++, cqr->data, first_trk, last_trk, cmd, basedev, 0) == -EAGAIN) { dasd_sfree_request(cqr, startdev); return ERR_PTR(-EAGAIN); } idaws = (dma64_t *)(cqr->data + sizeof(struct DE_eckd_data)); } LO_data = (struct LO_eckd_data *) (idaws + cidaw); recid = first_rec; if (private->uses_cdl == 0 || recid > 2*blk_per_trk) { ccw[-1].flags |= CCW_FLAG_CC; locate_record(ccw++, LO_data++, first_trk, first_offs + 1, last_rec - recid + 1, cmd, basedev, blksize); } rq_for_each_segment(bv, req, iter) { dst = bvec_virt(&bv); if (dasd_page_cache) { char *copy = kmem_cache_alloc(dasd_page_cache, GFP_DMA | __GFP_NOWARN); if (copy && rq_data_dir(req) == WRITE) memcpy(copy + bv.bv_offset, dst, bv.bv_len); if (copy) dst = copy + bv.bv_offset; } for (off = 0; off < bv.bv_len; off += blksize) { sector_t trkid = recid; unsigned int recoffs = sector_div(trkid, blk_per_trk); rcmd = cmd; count = blksize; if (private->uses_cdl && recid < 2*blk_per_trk) { if (dasd_eckd_cdl_special(blk_per_trk, recid)){ rcmd |= 0x8; count = dasd_eckd_cdl_reclen(recid); if (count < blksize && rq_data_dir(req) == READ) memset(dst + count, 0xe5, blksize - count); } ccw[-1].flags |= CCW_FLAG_CC; locate_record(ccw++, LO_data++, trkid, recoffs + 1, 1, rcmd, basedev, count); } if (private->uses_cdl && recid == 2*blk_per_trk) { ccw[-1].flags |= CCW_FLAG_CC; locate_record(ccw++, LO_data++, trkid, recoffs + 1, last_rec - recid + 1, cmd, basedev, count); } ccw[-1].flags |= CCW_FLAG_CC; ccw->cmd_code = rcmd; ccw->count = count; if (idal_is_needed(dst, blksize)) { ccw->cda = virt_to_dma32(idaws); ccw->flags = CCW_FLAG_IDA; idaws = idal_create_words(idaws, dst, blksize); } else { ccw->cda = virt_to_dma32(dst); ccw->flags = 0; } ccw++; dst += blksize; recid++; } } if (blk_noretry_request(req) || block->base->features & DASD_FEATURE_FAILFAST) set_bit(DASD_CQR_FLAGS_FAILFAST, &cqr->flags); cqr->startdev = startdev; cqr->memdev = startdev; cqr->block = block; cqr->expires = startdev->default_expires * HZ; cqr->lpm = dasd_path_get_ppm(startdev); cqr->retries = startdev->default_retries; cqr->buildclk = get_tod_clock(); cqr->status = DASD_CQR_FILLED; if (dasd_eckd_is_ese(basedev)) { set_bit(DASD_CQR_SUPPRESS_NRF, &cqr->flags); } return cqr; }"
1231----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53689/bad/blk-mq.c----blk_mq_elv_switch_back,"static void blk_mq_elv_switch_back(struct list_head *head, struct request_queue *q) { struct blk_mq_qe_pair *qe; struct elevator_type *t; qe = blk_lookup_qe_pair(head, q); if (!qe) return; t = qe->type; list_del(&qe->node); kfree(qe); <S2SV_StartVul> mutex_lock(&q->sysfs_lock); <S2SV_EndVul> elevator_switch(q, t); elevator_put(t); <S2SV_StartVul> mutex_unlock(&q->sysfs_lock); <S2SV_EndVul> }","- mutex_lock(&q->sysfs_lock);
- mutex_unlock(&q->sysfs_lock);
+ }","static void blk_mq_elv_switch_back(struct list_head *head, struct request_queue *q) { struct blk_mq_qe_pair *qe; struct elevator_type *t; qe = blk_lookup_qe_pair(head, q); if (!qe) return; t = qe->type; list_del(&qe->node); kfree(qe); elevator_switch(q, t); elevator_put(t); }"
1343----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56661/bad/udp_media.c----cleanup_bearer,"static void cleanup_bearer(struct work_struct *work) { struct udp_bearer *ub = container_of(work, struct udp_bearer, work); struct udp_replicast *rcast, *tmp; list_for_each_entry_safe(rcast, tmp, &ub->rcast.list, list) { dst_cache_destroy(&rcast->dst_cache); list_del_rcu(&rcast->list); kfree_rcu(rcast, rcu); } dst_cache_destroy(&ub->rcast.dst_cache); udp_tunnel_sock_release(ub->ubsock); synchronize_net(); <S2SV_StartVul> atomic_dec(&tipc_net(sock_net(ub->ubsock->sk))->wq_count); <S2SV_EndVul> kfree(ub); }","- atomic_dec(&tipc_net(sock_net(ub->ubsock->sk))->wq_count);
+ struct tipc_net *tn;
+ tn = tipc_net(sock_net(ub->ubsock->sk));
+ atomic_dec(&tn->wq_count);","static void cleanup_bearer(struct work_struct *work) { struct udp_bearer *ub = container_of(work, struct udp_bearer, work); struct udp_replicast *rcast, *tmp; struct tipc_net *tn; list_for_each_entry_safe(rcast, tmp, &ub->rcast.list, list) { dst_cache_destroy(&rcast->dst_cache); list_del_rcu(&rcast->list); kfree_rcu(rcast, rcu); } tn = tipc_net(sock_net(ub->ubsock->sk)); dst_cache_destroy(&ub->rcast.dst_cache); udp_tunnel_sock_release(ub->ubsock); synchronize_net(); atomic_dec(&tn->wq_count); kfree(ub); }"
567----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49891/bad/lpfc_hbadisc.c----lpfc_dev_loss_tmo_callbk,"lpfc_dev_loss_tmo_callbk(struct fc_rport *rport) { struct lpfc_nodelist *ndlp; struct lpfc_vport *vport; struct lpfc_hba *phba; struct lpfc_work_evt *evtp; unsigned long iflags; ndlp = ((struct lpfc_rport_data *)rport->dd_data)->pnode; if (!ndlp) return; vport = ndlp->vport; phba = vport->phba; lpfc_debugfs_disc_trc(vport, LPFC_DISC_TRC_RPORT, ""rport devlosscb: sid:x%x did:x%x flg:x%x"", ndlp->nlp_sid, ndlp->nlp_DID, ndlp->nlp_flag); lpfc_printf_vlog(ndlp->vport, KERN_INFO, LOG_NODE, ""3181 dev_loss_callbk x%06x, rport x%px flg x%x "" ""load_flag x%lx refcnt %u state %d xpt x%x\n"", ndlp->nlp_DID, ndlp->rport, ndlp->nlp_flag, vport->load_flag, kref_read(&ndlp->kref), ndlp->nlp_state, ndlp->fc4_xpt_flags); <S2SV_StartVul> if (test_bit(FC_UNLOADING, &vport->load_flag)) { <S2SV_EndVul> spin_lock_irqsave(&ndlp->lock, iflags); ndlp->rport = NULL; if (ndlp->fc4_xpt_flags & (NLP_XPT_REGD | SCSI_XPT_REGD)) { ndlp->fc4_xpt_flags &= ~SCSI_XPT_REGD; if (!(ndlp->fc4_xpt_flags & NVME_XPT_REGD)) ndlp->fc4_xpt_flags &= ~NLP_XPT_REGD; spin_unlock_irqrestore(&ndlp->lock, iflags); lpfc_nlp_put(ndlp); spin_lock_irqsave(&ndlp->lock, iflags); } if (!(ndlp->fc4_xpt_flags & NVME_XPT_REGD) && !(ndlp->nlp_flag & NLP_DROPPED)) { ndlp->nlp_flag |= NLP_DROPPED; spin_unlock_irqrestore(&ndlp->lock, iflags); lpfc_nlp_put(ndlp); return; } spin_unlock_irqrestore(&ndlp->lock, iflags); return; } if (ndlp->nlp_state == NLP_STE_MAPPED_NODE) return; if (ndlp->nlp_state == NLP_STE_UNMAPPED_NODE && ndlp->nlp_DID == Fabric_DID) return; if (rport->port_name != wwn_to_u64(ndlp->nlp_portname.u.wwn)) lpfc_printf_vlog(vport, KERN_ERR, LOG_TRACE_EVENT, ""6789 rport name %llx != node port name %llx"", rport->port_name, wwn_to_u64(ndlp->nlp_portname.u.wwn)); evtp = &ndlp->dev_loss_evt; if (!list_empty(&evtp->evt_listp)) { lpfc_printf_vlog(vport, KERN_ERR, LOG_TRACE_EVENT, ""6790 rport name %llx dev_loss_evt pending\n"", rport->port_name); return; } spin_lock_irqsave(&ndlp->lock, iflags); ndlp->nlp_flag |= NLP_IN_DEV_LOSS; if (ndlp->nlp_state != NLP_STE_PLOGI_ISSUE) ndlp->nlp_flag &= ~NLP_NPR_2B_DISC; ndlp->fc4_xpt_flags &= ~SCSI_XPT_REGD; ((struct lpfc_rport_data *)rport->dd_data)->pnode = NULL; ndlp->rport = NULL; spin_unlock_irqrestore(&ndlp->lock, iflags); if (phba->worker_thread) { evtp->evt_arg1 = lpfc_nlp_get(ndlp); spin_lock_irqsave(&phba->hbalock, iflags); if (evtp->evt_arg1) { evtp->evt = LPFC_EVT_DEV_LOSS; list_add_tail(&evtp->evt_listp, &phba->work_list); spin_unlock_irqrestore(&phba->hbalock, iflags); lpfc_worker_wake_up(phba); return; } spin_unlock_irqrestore(&phba->hbalock, iflags); } else { lpfc_printf_vlog(ndlp->vport, KERN_INFO, LOG_NODE, ""3188 worker thread is stopped %s x%06x, "" "" rport x%px flg x%x load_flag x%lx refcnt "" ""%d\n"", __func__, ndlp->nlp_DID, ndlp->rport, ndlp->nlp_flag, vport->load_flag, kref_read(&ndlp->kref)); if (!(ndlp->fc4_xpt_flags & NVME_XPT_REGD)) { spin_lock_irqsave(&ndlp->lock, iflags); ndlp->nlp_flag &= ~NLP_IN_DEV_LOSS; spin_unlock_irqrestore(&ndlp->lock, iflags); lpfc_disc_state_machine(vport, ndlp, NULL, NLP_EVT_DEVICE_RM); } } }","- if (test_bit(FC_UNLOADING, &vport->load_flag)) {
+ if (test_bit(FC_UNLOADING, &vport->load_flag) ||
+ !test_bit(HBA_SETUP, &phba->hba_flag)) {","lpfc_dev_loss_tmo_callbk(struct fc_rport *rport) { struct lpfc_nodelist *ndlp; struct lpfc_vport *vport; struct lpfc_hba *phba; struct lpfc_work_evt *evtp; unsigned long iflags; ndlp = ((struct lpfc_rport_data *)rport->dd_data)->pnode; if (!ndlp) return; vport = ndlp->vport; phba = vport->phba; lpfc_debugfs_disc_trc(vport, LPFC_DISC_TRC_RPORT, ""rport devlosscb: sid:x%x did:x%x flg:x%x"", ndlp->nlp_sid, ndlp->nlp_DID, ndlp->nlp_flag); lpfc_printf_vlog(ndlp->vport, KERN_INFO, LOG_NODE, ""3181 dev_loss_callbk x%06x, rport x%px flg x%x "" ""load_flag x%lx refcnt %u state %d xpt x%x\n"", ndlp->nlp_DID, ndlp->rport, ndlp->nlp_flag, vport->load_flag, kref_read(&ndlp->kref), ndlp->nlp_state, ndlp->fc4_xpt_flags); if (test_bit(FC_UNLOADING, &vport->load_flag) || !test_bit(HBA_SETUP, &phba->hba_flag)) { spin_lock_irqsave(&ndlp->lock, iflags); ndlp->rport = NULL; if (ndlp->fc4_xpt_flags & (NLP_XPT_REGD | SCSI_XPT_REGD)) { ndlp->fc4_xpt_flags &= ~SCSI_XPT_REGD; if (!(ndlp->fc4_xpt_flags & NVME_XPT_REGD)) ndlp->fc4_xpt_flags &= ~NLP_XPT_REGD; spin_unlock_irqrestore(&ndlp->lock, iflags); lpfc_nlp_put(ndlp); spin_lock_irqsave(&ndlp->lock, iflags); } if (!(ndlp->fc4_xpt_flags & NVME_XPT_REGD) && !(ndlp->nlp_flag & NLP_DROPPED)) { ndlp->nlp_flag |= NLP_DROPPED; spin_unlock_irqrestore(&ndlp->lock, iflags); lpfc_nlp_put(ndlp); return; } spin_unlock_irqrestore(&ndlp->lock, iflags); return; } if (ndlp->nlp_state == NLP_STE_MAPPED_NODE) return; if (ndlp->nlp_state == NLP_STE_UNMAPPED_NODE && ndlp->nlp_DID == Fabric_DID) return; if (rport->port_name != wwn_to_u64(ndlp->nlp_portname.u.wwn)) lpfc_printf_vlog(vport, KERN_ERR, LOG_TRACE_EVENT, ""6789 rport name %llx != node port name %llx"", rport->port_name, wwn_to_u64(ndlp->nlp_portname.u.wwn)); evtp = &ndlp->dev_loss_evt; if (!list_empty(&evtp->evt_listp)) { lpfc_printf_vlog(vport, KERN_ERR, LOG_TRACE_EVENT, ""6790 rport name %llx dev_loss_evt pending\n"", rport->port_name); return; } spin_lock_irqsave(&ndlp->lock, iflags); ndlp->nlp_flag |= NLP_IN_DEV_LOSS; if (ndlp->nlp_state != NLP_STE_PLOGI_ISSUE) ndlp->nlp_flag &= ~NLP_NPR_2B_DISC; ndlp->fc4_xpt_flags &= ~SCSI_XPT_REGD; ((struct lpfc_rport_data *)rport->dd_data)->pnode = NULL; ndlp->rport = NULL; spin_unlock_irqrestore(&ndlp->lock, iflags); if (phba->worker_thread) { evtp->evt_arg1 = lpfc_nlp_get(ndlp); spin_lock_irqsave(&phba->hbalock, iflags); if (evtp->evt_arg1) { evtp->evt = LPFC_EVT_DEV_LOSS; list_add_tail(&evtp->evt_listp, &phba->work_list); spin_unlock_irqrestore(&phba->hbalock, iflags); lpfc_worker_wake_up(phba); return; } spin_unlock_irqrestore(&phba->hbalock, iflags); } else { lpfc_printf_vlog(ndlp->vport, KERN_INFO, LOG_NODE, ""3188 worker thread is stopped %s x%06x, "" "" rport x%px flg x%x load_flag x%lx refcnt "" ""%d\n"", __func__, ndlp->nlp_DID, ndlp->rport, ndlp->nlp_flag, vport->load_flag, kref_read(&ndlp->kref)); if (!(ndlp->fc4_xpt_flags & NVME_XPT_REGD)) { spin_lock_irqsave(&ndlp->lock, iflags); ndlp->nlp_flag &= ~NLP_IN_DEV_LOSS; spin_unlock_irqrestore(&ndlp->lock, iflags); lpfc_disc_state_machine(vport, ndlp, NULL, NLP_EVT_DEVICE_RM); } } }"
410----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47141/bad/pinmux.c----pinmux_enable_setting,"int pinmux_enable_setting(const struct pinctrl_setting *setting) { struct pinctrl_dev *pctldev = setting->pctldev; const struct pinctrl_ops *pctlops = pctldev->desc->pctlops; const struct pinmux_ops *ops = pctldev->desc->pmxops; int ret = 0; const unsigned int *pins = NULL; unsigned int num_pins = 0; int i; struct pin_desc *desc; if (pctlops->get_group_pins) ret = pctlops->get_group_pins(pctldev, setting->data.mux.group, &pins, &num_pins); if (ret) { const char *gname; gname = pctlops->get_group_name(pctldev, setting->data.mux.group); dev_warn(pctldev->dev, ""could not get pins for group %s\n"", gname); num_pins = 0; } for (i = 0; i < num_pins; i++) { ret = pin_request(pctldev, pins[i], setting->dev_name, NULL); if (ret) { const char *gname; const char *pname; desc = pin_desc_get(pctldev, pins[i]); pname = desc ? desc->name : ""non-existing""; gname = pctlops->get_group_name(pctldev, setting->data.mux.group); dev_err_probe(pctldev->dev, ret, ""could not request pin %d (%s) from group %s on device %s\n"", pins[i], pname, gname, pinctrl_dev_get_name(pctldev)); goto err_pin_request; } } for (i = 0; i < num_pins; i++) { desc = pin_desc_get(pctldev, pins[i]); if (desc == NULL) { dev_warn(pctldev->dev, ""could not get pin desc for pin %d\n"", pins[i]); continue; <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> desc->mux_setting = &(setting->data.mux); <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> ret = ops->set_mux(pctldev, setting->data.mux.func, setting->data.mux.group); if (ret) goto err_set_mux; return 0; err_set_mux: for (i = 0; i < num_pins; i++) { desc = pin_desc_get(pctldev, pins[i]); <S2SV_StartVul> if (desc) <S2SV_EndVul> <S2SV_StartVul> desc->mux_setting = NULL; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> err_pin_request: while (--i >= 0) pin_free(pctldev, pins[i], NULL); return ret; }","- }
- desc->mux_setting = &(setting->data.mux);
- }
- if (desc)
- desc->mux_setting = NULL;
- }
+ }
+ scoped_guard(mutex, &desc->mux_lock)
+ desc->mux_setting = &(setting->data.mux);
+ }
+ if (desc) {
+ scoped_guard(mutex, &desc->mux_lock)
+ desc->mux_setting = NULL;
+ }
+ }","int pinmux_enable_setting(const struct pinctrl_setting *setting) { struct pinctrl_dev *pctldev = setting->pctldev; const struct pinctrl_ops *pctlops = pctldev->desc->pctlops; const struct pinmux_ops *ops = pctldev->desc->pmxops; int ret = 0; const unsigned int *pins = NULL; unsigned int num_pins = 0; int i; struct pin_desc *desc; if (pctlops->get_group_pins) ret = pctlops->get_group_pins(pctldev, setting->data.mux.group, &pins, &num_pins); if (ret) { const char *gname; gname = pctlops->get_group_name(pctldev, setting->data.mux.group); dev_warn(pctldev->dev, ""could not get pins for group %s\n"", gname); num_pins = 0; } for (i = 0; i < num_pins; i++) { ret = pin_request(pctldev, pins[i], setting->dev_name, NULL); if (ret) { const char *gname; const char *pname; desc = pin_desc_get(pctldev, pins[i]); pname = desc ? desc->name : ""non-existing""; gname = pctlops->get_group_name(pctldev, setting->data.mux.group); dev_err_probe(pctldev->dev, ret, ""could not request pin %d (%s) from group %s on device %s\n"", pins[i], pname, gname, pinctrl_dev_get_name(pctldev)); goto err_pin_request; } } for (i = 0; i < num_pins; i++) { desc = pin_desc_get(pctldev, pins[i]); if (desc == NULL) { dev_warn(pctldev->dev, ""could not get pin desc for pin %d\n"", pins[i]); continue; } scoped_guard(mutex, &desc->mux_lock) desc->mux_setting = &(setting->data.mux); } ret = ops->set_mux(pctldev, setting->data.mux.func, setting->data.mux.group); if (ret) goto err_set_mux; return 0; err_set_mux: for (i = 0; i < num_pins; i++) { desc = pin_desc_get(pctldev, pins[i]); if (desc) { scoped_guard(mutex, &desc->mux_lock) desc->mux_setting = NULL; } } err_pin_request: while (--i >= 0) pin_free(pctldev, pins[i], NULL); return ret; }"
1135----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53109/bad/nommu.c----delete_vma_from_mm,"static int delete_vma_from_mm(struct vm_area_struct *vma) { VMA_ITERATOR(vmi, vma->vm_mm, vma->vm_start); vma_iter_config(&vmi, vma->vm_start, vma->vm_end); <S2SV_StartVul> if (vma_iter_prealloc(&vmi, vma)) { <S2SV_EndVul> pr_warn(""Allocation of vma tree for process %d failed\n"", current->pid); return -ENOMEM; } cleanup_vma_from_mm(vma); vma_iter_clear(&vmi); return 0; }","- if (vma_iter_prealloc(&vmi, vma)) {
+ if (vma_iter_prealloc(&vmi, NULL)) {","static int delete_vma_from_mm(struct vm_area_struct *vma) { VMA_ITERATOR(vmi, vma->vm_mm, vma->vm_start); vma_iter_config(&vmi, vma->vm_start, vma->vm_end); if (vma_iter_prealloc(&vmi, NULL)) { pr_warn(""Allocation of vma tree for process %d failed\n"", current->pid); return -ENOMEM; } cleanup_vma_from_mm(vma); vma_iter_clear(&vmi); return 0; }"
165----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-45020/bad/verifier.c----stacksafe,"static bool stacksafe(struct bpf_verifier_env *env, struct bpf_func_state *old, struct bpf_func_state *cur, struct bpf_idmap *idmap, enum exact_level exact) { int i, spi; for (i = 0; i < old->allocated_stack; i++) { struct bpf_reg_state *old_reg, *cur_reg; spi = i / BPF_REG_SIZE; if (exact != NOT_EXACT && <S2SV_StartVul> old->stack[spi].slot_type[i % BPF_REG_SIZE] != <S2SV_EndVul> <S2SV_StartVul> cur->stack[spi].slot_type[i % BPF_REG_SIZE]) <S2SV_EndVul> return false; if (!(old->stack[spi].spilled_ptr.live & REG_LIVE_READ) && exact == NOT_EXACT) { i += BPF_REG_SIZE - 1; continue; } if (old->stack[spi].slot_type[i % BPF_REG_SIZE] == STACK_INVALID) continue; if (env->allow_uninit_stack && old->stack[spi].slot_type[i % BPF_REG_SIZE] == STACK_MISC) continue; if (i >= cur->allocated_stack) return false; old_reg = scalar_reg_for_stack(env, &old->stack[spi]); cur_reg = scalar_reg_for_stack(env, &cur->stack[spi]); if (old_reg && cur_reg) { if (!regsafe(env, old_reg, cur_reg, idmap, exact)) return false; i += BPF_REG_SIZE - 1; continue; } if (old->stack[spi].slot_type[i % BPF_REG_SIZE] == STACK_MISC && cur->stack[spi].slot_type[i % BPF_REG_SIZE] == STACK_ZERO) continue; if (old->stack[spi].slot_type[i % BPF_REG_SIZE] != cur->stack[spi].slot_type[i % BPF_REG_SIZE]) return false; if (i % BPF_REG_SIZE != BPF_REG_SIZE - 1) continue; switch (old->stack[spi].slot_type[BPF_REG_SIZE - 1]) { case STACK_SPILL: if (!regsafe(env, &old->stack[spi].spilled_ptr, &cur->stack[spi].spilled_ptr, idmap, exact)) return false; break; case STACK_DYNPTR: old_reg = &old->stack[spi].spilled_ptr; cur_reg = &cur->stack[spi].spilled_ptr; if (old_reg->dynptr.type != cur_reg->dynptr.type || old_reg->dynptr.first_slot != cur_reg->dynptr.first_slot || !check_ids(old_reg->ref_obj_id, cur_reg->ref_obj_id, idmap)) return false; break; case STACK_ITER: old_reg = &old->stack[spi].spilled_ptr; cur_reg = &cur->stack[spi].spilled_ptr; if (old_reg->iter.btf != cur_reg->iter.btf || old_reg->iter.btf_id != cur_reg->iter.btf_id || old_reg->iter.state != cur_reg->iter.state || !check_ids(old_reg->ref_obj_id, cur_reg->ref_obj_id, idmap)) return false; break; case STACK_MISC: case STACK_ZERO: case STACK_INVALID: continue; default: return false; } } return true; }","- old->stack[spi].slot_type[i % BPF_REG_SIZE] !=
- cur->stack[spi].slot_type[i % BPF_REG_SIZE])
+ (i >= cur->allocated_stack ||
+ old->stack[spi].slot_type[i % BPF_REG_SIZE] !=
+ cur->stack[spi].slot_type[i % BPF_REG_SIZE]))","static bool stacksafe(struct bpf_verifier_env *env, struct bpf_func_state *old, struct bpf_func_state *cur, struct bpf_idmap *idmap, enum exact_level exact) { int i, spi; for (i = 0; i < old->allocated_stack; i++) { struct bpf_reg_state *old_reg, *cur_reg; spi = i / BPF_REG_SIZE; if (exact != NOT_EXACT && (i >= cur->allocated_stack || old->stack[spi].slot_type[i % BPF_REG_SIZE] != cur->stack[spi].slot_type[i % BPF_REG_SIZE])) return false; if (!(old->stack[spi].spilled_ptr.live & REG_LIVE_READ) && exact == NOT_EXACT) { i += BPF_REG_SIZE - 1; continue; } if (old->stack[spi].slot_type[i % BPF_REG_SIZE] == STACK_INVALID) continue; if (env->allow_uninit_stack && old->stack[spi].slot_type[i % BPF_REG_SIZE] == STACK_MISC) continue; if (i >= cur->allocated_stack) return false; old_reg = scalar_reg_for_stack(env, &old->stack[spi]); cur_reg = scalar_reg_for_stack(env, &cur->stack[spi]); if (old_reg && cur_reg) { if (!regsafe(env, old_reg, cur_reg, idmap, exact)) return false; i += BPF_REG_SIZE - 1; continue; } if (old->stack[spi].slot_type[i % BPF_REG_SIZE] == STACK_MISC && cur->stack[spi].slot_type[i % BPF_REG_SIZE] == STACK_ZERO) continue; if (old->stack[spi].slot_type[i % BPF_REG_SIZE] != cur->stack[spi].slot_type[i % BPF_REG_SIZE]) return false; if (i % BPF_REG_SIZE != BPF_REG_SIZE - 1) continue; switch (old->stack[spi].slot_type[BPF_REG_SIZE - 1]) { case STACK_SPILL: if (!regsafe(env, &old->stack[spi].spilled_ptr, &cur->stack[spi].spilled_ptr, idmap, exact)) return false; break; case STACK_DYNPTR: old_reg = &old->stack[spi].spilled_ptr; cur_reg = &cur->stack[spi].spilled_ptr; if (old_reg->dynptr.type != cur_reg->dynptr.type || old_reg->dynptr.first_slot != cur_reg->dynptr.first_slot || !check_ids(old_reg->ref_obj_id, cur_reg->ref_obj_id, idmap)) return false; break; case STACK_ITER: old_reg = &old->stack[spi].spilled_ptr; cur_reg = &cur->stack[spi].spilled_ptr; if (old_reg->iter.btf != cur_reg->iter.btf || old_reg->iter.btf_id != cur_reg->iter.btf_id || old_reg->iter.state != cur_reg->iter.state || !check_ids(old_reg->ref_obj_id, cur_reg->ref_obj_id, idmap)) return false; break; case STACK_MISC: case STACK_ZERO: case STACK_INVALID: continue; default: return false; } } return true; }"
628----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49934/bad/inode.c----dump_mapping,"void dump_mapping(const struct address_space *mapping) { struct inode *host; const struct address_space_operations *a_ops; struct hlist_node *dentry_first; struct dentry *dentry_ptr; struct dentry dentry; unsigned long ino; if (get_kernel_nofault(host, &mapping->host) || get_kernel_nofault(a_ops, &mapping->a_ops)) { pr_warn(""invalid mapping:%px\n"", mapping); return; } if (!host) { pr_warn(""aops:%ps\n"", a_ops); return; } if (get_kernel_nofault(dentry_first, &host->i_dentry.first) || get_kernel_nofault(ino, &host->i_ino)) { pr_warn(""aops:%ps invalid inode:%px\n"", a_ops, host); return; } if (!dentry_first) { pr_warn(""aops:%ps ino:%lx\n"", a_ops, ino); return; } dentry_ptr = container_of(dentry_first, struct dentry, d_u.d_alias); if (get_kernel_nofault(dentry, dentry_ptr)) { pr_warn(""aops:%ps ino:%lx invalid dentry:%px\n"", a_ops, ino, dentry_ptr); return; } <S2SV_StartVul> pr_warn(""aops:%ps ino:%lx dentry name:\""%pd\""\n"", a_ops, ino, &dentry); <S2SV_EndVul> }","- pr_warn(""aops:%ps ino:%lx dentry name:\""%pd\""\n"", a_ops, ino, &dentry);
+ char fname[64] = {};
+ if (strncpy_from_kernel_nofault(fname, dentry.d_name.name, 63) < 0)
+ strscpy(fname, ""<invalid>"", 63);
+ pr_warn(""aops:%ps ino:%lx dentry name(?):\""%s\""\n"",
+ a_ops, ino, fname);","void dump_mapping(const struct address_space *mapping) { struct inode *host; const struct address_space_operations *a_ops; struct hlist_node *dentry_first; struct dentry *dentry_ptr; struct dentry dentry; char fname[64] = {}; unsigned long ino; if (get_kernel_nofault(host, &mapping->host) || get_kernel_nofault(a_ops, &mapping->a_ops)) { pr_warn(""invalid mapping:%px\n"", mapping); return; } if (!host) { pr_warn(""aops:%ps\n"", a_ops); return; } if (get_kernel_nofault(dentry_first, &host->i_dentry.first) || get_kernel_nofault(ino, &host->i_ino)) { pr_warn(""aops:%ps invalid inode:%px\n"", a_ops, host); return; } if (!dentry_first) { pr_warn(""aops:%ps ino:%lx\n"", a_ops, ino); return; } dentry_ptr = container_of(dentry_first, struct dentry, d_u.d_alias); if (get_kernel_nofault(dentry, dentry_ptr)) { pr_warn(""aops:%ps ino:%lx invalid dentry:%px\n"", a_ops, ino, dentry_ptr); return; } if (strncpy_from_kernel_nofault(fname, dentry.d_name.name, 63) < 0) strscpy(fname, ""<invalid>"", 63); pr_warn(""aops:%ps ino:%lx dentry name(?):\""%s\""\n"", a_ops, ino, fname); }"
1189----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53179/bad/smb2transport.c----smb3_calc_signature,"smb3_calc_signature(struct smb_rqst *rqst, struct TCP_Server_Info *server, bool allocate_crypto) { int rc; unsigned char smb3_signature[SMB2_CMACAES_SIZE]; unsigned char *sigptr = smb3_signature; struct kvec *iov = rqst->rq_iov; struct smb2_hdr *shdr = (struct smb2_hdr *)iov[0].iov_base; struct shash_desc *shash = NULL; struct smb_rqst drqst; u8 key[SMB3_SIGN_KEY_SIZE]; <S2SV_StartVul> rc = smb2_get_sign_key(le64_to_cpu(shdr->SessionId), server, key); <S2SV_EndVul> if (unlikely(rc)) { cifs_server_dbg(FYI, ""%s: Could not get signing key\n"", __func__); return rc; } if (allocate_crypto) { rc = cifs_alloc_hash(""cmac(aes)"", &shash); if (rc) return rc; } else { shash = server->secmech.aes_cmac; } memset(smb3_signature, 0x0, SMB2_CMACAES_SIZE); memset(shdr->Signature, 0x0, SMB2_SIGNATURE_SIZE); rc = crypto_shash_setkey(shash->tfm, key, SMB2_CMACAES_SIZE); if (rc) { cifs_server_dbg(VFS, ""%s: Could not set key for cmac aes\n"", __func__); goto out; } rc = crypto_shash_init(shash); if (rc) { cifs_server_dbg(VFS, ""%s: Could not init cmac aes\n"", __func__); goto out; } drqst = *rqst; if (drqst.rq_nvec >= 2 && iov[0].iov_len == 4) { rc = crypto_shash_update(shash, iov[0].iov_base, iov[0].iov_len); if (rc) { cifs_server_dbg(VFS, ""%s: Could not update with payload\n"", __func__); goto out; } drqst.rq_iov++; drqst.rq_nvec--; } rc = __cifs_calc_signature(&drqst, server, sigptr, shash); if (!rc) memcpy(shdr->Signature, sigptr, SMB2_SIGNATURE_SIZE); out: if (allocate_crypto) cifs_free_hash(&shash); return rc; }","- rc = smb2_get_sign_key(le64_to_cpu(shdr->SessionId), server, key);
+ rc = smb3_get_sign_key(le64_to_cpu(shdr->SessionId), server, key);
+ if (unlikely(rc)) {
+ return rc;","smb3_calc_signature(struct smb_rqst *rqst, struct TCP_Server_Info *server, bool allocate_crypto) { int rc; unsigned char smb3_signature[SMB2_CMACAES_SIZE]; unsigned char *sigptr = smb3_signature; struct kvec *iov = rqst->rq_iov; struct smb2_hdr *shdr = (struct smb2_hdr *)iov[0].iov_base; struct shash_desc *shash = NULL; struct smb_rqst drqst; u8 key[SMB3_SIGN_KEY_SIZE]; rc = smb3_get_sign_key(le64_to_cpu(shdr->SessionId), server, key); if (unlikely(rc)) { cifs_server_dbg(FYI, ""%s: Could not get signing key\n"", __func__); return rc; } if (allocate_crypto) { rc = cifs_alloc_hash(""cmac(aes)"", &shash); if (rc) return rc; } else { shash = server->secmech.aes_cmac; } memset(smb3_signature, 0x0, SMB2_CMACAES_SIZE); memset(shdr->Signature, 0x0, SMB2_SIGNATURE_SIZE); rc = crypto_shash_setkey(shash->tfm, key, SMB2_CMACAES_SIZE); if (rc) { cifs_server_dbg(VFS, ""%s: Could not set key for cmac aes\n"", __func__); goto out; } rc = crypto_shash_init(shash); if (rc) { cifs_server_dbg(VFS, ""%s: Could not init cmac aes\n"", __func__); goto out; } drqst = *rqst; if (drqst.rq_nvec >= 2 && iov[0].iov_len == 4) { rc = crypto_shash_update(shash, iov[0].iov_base, iov[0].iov_len); if (rc) { cifs_server_dbg(VFS, ""%s: Could not update with payload\n"", __func__); goto out; } drqst.rq_iov++; drqst.rq_nvec--; } rc = __cifs_calc_signature(&drqst, server, sigptr, shash); if (!rc) memcpy(shdr->Signature, sigptr, SMB2_SIGNATURE_SIZE); out: if (allocate_crypto) cifs_free_hash(&shash); return rc; }"
1460----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-57799/bad/phy-rockchip-samsung-hdptx.c----rk_hdptx_phy_probe,"static int rk_hdptx_phy_probe(struct platform_device *pdev) { struct phy_provider *phy_provider; struct device *dev = &pdev->dev; struct rk_hdptx_phy *hdptx; void __iomem *regs; int ret; hdptx = devm_kzalloc(dev, sizeof(*hdptx), GFP_KERNEL); if (!hdptx) return -ENOMEM; hdptx->dev = dev; regs = devm_platform_ioremap_resource(pdev, 0); if (IS_ERR(regs)) return dev_err_probe(dev, PTR_ERR(regs), ""Failed to ioremap resource\n""); ret = devm_clk_bulk_get_all(dev, &hdptx->clks); if (ret < 0) return dev_err_probe(dev, ret, ""Failed to get clocks\n""); if (ret == 0) return dev_err_probe(dev, -EINVAL, ""Missing clocks\n""); hdptx->nr_clks = ret; hdptx->regmap = devm_regmap_init_mmio(dev, regs, &rk_hdptx_phy_regmap_config); if (IS_ERR(hdptx->regmap)) return dev_err_probe(dev, PTR_ERR(hdptx->regmap), ""Failed to init regmap\n""); hdptx->rsts[RST_PHY].id = ""phy""; hdptx->rsts[RST_APB].id = ""apb""; hdptx->rsts[RST_INIT].id = ""init""; hdptx->rsts[RST_CMN].id = ""cmn""; hdptx->rsts[RST_LANE].id = ""lane""; hdptx->rsts[RST_ROPLL].id = ""ropll""; hdptx->rsts[RST_LCPLL].id = ""lcpll""; ret = devm_reset_control_bulk_get_exclusive(dev, RST_MAX, hdptx->rsts); if (ret) return dev_err_probe(dev, ret, ""Failed to get resets\n""); hdptx->grf = syscon_regmap_lookup_by_phandle(dev->of_node, ""rockchip,grf""); if (IS_ERR(hdptx->grf)) return dev_err_probe(dev, PTR_ERR(hdptx->grf), ""Could not get GRF syscon\n""); ret = devm_pm_runtime_enable(dev); if (ret) return dev_err_probe(dev, ret, ""Failed to enable runtime PM\n""); hdptx->phy = devm_phy_create(dev, NULL, &rk_hdptx_phy_ops); if (IS_ERR(hdptx->phy)) return dev_err_probe(dev, PTR_ERR(hdptx->phy), ""Failed to create HDMI PHY\n""); <S2SV_StartVul> platform_set_drvdata(pdev, hdptx); <S2SV_EndVul> phy_set_drvdata(hdptx->phy, hdptx); phy_set_bus_width(hdptx->phy, 8); phy_provider = devm_of_phy_provider_register(dev, of_phy_simple_xlate); if (IS_ERR(phy_provider)) return dev_err_probe(dev, PTR_ERR(phy_provider), ""Failed to register PHY provider\n""); reset_control_deassert(hdptx->rsts[RST_APB].rstc); reset_control_deassert(hdptx->rsts[RST_CMN].rstc); reset_control_deassert(hdptx->rsts[RST_INIT].rstc); return rk_hdptx_phy_clk_register(hdptx); }","- platform_set_drvdata(pdev, hdptx);
+ platform_set_drvdata(pdev, hdptx);","static int rk_hdptx_phy_probe(struct platform_device *pdev) { struct phy_provider *phy_provider; struct device *dev = &pdev->dev; struct rk_hdptx_phy *hdptx; void __iomem *regs; int ret; hdptx = devm_kzalloc(dev, sizeof(*hdptx), GFP_KERNEL); if (!hdptx) return -ENOMEM; hdptx->dev = dev; regs = devm_platform_ioremap_resource(pdev, 0); if (IS_ERR(regs)) return dev_err_probe(dev, PTR_ERR(regs), ""Failed to ioremap resource\n""); ret = devm_clk_bulk_get_all(dev, &hdptx->clks); if (ret < 0) return dev_err_probe(dev, ret, ""Failed to get clocks\n""); if (ret == 0) return dev_err_probe(dev, -EINVAL, ""Missing clocks\n""); hdptx->nr_clks = ret; hdptx->regmap = devm_regmap_init_mmio(dev, regs, &rk_hdptx_phy_regmap_config); if (IS_ERR(hdptx->regmap)) return dev_err_probe(dev, PTR_ERR(hdptx->regmap), ""Failed to init regmap\n""); hdptx->rsts[RST_PHY].id = ""phy""; hdptx->rsts[RST_APB].id = ""apb""; hdptx->rsts[RST_INIT].id = ""init""; hdptx->rsts[RST_CMN].id = ""cmn""; hdptx->rsts[RST_LANE].id = ""lane""; hdptx->rsts[RST_ROPLL].id = ""ropll""; hdptx->rsts[RST_LCPLL].id = ""lcpll""; ret = devm_reset_control_bulk_get_exclusive(dev, RST_MAX, hdptx->rsts); if (ret) return dev_err_probe(dev, ret, ""Failed to get resets\n""); hdptx->grf = syscon_regmap_lookup_by_phandle(dev->of_node, ""rockchip,grf""); if (IS_ERR(hdptx->grf)) return dev_err_probe(dev, PTR_ERR(hdptx->grf), ""Could not get GRF syscon\n""); platform_set_drvdata(pdev, hdptx); ret = devm_pm_runtime_enable(dev); if (ret) return dev_err_probe(dev, ret, ""Failed to enable runtime PM\n""); hdptx->phy = devm_phy_create(dev, NULL, &rk_hdptx_phy_ops); if (IS_ERR(hdptx->phy)) return dev_err_probe(dev, PTR_ERR(hdptx->phy), ""Failed to create HDMI PHY\n""); phy_set_drvdata(hdptx->phy, hdptx); phy_set_bus_width(hdptx->phy, 8); phy_provider = devm_of_phy_provider_register(dev, of_phy_simple_xlate); if (IS_ERR(phy_provider)) return dev_err_probe(dev, PTR_ERR(phy_provider), ""Failed to register PHY provider\n""); reset_control_deassert(hdptx->rsts[RST_APB].rstc); reset_control_deassert(hdptx->rsts[RST_CMN].rstc); reset_control_deassert(hdptx->rsts[RST_INIT].rstc); return rk_hdptx_phy_clk_register(hdptx); }"
716----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49992/bad/ltdc.c----ltdc_encoder_init,"static int ltdc_encoder_init(struct drm_device *ddev, struct drm_bridge *bridge) { struct drm_encoder *encoder; <S2SV_StartVul> int ret; <S2SV_EndVul> <S2SV_StartVul> encoder = devm_kzalloc(ddev->dev, sizeof(*encoder), GFP_KERNEL); <S2SV_EndVul> <S2SV_StartVul> if (!encoder) <S2SV_EndVul> <S2SV_StartVul> return -ENOMEM; <S2SV_EndVul> encoder->possible_crtcs = CRTC_MASK; encoder->possible_clones = 0; <S2SV_StartVul> drm_simple_encoder_init(ddev, encoder, DRM_MODE_ENCODER_DPI); <S2SV_EndVul> drm_encoder_helper_add(encoder, &ltdc_encoder_helper_funcs); ret = drm_bridge_attach(encoder, bridge, NULL, 0); <S2SV_StartVul> if (ret) { <S2SV_EndVul> <S2SV_StartVul> if (ret != -EPROBE_DEFER) <S2SV_EndVul> <S2SV_StartVul> drm_encoder_cleanup(encoder); <S2SV_EndVul> <S2SV_StartVul> return ret; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> DRM_DEBUG_DRIVER(""Bridge encoder:%d created\n"", encoder->base.id); return 0; }","- int ret;
- encoder = devm_kzalloc(ddev->dev, sizeof(*encoder), GFP_KERNEL);
- if (!encoder)
- return -ENOMEM;
- drm_simple_encoder_init(ddev, encoder, DRM_MODE_ENCODER_DPI);
- if (ret) {
- if (ret != -EPROBE_DEFER)
- drm_encoder_cleanup(encoder);
- return ret;
- }
+ encoder = drmm_simple_encoder_alloc(ddev, struct drm_encoder, dev,
+ DRM_MODE_ENCODER_DPI);
+ if (IS_ERR(encoder))
+ return PTR_ERR(encoder);
+ if (ret)
+ return ret;","static int ltdc_encoder_init(struct drm_device *ddev, struct drm_bridge *bridge) { struct drm_encoder *encoder; int ret; encoder = drmm_simple_encoder_alloc(ddev, struct drm_encoder, dev, DRM_MODE_ENCODER_DPI); if (IS_ERR(encoder)) return PTR_ERR(encoder); encoder->possible_crtcs = CRTC_MASK; encoder->possible_clones = 0; drm_encoder_helper_add(encoder, &ltdc_encoder_helper_funcs); ret = drm_bridge_attach(encoder, bridge, NULL, 0); if (ret) return ret; DRM_DEBUG_DRIVER(""Bridge encoder:%d created\n"", encoder->base.id); return 0; }"
952----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50194/bad/uprobes.c----arch_uprobe_analyze_insn,"int arch_uprobe_analyze_insn(struct arch_uprobe *auprobe, struct mm_struct *mm, unsigned long addr) { probe_opcode_t insn; if (mm->context.flags & MMCF_AARCH32) return -EOPNOTSUPP; else if (!IS_ALIGNED(addr, AARCH64_INSN_SIZE)) return -EINVAL; <S2SV_StartVul> insn = *(probe_opcode_t *)(&auprobe->insn[0]); <S2SV_EndVul> switch (arm_probe_decode_insn(insn, &auprobe->api)) { case INSN_REJECTED: return -EINVAL; case INSN_GOOD_NO_SLOT: auprobe->simulate = true; break; default: break; } return 0; }","- insn = *(probe_opcode_t *)(&auprobe->insn[0]);
+ insn = le32_to_cpu(auprobe->insn);","int arch_uprobe_analyze_insn(struct arch_uprobe *auprobe, struct mm_struct *mm, unsigned long addr) { probe_opcode_t insn; if (mm->context.flags & MMCF_AARCH32) return -EOPNOTSUPP; else if (!IS_ALIGNED(addr, AARCH64_INSN_SIZE)) return -EINVAL; insn = le32_to_cpu(auprobe->insn); switch (arm_probe_decode_insn(insn, &auprobe->api)) { case INSN_REJECTED: return -EINVAL; case INSN_GOOD_NO_SLOT: auprobe->simulate = true; break; default: break; } return 0; }"
1106----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53085/bad/tpm-interface.c----tpm_pm_suspend,"int tpm_pm_suspend(struct device *dev) { struct tpm_chip *chip = dev_get_drvdata(dev); int rc = 0; if (!chip) return -ENODEV; if (chip->flags & TPM_CHIP_FLAG_ALWAYS_POWERED) goto suspended; if ((chip->flags & TPM_CHIP_FLAG_FIRMWARE_POWER_MANAGED) && !pm_suspend_via_firmware()) goto suspended; <S2SV_StartVul> rc = tpm_try_get_ops(chip); <S2SV_EndVul> <S2SV_StartVul> if (!rc) { <S2SV_EndVul> <S2SV_StartVul> if (chip->flags & TPM_CHIP_FLAG_TPM2) { <S2SV_EndVul> <S2SV_StartVul> tpm2_end_auth_session(chip); <S2SV_EndVul> <S2SV_StartVul> tpm2_shutdown(chip, TPM2_SU_STATE); <S2SV_EndVul> <S2SV_StartVul> } else { <S2SV_EndVul> <S2SV_StartVul> rc = tpm1_pm_suspend(chip, tpm_suspend_pcr); <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> tpm_put_ops(chip); <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> suspended: chip->flags |= TPM_CHIP_FLAG_SUSPENDED; if (rc) dev_err(dev, ""Ignoring error %d while suspending\n"", rc); return 0; }","- rc = tpm_try_get_ops(chip);
- if (!rc) {
- if (chip->flags & TPM_CHIP_FLAG_TPM2) {
- tpm2_end_auth_session(chip);
- tpm2_shutdown(chip, TPM2_SU_STATE);
- } else {
- rc = tpm1_pm_suspend(chip, tpm_suspend_pcr);
- }
- tpm_put_ops(chip);
- }
+ rc = tpm_try_get_ops(chip);
+ if (rc) {
+ chip->flags |= TPM_CHIP_FLAG_SUSPENDED;
+ goto out;
+ }
+ goto suspended;
+ goto suspended;
+ if (chip->flags & TPM_CHIP_FLAG_TPM2) {
+ tpm2_end_auth_session(chip);
+ tpm2_shutdown(chip, TPM2_SU_STATE);
+ goto suspended;
+ }
+ rc = tpm1_pm_suspend(chip, tpm_suspend_pcr);
+ chip->flags |= TPM_CHIP_FLAG_SUSPENDED;
+ tpm_put_ops(chip);
+ out:","int tpm_pm_suspend(struct device *dev) { struct tpm_chip *chip = dev_get_drvdata(dev); int rc = 0; if (!chip) return -ENODEV; rc = tpm_try_get_ops(chip); if (rc) { chip->flags |= TPM_CHIP_FLAG_SUSPENDED; goto out; } if (chip->flags & TPM_CHIP_FLAG_ALWAYS_POWERED) goto suspended; if ((chip->flags & TPM_CHIP_FLAG_FIRMWARE_POWER_MANAGED) && !pm_suspend_via_firmware()) goto suspended; if (chip->flags & TPM_CHIP_FLAG_TPM2) { tpm2_end_auth_session(chip); tpm2_shutdown(chip, TPM2_SU_STATE); goto suspended; } rc = tpm1_pm_suspend(chip, tpm_suspend_pcr); suspended: chip->flags |= TPM_CHIP_FLAG_SUSPENDED; tpm_put_ops(chip); out: if (rc) dev_err(dev, ""Ignoring error %d while suspending\n"", rc); return 0; }"
362----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46823/bad/overflow_kunit.c----overflow_allocation_test,"static void overflow_allocation_test(struct kunit *test) { const char device_name[] = ""overflow-test""; struct device *dev; int count = 0; #define check_allocation_overflow(alloc) do { \ count++; \ test_ ## alloc(test, dev); \ } while (0) <S2SV_StartVul> dev = kunit_device_register(test, device_name); <S2SV_EndVul> KUNIT_ASSERT_FALSE_MSG(test, IS_ERR(dev), ""Cannot register test device\n""); check_allocation_overflow(kmalloc); check_allocation_overflow(kmalloc_node); check_allocation_overflow(kzalloc); check_allocation_overflow(kzalloc_node); check_allocation_overflow(__vmalloc); check_allocation_overflow(kvmalloc); check_allocation_overflow(kvmalloc_node); check_allocation_overflow(kvzalloc); check_allocation_overflow(kvzalloc_node); check_allocation_overflow(devm_kmalloc); check_allocation_overflow(devm_kzalloc); kunit_info(test, ""%d allocation overflow tests finished\n"", count); #undef check_allocation_overflow }","- dev = kunit_device_register(test, device_name);
+ dev = kunit_device_register(test, ""overflow-test"");","static void overflow_allocation_test(struct kunit *test) { struct device *dev; int count = 0; #define check_allocation_overflow(alloc) do { \ count++; \ test_ ## alloc(test, dev); \ } while (0) dev = kunit_device_register(test, ""overflow-test""); KUNIT_ASSERT_FALSE_MSG(test, IS_ERR(dev), ""Cannot register test device\n""); check_allocation_overflow(kmalloc); check_allocation_overflow(kmalloc_node); check_allocation_overflow(kzalloc); check_allocation_overflow(kzalloc_node); check_allocation_overflow(__vmalloc); check_allocation_overflow(kvmalloc); check_allocation_overflow(kvmalloc_node); check_allocation_overflow(kvzalloc); check_allocation_overflow(kvzalloc_node); check_allocation_overflow(devm_kmalloc); check_allocation_overflow(devm_kzalloc); kunit_info(test, ""%d allocation overflow tests finished\n"", count); #undef check_allocation_overflow }"
1259----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56541/bad/dp.c----ath12k_dp_cc_cleanup,"static void ath12k_dp_cc_cleanup(struct ath12k_base *ab) { struct ath12k_rx_desc_info *desc_info; struct ath12k_tx_desc_info *tx_desc_info, *tmp1; struct ath12k_dp *dp = &ab->dp; struct ath12k_skb_cb *skb_cb; struct sk_buff *skb; struct ath12k *ar; int i, j; u32 pool_id, tx_spt_page; if (!dp->spt_info) return; spin_lock_bh(&dp->rx_desc_lock); for (i = 0; i < ATH12K_NUM_RX_SPT_PAGES; i++) { desc_info = dp->spt_info->rxbaddr[i]; for (j = 0; j < ATH12K_MAX_SPT_ENTRIES; j++) { if (!desc_info[j].in_use) { list_del(&desc_info[j].list); continue; } skb = desc_info[j].skb; if (!skb) continue; dma_unmap_single(ab->dev, ATH12K_SKB_RXCB(skb)->paddr, skb->len + skb_tailroom(skb), DMA_FROM_DEVICE); dev_kfree_skb_any(skb); } } for (i = 0; i < ATH12K_NUM_RX_SPT_PAGES; i++) { if (!dp->spt_info->rxbaddr[i]) continue; kfree(dp->spt_info->rxbaddr[i]); dp->spt_info->rxbaddr[i] = NULL; } spin_unlock_bh(&dp->rx_desc_lock); for (i = 0; i < ATH12K_HW_MAX_QUEUES; i++) { spin_lock_bh(&dp->tx_desc_lock[i]); list_for_each_entry_safe(tx_desc_info, tmp1, &dp->tx_desc_used_list[i], list) { list_del(&tx_desc_info->list); skb = tx_desc_info->skb; if (!skb) continue; <S2SV_StartVul> skb_cb = ATH12K_SKB_CB(skb); <S2SV_EndVul> <S2SV_StartVul> ar = skb_cb->ar; <S2SV_EndVul> <S2SV_StartVul> if (atomic_dec_and_test(&ar->dp.num_tx_pending)) <S2SV_EndVul> <S2SV_StartVul> wake_up(&ar->dp.tx_empty_waitq); <S2SV_EndVul> dma_unmap_single(ab->dev, ATH12K_SKB_CB(skb)->paddr, skb->len, DMA_TO_DEVICE); dev_kfree_skb_any(skb); } spin_unlock_bh(&dp->tx_desc_lock[i]); } for (pool_id = 0; pool_id < ATH12K_HW_MAX_QUEUES; pool_id++) { spin_lock_bh(&dp->tx_desc_lock[pool_id]); for (i = 0; i < ATH12K_TX_SPT_PAGES_PER_POOL; i++) { tx_spt_page = i + pool_id * ATH12K_TX_SPT_PAGES_PER_POOL; if (!dp->spt_info->txbaddr[tx_spt_page]) continue; kfree(dp->spt_info->txbaddr[tx_spt_page]); dp->spt_info->txbaddr[tx_spt_page] = NULL; } spin_unlock_bh(&dp->tx_desc_lock[pool_id]); } for (i = 0; i < dp->num_spt_pages; i++) { if (!dp->spt_info[i].vaddr) continue; dma_free_coherent(ab->dev, ATH12K_PAGE_SIZE, dp->spt_info[i].vaddr, dp->spt_info[i].paddr); dp->spt_info[i].vaddr = NULL; } kfree(dp->spt_info); }","- skb_cb = ATH12K_SKB_CB(skb);
- ar = skb_cb->ar;
- if (atomic_dec_and_test(&ar->dp.num_tx_pending))
- wake_up(&ar->dp.tx_empty_waitq);
+ if (!(test_bit(ATH12K_FLAG_UNREGISTERING, &ab->dev_flags))) {
+ skb_cb = ATH12K_SKB_CB(skb);
+ ar = skb_cb->ar;
+ if (atomic_dec_and_test(&ar->dp.num_tx_pending))
+ wake_up(&ar->dp.tx_empty_waitq);
+ }","static void ath12k_dp_cc_cleanup(struct ath12k_base *ab) { struct ath12k_rx_desc_info *desc_info; struct ath12k_tx_desc_info *tx_desc_info, *tmp1; struct ath12k_dp *dp = &ab->dp; struct ath12k_skb_cb *skb_cb; struct sk_buff *skb; struct ath12k *ar; int i, j; u32 pool_id, tx_spt_page; if (!dp->spt_info) return; spin_lock_bh(&dp->rx_desc_lock); for (i = 0; i < ATH12K_NUM_RX_SPT_PAGES; i++) { desc_info = dp->spt_info->rxbaddr[i]; for (j = 0; j < ATH12K_MAX_SPT_ENTRIES; j++) { if (!desc_info[j].in_use) { list_del(&desc_info[j].list); continue; } skb = desc_info[j].skb; if (!skb) continue; dma_unmap_single(ab->dev, ATH12K_SKB_RXCB(skb)->paddr, skb->len + skb_tailroom(skb), DMA_FROM_DEVICE); dev_kfree_skb_any(skb); } } for (i = 0; i < ATH12K_NUM_RX_SPT_PAGES; i++) { if (!dp->spt_info->rxbaddr[i]) continue; kfree(dp->spt_info->rxbaddr[i]); dp->spt_info->rxbaddr[i] = NULL; } spin_unlock_bh(&dp->rx_desc_lock); for (i = 0; i < ATH12K_HW_MAX_QUEUES; i++) { spin_lock_bh(&dp->tx_desc_lock[i]); list_for_each_entry_safe(tx_desc_info, tmp1, &dp->tx_desc_used_list[i], list) { list_del(&tx_desc_info->list); skb = tx_desc_info->skb; if (!skb) continue; if (!(test_bit(ATH12K_FLAG_UNREGISTERING, &ab->dev_flags))) { skb_cb = ATH12K_SKB_CB(skb); ar = skb_cb->ar; if (atomic_dec_and_test(&ar->dp.num_tx_pending)) wake_up(&ar->dp.tx_empty_waitq); } dma_unmap_single(ab->dev, ATH12K_SKB_CB(skb)->paddr, skb->len, DMA_TO_DEVICE); dev_kfree_skb_any(skb); } spin_unlock_bh(&dp->tx_desc_lock[i]); } for (pool_id = 0; pool_id < ATH12K_HW_MAX_QUEUES; pool_id++) { spin_lock_bh(&dp->tx_desc_lock[pool_id]); for (i = 0; i < ATH12K_TX_SPT_PAGES_PER_POOL; i++) { tx_spt_page = i + pool_id * ATH12K_TX_SPT_PAGES_PER_POOL; if (!dp->spt_info->txbaddr[tx_spt_page]) continue; kfree(dp->spt_info->txbaddr[tx_spt_page]); dp->spt_info->txbaddr[tx_spt_page] = NULL; } spin_unlock_bh(&dp->tx_desc_lock[pool_id]); } for (i = 0; i < dp->num_spt_pages; i++) { if (!dp->spt_info[i].vaddr) continue; dma_free_coherent(ab->dev, ATH12K_PAGE_SIZE, dp->spt_info[i].vaddr, dp->spt_info[i].paddr); dp->spt_info[i].vaddr = NULL; } kfree(dp->spt_info); }"
590----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49912/bad/dc_resource.c----planes_changed_for_existing_stream,"static bool planes_changed_for_existing_stream(struct dc_state *context, struct dc_stream_state *stream, const struct dc_validation_set set[], int set_count) { int i, j; struct dc_stream_status *stream_status = NULL; for (i = 0; i < context->stream_count; i++) { if (context->streams[i] == stream) { stream_status = &context->stream_status[i]; break; } } <S2SV_StartVul> if (!stream_status) <S2SV_EndVul> ASSERT(0); for (i = 0; i < set_count; i++) if (set[i].stream == stream) break; if (i == set_count) ASSERT(0); if (set[i].plane_count != stream_status->plane_count) return true; for (j = 0; j < set[i].plane_count; j++) if (set[i].plane_states[j] != stream_status->plane_states[j]) return true; return false; }","- if (!stream_status)
+ }
+ if (!stream_status) {
+ return false;
+ }","static bool planes_changed_for_existing_stream(struct dc_state *context, struct dc_stream_state *stream, const struct dc_validation_set set[], int set_count) { int i, j; struct dc_stream_status *stream_status = NULL; for (i = 0; i < context->stream_count; i++) { if (context->streams[i] == stream) { stream_status = &context->stream_status[i]; break; } } if (!stream_status) { ASSERT(0); return false; } for (i = 0; i < set_count; i++) if (set[i].stream == stream) break; if (i == set_count) ASSERT(0); if (set[i].plane_count != stream_status->plane_count) return true; for (j = 0; j < set[i].plane_count; j++) if (set[i].plane_states[j] != stream_status->plane_states[j]) return true; return false; }"
1533----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2025-21685/bad/lenovo-yoga-tab2-pro-1380-fastcharger.c----yt2_1380_fc_serdev_probe,"static int yt2_1380_fc_serdev_probe(struct serdev_device *serdev) { struct device *dev = &serdev->dev; struct yt2_1380_fc *fc; int ret; fc = devm_kzalloc(dev, sizeof(*fc), GFP_KERNEL); if (!fc) return -ENOMEM; fc->dev = dev; fc->nb.notifier_call = yt2_1380_fc_extcon_evt; INIT_WORK(&fc->work, yt2_1380_fc_worker); fc->extcon = extcon_get_extcon_dev(YT2_1380_FC_EXTCON_NAME); if (IS_ERR(fc->extcon)) return dev_err_probe(dev, PTR_ERR(fc->extcon), ""getting extcon\n""); fc->pinctrl = devm_pinctrl_get(dev); if (IS_ERR(fc->pinctrl)) return dev_err_probe(dev, PTR_ERR(fc->pinctrl), ""getting pinctrl\n""); fc->gpio_state = pinctrl_lookup_state(fc->pinctrl, ""uart3_gpio""); fc->uart_state = pinctrl_lookup_state(fc->pinctrl, ""uart3_uart""); if (IS_ERR(fc->gpio_state) || IS_ERR(fc->uart_state)) return dev_err_probe(dev, -EINVAL, ""getting pinctrl states\n""); ret = yt2_1380_fc_set_gpio_mode(fc, true); if (ret) return ret; fc->uart3_txd = devm_gpiod_get(dev, ""uart3_txd"", GPIOD_OUT_HIGH); if (IS_ERR(fc->uart3_txd)) return dev_err_probe(dev, PTR_ERR(fc->uart3_txd), ""getting uart3_txd gpio\n""); fc->uart3_rxd = devm_gpiod_get(dev, ""uart3_rxd"", GPIOD_OUT_HIGH); if (IS_ERR(fc->uart3_rxd)) return dev_err_probe(dev, PTR_ERR(fc->uart3_rxd), ""getting uart3_rxd gpio\n""); ret = yt2_1380_fc_set_gpio_mode(fc, false); if (ret) return ret; ret = devm_serdev_device_open(dev, serdev); if (ret) return dev_err_probe(dev, ret, ""opening UART device\n""); serdev_device_set_baudrate(serdev, 600); serdev_device_set_flow_control(serdev, false); <S2SV_StartVul> serdev_device_set_drvdata(serdev, fc); <S2SV_EndVul> <S2SV_StartVul> serdev_device_set_client_ops(serdev, &yt2_1380_fc_serdev_ops); <S2SV_EndVul> ret = devm_extcon_register_notifier_all(dev, fc->extcon, &fc->nb); if (ret) return dev_err_probe(dev, ret, ""registering extcon notifier\n""); schedule_work(&fc->work); return 0; }","- serdev_device_set_drvdata(serdev, fc);
- serdev_device_set_client_ops(serdev, &yt2_1380_fc_serdev_ops);
+ serdev_device_set_drvdata(serdev, fc);
+ serdev_device_set_client_ops(serdev, &yt2_1380_fc_serdev_ops);","static int yt2_1380_fc_serdev_probe(struct serdev_device *serdev) { struct device *dev = &serdev->dev; struct yt2_1380_fc *fc; int ret; fc = devm_kzalloc(dev, sizeof(*fc), GFP_KERNEL); if (!fc) return -ENOMEM; fc->dev = dev; fc->nb.notifier_call = yt2_1380_fc_extcon_evt; INIT_WORK(&fc->work, yt2_1380_fc_worker); fc->extcon = extcon_get_extcon_dev(YT2_1380_FC_EXTCON_NAME); if (IS_ERR(fc->extcon)) return dev_err_probe(dev, PTR_ERR(fc->extcon), ""getting extcon\n""); fc->pinctrl = devm_pinctrl_get(dev); if (IS_ERR(fc->pinctrl)) return dev_err_probe(dev, PTR_ERR(fc->pinctrl), ""getting pinctrl\n""); fc->gpio_state = pinctrl_lookup_state(fc->pinctrl, ""uart3_gpio""); fc->uart_state = pinctrl_lookup_state(fc->pinctrl, ""uart3_uart""); if (IS_ERR(fc->gpio_state) || IS_ERR(fc->uart_state)) return dev_err_probe(dev, -EINVAL, ""getting pinctrl states\n""); ret = yt2_1380_fc_set_gpio_mode(fc, true); if (ret) return ret; fc->uart3_txd = devm_gpiod_get(dev, ""uart3_txd"", GPIOD_OUT_HIGH); if (IS_ERR(fc->uart3_txd)) return dev_err_probe(dev, PTR_ERR(fc->uart3_txd), ""getting uart3_txd gpio\n""); fc->uart3_rxd = devm_gpiod_get(dev, ""uart3_rxd"", GPIOD_OUT_HIGH); if (IS_ERR(fc->uart3_rxd)) return dev_err_probe(dev, PTR_ERR(fc->uart3_rxd), ""getting uart3_rxd gpio\n""); ret = yt2_1380_fc_set_gpio_mode(fc, false); if (ret) return ret; serdev_device_set_drvdata(serdev, fc); serdev_device_set_client_ops(serdev, &yt2_1380_fc_serdev_ops); ret = devm_serdev_device_open(dev, serdev); if (ret) return dev_err_probe(dev, ret, ""opening UART device\n""); serdev_device_set_baudrate(serdev, 600); serdev_device_set_flow_control(serdev, false); ret = devm_extcon_register_notifier_all(dev, fc->extcon, &fc->nb); if (ret) return dev_err_probe(dev, ret, ""registering extcon notifier\n""); schedule_work(&fc->work); return 0; }"
470----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47711/bad/af_unix.c----unix_ioctl,"static int unix_ioctl(struct socket *sock, unsigned int cmd, unsigned long arg) { struct sock *sk = sock->sk; long amount = 0; int err; switch (cmd) { case SIOCOUTQ: amount = unix_outq_len(sk); err = put_user(amount, (int __user *)arg); break; case SIOCINQ: amount = unix_inq_len(sk); if (amount < 0) err = amount; else err = put_user(amount, (int __user *)arg); break; case SIOCUNIXFILE: err = unix_open_file(sk); break; #if IS_ENABLED(CONFIG_AF_UNIX_OOB) case SIOCATMARK: { struct unix_sock *u = unix_sk(sk); struct sk_buff *skb; int answ = 0; mutex_lock(&u->iolock); skb = skb_peek(&sk->sk_receive_queue); if (skb) { struct sk_buff *oob_skb = READ_ONCE(u->oob_skb); if (skb == oob_skb || <S2SV_StartVul> (!oob_skb && !unix_skb_len(skb))) <S2SV_EndVul> answ = 1; } mutex_unlock(&u->iolock); err = put_user(answ, (int __user *)arg); } break; #endif default: err = -ENOIOCTLCMD; break; } return err; }","- (!oob_skb && !unix_skb_len(skb)))
+ struct sk_buff *next_skb;
+ next_skb = skb_peek_next(skb, &sk->sk_receive_queue);
+ (!unix_skb_len(skb) &&
+ (!oob_skb || next_skb == oob_skb)))","static int unix_ioctl(struct socket *sock, unsigned int cmd, unsigned long arg) { struct sock *sk = sock->sk; long amount = 0; int err; switch (cmd) { case SIOCOUTQ: amount = unix_outq_len(sk); err = put_user(amount, (int __user *)arg); break; case SIOCINQ: amount = unix_inq_len(sk); if (amount < 0) err = amount; else err = put_user(amount, (int __user *)arg); break; case SIOCUNIXFILE: err = unix_open_file(sk); break; #if IS_ENABLED(CONFIG_AF_UNIX_OOB) case SIOCATMARK: { struct unix_sock *u = unix_sk(sk); struct sk_buff *skb; int answ = 0; mutex_lock(&u->iolock); skb = skb_peek(&sk->sk_receive_queue); if (skb) { struct sk_buff *oob_skb = READ_ONCE(u->oob_skb); struct sk_buff *next_skb; next_skb = skb_peek_next(skb, &sk->sk_receive_queue); if (skb == oob_skb || (!unix_skb_len(skb) && (!oob_skb || next_skb == oob_skb))) answ = 1; } mutex_unlock(&u->iolock); err = put_user(answ, (int __user *)arg); } break; #endif default: err = -ENOIOCTLCMD; break; } return err; }"
316----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46782/bad/ila_xlat.c----ila_xlat_exit_net,"void ila_xlat_exit_net(struct net *net) { struct ila_net *ilan = net_generic(net, ila_net_id); rhashtable_free_and_destroy(&ilan->xlat.rhash_table, ila_free_cb, NULL); free_bucket_spinlocks(ilan->xlat.locks); <S2SV_StartVul> if (ilan->xlat.hooks_registered) <S2SV_EndVul> <S2SV_StartVul> nf_unregister_net_hooks(net, ila_nf_hook_ops, <S2SV_EndVul> <S2SV_StartVul> ARRAY_SIZE(ila_nf_hook_ops)); <S2SV_EndVul> }","- if (ilan->xlat.hooks_registered)
- nf_unregister_net_hooks(net, ila_nf_hook_ops,
- ARRAY_SIZE(ila_nf_hook_ops));
+ }","void ila_xlat_exit_net(struct net *net) { struct ila_net *ilan = net_generic(net, ila_net_id); rhashtable_free_and_destroy(&ilan->xlat.rhash_table, ila_free_cb, NULL); free_bucket_spinlocks(ilan->xlat.locks); }"
239----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46715/bad/industrialio-core.c----iio_read_channel_info,"static ssize_t iio_read_channel_info(struct device *dev, struct device_attribute *attr, char *buf) { struct iio_dev *indio_dev = dev_to_iio_dev(dev); struct iio_dev_attr *this_attr = to_iio_dev_attr(attr); int vals[INDIO_MAX_RAW_ELEMENTS]; int ret; int val_len = 2; if (indio_dev->info->read_raw_multi) ret = indio_dev->info->read_raw_multi(indio_dev, this_attr->c, INDIO_MAX_RAW_ELEMENTS, vals, &val_len, this_attr->address); <S2SV_StartVul> else <S2SV_EndVul> ret = indio_dev->info->read_raw(indio_dev, this_attr->c, &vals[0], &vals[1], this_attr->address); if (ret < 0) return ret; return iio_format_value(buf, ret, val_len, vals); }","- else
+ else if (indio_dev->info->read_raw)
+ else
+ return -EINVAL;","static ssize_t iio_read_channel_info(struct device *dev, struct device_attribute *attr, char *buf) { struct iio_dev *indio_dev = dev_to_iio_dev(dev); struct iio_dev_attr *this_attr = to_iio_dev_attr(attr); int vals[INDIO_MAX_RAW_ELEMENTS]; int ret; int val_len = 2; if (indio_dev->info->read_raw_multi) ret = indio_dev->info->read_raw_multi(indio_dev, this_attr->c, INDIO_MAX_RAW_ELEMENTS, vals, &val_len, this_attr->address); else if (indio_dev->info->read_raw) ret = indio_dev->info->read_raw(indio_dev, this_attr->c, &vals[0], &vals[1], this_attr->address); else return -EINVAL; if (ret < 0) return ret; return iio_format_value(buf, ret, val_len, vals); }"
469----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47711/bad/af_unix.c----*manage_oob,"static struct sk_buff *manage_oob(struct sk_buff *skb, struct sock *sk, int flags, int copied) { struct sk_buff *read_skb = NULL, *unread_skb = NULL; struct unix_sock *u = unix_sk(sk); if (likely(unix_skb_len(skb) && skb != READ_ONCE(u->oob_skb))) return skb; spin_lock(&sk->sk_receive_queue.lock); if (!unix_skb_len(skb)) { if (copied && (!u->oob_skb || skb == u->oob_skb)) { skb = NULL; } else if (flags & MSG_PEEK) { skb = skb_peek_next(skb, &sk->sk_receive_queue); } else { read_skb = skb; skb = skb_peek_next(skb, &sk->sk_receive_queue); __skb_unlink(read_skb, &sk->sk_receive_queue); } <S2SV_StartVul> goto unlock; <S2SV_EndVul> } if (skb != u->oob_skb) goto unlock; if (copied) { skb = NULL; } else if (!(flags & MSG_PEEK)) { WRITE_ONCE(u->oob_skb, NULL); if (!sock_flag(sk, SOCK_URGINLINE)) { __skb_unlink(skb, &sk->sk_receive_queue); unread_skb = skb; skb = skb_peek(&sk->sk_receive_queue); } } else if (!sock_flag(sk, SOCK_URGINLINE)) { skb = skb_peek_next(skb, &sk->sk_receive_queue); } unlock: spin_unlock(&sk->sk_receive_queue.lock); consume_skb(read_skb); kfree_skb(unread_skb); return skb; }","- goto unlock;
+ if (!skb)
+ goto unlock;","static struct sk_buff *manage_oob(struct sk_buff *skb, struct sock *sk, int flags, int copied) { struct sk_buff *read_skb = NULL, *unread_skb = NULL; struct unix_sock *u = unix_sk(sk); if (likely(unix_skb_len(skb) && skb != READ_ONCE(u->oob_skb))) return skb; spin_lock(&sk->sk_receive_queue.lock); if (!unix_skb_len(skb)) { if (copied && (!u->oob_skb || skb == u->oob_skb)) { skb = NULL; } else if (flags & MSG_PEEK) { skb = skb_peek_next(skb, &sk->sk_receive_queue); } else { read_skb = skb; skb = skb_peek_next(skb, &sk->sk_receive_queue); __skb_unlink(read_skb, &sk->sk_receive_queue); } if (!skb) goto unlock; } if (skb != u->oob_skb) goto unlock; if (copied) { skb = NULL; } else if (!(flags & MSG_PEEK)) { WRITE_ONCE(u->oob_skb, NULL); if (!sock_flag(sk, SOCK_URGINLINE)) { __skb_unlink(skb, &sk->sk_receive_queue); unread_skb = skb; skb = skb_peek(&sk->sk_receive_queue); } } else if (!sock_flag(sk, SOCK_URGINLINE)) { skb = skb_peek_next(skb, &sk->sk_receive_queue); } unlock: spin_unlock(&sk->sk_receive_queue.lock); consume_skb(read_skb); kfree_skb(unread_skb); return skb; }"
393----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46854/bad/dpaa_eth.c----dpaa_start_xmit,"dpaa_start_xmit(struct sk_buff *skb, struct net_device *net_dev) { const int queue_mapping = skb_get_queue_mapping(skb); <S2SV_StartVul> bool nonlinear = skb_is_nonlinear(skb); <S2SV_EndVul> struct rtnl_link_stats64 *percpu_stats; struct dpaa_percpu_priv *percpu_priv; struct netdev_queue *txq; struct dpaa_priv *priv; struct qm_fd fd; int offset = 0; int err = 0; priv = netdev_priv(net_dev); percpu_priv = this_cpu_ptr(priv->percpu_priv); percpu_stats = &percpu_priv->stats; qm_fd_clear_fd(&fd); if (!nonlinear) { if (skb_cow_head(skb, priv->tx_headroom)) goto enomem; WARN_ON(skb_is_nonlinear(skb)); } if (unlikely(nonlinear && (skb_shinfo(skb)->nr_frags >= DPAA_SGT_MAX_ENTRIES))) { if (__skb_linearize(skb)) goto enomem; nonlinear = skb_is_nonlinear(skb); } #ifdef CONFIG_DPAA_ERRATUM_A050385 if (unlikely(fman_has_errata_a050385())) { if (dpaa_a050385_wa_skb(net_dev, &skb)) goto enomem; nonlinear = skb_is_nonlinear(skb); } #endif if (nonlinear) { err = skb_to_sg_fd(priv, skb, &fd); percpu_priv->tx_frag_skbuffs++; } else { err = skb_to_contig_fd(priv, skb, &fd, &offset); } if (unlikely(err < 0)) goto skb_to_fd_failed; txq = netdev_get_tx_queue(net_dev, queue_mapping); txq_trans_cond_update(txq); if (priv->tx_tstamp && skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP) { fd.cmd |= cpu_to_be32(FM_FD_CMD_UPD); skb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS; } if (likely(dpaa_xmit(priv, percpu_stats, queue_mapping, &fd) == 0)) return NETDEV_TX_OK; dpaa_cleanup_tx_fd(priv, &fd, false); skb_to_fd_failed: enomem: percpu_stats->tx_errors++; dev_kfree_skb(skb); return NETDEV_TX_OK; }","- bool nonlinear = skb_is_nonlinear(skb);
+ bool nonlinear;
+ if (__skb_put_padto(skb, ETH_ZLEN, false))
+ goto enomem;
+ nonlinear = skb_is_nonlinear(skb);","dpaa_start_xmit(struct sk_buff *skb, struct net_device *net_dev) { const int queue_mapping = skb_get_queue_mapping(skb); struct rtnl_link_stats64 *percpu_stats; struct dpaa_percpu_priv *percpu_priv; struct netdev_queue *txq; struct dpaa_priv *priv; struct qm_fd fd; bool nonlinear; int offset = 0; int err = 0; priv = netdev_priv(net_dev); percpu_priv = this_cpu_ptr(priv->percpu_priv); percpu_stats = &percpu_priv->stats; qm_fd_clear_fd(&fd); if (__skb_put_padto(skb, ETH_ZLEN, false)) goto enomem; nonlinear = skb_is_nonlinear(skb); if (!nonlinear) { if (skb_cow_head(skb, priv->tx_headroom)) goto enomem; WARN_ON(skb_is_nonlinear(skb)); } if (unlikely(nonlinear && (skb_shinfo(skb)->nr_frags >= DPAA_SGT_MAX_ENTRIES))) { if (__skb_linearize(skb)) goto enomem; nonlinear = skb_is_nonlinear(skb); } #ifdef CONFIG_DPAA_ERRATUM_A050385 if (unlikely(fman_has_errata_a050385())) { if (dpaa_a050385_wa_skb(net_dev, &skb)) goto enomem; nonlinear = skb_is_nonlinear(skb); } #endif if (nonlinear) { err = skb_to_sg_fd(priv, skb, &fd); percpu_priv->tx_frag_skbuffs++; } else { err = skb_to_contig_fd(priv, skb, &fd, &offset); } if (unlikely(err < 0)) goto skb_to_fd_failed; txq = netdev_get_tx_queue(net_dev, queue_mapping); txq_trans_cond_update(txq); if (priv->tx_tstamp && skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP) { fd.cmd |= cpu_to_be32(FM_FD_CMD_UPD); skb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS; } if (likely(dpaa_xmit(priv, percpu_stats, queue_mapping, &fd) == 0)) return NETDEV_TX_OK; dpaa_cleanup_tx_fd(priv, &fd, false); skb_to_fd_failed: enomem: percpu_stats->tx_errors++; dev_kfree_skb(skb); return NETDEV_TX_OK; }"
1312----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56615/bad/devmap.c----dev_map_delete_elem,"static int dev_map_delete_elem(struct bpf_map *map, void *key) { struct bpf_dtab *dtab = container_of(map, struct bpf_dtab, map); struct bpf_dtab_netdev *old_dev; <S2SV_StartVul> int k = *(u32 *)key; <S2SV_EndVul> if (k >= map->max_entries) return -EINVAL; old_dev = unrcu_pointer(xchg(&dtab->netdev_map[k], NULL)); if (old_dev) call_rcu(&old_dev->rcu, __dev_map_entry_free); return 0; }","- int k = *(u32 *)key;
+ u32 k = *(u32 *)key;","static int dev_map_delete_elem(struct bpf_map *map, void *key) { struct bpf_dtab *dtab = container_of(map, struct bpf_dtab, map); struct bpf_dtab_netdev *old_dev; u32 k = *(u32 *)key; if (k >= map->max_entries) return -EINVAL; old_dev = unrcu_pointer(xchg(&dtab->netdev_map[k], NULL)); if (old_dev) call_rcu(&old_dev->rcu, __dev_map_entry_free); return 0; }"
842----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50104/bad/sdm845.c----sdm845_snd_startup,"static int sdm845_snd_startup(struct snd_pcm_substream *substream) { unsigned int fmt = SND_SOC_DAIFMT_BP_FP; unsigned int codec_dai_fmt = SND_SOC_DAIFMT_BC_FC; struct snd_soc_pcm_runtime *rtd = snd_soc_substream_to_rtd(substream); struct snd_soc_card *card = rtd->card; struct sdm845_snd_data *data = snd_soc_card_get_drvdata(card); struct snd_soc_dai *cpu_dai = snd_soc_rtd_to_cpu(rtd, 0); struct snd_soc_dai *codec_dai = snd_soc_rtd_to_codec(rtd, 0); int j; int ret; switch (cpu_dai->id) { case PRIMARY_MI2S_RX: case PRIMARY_MI2S_TX: codec_dai_fmt |= SND_SOC_DAIFMT_NB_NF; if (++(data->pri_mi2s_clk_count) == 1) { snd_soc_dai_set_sysclk(cpu_dai, Q6AFE_LPASS_CLK_ID_MCLK_1, DEFAULT_MCLK_RATE, SNDRV_PCM_STREAM_PLAYBACK); snd_soc_dai_set_sysclk(cpu_dai, Q6AFE_LPASS_CLK_ID_PRI_MI2S_IBIT, MI2S_BCLK_RATE, SNDRV_PCM_STREAM_PLAYBACK); } snd_soc_dai_set_fmt(cpu_dai, fmt); snd_soc_dai_set_fmt(codec_dai, codec_dai_fmt); break; case SECONDARY_MI2S_TX: codec_dai_fmt |= SND_SOC_DAIFMT_NB_NF | SND_SOC_DAIFMT_I2S; if (++(data->sec_mi2s_clk_count) == 1) { snd_soc_dai_set_sysclk(cpu_dai, Q6AFE_LPASS_CLK_ID_SEC_MI2S_IBIT, MI2S_BCLK_RATE, SNDRV_PCM_STREAM_CAPTURE); } snd_soc_dai_set_fmt(cpu_dai, fmt); snd_soc_dai_set_fmt(codec_dai, codec_dai_fmt); break; case QUATERNARY_MI2S_RX: snd_soc_dai_set_sysclk(cpu_dai, Q6AFE_LPASS_CLK_ID_QUAD_MI2S_IBIT, MI2S_BCLK_RATE, SNDRV_PCM_STREAM_PLAYBACK); snd_soc_dai_set_fmt(cpu_dai, fmt); break; case QUATERNARY_TDM_RX_0: case QUATERNARY_TDM_TX_0: if (++(data->quat_tdm_clk_count) == 1) { snd_soc_dai_set_sysclk(cpu_dai, Q6AFE_LPASS_CLK_ID_QUAD_TDM_IBIT, TDM_BCLK_RATE, SNDRV_PCM_STREAM_PLAYBACK); } codec_dai_fmt |= SND_SOC_DAIFMT_IB_NF | SND_SOC_DAIFMT_DSP_B; for_each_rtd_codec_dais(rtd, j, codec_dai) { if (!strcmp(codec_dai->component->name_prefix, ""Left"")) { ret = snd_soc_dai_set_fmt( codec_dai, codec_dai_fmt); if (ret < 0) { dev_err(rtd->dev, ""Left TDM fmt err:%d\n"", ret); return ret; } } if (!strcmp(codec_dai->component->name_prefix, ""Right"")) { ret = snd_soc_dai_set_fmt( codec_dai, codec_dai_fmt); if (ret < 0) { dev_err(rtd->dev, ""Right TDM slot err:%d\n"", ret); return ret; } } } break; case SLIMBUS_0_RX...SLIMBUS_6_TX: break; default: pr_err(""%s: invalid dai id 0x%x\n"", __func__, cpu_dai->id); break; } <S2SV_StartVul> return 0; <S2SV_EndVul> }","- return 0;
+ return qcom_snd_sdw_startup(substream);","static int sdm845_snd_startup(struct snd_pcm_substream *substream) { unsigned int fmt = SND_SOC_DAIFMT_BP_FP; unsigned int codec_dai_fmt = SND_SOC_DAIFMT_BC_FC; struct snd_soc_pcm_runtime *rtd = snd_soc_substream_to_rtd(substream); struct snd_soc_card *card = rtd->card; struct sdm845_snd_data *data = snd_soc_card_get_drvdata(card); struct snd_soc_dai *cpu_dai = snd_soc_rtd_to_cpu(rtd, 0); struct snd_soc_dai *codec_dai = snd_soc_rtd_to_codec(rtd, 0); int j; int ret; switch (cpu_dai->id) { case PRIMARY_MI2S_RX: case PRIMARY_MI2S_TX: codec_dai_fmt |= SND_SOC_DAIFMT_NB_NF; if (++(data->pri_mi2s_clk_count) == 1) { snd_soc_dai_set_sysclk(cpu_dai, Q6AFE_LPASS_CLK_ID_MCLK_1, DEFAULT_MCLK_RATE, SNDRV_PCM_STREAM_PLAYBACK); snd_soc_dai_set_sysclk(cpu_dai, Q6AFE_LPASS_CLK_ID_PRI_MI2S_IBIT, MI2S_BCLK_RATE, SNDRV_PCM_STREAM_PLAYBACK); } snd_soc_dai_set_fmt(cpu_dai, fmt); snd_soc_dai_set_fmt(codec_dai, codec_dai_fmt); break; case SECONDARY_MI2S_TX: codec_dai_fmt |= SND_SOC_DAIFMT_NB_NF | SND_SOC_DAIFMT_I2S; if (++(data->sec_mi2s_clk_count) == 1) { snd_soc_dai_set_sysclk(cpu_dai, Q6AFE_LPASS_CLK_ID_SEC_MI2S_IBIT, MI2S_BCLK_RATE, SNDRV_PCM_STREAM_CAPTURE); } snd_soc_dai_set_fmt(cpu_dai, fmt); snd_soc_dai_set_fmt(codec_dai, codec_dai_fmt); break; case QUATERNARY_MI2S_RX: snd_soc_dai_set_sysclk(cpu_dai, Q6AFE_LPASS_CLK_ID_QUAD_MI2S_IBIT, MI2S_BCLK_RATE, SNDRV_PCM_STREAM_PLAYBACK); snd_soc_dai_set_fmt(cpu_dai, fmt); break; case QUATERNARY_TDM_RX_0: case QUATERNARY_TDM_TX_0: if (++(data->quat_tdm_clk_count) == 1) { snd_soc_dai_set_sysclk(cpu_dai, Q6AFE_LPASS_CLK_ID_QUAD_TDM_IBIT, TDM_BCLK_RATE, SNDRV_PCM_STREAM_PLAYBACK); } codec_dai_fmt |= SND_SOC_DAIFMT_IB_NF | SND_SOC_DAIFMT_DSP_B; for_each_rtd_codec_dais(rtd, j, codec_dai) { if (!strcmp(codec_dai->component->name_prefix, ""Left"")) { ret = snd_soc_dai_set_fmt( codec_dai, codec_dai_fmt); if (ret < 0) { dev_err(rtd->dev, ""Left TDM fmt err:%d\n"", ret); return ret; } } if (!strcmp(codec_dai->component->name_prefix, ""Right"")) { ret = snd_soc_dai_set_fmt( codec_dai, codec_dai_fmt); if (ret < 0) { dev_err(rtd->dev, ""Right TDM slot err:%d\n"", ret); return ret; } } } break; case SLIMBUS_0_RX...SLIMBUS_6_TX: break; default: pr_err(""%s: invalid dai id 0x%x\n"", __func__, cpu_dai->id); break; } return qcom_snd_sdw_startup(substream); }"
664----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49964/bad/gup.c----memfd_pin_folios,"long memfd_pin_folios(struct file *memfd, loff_t start, loff_t end, struct folio **folios, unsigned int max_folios, pgoff_t *offset) { unsigned int flags, nr_folios, nr_found; unsigned int i, pgshift = PAGE_SHIFT; pgoff_t start_idx, end_idx, next_idx; struct folio *folio = NULL; struct folio_batch fbatch; <S2SV_StartVul> struct hstate *h; <S2SV_EndVul> long ret = -EINVAL; if (start < 0 || start > end || !max_folios) return -EINVAL; if (!memfd) return -EINVAL; if (!shmem_file(memfd) && !is_file_hugepages(memfd)) return -EINVAL; if (end >= i_size_read(file_inode(memfd))) return -EINVAL; if (is_file_hugepages(memfd)) { h = hstate_file(memfd); pgshift = huge_page_shift(h); } flags = memalloc_pin_save(); do { nr_folios = 0; start_idx = start >> pgshift; end_idx = end >> pgshift; if (is_file_hugepages(memfd)) { start_idx <<= huge_page_order(h); end_idx <<= huge_page_order(h); } folio_batch_init(&fbatch); while (start_idx <= end_idx && nr_folios < max_folios) { nr_found = filemap_get_folios_contig(memfd->f_mapping, &start_idx, end_idx, &fbatch); if (folio) { folio_put(folio); folio = NULL; } next_idx = 0; for (i = 0; i < nr_found; i++) { if (next_idx && next_idx != folio_index(fbatch.folios[i])) continue; folio = page_folio(&fbatch.folios[i]->page); if (try_grab_folio(folio, 1, FOLL_PIN)) { folio_batch_release(&fbatch); ret = -EINVAL; goto err; } if (nr_folios == 0) *offset = offset_in_folio(folio, start); folios[nr_folios] = folio; next_idx = folio_next_index(folio); if (++nr_folios == max_folios) break; } folio = NULL; folio_batch_release(&fbatch); if (!nr_found) { folio = memfd_alloc_folio(memfd, start_idx); if (IS_ERR(folio)) { ret = PTR_ERR(folio); if (ret != -EEXIST) goto err; } } } ret = check_and_migrate_movable_folios(nr_folios, folios); } while (ret == -EAGAIN); memalloc_pin_restore(flags); return ret ? ret : nr_folios; err: memalloc_pin_restore(flags); unpin_folios(folios, nr_folios); return ret; }","- struct hstate *h;
+ struct hstate *h = NULL;
+ folio_put(folio);
+ if (h)
+ folio_put(folio);","long memfd_pin_folios(struct file *memfd, loff_t start, loff_t end, struct folio **folios, unsigned int max_folios, pgoff_t *offset) { unsigned int flags, nr_folios, nr_found; unsigned int i, pgshift = PAGE_SHIFT; pgoff_t start_idx, end_idx, next_idx; struct folio *folio = NULL; struct folio_batch fbatch; struct hstate *h = NULL; long ret = -EINVAL; if (start < 0 || start > end || !max_folios) return -EINVAL; if (!memfd) return -EINVAL; if (!shmem_file(memfd) && !is_file_hugepages(memfd)) return -EINVAL; if (end >= i_size_read(file_inode(memfd))) return -EINVAL; if (is_file_hugepages(memfd)) { h = hstate_file(memfd); pgshift = huge_page_shift(h); } flags = memalloc_pin_save(); do { nr_folios = 0; start_idx = start >> pgshift; end_idx = end >> pgshift; if (is_file_hugepages(memfd)) { start_idx <<= huge_page_order(h); end_idx <<= huge_page_order(h); } folio_batch_init(&fbatch); while (start_idx <= end_idx && nr_folios < max_folios) { nr_found = filemap_get_folios_contig(memfd->f_mapping, &start_idx, end_idx, &fbatch); if (folio) { folio_put(folio); if (h) folio_put(folio); folio = NULL; } next_idx = 0; for (i = 0; i < nr_found; i++) { if (next_idx && next_idx != folio_index(fbatch.folios[i])) continue; folio = page_folio(&fbatch.folios[i]->page); if (try_grab_folio(folio, 1, FOLL_PIN)) { folio_batch_release(&fbatch); ret = -EINVAL; goto err; } if (nr_folios == 0) *offset = offset_in_folio(folio, start); folios[nr_folios] = folio; next_idx = folio_next_index(folio); if (++nr_folios == max_folios) break; } folio = NULL; folio_batch_release(&fbatch); if (!nr_found) { folio = memfd_alloc_folio(memfd, start_idx); if (IS_ERR(folio)) { ret = PTR_ERR(folio); if (ret != -EEXIST) goto err; } } } ret = check_and_migrate_movable_folios(nr_folios, folios); } while (ret == -EAGAIN); memalloc_pin_restore(flags); return ret ? ret : nr_folios; err: memalloc_pin_restore(flags); unpin_folios(folios, nr_folios); return ret; }"
739----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50015/bad/file.c----ext4_dax_write_iter,"ext4_dax_write_iter(struct kiocb *iocb, struct iov_iter *from) { ssize_t ret; size_t count; loff_t offset; handle_t *handle; bool extend = false; struct inode *inode = file_inode(iocb->ki_filp); if (iocb->ki_flags & IOCB_NOWAIT) { if (!inode_trylock(inode)) return -EAGAIN; } else { inode_lock(inode); } ret = ext4_write_checks(iocb, from); if (ret <= 0) goto out; offset = iocb->ki_pos; count = iov_iter_count(from); if (offset + count > EXT4_I(inode)->i_disksize) { handle = ext4_journal_start(inode, EXT4_HT_INODE, 2); if (IS_ERR(handle)) { ret = PTR_ERR(handle); goto out; } ret = ext4_orphan_add(handle, inode); if (ret) { ext4_journal_stop(handle); goto out; } extend = true; ext4_journal_stop(handle); } ret = dax_iomap_rw(iocb, from, &ext4_iomap_ops); if (extend) { ret = ext4_handle_inode_extension(inode, offset, ret); <S2SV_StartVul> ext4_inode_extension_cleanup(inode, ret); <S2SV_EndVul> } out: inode_unlock(inode); if (ret > 0) ret = generic_write_sync(iocb, ret); return ret; }","- ext4_inode_extension_cleanup(inode, ret);
+ ext4_inode_extension_cleanup(inode, ret < (ssize_t)count);","ext4_dax_write_iter(struct kiocb *iocb, struct iov_iter *from) { ssize_t ret; size_t count; loff_t offset; handle_t *handle; bool extend = false; struct inode *inode = file_inode(iocb->ki_filp); if (iocb->ki_flags & IOCB_NOWAIT) { if (!inode_trylock(inode)) return -EAGAIN; } else { inode_lock(inode); } ret = ext4_write_checks(iocb, from); if (ret <= 0) goto out; offset = iocb->ki_pos; count = iov_iter_count(from); if (offset + count > EXT4_I(inode)->i_disksize) { handle = ext4_journal_start(inode, EXT4_HT_INODE, 2); if (IS_ERR(handle)) { ret = PTR_ERR(handle); goto out; } ret = ext4_orphan_add(handle, inode); if (ret) { ext4_journal_stop(handle); goto out; } extend = true; ext4_journal_stop(handle); } ret = dax_iomap_rw(iocb, from, &ext4_iomap_ops); if (extend) { ret = ext4_handle_inode_extension(inode, offset, ret); ext4_inode_extension_cleanup(inode, ret < (ssize_t)count); } out: inode_unlock(inode); if (ret > 0) ret = generic_write_sync(iocb, ret); return ret; }"
869----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50126/bad/sch_taprio.c----taprio_dump,"static int taprio_dump(struct Qdisc *sch, struct sk_buff *skb) { struct taprio_sched *q = qdisc_priv(sch); struct net_device *dev = qdisc_dev(sch); struct sched_gate_list *oper, *admin; struct tc_mqprio_qopt opt = { 0 }; struct nlattr *nest, *sched_nest; unsigned int i; oper = rtnl_dereference(q->oper_sched); admin = rtnl_dereference(q->admin_sched); opt.num_tc = netdev_get_num_tc(dev); memcpy(opt.prio_tc_map, dev->prio_tc_map, sizeof(opt.prio_tc_map)); for (i = 0; i < netdev_get_num_tc(dev); i++) { opt.count[i] = dev->tc_to_txq[i].count; opt.offset[i] = dev->tc_to_txq[i].offset; } nest = nla_nest_start_noflag(skb, TCA_OPTIONS); if (!nest) goto start_error; if (nla_put(skb, TCA_TAPRIO_ATTR_PRIOMAP, sizeof(opt), &opt)) goto options_error; if (!FULL_OFFLOAD_IS_ENABLED(q->flags) && nla_put_s32(skb, TCA_TAPRIO_ATTR_SCHED_CLOCKID, q->clockid)) goto options_error; if (q->flags && nla_put_u32(skb, TCA_TAPRIO_ATTR_FLAGS, q->flags)) goto options_error; if (q->txtime_delay && nla_put_u32(skb, TCA_TAPRIO_ATTR_TXTIME_DELAY, q->txtime_delay)) <S2SV_StartVul> goto options_error; <S2SV_EndVul> if (taprio_dump_tc_entries(q, skb)) goto options_error; if (oper && dump_schedule(skb, oper)) goto options_error; if (!admin) goto done; sched_nest = nla_nest_start_noflag(skb, TCA_TAPRIO_ATTR_ADMIN_SCHED); if (!sched_nest) goto options_error; if (dump_schedule(skb, admin)) goto admin_error; nla_nest_end(skb, sched_nest); done: return nla_nest_end(skb, nest); admin_error: nla_nest_cancel(skb, sched_nest); options_error: <S2SV_StartVul> nla_nest_cancel(skb, nest); <S2SV_EndVul> start_error: <S2SV_StartVul> return -ENOSPC; <S2SV_EndVul> }","- goto options_error;
- nla_nest_cancel(skb, nest);
- return -ENOSPC;
+ admin_error:
+ options_error_rcu:
+ options_error:
+ start_error:","static int taprio_dump(struct Qdisc *sch, struct sk_buff *skb) { struct taprio_sched *q = qdisc_priv(sch); struct net_device *dev = qdisc_dev(sch); struct sched_gate_list *oper, *admin; struct tc_mqprio_qopt opt = { 0 }; struct nlattr *nest, *sched_nest; unsigned int i; opt.num_tc = netdev_get_num_tc(dev); memcpy(opt.prio_tc_map, dev->prio_tc_map, sizeof(opt.prio_tc_map)); for (i = 0; i < netdev_get_num_tc(dev); i++) { opt.count[i] = dev->tc_to_txq[i].count; opt.offset[i] = dev->tc_to_txq[i].offset; } nest = nla_nest_start_noflag(skb, TCA_OPTIONS); if (!nest) goto start_error; if (nla_put(skb, TCA_TAPRIO_ATTR_PRIOMAP, sizeof(opt), &opt)) goto options_error; if (!FULL_OFFLOAD_IS_ENABLED(q->flags) && nla_put_s32(skb, TCA_TAPRIO_ATTR_SCHED_CLOCKID, q->clockid)) goto options_error; if (q->flags && nla_put_u32(skb, TCA_TAPRIO_ATTR_FLAGS, q->flags)) goto options_error; if (q->txtime_delay && nla_put_u32(skb, TCA_TAPRIO_ATTR_TXTIME_DELAY, q->txtime_delay)) goto options_error; rcu_read_lock(); oper = rtnl_dereference(q->oper_sched); admin = rtnl_dereference(q->admin_sched); if (taprio_dump_tc_entries(q, skb)) goto options_error_rcu; if (oper && dump_schedule(skb, oper)) goto options_error_rcu; if (!admin) goto done; sched_nest = nla_nest_start_noflag(skb, TCA_TAPRIO_ATTR_ADMIN_SCHED); if (!sched_nest) goto options_error_rcu; if (dump_schedule(skb, admin)) goto admin_error; nla_nest_end(skb, sched_nest); done: rcu_read_unlock(); return nla_nest_end(skb, nest); admin_error: nla_nest_cancel(skb, sched_nest); options_error_rcu: rcu_read_unlock(); options_error: nla_nest_cancel(skb, nest); start_error: return -ENOSPC; }"
1327----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56643/bad/feat.c----dccp_feat_change_recv,"static u8 dccp_feat_change_recv(struct list_head *fn, u8 is_mandatory, u8 opt, u8 feat, u8 *val, u8 len, const bool server) { u8 defval, type = dccp_feat_type(feat); const bool local = (opt == DCCPO_CHANGE_R); struct dccp_feat_entry *entry; dccp_feat_val fval; if (len == 0 || type == FEAT_UNKNOWN) goto unknown_feature_or_value; dccp_feat_print_opt(opt, feat, val, len, is_mandatory); if (type == FEAT_NN) { if (local || len > sizeof(fval.nn)) goto unknown_feature_or_value; fval.nn = dccp_decode_value_var(val, len); if (!dccp_feat_is_valid_nn_val(feat, fval.nn)) goto unknown_feature_or_value; return dccp_feat_push_confirm(fn, feat, local, &fval); } entry = dccp_feat_list_lookup(fn, feat, local); if (entry == NULL) { if (dccp_feat_clone_sp_val(&fval, val, 1)) return DCCP_RESET_CODE_TOO_BUSY; if (len > 1 && server) { defval = dccp_feat_default_value(feat); if (dccp_feat_preflist_match(&defval, 1, val, len) > -1) fval.sp.vec[0] = defval; } else if (!dccp_feat_is_valid_sp_val(feat, fval.sp.vec[0])) { kfree(fval.sp.vec); goto unknown_feature_or_value; } if (feat == DCCPF_CCID && !ccid_support_check(fval.sp.vec, 1)) { kfree(fval.sp.vec); goto not_valid_or_not_known; } <S2SV_StartVul> return dccp_feat_push_confirm(fn, feat, local, &fval); <S2SV_EndVul> } else if (entry->state == FEAT_UNSTABLE) { return 0; } if (dccp_feat_reconcile(&entry->val, val, len, server, true)) { entry->empty_confirm = false; } else if (is_mandatory) { return DCCP_RESET_CODE_MANDATORY_ERROR; } else if (entry->state == FEAT_INITIALISING) { WARN_ON(!server); defval = dccp_feat_default_value(feat); if (!dccp_feat_reconcile(&entry->val, &defval, 1, server, true)) return DCCP_RESET_CODE_OPTION_ERROR; entry->empty_confirm = true; } entry->needs_confirm = true; entry->needs_mandatory = false; entry->state = FEAT_STABLE; return 0; unknown_feature_or_value: if (!is_mandatory) return dccp_push_empty_confirm(fn, feat, local); not_valid_or_not_known: return is_mandatory ? DCCP_RESET_CODE_MANDATORY_ERROR : DCCP_RESET_CODE_OPTION_ERROR; }","- return dccp_feat_push_confirm(fn, feat, local, &fval);
+ }
+ if (dccp_feat_push_confirm(fn, feat, local, &fval)) {
+ kfree(fval.sp.vec);
+ return DCCP_RESET_CODE_TOO_BUSY;
+ }
+ return 0;
+ return 0;
+ }","static u8 dccp_feat_change_recv(struct list_head *fn, u8 is_mandatory, u8 opt, u8 feat, u8 *val, u8 len, const bool server) { u8 defval, type = dccp_feat_type(feat); const bool local = (opt == DCCPO_CHANGE_R); struct dccp_feat_entry *entry; dccp_feat_val fval; if (len == 0 || type == FEAT_UNKNOWN) goto unknown_feature_or_value; dccp_feat_print_opt(opt, feat, val, len, is_mandatory); if (type == FEAT_NN) { if (local || len > sizeof(fval.nn)) goto unknown_feature_or_value; fval.nn = dccp_decode_value_var(val, len); if (!dccp_feat_is_valid_nn_val(feat, fval.nn)) goto unknown_feature_or_value; return dccp_feat_push_confirm(fn, feat, local, &fval); } entry = dccp_feat_list_lookup(fn, feat, local); if (entry == NULL) { if (dccp_feat_clone_sp_val(&fval, val, 1)) return DCCP_RESET_CODE_TOO_BUSY; if (len > 1 && server) { defval = dccp_feat_default_value(feat); if (dccp_feat_preflist_match(&defval, 1, val, len) > -1) fval.sp.vec[0] = defval; } else if (!dccp_feat_is_valid_sp_val(feat, fval.sp.vec[0])) { kfree(fval.sp.vec); goto unknown_feature_or_value; } if (feat == DCCPF_CCID && !ccid_support_check(fval.sp.vec, 1)) { kfree(fval.sp.vec); goto not_valid_or_not_known; } if (dccp_feat_push_confirm(fn, feat, local, &fval)) { kfree(fval.sp.vec); return DCCP_RESET_CODE_TOO_BUSY; } return 0; } else if (entry->state == FEAT_UNSTABLE) { return 0; } if (dccp_feat_reconcile(&entry->val, val, len, server, true)) { entry->empty_confirm = false; } else if (is_mandatory) { return DCCP_RESET_CODE_MANDATORY_ERROR; } else if (entry->state == FEAT_INITIALISING) { WARN_ON(!server); defval = dccp_feat_default_value(feat); if (!dccp_feat_reconcile(&entry->val, &defval, 1, server, true)) return DCCP_RESET_CODE_OPTION_ERROR; entry->empty_confirm = true; } entry->needs_confirm = true; entry->needs_mandatory = false; entry->state = FEAT_STABLE; return 0; unknown_feature_or_value: if (!is_mandatory) return dccp_push_empty_confirm(fn, feat, local); not_valid_or_not_known: return is_mandatory ? DCCP_RESET_CODE_MANDATORY_ERROR : DCCP_RESET_CODE_OPTION_ERROR; }"
336----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46793/bad/cht_bsw_rt5645.c----snd_cht_mc_probe,"static int snd_cht_mc_probe(struct platform_device *pdev) { int ret_val = 0; struct cht_mc_private *drv; struct snd_soc_acpi_mach *mach = pdev->dev.platform_data; const char *platform_name; struct acpi_device *adev; bool sof_parent; int dai_index = 0; int i; drv = devm_kzalloc(&pdev->dev, sizeof(*drv), GFP_KERNEL); if (!drv) return -ENOMEM; strcpy(drv->codec_name, RT5672_I2C_DEFAULT); for (i = 0; i < ARRAY_SIZE(cht_dailink); i++) { if (cht_dailink[i].codecs->name && !strcmp(cht_dailink[i].codecs->name, RT5672_I2C_DEFAULT)) { dai_index = i; break; } } adev = acpi_dev_get_first_match_dev(mach->id, NULL, -1); if (adev) { snprintf(drv->codec_name, sizeof(drv->codec_name), ""i2c-%s"", acpi_dev_name(adev)); cht_dailink[dai_index].codecs->name = drv->codec_name; } acpi_dev_put(adev); if (soc_intel_is_byt() && mach->mach_params.acpi_ipc_irq_index == 0) { cht_dailink[dai_index].cpus->dai_name = ""ssp0-port""; drv->use_ssp0 = true; } snd_soc_card_cht.dev = &pdev->dev; <S2SV_StartVul> platform_name = mach->mach_params.platform; <S2SV_EndVul> ret_val = snd_soc_fixup_dai_links_platform_name(&snd_soc_card_cht, platform_name); if (ret_val) return ret_val; snd_soc_card_cht.components = rt5670_components(); drv->mclk = devm_clk_get(&pdev->dev, ""pmc_plt_clk_3""); if (IS_ERR(drv->mclk)) { dev_err(&pdev->dev, ""Failed to get MCLK from pmc_plt_clk_3: %ld\n"", PTR_ERR(drv->mclk)); return PTR_ERR(drv->mclk); } snd_soc_card_set_drvdata(&snd_soc_card_cht, drv); sof_parent = snd_soc_acpi_sof_parent(&pdev->dev); if (sof_parent) { snd_soc_card_cht.name = SOF_CARD_NAME; snd_soc_card_cht.driver_name = SOF_DRIVER_NAME; } else { snd_soc_card_cht.name = CARD_NAME; snd_soc_card_cht.driver_name = DRIVER_NAME; } if (sof_parent) pdev->dev.driver->pm = &snd_soc_pm_ops; ret_val = devm_snd_soc_register_card(&pdev->dev, &snd_soc_card_cht); if (ret_val) { dev_err(&pdev->dev, ""snd_soc_register_card failed %d\n"", ret_val); return ret_val; } platform_set_drvdata(pdev, &snd_soc_card_cht); return ret_val; }","- platform_name = mach->mach_params.platform;
+ platform_name = mach->mach_params.platform;","static int snd_cht_mc_probe(struct platform_device *pdev) { int ret_val = 0; struct cht_mc_private *drv; struct snd_soc_acpi_mach *mach = pdev->dev.platform_data; const char *platform_name; struct acpi_device *adev; bool sof_parent; int dai_index = 0; int i; drv = devm_kzalloc(&pdev->dev, sizeof(*drv), GFP_KERNEL); if (!drv) return -ENOMEM; strcpy(drv->codec_name, RT5672_I2C_DEFAULT); for (i = 0; i < ARRAY_SIZE(cht_dailink); i++) { if (cht_dailink[i].num_codecs && !strcmp(cht_dailink[i].codecs->name, RT5672_I2C_DEFAULT)) { dai_index = i; break; } } adev = acpi_dev_get_first_match_dev(mach->id, NULL, -1); if (adev) { snprintf(drv->codec_name, sizeof(drv->codec_name), ""i2c-%s"", acpi_dev_name(adev)); cht_dailink[dai_index].codecs->name = drv->codec_name; } acpi_dev_put(adev); if (soc_intel_is_byt() && mach->mach_params.acpi_ipc_irq_index == 0) { cht_dailink[dai_index].cpus->dai_name = ""ssp0-port""; drv->use_ssp0 = true; } snd_soc_card_cht.dev = &pdev->dev; platform_name = mach->mach_params.platform; ret_val = snd_soc_fixup_dai_links_platform_name(&snd_soc_card_cht, platform_name); if (ret_val) return ret_val; snd_soc_card_cht.components = rt5670_components(); drv->mclk = devm_clk_get(&pdev->dev, ""pmc_plt_clk_3""); if (IS_ERR(drv->mclk)) { dev_err(&pdev->dev, ""Failed to get MCLK from pmc_plt_clk_3: %ld\n"", PTR_ERR(drv->mclk)); return PTR_ERR(drv->mclk); } snd_soc_card_set_drvdata(&snd_soc_card_cht, drv); sof_parent = snd_soc_acpi_sof_parent(&pdev->dev); if (sof_parent) { snd_soc_card_cht.name = SOF_CARD_NAME; snd_soc_card_cht.driver_name = SOF_DRIVER_NAME; } else { snd_soc_card_cht.name = CARD_NAME; snd_soc_card_cht.driver_name = DRIVER_NAME; } if (sof_parent) pdev->dev.driver->pm = &snd_soc_pm_ops; ret_val = devm_snd_soc_register_card(&pdev->dev, &snd_soc_card_cht); if (ret_val) { dev_err(&pdev->dev, ""snd_soc_register_card failed %d\n"", ret_val); return ret_val; } platform_set_drvdata(pdev, &snd_soc_card_cht); return ret_val; }"
40----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-43907/bad/vega10_hwmgr.c----vega10_apply_state_adjust_rules,"static int vega10_apply_state_adjust_rules(struct pp_hwmgr *hwmgr, struct pp_power_state *request_ps, const struct pp_power_state *current_ps) { struct amdgpu_device *adev = hwmgr->adev; <S2SV_StartVul> struct vega10_power_state *vega10_ps = <S2SV_EndVul> <S2SV_StartVul> cast_phw_vega10_power_state(&request_ps->hardware); <S2SV_EndVul> uint32_t sclk; uint32_t mclk; struct PP_Clocks minimum_clocks = {0}; bool disable_mclk_switching; bool disable_mclk_switching_for_frame_lock; bool disable_mclk_switching_for_vr; bool force_mclk_high; const struct phm_clock_and_voltage_limits *max_limits; uint32_t i; struct vega10_hwmgr *data = hwmgr->backend; struct phm_ppt_v2_information *table_info = (struct phm_ppt_v2_information *)(hwmgr->pptable); int32_t count; uint32_t stable_pstate_sclk_dpm_percentage; uint32_t stable_pstate_sclk = 0, stable_pstate_mclk = 0; uint32_t latency; data->battery_state = (PP_StateUILabel_Battery == request_ps->classification.ui_label); if (vega10_ps->performance_level_count != 2) pr_info(""VI should always have 2 performance levels""); max_limits = adev->pm.ac_power ? &(hwmgr->dyn_state.max_clock_voltage_on_ac) : &(hwmgr->dyn_state.max_clock_voltage_on_dc); if (!adev->pm.ac_power) { for (i = 0; i < vega10_ps->performance_level_count; i++) { if (vega10_ps->performance_levels[i].mem_clock > max_limits->mclk) vega10_ps->performance_levels[i].mem_clock = max_limits->mclk; if (vega10_ps->performance_levels[i].gfx_clock > max_limits->sclk) vega10_ps->performance_levels[i].gfx_clock = max_limits->sclk; } } minimum_clocks.engineClock = hwmgr->display_config->min_core_set_clock; minimum_clocks.memoryClock = hwmgr->display_config->min_mem_set_clock; if (PP_CAP(PHM_PlatformCaps_StablePState)) { stable_pstate_sclk_dpm_percentage = data->registry_data.stable_pstate_sclk_dpm_percentage; PP_ASSERT_WITH_CODE( data->registry_data.stable_pstate_sclk_dpm_percentage >= 1 && data->registry_data.stable_pstate_sclk_dpm_percentage <= 100, ""percent sclk value must range from 1% to 100%, setting default value"", stable_pstate_sclk_dpm_percentage = 75); max_limits = &(hwmgr->dyn_state.max_clock_voltage_on_ac); stable_pstate_sclk = (max_limits->sclk * stable_pstate_sclk_dpm_percentage) / 100; for (count = table_info->vdd_dep_on_sclk->count - 1; count >= 0; count--) { if (stable_pstate_sclk >= table_info->vdd_dep_on_sclk->entries[count].clk) { stable_pstate_sclk = table_info->vdd_dep_on_sclk->entries[count].clk; break; } } if (count < 0) stable_pstate_sclk = table_info->vdd_dep_on_sclk->entries[0].clk; stable_pstate_mclk = max_limits->mclk; minimum_clocks.engineClock = stable_pstate_sclk; minimum_clocks.memoryClock = stable_pstate_mclk; } disable_mclk_switching_for_frame_lock = PP_CAP(PHM_PlatformCaps_DisableMclkSwitchingForFrameLock); disable_mclk_switching_for_vr = PP_CAP(PHM_PlatformCaps_DisableMclkSwitchForVR); force_mclk_high = PP_CAP(PHM_PlatformCaps_ForceMclkHigh); if (hwmgr->display_config->num_display == 0) disable_mclk_switching = false; else disable_mclk_switching = ((1 < hwmgr->display_config->num_display) && !hwmgr->display_config->multi_monitor_in_sync) || disable_mclk_switching_for_frame_lock || disable_mclk_switching_for_vr || force_mclk_high; sclk = vega10_ps->performance_levels[0].gfx_clock; mclk = vega10_ps->performance_levels[0].mem_clock; if (sclk < minimum_clocks.engineClock) sclk = (minimum_clocks.engineClock > max_limits->sclk) ? max_limits->sclk : minimum_clocks.engineClock; if (mclk < minimum_clocks.memoryClock) mclk = (minimum_clocks.memoryClock > max_limits->mclk) ? max_limits->mclk : minimum_clocks.memoryClock; vega10_ps->performance_levels[0].gfx_clock = sclk; vega10_ps->performance_levels[0].mem_clock = mclk; if (vega10_ps->performance_levels[1].gfx_clock < vega10_ps->performance_levels[0].gfx_clock) vega10_ps->performance_levels[0].gfx_clock = vega10_ps->performance_levels[1].gfx_clock; if (disable_mclk_switching) { if (mclk < vega10_ps->performance_levels[1].mem_clock) mclk = vega10_ps->performance_levels[1].mem_clock; latency = hwmgr->display_config->dce_tolerable_mclk_in_active_latency; for (i = 0; i < data->mclk_latency_table.count; i++) { if ((data->mclk_latency_table.entries[i].latency <= latency) && (data->mclk_latency_table.entries[i].frequency >= vega10_ps->performance_levels[0].mem_clock) && (data->mclk_latency_table.entries[i].frequency <= vega10_ps->performance_levels[1].mem_clock)) mclk = data->mclk_latency_table.entries[i].frequency; } vega10_ps->performance_levels[0].mem_clock = mclk; } else { if (vega10_ps->performance_levels[1].mem_clock < vega10_ps->performance_levels[0].mem_clock) vega10_ps->performance_levels[0].mem_clock = vega10_ps->performance_levels[1].mem_clock; } if (PP_CAP(PHM_PlatformCaps_StablePState)) { for (i = 0; i < vega10_ps->performance_level_count; i++) { vega10_ps->performance_levels[i].gfx_clock = stable_pstate_sclk; vega10_ps->performance_levels[i].mem_clock = stable_pstate_mclk; } } return 0; }","- struct vega10_power_state *vega10_ps =
- cast_phw_vega10_power_state(&request_ps->hardware);
+ struct vega10_power_state *vega10_ps;
+ vega10_ps = cast_phw_vega10_power_state(&request_ps->hardware);
+ if (!vega10_ps)
+ return -EINVAL;","static int vega10_apply_state_adjust_rules(struct pp_hwmgr *hwmgr, struct pp_power_state *request_ps, const struct pp_power_state *current_ps) { struct amdgpu_device *adev = hwmgr->adev; struct vega10_power_state *vega10_ps; uint32_t sclk; uint32_t mclk; struct PP_Clocks minimum_clocks = {0}; bool disable_mclk_switching; bool disable_mclk_switching_for_frame_lock; bool disable_mclk_switching_for_vr; bool force_mclk_high; const struct phm_clock_and_voltage_limits *max_limits; uint32_t i; struct vega10_hwmgr *data = hwmgr->backend; struct phm_ppt_v2_information *table_info = (struct phm_ppt_v2_information *)(hwmgr->pptable); int32_t count; uint32_t stable_pstate_sclk_dpm_percentage; uint32_t stable_pstate_sclk = 0, stable_pstate_mclk = 0; uint32_t latency; vega10_ps = cast_phw_vega10_power_state(&request_ps->hardware); if (!vega10_ps) return -EINVAL; data->battery_state = (PP_StateUILabel_Battery == request_ps->classification.ui_label); if (vega10_ps->performance_level_count != 2) pr_info(""VI should always have 2 performance levels""); max_limits = adev->pm.ac_power ? &(hwmgr->dyn_state.max_clock_voltage_on_ac) : &(hwmgr->dyn_state.max_clock_voltage_on_dc); if (!adev->pm.ac_power) { for (i = 0; i < vega10_ps->performance_level_count; i++) { if (vega10_ps->performance_levels[i].mem_clock > max_limits->mclk) vega10_ps->performance_levels[i].mem_clock = max_limits->mclk; if (vega10_ps->performance_levels[i].gfx_clock > max_limits->sclk) vega10_ps->performance_levels[i].gfx_clock = max_limits->sclk; } } minimum_clocks.engineClock = hwmgr->display_config->min_core_set_clock; minimum_clocks.memoryClock = hwmgr->display_config->min_mem_set_clock; if (PP_CAP(PHM_PlatformCaps_StablePState)) { stable_pstate_sclk_dpm_percentage = data->registry_data.stable_pstate_sclk_dpm_percentage; PP_ASSERT_WITH_CODE( data->registry_data.stable_pstate_sclk_dpm_percentage >= 1 && data->registry_data.stable_pstate_sclk_dpm_percentage <= 100, ""percent sclk value must range from 1% to 100%, setting default value"", stable_pstate_sclk_dpm_percentage = 75); max_limits = &(hwmgr->dyn_state.max_clock_voltage_on_ac); stable_pstate_sclk = (max_limits->sclk * stable_pstate_sclk_dpm_percentage) / 100; for (count = table_info->vdd_dep_on_sclk->count - 1; count >= 0; count--) { if (stable_pstate_sclk >= table_info->vdd_dep_on_sclk->entries[count].clk) { stable_pstate_sclk = table_info->vdd_dep_on_sclk->entries[count].clk; break; } } if (count < 0) stable_pstate_sclk = table_info->vdd_dep_on_sclk->entries[0].clk; stable_pstate_mclk = max_limits->mclk; minimum_clocks.engineClock = stable_pstate_sclk; minimum_clocks.memoryClock = stable_pstate_mclk; } disable_mclk_switching_for_frame_lock = PP_CAP(PHM_PlatformCaps_DisableMclkSwitchingForFrameLock); disable_mclk_switching_for_vr = PP_CAP(PHM_PlatformCaps_DisableMclkSwitchForVR); force_mclk_high = PP_CAP(PHM_PlatformCaps_ForceMclkHigh); if (hwmgr->display_config->num_display == 0) disable_mclk_switching = false; else disable_mclk_switching = ((1 < hwmgr->display_config->num_display) && !hwmgr->display_config->multi_monitor_in_sync) || disable_mclk_switching_for_frame_lock || disable_mclk_switching_for_vr || force_mclk_high; sclk = vega10_ps->performance_levels[0].gfx_clock; mclk = vega10_ps->performance_levels[0].mem_clock; if (sclk < minimum_clocks.engineClock) sclk = (minimum_clocks.engineClock > max_limits->sclk) ? max_limits->sclk : minimum_clocks.engineClock; if (mclk < minimum_clocks.memoryClock) mclk = (minimum_clocks.memoryClock > max_limits->mclk) ? max_limits->mclk : minimum_clocks.memoryClock; vega10_ps->performance_levels[0].gfx_clock = sclk; vega10_ps->performance_levels[0].mem_clock = mclk; if (vega10_ps->performance_levels[1].gfx_clock < vega10_ps->performance_levels[0].gfx_clock) vega10_ps->performance_levels[0].gfx_clock = vega10_ps->performance_levels[1].gfx_clock; if (disable_mclk_switching) { if (mclk < vega10_ps->performance_levels[1].mem_clock) mclk = vega10_ps->performance_levels[1].mem_clock; latency = hwmgr->display_config->dce_tolerable_mclk_in_active_latency; for (i = 0; i < data->mclk_latency_table.count; i++) { if ((data->mclk_latency_table.entries[i].latency <= latency) && (data->mclk_latency_table.entries[i].frequency >= vega10_ps->performance_levels[0].mem_clock) && (data->mclk_latency_table.entries[i].frequency <= vega10_ps->performance_levels[1].mem_clock)) mclk = data->mclk_latency_table.entries[i].frequency; } vega10_ps->performance_levels[0].mem_clock = mclk; } else { if (vega10_ps->performance_levels[1].mem_clock < vega10_ps->performance_levels[0].mem_clock) vega10_ps->performance_levels[0].mem_clock = vega10_ps->performance_levels[1].mem_clock; } if (PP_CAP(PHM_PlatformCaps_StablePState)) { for (i = 0; i < vega10_ps->performance_level_count; i++) { vega10_ps->performance_levels[i].gfx_clock = stable_pstate_sclk; vega10_ps->performance_levels[i].mem_clock = stable_pstate_mclk; } } return 0; }"
1536----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2025-21688/bad/v3d_irq.c----v3d_hub_irq,"v3d_hub_irq(int irq, void *arg) { struct v3d_dev *v3d = arg; u32 intsts; irqreturn_t status = IRQ_NONE; intsts = V3D_READ(V3D_HUB_INT_STS); V3D_WRITE(V3D_HUB_INT_CLR, intsts); if (intsts & V3D_HUB_INT_TFUC) { struct v3d_fence *fence = to_v3d_fence(v3d->tfu_job->base.irq_fence); trace_v3d_tfu_irq(&v3d->drm, fence->seqno); <S2SV_StartVul> dma_fence_signal(&fence->base); <S2SV_EndVul> v3d->tfu_job = NULL; status = IRQ_HANDLED; } if (intsts & (V3D_HUB_INT_MMU_WRV | V3D_HUB_INT_MMU_PTI | V3D_HUB_INT_MMU_CAP)) { u32 axi_id = V3D_READ(V3D_MMU_VIO_ID); u64 vio_addr = ((u64)V3D_READ(V3D_MMU_VIO_ADDR) << (v3d->va_width - 32)); static const char *const v3d41_axi_ids[] = { ""L2T"", ""PTB"", ""PSE"", ""TLB"", ""CLE"", ""TFU"", ""MMU"", ""GMP"", }; const char *client = ""?""; V3D_WRITE(V3D_MMU_CTL, V3D_READ(V3D_MMU_CTL) & (V3D_MMU_CTL_CAP_EXCEEDED | V3D_MMU_CTL_PT_INVALID | V3D_MMU_CTL_WRITE_VIOLATION)); if (v3d->ver >= 41) { axi_id = axi_id >> 5; if (axi_id < ARRAY_SIZE(v3d41_axi_ids)) client = v3d41_axi_ids[axi_id]; } dev_err(v3d->dev, ""MMU error from client %s (%d) at 0x%llx%s%s%s\n"", client, axi_id, (long long)vio_addr, ((intsts & V3D_HUB_INT_MMU_WRV) ? "", write violation"" : """"), ((intsts & V3D_HUB_INT_MMU_PTI) ? "", pte invalid"" : """"), ((intsts & V3D_HUB_INT_MMU_CAP) ? "", cap exceeded"" : """")); status = IRQ_HANDLED; } return status; }","- dma_fence_signal(&fence->base);
+ dma_fence_signal(&fence->base);","v3d_hub_irq(int irq, void *arg) { struct v3d_dev *v3d = arg; u32 intsts; irqreturn_t status = IRQ_NONE; intsts = V3D_READ(V3D_HUB_INT_STS); V3D_WRITE(V3D_HUB_INT_CLR, intsts); if (intsts & V3D_HUB_INT_TFUC) { struct v3d_fence *fence = to_v3d_fence(v3d->tfu_job->base.irq_fence); trace_v3d_tfu_irq(&v3d->drm, fence->seqno); v3d->tfu_job = NULL; dma_fence_signal(&fence->base); status = IRQ_HANDLED; } if (intsts & (V3D_HUB_INT_MMU_WRV | V3D_HUB_INT_MMU_PTI | V3D_HUB_INT_MMU_CAP)) { u32 axi_id = V3D_READ(V3D_MMU_VIO_ID); u64 vio_addr = ((u64)V3D_READ(V3D_MMU_VIO_ADDR) << (v3d->va_width - 32)); static const char *const v3d41_axi_ids[] = { ""L2T"", ""PTB"", ""PSE"", ""TLB"", ""CLE"", ""TFU"", ""MMU"", ""GMP"", }; const char *client = ""?""; V3D_WRITE(V3D_MMU_CTL, V3D_READ(V3D_MMU_CTL) & (V3D_MMU_CTL_CAP_EXCEEDED | V3D_MMU_CTL_PT_INVALID | V3D_MMU_CTL_WRITE_VIOLATION)); if (v3d->ver >= 41) { axi_id = axi_id >> 5; if (axi_id < ARRAY_SIZE(v3d41_axi_ids)) client = v3d41_axi_ids[axi_id]; } dev_err(v3d->dev, ""MMU error from client %s (%d) at 0x%llx%s%s%s\n"", client, axi_id, (long long)vio_addr, ((intsts & V3D_HUB_INT_MMU_WRV) ? "", write violation"" : """"), ((intsts & V3D_HUB_INT_MMU_PTI) ? "", pte invalid"" : """"), ((intsts & V3D_HUB_INT_MMU_CAP) ? "", cap exceeded"" : """")); status = IRQ_HANDLED; } return status; }"
928----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50174/bad/panthor_sched.c----panthor_group_get_state,"int panthor_group_get_state(struct panthor_file *pfile, struct drm_panthor_group_get_state *get_state) { struct panthor_group_pool *gpool = pfile->groups; struct panthor_device *ptdev = pfile->ptdev; struct panthor_scheduler *sched = ptdev->scheduler; struct panthor_group *group; if (get_state->pad) return -EINVAL; <S2SV_StartVul> group = group_get(xa_load(&gpool->xa, get_state->group_handle)); <S2SV_EndVul> if (!group) return -EINVAL; memset(get_state, 0, sizeof(*get_state)); mutex_lock(&sched->lock); if (group->timedout) get_state->state |= DRM_PANTHOR_GROUP_STATE_TIMEDOUT; if (group->fatal_queues) { get_state->state |= DRM_PANTHOR_GROUP_STATE_FATAL_FAULT; get_state->fatal_queues = group->fatal_queues; } mutex_unlock(&sched->lock); group_put(group); return 0; }","- group = group_get(xa_load(&gpool->xa, get_state->group_handle));
+ group = group_from_handle(gpool, get_state->group_handle);","int panthor_group_get_state(struct panthor_file *pfile, struct drm_panthor_group_get_state *get_state) { struct panthor_group_pool *gpool = pfile->groups; struct panthor_device *ptdev = pfile->ptdev; struct panthor_scheduler *sched = ptdev->scheduler; struct panthor_group *group; if (get_state->pad) return -EINVAL; group = group_from_handle(gpool, get_state->group_handle); if (!group) return -EINVAL; memset(get_state, 0, sizeof(*get_state)); mutex_lock(&sched->lock); if (group->timedout) get_state->state |= DRM_PANTHOR_GROUP_STATE_TIMEDOUT; if (group->fatal_queues) { get_state->state |= DRM_PANTHOR_GROUP_STATE_FATAL_FAULT; get_state->fatal_queues = group->fatal_queues; } mutex_unlock(&sched->lock); group_put(group); return 0; }"
1467----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-57874/bad/ptrace.c----tagged_addr_ctrl_get,"static int tagged_addr_ctrl_get(struct task_struct *target, const struct user_regset *regset, struct membuf to) { long ctrl = get_tagged_addr_ctrl(target); <S2SV_StartVul> if (IS_ERR_VALUE(ctrl)) <S2SV_EndVul> return ctrl; return membuf_write(&to, &ctrl, sizeof(ctrl)); }","- if (IS_ERR_VALUE(ctrl))
+ if (WARN_ON_ONCE(IS_ERR_VALUE(ctrl)))
+ return ctrl;","static int tagged_addr_ctrl_get(struct task_struct *target, const struct user_regset *regset, struct membuf to) { long ctrl = get_tagged_addr_ctrl(target); if (WARN_ON_ONCE(IS_ERR_VALUE(ctrl))) return ctrl; return membuf_write(&to, &ctrl, sizeof(ctrl)); }"
892----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50142/bad/xfrm_user.c----verify_newsa_info,"static int verify_newsa_info(struct xfrm_usersa_info *p, struct nlattr **attrs) { int err; err = -EINVAL; switch (p->family) { case AF_INET: break; case AF_INET6: #if IS_ENABLED(CONFIG_IPV6) break; #else err = -EAFNOSUPPORT; goto out; #endif default: goto out; } <S2SV_StartVul> switch (p->sel.family) { <S2SV_EndVul> case AF_UNSPEC: break; case AF_INET: if (p->sel.prefixlen_d > 32 || p->sel.prefixlen_s > 32) goto out; break; case AF_INET6: #if IS_ENABLED(CONFIG_IPV6) if (p->sel.prefixlen_d > 128 || p->sel.prefixlen_s > 128) goto out; break; #else err = -EAFNOSUPPORT; goto out; #endif default: goto out; } err = -EINVAL; switch (p->id.proto) { case IPPROTO_AH: if ((!attrs[XFRMA_ALG_AUTH] && !attrs[XFRMA_ALG_AUTH_TRUNC]) || attrs[XFRMA_ALG_AEAD] || attrs[XFRMA_ALG_CRYPT] || attrs[XFRMA_ALG_COMP] || attrs[XFRMA_TFCPAD]) goto out; break; case IPPROTO_ESP: if (attrs[XFRMA_ALG_COMP]) goto out; if (!attrs[XFRMA_ALG_AUTH] && !attrs[XFRMA_ALG_AUTH_TRUNC] && !attrs[XFRMA_ALG_CRYPT] && !attrs[XFRMA_ALG_AEAD]) goto out; if ((attrs[XFRMA_ALG_AUTH] || attrs[XFRMA_ALG_AUTH_TRUNC] || attrs[XFRMA_ALG_CRYPT]) && attrs[XFRMA_ALG_AEAD]) goto out; if (attrs[XFRMA_TFCPAD] && p->mode != XFRM_MODE_TUNNEL) goto out; break; case IPPROTO_COMP: if (!attrs[XFRMA_ALG_COMP] || attrs[XFRMA_ALG_AEAD] || attrs[XFRMA_ALG_AUTH] || attrs[XFRMA_ALG_AUTH_TRUNC] || attrs[XFRMA_ALG_CRYPT] || attrs[XFRMA_TFCPAD] || (ntohl(p->id.spi) >= 0x10000)) goto out; break; #if IS_ENABLED(CONFIG_IPV6) case IPPROTO_DSTOPTS: case IPPROTO_ROUTING: if (attrs[XFRMA_ALG_COMP] || attrs[XFRMA_ALG_AUTH] || attrs[XFRMA_ALG_AUTH_TRUNC] || attrs[XFRMA_ALG_AEAD] || attrs[XFRMA_ALG_CRYPT] || attrs[XFRMA_ENCAP] || attrs[XFRMA_SEC_CTX] || attrs[XFRMA_TFCPAD] || !attrs[XFRMA_COADDR]) goto out; break; #endif default: goto out; } if ((err = verify_aead(attrs))) goto out; if ((err = verify_auth_trunc(attrs))) goto out; if ((err = verify_one_alg(attrs, XFRMA_ALG_AUTH))) goto out; if ((err = verify_one_alg(attrs, XFRMA_ALG_CRYPT))) goto out; if ((err = verify_one_alg(attrs, XFRMA_ALG_COMP))) goto out; if ((err = verify_sec_ctx_len(attrs))) goto out; if ((err = verify_replay(p, attrs))) goto out; err = -EINVAL; switch (p->mode) { case XFRM_MODE_TRANSPORT: case XFRM_MODE_TUNNEL: case XFRM_MODE_ROUTEOPTIMIZATION: case XFRM_MODE_BEET: break; default: goto out; } err = 0; if (attrs[XFRMA_MTIMER_THRESH]) if (!attrs[XFRMA_ENCAP]) err = -EINVAL; out: return err; }","- switch (p->sel.family) {
+ u16 family = p->sel.family;
+ if (!family && !(p->flags & XFRM_STATE_AF_UNSPEC))
+ family = p->family;
+ switch (family) {","static int verify_newsa_info(struct xfrm_usersa_info *p, struct nlattr **attrs) { int err; u16 family = p->sel.family; err = -EINVAL; switch (p->family) { case AF_INET: break; case AF_INET6: #if IS_ENABLED(CONFIG_IPV6) break; #else err = -EAFNOSUPPORT; goto out; #endif default: goto out; } if (!family && !(p->flags & XFRM_STATE_AF_UNSPEC)) family = p->family; switch (family) { case AF_UNSPEC: break; case AF_INET: if (p->sel.prefixlen_d > 32 || p->sel.prefixlen_s > 32) goto out; break; case AF_INET6: #if IS_ENABLED(CONFIG_IPV6) if (p->sel.prefixlen_d > 128 || p->sel.prefixlen_s > 128) goto out; break; #else err = -EAFNOSUPPORT; goto out; #endif default: goto out; } err = -EINVAL; switch (p->id.proto) { case IPPROTO_AH: if ((!attrs[XFRMA_ALG_AUTH] && !attrs[XFRMA_ALG_AUTH_TRUNC]) || attrs[XFRMA_ALG_AEAD] || attrs[XFRMA_ALG_CRYPT] || attrs[XFRMA_ALG_COMP] || attrs[XFRMA_TFCPAD]) goto out; break; case IPPROTO_ESP: if (attrs[XFRMA_ALG_COMP]) goto out; if (!attrs[XFRMA_ALG_AUTH] && !attrs[XFRMA_ALG_AUTH_TRUNC] && !attrs[XFRMA_ALG_CRYPT] && !attrs[XFRMA_ALG_AEAD]) goto out; if ((attrs[XFRMA_ALG_AUTH] || attrs[XFRMA_ALG_AUTH_TRUNC] || attrs[XFRMA_ALG_CRYPT]) && attrs[XFRMA_ALG_AEAD]) goto out; if (attrs[XFRMA_TFCPAD] && p->mode != XFRM_MODE_TUNNEL) goto out; break; case IPPROTO_COMP: if (!attrs[XFRMA_ALG_COMP] || attrs[XFRMA_ALG_AEAD] || attrs[XFRMA_ALG_AUTH] || attrs[XFRMA_ALG_AUTH_TRUNC] || attrs[XFRMA_ALG_CRYPT] || attrs[XFRMA_TFCPAD] || (ntohl(p->id.spi) >= 0x10000)) goto out; break; #if IS_ENABLED(CONFIG_IPV6) case IPPROTO_DSTOPTS: case IPPROTO_ROUTING: if (attrs[XFRMA_ALG_COMP] || attrs[XFRMA_ALG_AUTH] || attrs[XFRMA_ALG_AUTH_TRUNC] || attrs[XFRMA_ALG_AEAD] || attrs[XFRMA_ALG_CRYPT] || attrs[XFRMA_ENCAP] || attrs[XFRMA_SEC_CTX] || attrs[XFRMA_TFCPAD] || !attrs[XFRMA_COADDR]) goto out; break; #endif default: goto out; } if ((err = verify_aead(attrs))) goto out; if ((err = verify_auth_trunc(attrs))) goto out; if ((err = verify_one_alg(attrs, XFRMA_ALG_AUTH))) goto out; if ((err = verify_one_alg(attrs, XFRMA_ALG_CRYPT))) goto out; if ((err = verify_one_alg(attrs, XFRMA_ALG_COMP))) goto out; if ((err = verify_sec_ctx_len(attrs))) goto out; if ((err = verify_replay(p, attrs))) goto out; err = -EINVAL; switch (p->mode) { case XFRM_MODE_TRANSPORT: case XFRM_MODE_TUNNEL: case XFRM_MODE_ROUTEOPTIMIZATION: case XFRM_MODE_BEET: break; default: goto out; } err = 0; if (attrs[XFRMA_MTIMER_THRESH]) if (!attrs[XFRMA_ENCAP]) err = -EINVAL; out: return err; }"
587----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49909/bad/dcn32_hwseq.c----dcn32_set_output_transfer_func,"bool dcn32_set_output_transfer_func(struct dc *dc, struct pipe_ctx *pipe_ctx, const struct dc_stream_state *stream) { int mpcc_id = pipe_ctx->plane_res.hubp->inst; struct mpc *mpc = pipe_ctx->stream_res.opp->ctx->dc->res_pool->mpc; const struct pwl_params *params = NULL; bool ret = false; if (resource_is_pipe_type(pipe_ctx, OPP_HEAD)) { ret = dcn32_set_mpc_shaper_3dlut(pipe_ctx, stream); if (ret == false && mpc->funcs->set_output_gamma) { if (stream->out_transfer_func.type == TF_TYPE_HWPWL) params = &stream->out_transfer_func.pwl; else if (pipe_ctx->stream->out_transfer_func.type == TF_TYPE_DISTRIBUTED_POINTS && cm3_helper_translate_curve_to_hw_format( &stream->out_transfer_func, &mpc->blender_params, false)) params = &mpc->blender_params; if (stream->out_transfer_func.type == TF_TYPE_PREDEFINED) BREAK_TO_DEBUGGER(); } } <S2SV_StartVul> mpc->funcs->set_output_gamma(mpc, mpcc_id, params); <S2SV_EndVul> return ret; }","- mpc->funcs->set_output_gamma(mpc, mpcc_id, params);
+ if (mpc->funcs->set_output_gamma)
+ mpc->funcs->set_output_gamma(mpc, mpcc_id, params);","bool dcn32_set_output_transfer_func(struct dc *dc, struct pipe_ctx *pipe_ctx, const struct dc_stream_state *stream) { int mpcc_id = pipe_ctx->plane_res.hubp->inst; struct mpc *mpc = pipe_ctx->stream_res.opp->ctx->dc->res_pool->mpc; const struct pwl_params *params = NULL; bool ret = false; if (resource_is_pipe_type(pipe_ctx, OPP_HEAD)) { ret = dcn32_set_mpc_shaper_3dlut(pipe_ctx, stream); if (ret == false && mpc->funcs->set_output_gamma) { if (stream->out_transfer_func.type == TF_TYPE_HWPWL) params = &stream->out_transfer_func.pwl; else if (pipe_ctx->stream->out_transfer_func.type == TF_TYPE_DISTRIBUTED_POINTS && cm3_helper_translate_curve_to_hw_format( &stream->out_transfer_func, &mpc->blender_params, false)) params = &mpc->blender_params; if (stream->out_transfer_func.type == TF_TYPE_PREDEFINED) BREAK_TO_DEBUGGER(); } } if (mpc->funcs->set_output_gamma) mpc->funcs->set_output_gamma(mpc, mpcc_id, params); return ret; }"
31----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-43903/bad/amdgpu_dm_plane.c----amdgpu_dm_plane_handle_cursor_update,"void amdgpu_dm_plane_handle_cursor_update(struct drm_plane *plane, struct drm_plane_state *old_plane_state) { struct amdgpu_device *adev = drm_to_adev(plane->dev); struct amdgpu_framebuffer *afb = to_amdgpu_framebuffer(plane->state->fb); <S2SV_StartVul> struct drm_crtc *crtc; <S2SV_EndVul> <S2SV_StartVul> struct dm_crtc_state *crtc_state; <S2SV_EndVul> <S2SV_StartVul> struct amdgpu_crtc *amdgpu_crtc; <S2SV_EndVul> <S2SV_StartVul> u64 address; <S2SV_EndVul> struct dc_cursor_position position = {0}; struct dc_cursor_attributes attributes; int ret; <S2SV_StartVul> if (!afb) <S2SV_EndVul> <S2SV_StartVul> return; <S2SV_EndVul> <S2SV_StartVul> crtc = plane->state->crtc ? plane->state->crtc : old_plane_state->crtc; <S2SV_EndVul> <S2SV_StartVul> crtc_state = crtc ? to_dm_crtc_state(crtc->state) : NULL; <S2SV_EndVul> <S2SV_StartVul> amdgpu_crtc = to_amdgpu_crtc(crtc); <S2SV_EndVul> <S2SV_StartVul> address = afb->address; <S2SV_EndVul> if (!plane->state->fb && !old_plane_state->fb) <S2SV_StartVul> return; <S2SV_EndVul> drm_dbg_atomic(plane->dev, ""crtc_id=%d with size %d to %d\n"", amdgpu_crtc->crtc_id, plane->state->crtc_w, plane->state->crtc_h); ret = amdgpu_dm_plane_get_cursor_position(plane, crtc, &position); if (ret) return; if (!position.enable) { if (crtc_state && crtc_state->stream) { mutex_lock(&adev->dm.dc_lock); dc_stream_set_cursor_position(crtc_state->stream, &position); mutex_unlock(&adev->dm.dc_lock); } return; } amdgpu_crtc->cursor_width = plane->state->crtc_w; amdgpu_crtc->cursor_height = plane->state->crtc_h; memset(&attributes, 0, sizeof(attributes)); attributes.address.high_part = upper_32_bits(address); attributes.address.low_part = lower_32_bits(address); attributes.width = plane->state->crtc_w; attributes.height = plane->state->crtc_h; attributes.color_format = CURSOR_MODE_COLOR_PRE_MULTIPLIED_ALPHA; attributes.rotation_angle = 0; attributes.attribute_flags.value = 0; if (crtc_state->cm_is_degamma_srgb && adev->dm.dc->caps.color.dpp.gamma_corr) attributes.attribute_flags.bits.ENABLE_CURSOR_DEGAMMA = 1; attributes.pitch = afb->base.pitches[0] / afb->base.format->cpp[0]; if (crtc_state->stream) { mutex_lock(&adev->dm.dc_lock); if (!dc_stream_set_cursor_attributes(crtc_state->stream, &attributes)) DRM_ERROR(""DC failed to set cursor attributes\n""); if (!dc_stream_set_cursor_position(crtc_state->stream, &position)) DRM_ERROR(""DC failed to set cursor position\n""); mutex_unlock(&adev->dm.dc_lock); } }","- struct drm_crtc *crtc;
- struct dm_crtc_state *crtc_state;
- struct amdgpu_crtc *amdgpu_crtc;
- u64 address;
- if (!afb)
- return;
- crtc = plane->state->crtc ? plane->state->crtc : old_plane_state->crtc;
- crtc_state = crtc ? to_dm_crtc_state(crtc->state) : NULL;
- amdgpu_crtc = to_amdgpu_crtc(crtc);
- address = afb->address;
- return;
+ struct drm_crtc *crtc = afb ? plane->state->crtc : old_plane_state->crtc;
+ struct dm_crtc_state *crtc_state = crtc ? to_dm_crtc_state(crtc->state) : NULL;
+ struct amdgpu_crtc *amdgpu_crtc = to_amdgpu_crtc(crtc);
+ uint64_t address = afb ? afb->address : 0;","void amdgpu_dm_plane_handle_cursor_update(struct drm_plane *plane, struct drm_plane_state *old_plane_state) { struct amdgpu_device *adev = drm_to_adev(plane->dev); struct amdgpu_framebuffer *afb = to_amdgpu_framebuffer(plane->state->fb); struct drm_crtc *crtc = afb ? plane->state->crtc : old_plane_state->crtc; struct dm_crtc_state *crtc_state = crtc ? to_dm_crtc_state(crtc->state) : NULL; struct amdgpu_crtc *amdgpu_crtc = to_amdgpu_crtc(crtc); uint64_t address = afb ? afb->address : 0; struct dc_cursor_position position = {0}; struct dc_cursor_attributes attributes; int ret; if (!plane->state->fb && !old_plane_state->fb) return; drm_dbg_atomic(plane->dev, ""crtc_id=%d with size %d to %d\n"", amdgpu_crtc->crtc_id, plane->state->crtc_w, plane->state->crtc_h); ret = amdgpu_dm_plane_get_cursor_position(plane, crtc, &position); if (ret) return; if (!position.enable) { if (crtc_state && crtc_state->stream) { mutex_lock(&adev->dm.dc_lock); dc_stream_set_cursor_position(crtc_state->stream, &position); mutex_unlock(&adev->dm.dc_lock); } return; } amdgpu_crtc->cursor_width = plane->state->crtc_w; amdgpu_crtc->cursor_height = plane->state->crtc_h; memset(&attributes, 0, sizeof(attributes)); attributes.address.high_part = upper_32_bits(address); attributes.address.low_part = lower_32_bits(address); attributes.width = plane->state->crtc_w; attributes.height = plane->state->crtc_h; attributes.color_format = CURSOR_MODE_COLOR_PRE_MULTIPLIED_ALPHA; attributes.rotation_angle = 0; attributes.attribute_flags.value = 0; if (crtc_state->cm_is_degamma_srgb && adev->dm.dc->caps.color.dpp.gamma_corr) attributes.attribute_flags.bits.ENABLE_CURSOR_DEGAMMA = 1; attributes.pitch = afb->base.pitches[0] / afb->base.format->cpp[0]; if (crtc_state->stream) { mutex_lock(&adev->dm.dc_lock); if (!dc_stream_set_cursor_attributes(crtc_state->stream, &attributes)) DRM_ERROR(""DC failed to set cursor attributes\n""); if (!dc_stream_set_cursor_position(crtc_state->stream, &position)) DRM_ERROR(""DC failed to set cursor position\n""); mutex_unlock(&adev->dm.dc_lock); } }"
1367----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56702/bad/verifier.c----check_ptr_to_btf_access,"static int check_ptr_to_btf_access(struct bpf_verifier_env *env, struct bpf_reg_state *regs, int regno, int off, int size, enum bpf_access_type atype, int value_regno) { struct bpf_reg_state *reg = regs + regno; const struct btf_type *t = btf_type_by_id(reg->btf, reg->btf_id); const char *tname = btf_name_by_offset(reg->btf, t->name_off); const char *field_name = NULL; enum bpf_type_flag flag = 0; u32 btf_id = 0; int ret; if (!env->allow_ptr_leaks) { verbose(env, ""'struct %s' access is allowed only to CAP_PERFMON and CAP_SYS_ADMIN\n"", tname); return -EPERM; } if (!env->prog->gpl_compatible && btf_is_kernel(reg->btf)) { verbose(env, ""Cannot access kernel 'struct %s' from non-GPL compatible program\n"", tname); return -EINVAL; } if (off < 0) { verbose(env, ""R%d is ptr_%s invalid negative access: off=%d\n"", regno, tname, off); return -EACCES; } if (!tnum_is_const(reg->var_off) || reg->var_off.value) { char tn_buf[48]; tnum_strn(tn_buf, sizeof(tn_buf), reg->var_off); verbose(env, ""R%d is ptr_%s invalid variable offset: off=%d, var_off=%s\n"", regno, tname, off, tn_buf); return -EACCES; } if (reg->type & MEM_USER) { verbose(env, ""R%d is ptr_%s access user memory: off=%d\n"", regno, tname, off); return -EACCES; } if (reg->type & MEM_PERCPU) { verbose(env, ""R%d is ptr_%s access percpu memory: off=%d\n"", regno, tname, off); return -EACCES; } if (env->ops->btf_struct_access && !type_is_alloc(reg->type) && atype == BPF_WRITE) { if (!btf_is_kernel(reg->btf)) { verbose(env, ""verifier internal error: reg->btf must be kernel btf\n""); return -EFAULT; } ret = env->ops->btf_struct_access(&env->log, reg, off, size); } else { if (atype != BPF_READ && !type_is_ptr_alloc_obj(reg->type)) { verbose(env, ""only read is supported\n""); return -EACCES; } if (type_is_alloc(reg->type) && !type_is_non_owning_ref(reg->type) && !(reg->type & MEM_RCU) && !reg->ref_obj_id) { verbose(env, ""verifier internal error: ref_obj_id for allocated object must be non-zero\n""); return -EFAULT; } ret = btf_struct_access(&env->log, reg, off, size, atype, &btf_id, &flag, &field_name); } if (ret < 0) return ret; if (ret != PTR_TO_BTF_ID) { } else if (type_flag(reg->type) & PTR_UNTRUSTED) { flag = PTR_UNTRUSTED; } else if (is_trusted_reg(reg) || is_rcu_reg(reg)) { if (type_is_trusted(env, reg, field_name, btf_id)) { flag |= PTR_TRUSTED; } else if (type_is_trusted_or_null(env, reg, field_name, btf_id)) { flag |= PTR_TRUSTED | PTR_MAYBE_NULL; } else if (in_rcu_cs(env) && !type_may_be_null(reg->type)) { if (type_is_rcu(env, reg, field_name, btf_id)) { flag |= MEM_RCU; } else if (flag & MEM_RCU || type_is_rcu_or_null(env, reg, field_name, btf_id)) { flag |= MEM_RCU | PTR_MAYBE_NULL; if (type_is_rcu_or_null(env, reg, field_name, btf_id) && flag & PTR_UNTRUSTED) flag &= ~PTR_UNTRUSTED; } else if (flag & (MEM_PERCPU | MEM_USER)) { } else { clear_trusted_flags(&flag); } } else { flag = PTR_UNTRUSTED; } } else { clear_trusted_flags(&flag); } <S2SV_StartVul> if (atype == BPF_READ && value_regno >= 0) <S2SV_EndVul> mark_btf_ld_reg(env, regs, value_regno, ret, reg->btf, btf_id, flag); return 0; }","- if (atype == BPF_READ && value_regno >= 0)
+ bool mask;
+ mask = mask_raw_tp_reg(env, reg);
+ }
+ if (atype == BPF_READ && value_regno >= 0) {
+ if (regno == value_regno)
+ mask = false;
+ }
+ unmask_raw_tp_reg(reg, mask);
+ }","static int check_ptr_to_btf_access(struct bpf_verifier_env *env, struct bpf_reg_state *regs, int regno, int off, int size, enum bpf_access_type atype, int value_regno) { struct bpf_reg_state *reg = regs + regno; const struct btf_type *t = btf_type_by_id(reg->btf, reg->btf_id); const char *tname = btf_name_by_offset(reg->btf, t->name_off); const char *field_name = NULL; enum bpf_type_flag flag = 0; u32 btf_id = 0; bool mask; int ret; if (!env->allow_ptr_leaks) { verbose(env, ""'struct %s' access is allowed only to CAP_PERFMON and CAP_SYS_ADMIN\n"", tname); return -EPERM; } if (!env->prog->gpl_compatible && btf_is_kernel(reg->btf)) { verbose(env, ""Cannot access kernel 'struct %s' from non-GPL compatible program\n"", tname); return -EINVAL; } if (off < 0) { verbose(env, ""R%d is ptr_%s invalid negative access: off=%d\n"", regno, tname, off); return -EACCES; } if (!tnum_is_const(reg->var_off) || reg->var_off.value) { char tn_buf[48]; tnum_strn(tn_buf, sizeof(tn_buf), reg->var_off); verbose(env, ""R%d is ptr_%s invalid variable offset: off=%d, var_off=%s\n"", regno, tname, off, tn_buf); return -EACCES; } if (reg->type & MEM_USER) { verbose(env, ""R%d is ptr_%s access user memory: off=%d\n"", regno, tname, off); return -EACCES; } if (reg->type & MEM_PERCPU) { verbose(env, ""R%d is ptr_%s access percpu memory: off=%d\n"", regno, tname, off); return -EACCES; } if (env->ops->btf_struct_access && !type_is_alloc(reg->type) && atype == BPF_WRITE) { if (!btf_is_kernel(reg->btf)) { verbose(env, ""verifier internal error: reg->btf must be kernel btf\n""); return -EFAULT; } ret = env->ops->btf_struct_access(&env->log, reg, off, size); } else { if (atype != BPF_READ && !type_is_ptr_alloc_obj(reg->type)) { verbose(env, ""only read is supported\n""); return -EACCES; } if (type_is_alloc(reg->type) && !type_is_non_owning_ref(reg->type) && !(reg->type & MEM_RCU) && !reg->ref_obj_id) { verbose(env, ""verifier internal error: ref_obj_id for allocated object must be non-zero\n""); return -EFAULT; } ret = btf_struct_access(&env->log, reg, off, size, atype, &btf_id, &flag, &field_name); } if (ret < 0) return ret; mask = mask_raw_tp_reg(env, reg); if (ret != PTR_TO_BTF_ID) { } else if (type_flag(reg->type) & PTR_UNTRUSTED) { flag = PTR_UNTRUSTED; } else if (is_trusted_reg(reg) || is_rcu_reg(reg)) { if (type_is_trusted(env, reg, field_name, btf_id)) { flag |= PTR_TRUSTED; } else if (type_is_trusted_or_null(env, reg, field_name, btf_id)) { flag |= PTR_TRUSTED | PTR_MAYBE_NULL; } else if (in_rcu_cs(env) && !type_may_be_null(reg->type)) { if (type_is_rcu(env, reg, field_name, btf_id)) { flag |= MEM_RCU; } else if (flag & MEM_RCU || type_is_rcu_or_null(env, reg, field_name, btf_id)) { flag |= MEM_RCU | PTR_MAYBE_NULL; if (type_is_rcu_or_null(env, reg, field_name, btf_id) && flag & PTR_UNTRUSTED) flag &= ~PTR_UNTRUSTED; } else if (flag & (MEM_PERCPU | MEM_USER)) { } else { clear_trusted_flags(&flag); } } else { flag = PTR_UNTRUSTED; } } else { clear_trusted_flags(&flag); } if (atype == BPF_READ && value_regno >= 0) { mark_btf_ld_reg(env, regs, value_regno, ret, reg->btf, btf_id, flag); if (regno == value_regno) mask = false; } unmask_raw_tp_reg(reg, mask); return 0; }"
984----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50222/bad/iov_iter.c----copy_page_from_iter_atomic,"size_t copy_page_from_iter_atomic(struct page *page, size_t offset, size_t bytes, struct iov_iter *i) { size_t n, copied = 0; if (!page_copy_sane(page, offset, bytes)) return 0; if (WARN_ON_ONCE(!i->data_source)) return 0; do { char *p; n = bytes - copied; <S2SV_StartVul> if (PageHighMem(page)) { <S2SV_EndVul> page += offset / PAGE_SIZE; offset %= PAGE_SIZE; n = min_t(size_t, n, PAGE_SIZE - offset); } p = kmap_atomic(page) + offset; n = __copy_from_iter(p, n, i); kunmap_atomic(p); copied += n; offset += n; <S2SV_StartVul> } while (PageHighMem(page) && copied != bytes && n > 0); <S2SV_EndVul> return copied; }","- if (PageHighMem(page)) {
- } while (PageHighMem(page) && copied != bytes && n > 0);
+ bool uses_kmap = IS_ENABLED(CONFIG_DEBUG_KMAP_LOCAL_FORCE_MAP) ||
+ PageHighMem(page);
+ if (uses_kmap) {
+ } while (uses_kmap && copied != bytes && n > 0);","size_t copy_page_from_iter_atomic(struct page *page, size_t offset, size_t bytes, struct iov_iter *i) { size_t n, copied = 0; bool uses_kmap = IS_ENABLED(CONFIG_DEBUG_KMAP_LOCAL_FORCE_MAP) || PageHighMem(page); if (!page_copy_sane(page, offset, bytes)) return 0; if (WARN_ON_ONCE(!i->data_source)) return 0; do { char *p; n = bytes - copied; if (uses_kmap) { page += offset / PAGE_SIZE; offset %= PAGE_SIZE; n = min_t(size_t, n, PAGE_SIZE - offset); } p = kmap_atomic(page) + offset; n = __copy_from_iter(p, n, i); kunmap_atomic(p); copied += n; offset += n; } while (uses_kmap && copied != bytes && n > 0); return copied; }"
494----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47735/bad/hns_roce_qp.c----hns_roce_lock_cqs,"void hns_roce_lock_cqs(struct hns_roce_cq *send_cq, struct hns_roce_cq *recv_cq) __acquires(&send_cq->lock) __acquires(&recv_cq->lock) { if (unlikely(send_cq == NULL && recv_cq == NULL)) { __acquire(&send_cq->lock); __acquire(&recv_cq->lock); } else if (unlikely(send_cq != NULL && recv_cq == NULL)) { <S2SV_StartVul> spin_lock_irq(&send_cq->lock); <S2SV_EndVul> __acquire(&recv_cq->lock); } else if (unlikely(send_cq == NULL && recv_cq != NULL)) { <S2SV_StartVul> spin_lock_irq(&recv_cq->lock); <S2SV_EndVul> __acquire(&send_cq->lock); } else if (send_cq == recv_cq) { <S2SV_StartVul> spin_lock_irq(&send_cq->lock); <S2SV_EndVul> __acquire(&recv_cq->lock); } else if (send_cq->cqn < recv_cq->cqn) { <S2SV_StartVul> spin_lock_irq(&send_cq->lock); <S2SV_EndVul> spin_lock_nested(&recv_cq->lock, SINGLE_DEPTH_NESTING); } else { <S2SV_StartVul> spin_lock_irq(&recv_cq->lock); <S2SV_EndVul> spin_lock_nested(&send_cq->lock, SINGLE_DEPTH_NESTING); } }","- spin_lock_irq(&send_cq->lock);
- spin_lock_irq(&recv_cq->lock);
- spin_lock_irq(&send_cq->lock);
- spin_lock_irq(&send_cq->lock);
- spin_lock_irq(&recv_cq->lock);
+ spin_lock(&send_cq->lock);
+ spin_lock(&recv_cq->lock);
+ spin_lock(&send_cq->lock);
+ spin_lock(&send_cq->lock);
+ spin_lock(&recv_cq->lock);","void hns_roce_lock_cqs(struct hns_roce_cq *send_cq, struct hns_roce_cq *recv_cq) __acquires(&send_cq->lock) __acquires(&recv_cq->lock) { if (unlikely(send_cq == NULL && recv_cq == NULL)) { __acquire(&send_cq->lock); __acquire(&recv_cq->lock); } else if (unlikely(send_cq != NULL && recv_cq == NULL)) { spin_lock(&send_cq->lock); __acquire(&recv_cq->lock); } else if (unlikely(send_cq == NULL && recv_cq != NULL)) { spin_lock(&recv_cq->lock); __acquire(&send_cq->lock); } else if (send_cq == recv_cq) { spin_lock(&send_cq->lock); __acquire(&recv_cq->lock); } else if (send_cq->cqn < recv_cq->cqn) { spin_lock(&send_cq->lock); spin_lock_nested(&recv_cq->lock, SINGLE_DEPTH_NESTING); } else { spin_lock(&recv_cq->lock); spin_lock_nested(&send_cq->lock, SINGLE_DEPTH_NESTING); } }"
74----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44947/bad/dev.c----fuse_notify_store,"static int fuse_notify_store(struct fuse_conn *fc, unsigned int size, struct fuse_copy_state *cs) { struct fuse_notify_store_out outarg; struct inode *inode; struct address_space *mapping; u64 nodeid; int err; pgoff_t index; unsigned int offset; unsigned int num; loff_t file_size; loff_t end; err = -EINVAL; if (size < sizeof(outarg)) goto out_finish; err = fuse_copy_one(cs, &outarg, sizeof(outarg)); if (err) goto out_finish; err = -EINVAL; if (size - sizeof(outarg) != outarg.size) goto out_finish; nodeid = outarg.nodeid; down_read(&fc->killsb); err = -ENOENT; inode = fuse_ilookup(fc, nodeid, NULL); if (!inode) goto out_up_killsb; mapping = inode->i_mapping; index = outarg.offset >> PAGE_SHIFT; offset = outarg.offset & ~PAGE_MASK; file_size = i_size_read(inode); end = outarg.offset + outarg.size; if (end > file_size) { file_size = end; fuse_write_update_attr(inode, file_size, outarg.size); } num = outarg.size; while (num) { struct page *page; unsigned int this_num; err = -ENOMEM; page = find_or_create_page(mapping, index, mapping_gfp_mask(mapping)); if (!page) goto out_iput; this_num = min_t(unsigned, num, PAGE_SIZE - offset); err = fuse_copy_page(cs, &page, offset, this_num, 0); <S2SV_StartVul> if (!err && offset == 0 && <S2SV_EndVul> <S2SV_StartVul> (this_num == PAGE_SIZE || file_size == end)) <S2SV_EndVul> SetPageUptodate(page); unlock_page(page); put_page(page); if (err) goto out_iput; num -= this_num; offset = 0; index++; } err = 0; out_iput: iput(inode); out_up_killsb: up_read(&fc->killsb); out_finish: fuse_copy_finish(cs); return err; }","- if (!err && offset == 0 &&
- (this_num == PAGE_SIZE || file_size == end))
+ if (!PageUptodate(page) && !err && offset == 0 &&
+ (this_num == PAGE_SIZE || file_size == end)) {
+ zero_user_segment(page, this_num, PAGE_SIZE);
+ }","static int fuse_notify_store(struct fuse_conn *fc, unsigned int size, struct fuse_copy_state *cs) { struct fuse_notify_store_out outarg; struct inode *inode; struct address_space *mapping; u64 nodeid; int err; pgoff_t index; unsigned int offset; unsigned int num; loff_t file_size; loff_t end; err = -EINVAL; if (size < sizeof(outarg)) goto out_finish; err = fuse_copy_one(cs, &outarg, sizeof(outarg)); if (err) goto out_finish; err = -EINVAL; if (size - sizeof(outarg) != outarg.size) goto out_finish; nodeid = outarg.nodeid; down_read(&fc->killsb); err = -ENOENT; inode = fuse_ilookup(fc, nodeid, NULL); if (!inode) goto out_up_killsb; mapping = inode->i_mapping; index = outarg.offset >> PAGE_SHIFT; offset = outarg.offset & ~PAGE_MASK; file_size = i_size_read(inode); end = outarg.offset + outarg.size; if (end > file_size) { file_size = end; fuse_write_update_attr(inode, file_size, outarg.size); } num = outarg.size; while (num) { struct page *page; unsigned int this_num; err = -ENOMEM; page = find_or_create_page(mapping, index, mapping_gfp_mask(mapping)); if (!page) goto out_iput; this_num = min_t(unsigned, num, PAGE_SIZE - offset); err = fuse_copy_page(cs, &page, offset, this_num, 0); if (!PageUptodate(page) && !err && offset == 0 && (this_num == PAGE_SIZE || file_size == end)) { zero_user_segment(page, this_num, PAGE_SIZE); SetPageUptodate(page); } unlock_page(page); put_page(page); if (err) goto out_iput; num -= this_num; offset = 0; index++; } err = 0; out_iput: iput(inode); out_up_killsb: up_read(&fc->killsb); out_finish: fuse_copy_finish(cs); return err; }"
521----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47757/bad/btree.c----nilfs_btree_check_delete,"static int nilfs_btree_check_delete(struct nilfs_bmap *btree, __u64 key) { struct buffer_head *bh; struct nilfs_btree_node *root, *node; __u64 maxkey, nextmaxkey; __u64 ptr; int nchildren, ret; root = nilfs_btree_get_root(btree); switch (nilfs_btree_height(btree)) { case 2: bh = NULL; node = root; break; case 3: <S2SV_StartVul> nchildren = nilfs_btree_node_get_nchildren(root); <S2SV_EndVul> if (nchildren > 1) return 0; ptr = nilfs_btree_node_get_ptr(root, nchildren - 1, NILFS_BTREE_ROOT_NCHILDREN_MAX); ret = nilfs_btree_get_block(btree, ptr, &bh); if (ret < 0) return ret; node = (struct nilfs_btree_node *)bh->b_data; break; default: return 0; } <S2SV_StartVul> nchildren = nilfs_btree_node_get_nchildren(node); <S2SV_EndVul> maxkey = nilfs_btree_node_get_key(node, nchildren - 1); nextmaxkey = (nchildren > 1) ? nilfs_btree_node_get_key(node, nchildren - 2) : 0; brelse(bh); return (maxkey == key) && (nextmaxkey < NILFS_BMAP_LARGE_LOW); }","- nchildren = nilfs_btree_node_get_nchildren(root);
- nchildren = nilfs_btree_node_get_nchildren(node);
+ nchildren = nilfs_btree_node_get_nchildren(root);
+ if (unlikely(nchildren == 0))
+ return 0;
+ return 0;
+ nchildren = nilfs_btree_node_get_nchildren(node);
+ return 0;","static int nilfs_btree_check_delete(struct nilfs_bmap *btree, __u64 key) { struct buffer_head *bh; struct nilfs_btree_node *root, *node; __u64 maxkey, nextmaxkey; __u64 ptr; int nchildren, ret; root = nilfs_btree_get_root(btree); nchildren = nilfs_btree_node_get_nchildren(root); if (unlikely(nchildren == 0)) return 0; switch (nilfs_btree_height(btree)) { case 2: bh = NULL; node = root; break; case 3: if (nchildren > 1) return 0; ptr = nilfs_btree_node_get_ptr(root, nchildren - 1, NILFS_BTREE_ROOT_NCHILDREN_MAX); ret = nilfs_btree_get_block(btree, ptr, &bh); if (ret < 0) return ret; node = (struct nilfs_btree_node *)bh->b_data; nchildren = nilfs_btree_node_get_nchildren(node); break; default: return 0; } maxkey = nilfs_btree_node_get_key(node, nchildren - 1); nextmaxkey = (nchildren > 1) ? nilfs_btree_node_get_key(node, nchildren - 2) : 0; brelse(bh); return (maxkey == key) && (nextmaxkey < NILFS_BMAP_LARGE_LOW); }"
1385----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56719/bad/stmmac_main.c----stmmac_tso_xmit,"static netdev_tx_t stmmac_tso_xmit(struct sk_buff *skb, struct net_device *dev) { struct dma_desc *desc, *first, *mss_desc = NULL; struct stmmac_priv *priv = netdev_priv(dev); int tmp_pay_len = 0, first_tx, nfrags; unsigned int first_entry, tx_packets; struct stmmac_txq_stats *txq_stats; struct stmmac_tx_queue *tx_q; u32 pay_len, mss, queue; u8 proto_hdr_len, hdr; <S2SV_StartVul> dma_addr_t des; <S2SV_EndVul> bool set_ic; int i; if (skb_vlan_tag_present(skb)) { skb = __vlan_hwaccel_push_inside(skb); if (unlikely(!skb)) { priv->xstats.tx_dropped++; return NETDEV_TX_OK; } } nfrags = skb_shinfo(skb)->nr_frags; queue = skb_get_queue_mapping(skb); tx_q = &priv->dma_conf.tx_queue[queue]; txq_stats = &priv->xstats.txq_stats[queue]; first_tx = tx_q->cur_tx; if (skb_shinfo(skb)->gso_type & SKB_GSO_UDP_L4) { proto_hdr_len = skb_transport_offset(skb) + sizeof(struct udphdr); hdr = sizeof(struct udphdr); } else { proto_hdr_len = skb_tcp_all_headers(skb); hdr = tcp_hdrlen(skb); } if (unlikely(stmmac_tx_avail(priv, queue) < (((skb->len - proto_hdr_len) / TSO_MAX_BUFF_SIZE + 1)))) { if (!netif_tx_queue_stopped(netdev_get_tx_queue(dev, queue))) { netif_tx_stop_queue(netdev_get_tx_queue(priv->dev, queue)); netdev_err(priv->dev, ""%s: Tx Ring full when queue awake\n"", __func__); } return NETDEV_TX_BUSY; } pay_len = skb_headlen(skb) - proto_hdr_len; mss = skb_shinfo(skb)->gso_size; if (mss != tx_q->mss) { if (tx_q->tbs & STMMAC_TBS_AVAIL) mss_desc = &tx_q->dma_entx[tx_q->cur_tx].basic; else mss_desc = &tx_q->dma_tx[tx_q->cur_tx]; stmmac_set_mss(priv, mss_desc, mss); tx_q->mss = mss; tx_q->cur_tx = STMMAC_GET_ENTRY(tx_q->cur_tx, priv->dma_conf.dma_tx_size); WARN_ON(tx_q->tx_skbuff[tx_q->cur_tx]); } if (netif_msg_tx_queued(priv)) { pr_info(""%s: hdrlen %d, hdr_len %d, pay_len %d, mss %d\n"", __func__, hdr, proto_hdr_len, pay_len, mss); pr_info(""\tskb->len %d, skb->data_len %d\n"", skb->len, skb->data_len); } first_entry = tx_q->cur_tx; WARN_ON(tx_q->tx_skbuff[first_entry]); if (tx_q->tbs & STMMAC_TBS_AVAIL) desc = &tx_q->dma_entx[first_entry].basic; else desc = &tx_q->dma_tx[first_entry]; first = desc; des = dma_map_single(priv->device, skb->data, skb_headlen(skb), DMA_TO_DEVICE); if (dma_mapping_error(priv->device, des)) goto dma_map_err; if (priv->dma_cap.addr64 <= 32) { first->des0 = cpu_to_le32(des); if (pay_len) first->des1 = cpu_to_le32(des + proto_hdr_len); tmp_pay_len = pay_len - TSO_MAX_BUFF_SIZE; } else { stmmac_set_desc_addr(priv, first, des); tmp_pay_len = pay_len; <S2SV_StartVul> des += proto_hdr_len; <S2SV_EndVul> pay_len = 0; } <S2SV_StartVul> stmmac_tso_allocator(priv, des, tmp_pay_len, (nfrags == 0), queue); <S2SV_EndVul> tx_q->tx_skbuff_dma[tx_q->cur_tx].buf = des; tx_q->tx_skbuff_dma[tx_q->cur_tx].len = skb_headlen(skb); tx_q->tx_skbuff_dma[tx_q->cur_tx].map_as_page = false; tx_q->tx_skbuff_dma[tx_q->cur_tx].buf_type = STMMAC_TXBUF_T_SKB; for (i = 0; i < nfrags; i++) { const skb_frag_t *frag = &skb_shinfo(skb)->frags[i]; des = skb_frag_dma_map(priv->device, frag, 0, skb_frag_size(frag), DMA_TO_DEVICE); if (dma_mapping_error(priv->device, des)) goto dma_map_err; stmmac_tso_allocator(priv, des, skb_frag_size(frag), (i == nfrags - 1), queue); tx_q->tx_skbuff_dma[tx_q->cur_tx].buf = des; tx_q->tx_skbuff_dma[tx_q->cur_tx].len = skb_frag_size(frag); tx_q->tx_skbuff_dma[tx_q->cur_tx].map_as_page = true; tx_q->tx_skbuff_dma[tx_q->cur_tx].buf_type = STMMAC_TXBUF_T_SKB; } tx_q->tx_skbuff_dma[tx_q->cur_tx].last_segment = true; tx_q->tx_skbuff[tx_q->cur_tx] = skb; tx_q->tx_skbuff_dma[tx_q->cur_tx].buf_type = STMMAC_TXBUF_T_SKB; tx_packets = (tx_q->cur_tx + 1) - first_tx; tx_q->tx_count_frames += tx_packets; if ((skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP) && priv->hwts_tx_en) set_ic = true; else if (!priv->tx_coal_frames[queue]) set_ic = false; else if (tx_packets > priv->tx_coal_frames[queue]) set_ic = true; else if ((tx_q->tx_count_frames % priv->tx_coal_frames[queue]) < tx_packets) set_ic = true; else set_ic = false; if (set_ic) { if (tx_q->tbs & STMMAC_TBS_AVAIL) desc = &tx_q->dma_entx[tx_q->cur_tx].basic; else desc = &tx_q->dma_tx[tx_q->cur_tx]; tx_q->tx_count_frames = 0; stmmac_set_tx_ic(priv, desc); } tx_q->cur_tx = STMMAC_GET_ENTRY(tx_q->cur_tx, priv->dma_conf.dma_tx_size); if (unlikely(stmmac_tx_avail(priv, queue) <= (MAX_SKB_FRAGS + 1))) { netif_dbg(priv, hw, priv->dev, ""%s: stop transmitted packets\n"", __func__); netif_tx_stop_queue(netdev_get_tx_queue(priv->dev, queue)); } u64_stats_update_begin(&txq_stats->q_syncp); u64_stats_add(&txq_stats->q.tx_bytes, skb->len); u64_stats_inc(&txq_stats->q.tx_tso_frames); u64_stats_add(&txq_stats->q.tx_tso_nfrags, nfrags); if (set_ic) u64_stats_inc(&txq_stats->q.tx_set_ic_bit); u64_stats_update_end(&txq_stats->q_syncp); if (priv->sarc_type) stmmac_set_desc_sarc(priv, first, priv->sarc_type); skb_tx_timestamp(skb); if (unlikely((skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP) && priv->hwts_tx_en)) { skb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS; stmmac_enable_tx_timestamp(priv, first); } stmmac_prepare_tso_tx_desc(priv, first, 1, proto_hdr_len, pay_len, 1, tx_q->tx_skbuff_dma[first_entry].last_segment, hdr / 4, (skb->len - proto_hdr_len)); if (mss_desc) { dma_wmb(); stmmac_set_tx_owner(priv, mss_desc); } if (netif_msg_pktdata(priv)) { pr_info(""%s: curr=%d dirty=%d f=%d, e=%d, f_p=%p, nfrags %d\n"", __func__, tx_q->cur_tx, tx_q->dirty_tx, first_entry, tx_q->cur_tx, first, nfrags); pr_info("">>> frame to be transmitted: ""); print_pkt(skb->data, skb_headlen(skb)); } netdev_tx_sent_queue(netdev_get_tx_queue(dev, queue), skb->len); stmmac_flush_tx_descriptors(priv, queue); stmmac_tx_timer_arm(priv, queue); return NETDEV_TX_OK; dma_map_err: dev_err(priv->device, ""Tx dma map failed\n""); dev_kfree_skb(skb); priv->xstats.tx_dropped++; return NETDEV_TX_OK; }","- dma_addr_t des;
- des += proto_hdr_len;
- stmmac_tso_allocator(priv, des, tmp_pay_len, (nfrags == 0), queue);
+ dma_addr_t tso_des, des;
+ tso_des = des;
+ tso_des = des + proto_hdr_len;
+ stmmac_tso_allocator(priv, tso_des, tmp_pay_len, (nfrags == 0), queue);","static netdev_tx_t stmmac_tso_xmit(struct sk_buff *skb, struct net_device *dev) { struct dma_desc *desc, *first, *mss_desc = NULL; struct stmmac_priv *priv = netdev_priv(dev); int tmp_pay_len = 0, first_tx, nfrags; unsigned int first_entry, tx_packets; struct stmmac_txq_stats *txq_stats; struct stmmac_tx_queue *tx_q; u32 pay_len, mss, queue; dma_addr_t tso_des, des; u8 proto_hdr_len, hdr; bool set_ic; int i; if (skb_vlan_tag_present(skb)) { skb = __vlan_hwaccel_push_inside(skb); if (unlikely(!skb)) { priv->xstats.tx_dropped++; return NETDEV_TX_OK; } } nfrags = skb_shinfo(skb)->nr_frags; queue = skb_get_queue_mapping(skb); tx_q = &priv->dma_conf.tx_queue[queue]; txq_stats = &priv->xstats.txq_stats[queue]; first_tx = tx_q->cur_tx; if (skb_shinfo(skb)->gso_type & SKB_GSO_UDP_L4) { proto_hdr_len = skb_transport_offset(skb) + sizeof(struct udphdr); hdr = sizeof(struct udphdr); } else { proto_hdr_len = skb_tcp_all_headers(skb); hdr = tcp_hdrlen(skb); } if (unlikely(stmmac_tx_avail(priv, queue) < (((skb->len - proto_hdr_len) / TSO_MAX_BUFF_SIZE + 1)))) { if (!netif_tx_queue_stopped(netdev_get_tx_queue(dev, queue))) { netif_tx_stop_queue(netdev_get_tx_queue(priv->dev, queue)); netdev_err(priv->dev, ""%s: Tx Ring full when queue awake\n"", __func__); } return NETDEV_TX_BUSY; } pay_len = skb_headlen(skb) - proto_hdr_len; mss = skb_shinfo(skb)->gso_size; if (mss != tx_q->mss) { if (tx_q->tbs & STMMAC_TBS_AVAIL) mss_desc = &tx_q->dma_entx[tx_q->cur_tx].basic; else mss_desc = &tx_q->dma_tx[tx_q->cur_tx]; stmmac_set_mss(priv, mss_desc, mss); tx_q->mss = mss; tx_q->cur_tx = STMMAC_GET_ENTRY(tx_q->cur_tx, priv->dma_conf.dma_tx_size); WARN_ON(tx_q->tx_skbuff[tx_q->cur_tx]); } if (netif_msg_tx_queued(priv)) { pr_info(""%s: hdrlen %d, hdr_len %d, pay_len %d, mss %d\n"", __func__, hdr, proto_hdr_len, pay_len, mss); pr_info(""\tskb->len %d, skb->data_len %d\n"", skb->len, skb->data_len); } first_entry = tx_q->cur_tx; WARN_ON(tx_q->tx_skbuff[first_entry]); if (tx_q->tbs & STMMAC_TBS_AVAIL) desc = &tx_q->dma_entx[first_entry].basic; else desc = &tx_q->dma_tx[first_entry]; first = desc; des = dma_map_single(priv->device, skb->data, skb_headlen(skb), DMA_TO_DEVICE); if (dma_mapping_error(priv->device, des)) goto dma_map_err; if (priv->dma_cap.addr64 <= 32) { first->des0 = cpu_to_le32(des); if (pay_len) first->des1 = cpu_to_le32(des + proto_hdr_len); tmp_pay_len = pay_len - TSO_MAX_BUFF_SIZE; tso_des = des; } else { stmmac_set_desc_addr(priv, first, des); tmp_pay_len = pay_len; tso_des = des + proto_hdr_len; pay_len = 0; } stmmac_tso_allocator(priv, tso_des, tmp_pay_len, (nfrags == 0), queue); tx_q->tx_skbuff_dma[tx_q->cur_tx].buf = des; tx_q->tx_skbuff_dma[tx_q->cur_tx].len = skb_headlen(skb); tx_q->tx_skbuff_dma[tx_q->cur_tx].map_as_page = false; tx_q->tx_skbuff_dma[tx_q->cur_tx].buf_type = STMMAC_TXBUF_T_SKB; for (i = 0; i < nfrags; i++) { const skb_frag_t *frag = &skb_shinfo(skb)->frags[i]; des = skb_frag_dma_map(priv->device, frag, 0, skb_frag_size(frag), DMA_TO_DEVICE); if (dma_mapping_error(priv->device, des)) goto dma_map_err; stmmac_tso_allocator(priv, des, skb_frag_size(frag), (i == nfrags - 1), queue); tx_q->tx_skbuff_dma[tx_q->cur_tx].buf = des; tx_q->tx_skbuff_dma[tx_q->cur_tx].len = skb_frag_size(frag); tx_q->tx_skbuff_dma[tx_q->cur_tx].map_as_page = true; tx_q->tx_skbuff_dma[tx_q->cur_tx].buf_type = STMMAC_TXBUF_T_SKB; } tx_q->tx_skbuff_dma[tx_q->cur_tx].last_segment = true; tx_q->tx_skbuff[tx_q->cur_tx] = skb; tx_q->tx_skbuff_dma[tx_q->cur_tx].buf_type = STMMAC_TXBUF_T_SKB; tx_packets = (tx_q->cur_tx + 1) - first_tx; tx_q->tx_count_frames += tx_packets; if ((skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP) && priv->hwts_tx_en) set_ic = true; else if (!priv->tx_coal_frames[queue]) set_ic = false; else if (tx_packets > priv->tx_coal_frames[queue]) set_ic = true; else if ((tx_q->tx_count_frames % priv->tx_coal_frames[queue]) < tx_packets) set_ic = true; else set_ic = false; if (set_ic) { if (tx_q->tbs & STMMAC_TBS_AVAIL) desc = &tx_q->dma_entx[tx_q->cur_tx].basic; else desc = &tx_q->dma_tx[tx_q->cur_tx]; tx_q->tx_count_frames = 0; stmmac_set_tx_ic(priv, desc); } tx_q->cur_tx = STMMAC_GET_ENTRY(tx_q->cur_tx, priv->dma_conf.dma_tx_size); if (unlikely(stmmac_tx_avail(priv, queue) <= (MAX_SKB_FRAGS + 1))) { netif_dbg(priv, hw, priv->dev, ""%s: stop transmitted packets\n"", __func__); netif_tx_stop_queue(netdev_get_tx_queue(priv->dev, queue)); } u64_stats_update_begin(&txq_stats->q_syncp); u64_stats_add(&txq_stats->q.tx_bytes, skb->len); u64_stats_inc(&txq_stats->q.tx_tso_frames); u64_stats_add(&txq_stats->q.tx_tso_nfrags, nfrags); if (set_ic) u64_stats_inc(&txq_stats->q.tx_set_ic_bit); u64_stats_update_end(&txq_stats->q_syncp); if (priv->sarc_type) stmmac_set_desc_sarc(priv, first, priv->sarc_type); skb_tx_timestamp(skb); if (unlikely((skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP) && priv->hwts_tx_en)) { skb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS; stmmac_enable_tx_timestamp(priv, first); } stmmac_prepare_tso_tx_desc(priv, first, 1, proto_hdr_len, pay_len, 1, tx_q->tx_skbuff_dma[first_entry].last_segment, hdr / 4, (skb->len - proto_hdr_len)); if (mss_desc) { dma_wmb(); stmmac_set_tx_owner(priv, mss_desc); } if (netif_msg_pktdata(priv)) { pr_info(""%s: curr=%d dirty=%d f=%d, e=%d, f_p=%p, nfrags %d\n"", __func__, tx_q->cur_tx, tx_q->dirty_tx, first_entry, tx_q->cur_tx, first, nfrags); pr_info("">>> frame to be transmitted: ""); print_pkt(skb->data, skb_headlen(skb)); } netdev_tx_sent_queue(netdev_get_tx_queue(dev, queue), skb->len); stmmac_flush_tx_descriptors(priv, queue); stmmac_tx_timer_arm(priv, queue); return NETDEV_TX_OK; dma_map_err: dev_err(priv->device, ""Tx dma map failed\n""); dev_kfree_skb(skb); priv->xstats.tx_dropped++; return NETDEV_TX_OK; }"
1190----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53180/bad/pcm_native.c----snd_pcm_mmap_data_fault,"static vm_fault_t snd_pcm_mmap_data_fault(struct vm_fault *vmf) { struct snd_pcm_substream *substream = vmf->vma->vm_private_data; struct snd_pcm_runtime *runtime; unsigned long offset; struct page * page; size_t dma_bytes; if (substream == NULL) return VM_FAULT_SIGBUS; runtime = substream->runtime; offset = vmf->pgoff << PAGE_SHIFT; dma_bytes = PAGE_ALIGN(runtime->dma_bytes); if (offset > dma_bytes - PAGE_SIZE) return VM_FAULT_SIGBUS; if (substream->ops->page) page = substream->ops->page(substream, offset); <S2SV_StartVul> else if (!snd_pcm_get_dma_buf(substream)) <S2SV_EndVul> page = virt_to_page(runtime->dma_area + offset); <S2SV_StartVul> else <S2SV_EndVul> page = snd_sgbuf_get_page(snd_pcm_get_dma_buf(substream), offset); if (!page) return VM_FAULT_SIGBUS; get_page(page); vmf->page = page; return 0; }","- else if (!snd_pcm_get_dma_buf(substream))
- else
+ else if (!snd_pcm_get_dma_buf(substream)) {
+ if (WARN_ON_ONCE(!runtime->dma_area))
+ return VM_FAULT_SIGBUS;
+ } else
+ return VM_FAULT_SIGBUS;","static vm_fault_t snd_pcm_mmap_data_fault(struct vm_fault *vmf) { struct snd_pcm_substream *substream = vmf->vma->vm_private_data; struct snd_pcm_runtime *runtime; unsigned long offset; struct page * page; size_t dma_bytes; if (substream == NULL) return VM_FAULT_SIGBUS; runtime = substream->runtime; offset = vmf->pgoff << PAGE_SHIFT; dma_bytes = PAGE_ALIGN(runtime->dma_bytes); if (offset > dma_bytes - PAGE_SIZE) return VM_FAULT_SIGBUS; if (substream->ops->page) page = substream->ops->page(substream, offset); else if (!snd_pcm_get_dma_buf(substream)) { if (WARN_ON_ONCE(!runtime->dma_area)) return VM_FAULT_SIGBUS; page = virt_to_page(runtime->dma_area + offset); } else page = snd_sgbuf_get_page(snd_pcm_get_dma_buf(substream), offset); if (!page) return VM_FAULT_SIGBUS; get_page(page); vmf->page = page; return 0; }"
715----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49992/bad/drv.c----drv_load,"static int drv_load(struct drm_device *ddev) { struct platform_device *pdev = to_platform_device(ddev->dev); struct ltdc_device *ldev; int ret; DRM_DEBUG(""%s\n"", __func__); <S2SV_StartVul> ldev = devm_kzalloc(ddev->dev, sizeof(*ldev), GFP_KERNEL); <S2SV_EndVul> if (!ldev) return -ENOMEM; ddev->dev_private = (void *)ldev; ret = drmm_mode_config_init(ddev); if (ret) return ret; ddev->mode_config.min_width = 0; ddev->mode_config.min_height = 0; ddev->mode_config.max_width = STM_MAX_FB_WIDTH; ddev->mode_config.max_height = STM_MAX_FB_HEIGHT; ddev->mode_config.funcs = &drv_mode_config_funcs; ddev->mode_config.normalize_zpos = true; ret = ltdc_load(ddev); if (ret) return ret; drm_mode_config_reset(ddev); drm_kms_helper_poll_init(ddev); platform_set_drvdata(pdev, ddev); return 0; }","- ldev = devm_kzalloc(ddev->dev, sizeof(*ldev), GFP_KERNEL);
+ ldev = drmm_kzalloc(ddev, sizeof(*ldev), GFP_KERNEL);","static int drv_load(struct drm_device *ddev) { struct platform_device *pdev = to_platform_device(ddev->dev); struct ltdc_device *ldev; int ret; DRM_DEBUG(""%s\n"", __func__); ldev = drmm_kzalloc(ddev, sizeof(*ldev), GFP_KERNEL); if (!ldev) return -ENOMEM; ddev->dev_private = (void *)ldev; ret = drmm_mode_config_init(ddev); if (ret) return ret; ddev->mode_config.min_width = 0; ddev->mode_config.min_height = 0; ddev->mode_config.max_width = STM_MAX_FB_WIDTH; ddev->mode_config.max_height = STM_MAX_FB_HEIGHT; ddev->mode_config.funcs = &drv_mode_config_funcs; ddev->mode_config.normalize_zpos = true; ret = ltdc_load(ddev); if (ret) return ret; drm_mode_config_reset(ddev); drm_kms_helper_poll_init(ddev); platform_set_drvdata(pdev, ddev); return 0; }"
938----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50186/bad/socket.c----__sock_create,"int __sock_create(struct net *net, int family, int type, int protocol, struct socket **res, int kern) { int err; struct socket *sock; const struct net_proto_family *pf; if (family < 0 || family >= NPROTO) return -EAFNOSUPPORT; if (type < 0 || type >= SOCK_MAX) return -EINVAL; if (family == PF_INET && type == SOCK_PACKET) { pr_info_once(""%s uses obsolete (PF_INET,SOCK_PACKET)\n"", current->comm); family = PF_PACKET; } err = security_socket_create(family, type, protocol, kern); if (err) return err; sock = sock_alloc(); if (!sock) { net_warn_ratelimited(""socket: no more sockets\n""); return -ENFILE; } sock->type = type; #ifdef CONFIG_MODULES if (rcu_access_pointer(net_families[family]) == NULL) request_module(""net-pf-%d"", family); #endif rcu_read_lock(); pf = rcu_dereference(net_families[family]); err = -EAFNOSUPPORT; if (!pf) goto out_release; if (!try_module_get(pf->owner)) goto out_release; rcu_read_unlock(); err = pf->create(net, sock, protocol, kern); <S2SV_StartVul> if (err < 0) <S2SV_EndVul> goto out_module_put; if (!try_module_get(sock->ops->owner)) goto out_module_busy; module_put(pf->owner); err = security_socket_post_create(sock, family, type, protocol, kern); if (err) goto out_sock_release; *res = sock; return 0; out_module_busy: err = -EAFNOSUPPORT; out_module_put: sock->ops = NULL; module_put(pf->owner); out_sock_release: sock_release(sock); return err; out_release: rcu_read_unlock(); goto out_sock_release; }","- if (err < 0)
+ if (err < 0) {
+ sock->sk = NULL;
+ }","int __sock_create(struct net *net, int family, int type, int protocol, struct socket **res, int kern) { int err; struct socket *sock; const struct net_proto_family *pf; if (family < 0 || family >= NPROTO) return -EAFNOSUPPORT; if (type < 0 || type >= SOCK_MAX) return -EINVAL; if (family == PF_INET && type == SOCK_PACKET) { pr_info_once(""%s uses obsolete (PF_INET,SOCK_PACKET)\n"", current->comm); family = PF_PACKET; } err = security_socket_create(family, type, protocol, kern); if (err) return err; sock = sock_alloc(); if (!sock) { net_warn_ratelimited(""socket: no more sockets\n""); return -ENFILE; } sock->type = type; #ifdef CONFIG_MODULES if (rcu_access_pointer(net_families[family]) == NULL) request_module(""net-pf-%d"", family); #endif rcu_read_lock(); pf = rcu_dereference(net_families[family]); err = -EAFNOSUPPORT; if (!pf) goto out_release; if (!try_module_get(pf->owner)) goto out_release; rcu_read_unlock(); err = pf->create(net, sock, protocol, kern); if (err < 0) { sock->sk = NULL; goto out_module_put; } if (!try_module_get(sock->ops->owner)) goto out_module_busy; module_put(pf->owner); err = security_socket_post_create(sock, family, type, protocol, kern); if (err) goto out_sock_release; *res = sock; return 0; out_module_busy: err = -EAFNOSUPPORT; out_module_put: sock->ops = NULL; module_put(pf->owner); out_sock_release: sock_release(sock); return err; out_release: rcu_read_unlock(); goto out_sock_release; }"
881----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50134/bad/hgsmi_base.c----hgsmi_update_pointer_shape,"int hgsmi_update_pointer_shape(struct gen_pool *ctx, u32 flags, u32 hot_x, u32 hot_y, u32 width, u32 height, u8 *pixels, u32 len) { struct vbva_mouse_pointer_shape *p; u32 pixel_len = 0; int rc; if (flags & VBOX_MOUSE_POINTER_SHAPE) { pixel_len = ((((width + 7) / 8) * height + 3) & ~3) + width * 4 * height; if (pixel_len > len) return -EINVAL; flags |= VBOX_MOUSE_POINTER_VISIBLE; } <S2SV_StartVul> p = hgsmi_buffer_alloc(ctx, sizeof(*p) + pixel_len, HGSMI_CH_VBVA, <S2SV_EndVul> VBVA_MOUSE_POINTER_SHAPE); if (!p) return -ENOMEM; p->result = VINF_SUCCESS; p->flags = flags; p->hot_X = hot_x; p->hot_y = hot_y; p->width = width; p->height = height; if (pixel_len) memcpy(p->data, pixels, pixel_len); hgsmi_buffer_submit(ctx, p); switch (p->result) { case VINF_SUCCESS: rc = 0; break; case VERR_NO_MEMORY: rc = -ENOMEM; break; case VERR_NOT_SUPPORTED: rc = -EBUSY; break; default: rc = -EINVAL; } hgsmi_buffer_free(ctx, p); return rc; }","- p = hgsmi_buffer_alloc(ctx, sizeof(*p) + pixel_len, HGSMI_CH_VBVA,
+ p = hgsmi_buffer_alloc(ctx, sizeof(*p) + pixel_len + 4, HGSMI_CH_VBVA,","int hgsmi_update_pointer_shape(struct gen_pool *ctx, u32 flags, u32 hot_x, u32 hot_y, u32 width, u32 height, u8 *pixels, u32 len) { struct vbva_mouse_pointer_shape *p; u32 pixel_len = 0; int rc; if (flags & VBOX_MOUSE_POINTER_SHAPE) { pixel_len = ((((width + 7) / 8) * height + 3) & ~3) + width * 4 * height; if (pixel_len > len) return -EINVAL; flags |= VBOX_MOUSE_POINTER_VISIBLE; } p = hgsmi_buffer_alloc(ctx, sizeof(*p) + pixel_len + 4, HGSMI_CH_VBVA, VBVA_MOUSE_POINTER_SHAPE); if (!p) return -ENOMEM; p->result = VINF_SUCCESS; p->flags = flags; p->hot_X = hot_x; p->hot_y = hot_y; p->width = width; p->height = height; if (pixel_len) memcpy(p->data, pixels, pixel_len); hgsmi_buffer_submit(ctx, p); switch (p->result) { case VINF_SUCCESS: rc = 0; break; case VERR_NO_MEMORY: rc = -ENOMEM; break; case VERR_NOT_SUPPORTED: rc = -EBUSY; break; default: rc = -EINVAL; } hgsmi_buffer_free(ctx, p); return rc; }"
778----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50056/bad/uvc_v4l2.c----uvc_v4l2_try_format,"uvc_v4l2_try_format(struct file *file, void *fh, struct v4l2_format *fmt) { struct video_device *vdev = video_devdata(file); struct uvc_device *uvc = video_get_drvdata(vdev); struct uvc_video *video = &uvc->video; struct uvcg_format *uformat; struct uvcg_frame *uframe; u8 *fcc; if (fmt->type != video->queue.queue.type) return -EINVAL; fcc = (u8 *)&fmt->fmt.pix.pixelformat; uvcg_dbg(&uvc->func, ""Trying format 0x%08x (%c%c%c%c): %ux%u\n"", fmt->fmt.pix.pixelformat, fcc[0], fcc[1], fcc[2], fcc[3], fmt->fmt.pix.width, fmt->fmt.pix.height); uformat = find_format_by_pix(uvc, fmt->fmt.pix.pixelformat); if (!uformat) return -EINVAL; uframe = find_closest_frame_by_size(uvc, uformat, fmt->fmt.pix.width, fmt->fmt.pix.height); if (!uframe) return -EINVAL; if (uformat->type == UVCG_UNCOMPRESSED) { struct uvcg_uncompressed *u = to_uvcg_uncompressed(&uformat->group.cg_item); if (!u) return 0; v4l2_fill_pixfmt(&fmt->fmt.pix, fmt->fmt.pix.pixelformat, uframe->frame.w_width, uframe->frame.w_height); if (fmt->fmt.pix.sizeimage != (uvc_v4l2_get_bytesperline(uformat, uframe) * uframe->frame.w_height)) return -EINVAL; } else { fmt->fmt.pix.width = uframe->frame.w_width; fmt->fmt.pix.height = uframe->frame.w_height; fmt->fmt.pix.bytesperline = uvc_v4l2_get_bytesperline(uformat, uframe); fmt->fmt.pix.sizeimage = uvc_get_frame_size(uformat, uframe); <S2SV_StartVul> fmt->fmt.pix.pixelformat = to_uvc_format(uformat)->fcc; <S2SV_EndVul> } fmt->fmt.pix.field = V4L2_FIELD_NONE; fmt->fmt.pix.colorspace = V4L2_COLORSPACE_SRGB; fmt->fmt.pix.priv = 0; return 0; }","- fmt->fmt.pix.pixelformat = to_uvc_format(uformat)->fcc;
+ const struct uvc_format_desc *fmtdesc;
+ fmtdesc = to_uvc_format(uformat);
+ if (IS_ERR(fmtdesc))
+ return PTR_ERR(fmtdesc);
+ fmt->fmt.pix.pixelformat = fmtdesc->fcc;","uvc_v4l2_try_format(struct file *file, void *fh, struct v4l2_format *fmt) { struct video_device *vdev = video_devdata(file); struct uvc_device *uvc = video_get_drvdata(vdev); struct uvc_video *video = &uvc->video; struct uvcg_format *uformat; struct uvcg_frame *uframe; const struct uvc_format_desc *fmtdesc; u8 *fcc; if (fmt->type != video->queue.queue.type) return -EINVAL; fcc = (u8 *)&fmt->fmt.pix.pixelformat; uvcg_dbg(&uvc->func, ""Trying format 0x%08x (%c%c%c%c): %ux%u\n"", fmt->fmt.pix.pixelformat, fcc[0], fcc[1], fcc[2], fcc[3], fmt->fmt.pix.width, fmt->fmt.pix.height); uformat = find_format_by_pix(uvc, fmt->fmt.pix.pixelformat); if (!uformat) return -EINVAL; uframe = find_closest_frame_by_size(uvc, uformat, fmt->fmt.pix.width, fmt->fmt.pix.height); if (!uframe) return -EINVAL; if (uformat->type == UVCG_UNCOMPRESSED) { struct uvcg_uncompressed *u = to_uvcg_uncompressed(&uformat->group.cg_item); if (!u) return 0; v4l2_fill_pixfmt(&fmt->fmt.pix, fmt->fmt.pix.pixelformat, uframe->frame.w_width, uframe->frame.w_height); if (fmt->fmt.pix.sizeimage != (uvc_v4l2_get_bytesperline(uformat, uframe) * uframe->frame.w_height)) return -EINVAL; } else { fmt->fmt.pix.width = uframe->frame.w_width; fmt->fmt.pix.height = uframe->frame.w_height; fmt->fmt.pix.bytesperline = uvc_v4l2_get_bytesperline(uformat, uframe); fmt->fmt.pix.sizeimage = uvc_get_frame_size(uformat, uframe); fmtdesc = to_uvc_format(uformat); if (IS_ERR(fmtdesc)) return PTR_ERR(fmtdesc); fmt->fmt.pix.pixelformat = fmtdesc->fcc; } fmt->fmt.pix.field = V4L2_FIELD_NONE; fmt->fmt.pix.colorspace = V4L2_COLORSPACE_SRGB; fmt->fmt.pix.priv = 0; return 0; }"
663----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49963/bad/bcm2835-mailbox.c----bcm2835_mbox_probe,"static int bcm2835_mbox_probe(struct platform_device *pdev) { struct device *dev = &pdev->dev; int ret = 0; struct resource *iomem; struct bcm2835_mbox *mbox; mbox = devm_kzalloc(dev, sizeof(*mbox), GFP_KERNEL); if (mbox == NULL) return -ENOMEM; spin_lock_init(&mbox->lock); ret = devm_request_irq(dev, irq_of_parse_and_map(dev->of_node, 0), <S2SV_StartVul> bcm2835_mbox_irq, 0, dev_name(dev), mbox); <S2SV_EndVul> if (ret) { dev_err(dev, ""Failed to register a mailbox IRQ handler: %d\n"", ret); return -ENODEV; } iomem = platform_get_resource(pdev, IORESOURCE_MEM, 0); mbox->regs = devm_ioremap_resource(&pdev->dev, iomem); if (IS_ERR(mbox->regs)) { ret = PTR_ERR(mbox->regs); dev_err(&pdev->dev, ""Failed to remap mailbox regs: %d\n"", ret); return ret; } mbox->controller.txdone_poll = true; mbox->controller.txpoll_period = 5; mbox->controller.ops = &bcm2835_mbox_chan_ops; mbox->controller.of_xlate = &bcm2835_mbox_index_xlate; mbox->controller.dev = dev; mbox->controller.num_chans = 1; mbox->controller.chans = devm_kzalloc(dev, sizeof(*mbox->controller.chans), GFP_KERNEL); if (!mbox->controller.chans) return -ENOMEM; ret = devm_mbox_controller_register(dev, &mbox->controller); if (ret) return ret; platform_set_drvdata(pdev, mbox); dev_info(dev, ""mailbox enabled\n""); return ret; }","- bcm2835_mbox_irq, 0, dev_name(dev), mbox);
+ bcm2835_mbox_irq, IRQF_NO_SUSPEND, dev_name(dev),
+ mbox);","static int bcm2835_mbox_probe(struct platform_device *pdev) { struct device *dev = &pdev->dev; int ret = 0; struct resource *iomem; struct bcm2835_mbox *mbox; mbox = devm_kzalloc(dev, sizeof(*mbox), GFP_KERNEL); if (mbox == NULL) return -ENOMEM; spin_lock_init(&mbox->lock); ret = devm_request_irq(dev, irq_of_parse_and_map(dev->of_node, 0), bcm2835_mbox_irq, IRQF_NO_SUSPEND, dev_name(dev), mbox); if (ret) { dev_err(dev, ""Failed to register a mailbox IRQ handler: %d\n"", ret); return -ENODEV; } iomem = platform_get_resource(pdev, IORESOURCE_MEM, 0); mbox->regs = devm_ioremap_resource(&pdev->dev, iomem); if (IS_ERR(mbox->regs)) { ret = PTR_ERR(mbox->regs); dev_err(&pdev->dev, ""Failed to remap mailbox regs: %d\n"", ret); return ret; } mbox->controller.txdone_poll = true; mbox->controller.txpoll_period = 5; mbox->controller.ops = &bcm2835_mbox_chan_ops; mbox->controller.of_xlate = &bcm2835_mbox_index_xlate; mbox->controller.dev = dev; mbox->controller.num_chans = 1; mbox->controller.chans = devm_kzalloc(dev, sizeof(*mbox->controller.chans), GFP_KERNEL); if (!mbox->controller.chans) return -ENOMEM; ret = devm_mbox_controller_register(dev, &mbox->controller); if (ret) return ret; platform_set_drvdata(pdev, mbox); dev_info(dev, ""mailbox enabled\n""); return ret; }"
1261----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56542/bad/amdgpu_dm_helpers.c----dm_helpers_free_gpu_mem,"void dm_helpers_free_gpu_mem( struct dc_context *ctx, enum dc_gpu_mem_alloc_type type, void *pvMem) { struct amdgpu_device *adev = ctx->driver_context; <S2SV_StartVul> struct dal_allocation *da; <S2SV_EndVul> <S2SV_StartVul> list_for_each_entry(da, &adev->dm.da_list, list) { <S2SV_EndVul> <S2SV_StartVul> if (pvMem == da->cpu_ptr) { <S2SV_EndVul> <S2SV_StartVul> amdgpu_bo_free_kernel(&da->bo, &da->gpu_addr, &da->cpu_ptr); <S2SV_EndVul> <S2SV_StartVul> list_del(&da->list); <S2SV_EndVul> <S2SV_StartVul> kfree(da); <S2SV_EndVul> <S2SV_StartVul> break; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul>","- struct dal_allocation *da;
- list_for_each_entry(da, &adev->dm.da_list, list) {
- if (pvMem == da->cpu_ptr) {
- amdgpu_bo_free_kernel(&da->bo, &da->gpu_addr, &da->cpu_ptr);
- list_del(&da->list);
- kfree(da);
- break;
- }
- }
- }
+ dm_free_gpu_mem(adev, type, pvMem);","void dm_helpers_free_gpu_mem( struct dc_context *ctx, enum dc_gpu_mem_alloc_type type, void *pvMem) { struct amdgpu_device *adev = ctx->driver_context; dm_free_gpu_mem(adev, type, pvMem); }"
392----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46853/bad/spi-nxp-fspi.c----nxp_fspi_fill_txfifo,"static void nxp_fspi_fill_txfifo(struct nxp_fspi *f, const struct spi_mem_op *op) { void __iomem *base = f->iobase; int i, ret; u8 *buf = (u8 *) op->data.buf.out; fspi_writel(f, FSPI_IPTXFCR_CLR, base + FSPI_IPTXFCR); for (i = 0; i < ALIGN_DOWN(op->data.nbytes, 8); i += 8) { ret = fspi_readl_poll_tout(f, f->iobase + FSPI_INTR, FSPI_INTR_IPTXWE, 0, POLL_TOUT, true); WARN_ON(ret); fspi_writel(f, *(u32 *) (buf + i), base + FSPI_TFDR); fspi_writel(f, *(u32 *) (buf + i + 4), base + FSPI_TFDR + 4); fspi_writel(f, FSPI_INTR_IPTXWE, base + FSPI_INTR); } if (i < op->data.nbytes) { u32 data = 0; int j; ret = fspi_readl_poll_tout(f, f->iobase + FSPI_INTR, FSPI_INTR_IPTXWE, 0, POLL_TOUT, true); WARN_ON(ret); <S2SV_StartVul> for (j = 0; j < ALIGN(op->data.nbytes - i, 4); j += 4) { <S2SV_EndVul> <S2SV_StartVul> memcpy(&data, buf + i + j, 4); <S2SV_EndVul> fspi_writel(f, data, base + FSPI_TFDR + j); } fspi_writel(f, FSPI_INTR_IPTXWE, base + FSPI_INTR); } }","- for (j = 0; j < ALIGN(op->data.nbytes - i, 4); j += 4) {
- memcpy(&data, buf + i + j, 4);
+ int remaining = op->data.nbytes - i;
+ for (j = 0; j < ALIGN(remaining, 4); j += 4) {
+ memcpy(&data, buf + i + j, min_t(int, 4, remaining - j));","static void nxp_fspi_fill_txfifo(struct nxp_fspi *f, const struct spi_mem_op *op) { void __iomem *base = f->iobase; int i, ret; u8 *buf = (u8 *) op->data.buf.out; fspi_writel(f, FSPI_IPTXFCR_CLR, base + FSPI_IPTXFCR); for (i = 0; i < ALIGN_DOWN(op->data.nbytes, 8); i += 8) { ret = fspi_readl_poll_tout(f, f->iobase + FSPI_INTR, FSPI_INTR_IPTXWE, 0, POLL_TOUT, true); WARN_ON(ret); fspi_writel(f, *(u32 *) (buf + i), base + FSPI_TFDR); fspi_writel(f, *(u32 *) (buf + i + 4), base + FSPI_TFDR + 4); fspi_writel(f, FSPI_INTR_IPTXWE, base + FSPI_INTR); } if (i < op->data.nbytes) { u32 data = 0; int j; int remaining = op->data.nbytes - i; ret = fspi_readl_poll_tout(f, f->iobase + FSPI_INTR, FSPI_INTR_IPTXWE, 0, POLL_TOUT, true); WARN_ON(ret); for (j = 0; j < ALIGN(remaining, 4); j += 4) { memcpy(&data, buf + i + j, min_t(int, 4, remaining - j)); fspi_writel(f, data, base + FSPI_TFDR + j); } fspi_writel(f, FSPI_INTR_IPTXWE, base + FSPI_INTR); } }"
1169----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53141/bad/ip_set_bitmap_ip.c----bitmap_ip_uadt,"bitmap_ip_uadt(struct ip_set *set, struct nlattr *tb[], enum ipset_adt adt, u32 *lineno, u32 flags, bool retried) { struct bitmap_ip *map = set->data; ipset_adtfn adtfn = set->variant->adt[adt]; u32 ip = 0, ip_to = 0; struct bitmap_ip_adt_elem e = { .id = 0 }; struct ip_set_ext ext = IP_SET_INIT_UEXT(set); int ret = 0; if (tb[IPSET_ATTR_LINENO]) *lineno = nla_get_u32(tb[IPSET_ATTR_LINENO]); if (unlikely(!tb[IPSET_ATTR_IP])) return -IPSET_ERR_PROTOCOL; ret = ip_set_get_hostipaddr4(tb[IPSET_ATTR_IP], &ip); if (ret) return ret; ret = ip_set_get_extensions(set, tb, &ext); if (ret) return ret; if (ip < map->first_ip || ip > map->last_ip) return -IPSET_ERR_BITMAP_RANGE; if (adt == IPSET_TEST) { e.id = ip_to_id(map, ip); return adtfn(set, &e, &ext, &ext, flags); } if (tb[IPSET_ATTR_IP_TO]) { ret = ip_set_get_hostipaddr4(tb[IPSET_ATTR_IP_TO], &ip_to); if (ret) return ret; <S2SV_StartVul> if (ip > ip_to) { <S2SV_EndVul> swap(ip, ip_to); <S2SV_StartVul> if (ip < map->first_ip) <S2SV_EndVul> <S2SV_StartVul> return -IPSET_ERR_BITMAP_RANGE; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> } else if (tb[IPSET_ATTR_CIDR]) { u8 cidr = nla_get_u8(tb[IPSET_ATTR_CIDR]); if (!cidr || cidr > HOST_MASK) return -IPSET_ERR_INVALID_CIDR; ip_set_mask_from_to(ip, ip_to, cidr); } else { ip_to = ip; <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> if (ip_to > map->last_ip) <S2SV_EndVul> <S2SV_StartVul> return -IPSET_ERR_BITMAP_RANGE; <S2SV_EndVul> for (; !before(ip_to, ip); ip += map->hosts) { e.id = ip_to_id(map, ip); ret = adtfn(set, &e, &ext, &ext, flags); if (ret && !ip_set_eexist(ret, flags)) return ret; ret = 0; } return ret; }","- if (ip > ip_to) {
- if (ip < map->first_ip)
- return -IPSET_ERR_BITMAP_RANGE;
- }
- }
- if (ip_to > map->last_ip)
- return -IPSET_ERR_BITMAP_RANGE;
+ if (ip > ip_to)
+ if (ip < map->first_ip || ip_to > map->last_ip)","bitmap_ip_uadt(struct ip_set *set, struct nlattr *tb[], enum ipset_adt adt, u32 *lineno, u32 flags, bool retried) { struct bitmap_ip *map = set->data; ipset_adtfn adtfn = set->variant->adt[adt]; u32 ip = 0, ip_to = 0; struct bitmap_ip_adt_elem e = { .id = 0 }; struct ip_set_ext ext = IP_SET_INIT_UEXT(set); int ret = 0; if (tb[IPSET_ATTR_LINENO]) *lineno = nla_get_u32(tb[IPSET_ATTR_LINENO]); if (unlikely(!tb[IPSET_ATTR_IP])) return -IPSET_ERR_PROTOCOL; ret = ip_set_get_hostipaddr4(tb[IPSET_ATTR_IP], &ip); if (ret) return ret; ret = ip_set_get_extensions(set, tb, &ext); if (ret) return ret; if (ip < map->first_ip || ip > map->last_ip) return -IPSET_ERR_BITMAP_RANGE; if (adt == IPSET_TEST) { e.id = ip_to_id(map, ip); return adtfn(set, &e, &ext, &ext, flags); } if (tb[IPSET_ATTR_IP_TO]) { ret = ip_set_get_hostipaddr4(tb[IPSET_ATTR_IP_TO], &ip_to); if (ret) return ret; if (ip > ip_to) swap(ip, ip_to); } else if (tb[IPSET_ATTR_CIDR]) { u8 cidr = nla_get_u8(tb[IPSET_ATTR_CIDR]); if (!cidr || cidr > HOST_MASK) return -IPSET_ERR_INVALID_CIDR; ip_set_mask_from_to(ip, ip_to, cidr); } else { ip_to = ip; } if (ip < map->first_ip || ip_to > map->last_ip) return -IPSET_ERR_BITMAP_RANGE; for (; !before(ip_to, ip); ip += map->hosts) { e.id = ip_to_id(map, ip); ret = adtfn(set, &e, &ext, &ext, flags); if (ret && !ip_set_eexist(ret, flags)) return ret; ret = 0; } return ret; }"
142----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-45002/bad/osnoise_top.c----*osnoise_init_top,"struct osnoise_tool *osnoise_init_top(struct osnoise_top_params *params) { struct osnoise_tool *tool; int nr_cpus; nr_cpus = sysconf(_SC_NPROCESSORS_CONF); tool = osnoise_init_tool(""osnoise_top""); if (!tool) return NULL; tool->data = osnoise_alloc_top(nr_cpus); <S2SV_StartVul> if (!tool->data) <S2SV_EndVul> <S2SV_StartVul> goto out_err; <S2SV_EndVul> tool->params = params; tep_register_event_handler(tool->trace.tep, -1, ""ftrace"", ""osnoise"", osnoise_top_handler, NULL); return tool; <S2SV_StartVul> out_err: <S2SV_EndVul> <S2SV_StartVul> osnoise_free_top(tool->data); <S2SV_EndVul> <S2SV_StartVul> osnoise_destroy_tool(tool); <S2SV_EndVul> <S2SV_StartVul> return NULL; <S2SV_EndVul> }","- if (!tool->data)
- goto out_err;
- out_err:
- osnoise_free_top(tool->data);
- osnoise_destroy_tool(tool);
- return NULL;
+ if (!tool->data) {
+ osnoise_destroy_tool(tool);
+ return NULL;
+ }
+ }","struct osnoise_tool *osnoise_init_top(struct osnoise_top_params *params) { struct osnoise_tool *tool; int nr_cpus; nr_cpus = sysconf(_SC_NPROCESSORS_CONF); tool = osnoise_init_tool(""osnoise_top""); if (!tool) return NULL; tool->data = osnoise_alloc_top(nr_cpus); if (!tool->data) { osnoise_destroy_tool(tool); return NULL; } tool->params = params; tep_register_event_handler(tool->trace.tep, -1, ""ftrace"", ""osnoise"", osnoise_top_handler, NULL); return tool; }"
126----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44984/bad/bnxt_xdp.c----bnxt_rx_xdp,"bool bnxt_rx_xdp(struct bnxt *bp, struct bnxt_rx_ring_info *rxr, u16 cons, struct xdp_buff xdp, struct page *page, u8 **data_ptr, unsigned int *len, u8 *event) { struct bpf_prog *xdp_prog = READ_ONCE(rxr->xdp_prog); struct bnxt_tx_ring_info *txr; struct bnxt_sw_rx_bd *rx_buf; struct pci_dev *pdev; dma_addr_t mapping; u32 tx_needed = 1; void *orig_data; u32 tx_avail; u32 offset; u32 act; if (!xdp_prog) return false; pdev = bp->pdev; offset = bp->rx_offset; txr = rxr->bnapi->tx_ring; orig_data = xdp.data; act = bpf_prog_run_xdp(xdp_prog, &xdp); tx_avail = bnxt_tx_avail(bp, txr); if (tx_avail != bp->tx_ring_size) *event &= ~BNXT_RX_EVENT; *len = xdp.data_end - xdp.data; if (orig_data != xdp.data) { offset = xdp.data - xdp.data_hard_start; *data_ptr = xdp.data_hard_start + offset; } switch (act) { case XDP_PASS: return false; case XDP_TX: rx_buf = &rxr->rx_buf_ring[cons]; mapping = rx_buf->mapping - bp->rx_dma_offset; *event = 0; if (unlikely(xdp_buff_has_frags(&xdp))) { struct skb_shared_info *sinfo = xdp_get_shared_info_from_buff(&xdp); tx_needed += sinfo->nr_frags; *event = BNXT_AGG_EVENT; } if (tx_avail < tx_needed) { trace_xdp_exception(bp->dev, xdp_prog, act); bnxt_xdp_buff_frags_free(rxr, &xdp); bnxt_reuse_rx_data(rxr, cons, page); return true; } dma_sync_single_for_device(&pdev->dev, mapping + offset, *len, bp->rx_dir); *event |= BNXT_TX_EVENT; __bnxt_xmit_xdp(bp, txr, mapping + offset, *len, NEXT_RX(rxr->rx_prod), &xdp); bnxt_reuse_rx_data(rxr, cons, page); return true; case XDP_REDIRECT: <S2SV_StartVul> rx_buf = &rxr->rx_buf_ring[cons]; <S2SV_EndVul> <S2SV_StartVul> mapping = rx_buf->mapping - bp->rx_dma_offset; <S2SV_EndVul> <S2SV_StartVul> dma_unmap_page_attrs(&pdev->dev, mapping, <S2SV_EndVul> <S2SV_StartVul> BNXT_RX_PAGE_SIZE, bp->rx_dir, <S2SV_EndVul> <S2SV_StartVul> DMA_ATTR_WEAK_ORDERING); <S2SV_EndVul> if (bnxt_alloc_rx_data(bp, rxr, rxr->rx_prod, GFP_ATOMIC)) { trace_xdp_exception(bp->dev, xdp_prog, act); bnxt_xdp_buff_frags_free(rxr, &xdp); bnxt_reuse_rx_data(rxr, cons, page); return true; } if (xdp_do_redirect(bp->dev, &xdp, xdp_prog)) { trace_xdp_exception(bp->dev, xdp_prog, act); page_pool_recycle_direct(rxr->page_pool, page); return true; } *event |= BNXT_REDIRECT_EVENT; break; default: bpf_warn_invalid_xdp_action(bp->dev, xdp_prog, act); fallthrough; case XDP_ABORTED: trace_xdp_exception(bp->dev, xdp_prog, act); fallthrough; case XDP_DROP: bnxt_xdp_buff_frags_free(rxr, &xdp); bnxt_reuse_rx_data(rxr, cons, page); break; } return true; }","- rx_buf = &rxr->rx_buf_ring[cons];
- mapping = rx_buf->mapping - bp->rx_dma_offset;
- dma_unmap_page_attrs(&pdev->dev, mapping,
- BNXT_RX_PAGE_SIZE, bp->rx_dir,
- DMA_ATTR_WEAK_ORDERING);","bool bnxt_rx_xdp(struct bnxt *bp, struct bnxt_rx_ring_info *rxr, u16 cons, struct xdp_buff xdp, struct page *page, u8 **data_ptr, unsigned int *len, u8 *event) { struct bpf_prog *xdp_prog = READ_ONCE(rxr->xdp_prog); struct bnxt_tx_ring_info *txr; struct bnxt_sw_rx_bd *rx_buf; struct pci_dev *pdev; dma_addr_t mapping; u32 tx_needed = 1; void *orig_data; u32 tx_avail; u32 offset; u32 act; if (!xdp_prog) return false; pdev = bp->pdev; offset = bp->rx_offset; txr = rxr->bnapi->tx_ring; orig_data = xdp.data; act = bpf_prog_run_xdp(xdp_prog, &xdp); tx_avail = bnxt_tx_avail(bp, txr); if (tx_avail != bp->tx_ring_size) *event &= ~BNXT_RX_EVENT; *len = xdp.data_end - xdp.data; if (orig_data != xdp.data) { offset = xdp.data - xdp.data_hard_start; *data_ptr = xdp.data_hard_start + offset; } switch (act) { case XDP_PASS: return false; case XDP_TX: rx_buf = &rxr->rx_buf_ring[cons]; mapping = rx_buf->mapping - bp->rx_dma_offset; *event = 0; if (unlikely(xdp_buff_has_frags(&xdp))) { struct skb_shared_info *sinfo = xdp_get_shared_info_from_buff(&xdp); tx_needed += sinfo->nr_frags; *event = BNXT_AGG_EVENT; } if (tx_avail < tx_needed) { trace_xdp_exception(bp->dev, xdp_prog, act); bnxt_xdp_buff_frags_free(rxr, &xdp); bnxt_reuse_rx_data(rxr, cons, page); return true; } dma_sync_single_for_device(&pdev->dev, mapping + offset, *len, bp->rx_dir); *event |= BNXT_TX_EVENT; __bnxt_xmit_xdp(bp, txr, mapping + offset, *len, NEXT_RX(rxr->rx_prod), &xdp); bnxt_reuse_rx_data(rxr, cons, page); return true; case XDP_REDIRECT: if (bnxt_alloc_rx_data(bp, rxr, rxr->rx_prod, GFP_ATOMIC)) { trace_xdp_exception(bp->dev, xdp_prog, act); bnxt_xdp_buff_frags_free(rxr, &xdp); bnxt_reuse_rx_data(rxr, cons, page); return true; } if (xdp_do_redirect(bp->dev, &xdp, xdp_prog)) { trace_xdp_exception(bp->dev, xdp_prog, act); page_pool_recycle_direct(rxr->page_pool, page); return true; } *event |= BNXT_REDIRECT_EVENT; break; default: bpf_warn_invalid_xdp_action(bp->dev, xdp_prog, act); fallthrough; case XDP_ABORTED: trace_xdp_exception(bp->dev, xdp_prog, act); fallthrough; case XDP_DROP: bnxt_xdp_buff_frags_free(rxr, &xdp); bnxt_reuse_rx_data(rxr, cons, page); break; } return true; }"
815----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50086/bad/user_session.c----*ksmbd_session_lookup_slowpath,struct ksmbd_session *ksmbd_session_lookup_slowpath(unsigned long long id) { struct ksmbd_session *sess; down_read(&sessions_table_lock); sess = __session_lookup(id); <S2SV_StartVul> if (sess) <S2SV_EndVul> <S2SV_StartVul> sess->last_active = jiffies; <S2SV_EndVul> up_read(&sessions_table_lock); return sess; },"- if (sess)
- sess->last_active = jiffies;",struct ksmbd_session *ksmbd_session_lookup_slowpath(unsigned long long id) { struct ksmbd_session *sess; down_read(&sessions_table_lock); sess = __session_lookup(id); up_read(&sessions_table_lock); return sess; }
1285----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56582/bad/inode.c----btrfs_encoded_read_endio,"static void btrfs_encoded_read_endio(struct btrfs_bio *bbio) { struct btrfs_encoded_read_private *priv = bbio->private; if (bbio->bio.bi_status) { WRITE_ONCE(priv->status, bbio->bio.bi_status); } <S2SV_StartVul> if (!atomic_dec_return(&priv->pending)) <S2SV_EndVul> wake_up(&priv->wait); bio_put(&bbio->bio); }","- if (!atomic_dec_return(&priv->pending))
+ if (atomic_dec_and_test(&priv->pending))","static void btrfs_encoded_read_endio(struct btrfs_bio *bbio) { struct btrfs_encoded_read_private *priv = bbio->private; if (bbio->bio.bi_status) { WRITE_ONCE(priv->status, bbio->bio.bi_status); } if (atomic_dec_and_test(&priv->pending)) wake_up(&priv->wait); bio_put(&bbio->bio); }"
941----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50189/bad/amd_sfh_client.c----amd_sfh_hid_client_deinit,"int amd_sfh_hid_client_deinit(struct amd_mp2_dev *privdata) { struct amdtp_cl_data *cl_data = privdata->cl_data; <S2SV_StartVul> struct amd_input_data *in_data = cl_data->in_data; <S2SV_EndVul> int i, status; <S2SV_StartVul> for (i = 0; i < cl_data->num_hid_devices; i++) { <S2SV_EndVul> if (cl_data->sensor_sts[i] == SENSOR_ENABLED) { privdata->mp2_ops->stop(privdata, cl_data->sensor_idx[i]); status = amd_sfh_wait_for_response (privdata, cl_data->sensor_idx[i], SENSOR_DISABLED); if (status != SENSOR_ENABLED) cl_data->sensor_sts[i] = SENSOR_DISABLED; dev_dbg(&privdata->pdev->dev, ""stopping sid 0x%x status 0x%x\n"", cl_data->sensor_idx[i], cl_data->sensor_sts[i]); } } cancel_delayed_work_sync(&cl_data->work); cancel_delayed_work_sync(&cl_data->work_buffer); amdtp_hid_remove(cl_data); <S2SV_StartVul> for (i = 0; i < cl_data->num_hid_devices; i++) { <S2SV_EndVul> <S2SV_StartVul> if (in_data->sensor_virt_addr[i]) { <S2SV_EndVul> <S2SV_StartVul> dma_free_coherent(&privdata->pdev->dev, 8 * sizeof(int), <S2SV_EndVul> <S2SV_StartVul> in_data->sensor_virt_addr[i], <S2SV_EndVul> <S2SV_StartVul> cl_data->sensor_dma_addr[i]); <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> return 0; <S2SV_StartVul> } <S2SV_EndVul>","- struct amd_input_data *in_data = cl_data->in_data;
- for (i = 0; i < cl_data->num_hid_devices; i++) {
- for (i = 0; i < cl_data->num_hid_devices; i++) {
- if (in_data->sensor_virt_addr[i]) {
- dma_free_coherent(&privdata->pdev->dev, 8 * sizeof(int),
- in_data->sensor_virt_addr[i],
- cl_data->sensor_dma_addr[i]);
- }
- }
- }","int amd_sfh_hid_client_deinit(struct amd_mp2_dev *privdata) { struct amdtp_cl_data *cl_data = privdata->cl_data; int i, status; for (i = 0; i < cl_data->num_hid_devices; i++) { if (cl_data->sensor_sts[i] == SENSOR_ENABLED) { privdata->mp2_ops->stop(privdata, cl_data->sensor_idx[i]); status = amd_sfh_wait_for_response (privdata, cl_data->sensor_idx[i], SENSOR_DISABLED); if (status != SENSOR_ENABLED) cl_data->sensor_sts[i] = SENSOR_DISABLED; dev_dbg(&privdata->pdev->dev, ""stopping sid 0x%x status 0x%x\n"", cl_data->sensor_idx[i], cl_data->sensor_sts[i]); } } cancel_delayed_work_sync(&cl_data->work); cancel_delayed_work_sync(&cl_data->work_buffer); amdtp_hid_remove(cl_data); return 0; }"
1228----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53239/bad/chip.c----usb6fire_chip_disconnect,static void usb6fire_chip_disconnect(struct usb_interface *intf) { struct sfire_chip *chip; chip = usb_get_intfdata(intf); if (chip) { chip->intf_count--; if (!chip->intf_count) { mutex_lock(&register_mutex); devices[chip->regidx] = NULL; chips[chip->regidx] = NULL; mutex_unlock(&register_mutex); chip->shutdown = true; usb6fire_chip_abort(chip); <S2SV_StartVul> usb6fire_chip_destroy(chip); <S2SV_EndVul> } } },- usb6fire_chip_destroy(chip);,static void usb6fire_chip_disconnect(struct usb_interface *intf) { struct sfire_chip *chip; chip = usb_get_intfdata(intf); if (chip) { chip->intf_count--; if (!chip->intf_count) { mutex_lock(&register_mutex); devices[chip->regidx] = NULL; chips[chip->regidx] = NULL; mutex_unlock(&register_mutex); chip->shutdown = true; usb6fire_chip_abort(chip); } } }
38----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-43907/bad/smu7_hwmgr.c----smu7_apply_state_adjust_rules,"static int smu7_apply_state_adjust_rules(struct pp_hwmgr *hwmgr, struct pp_power_state *request_ps, const struct pp_power_state *current_ps) { struct amdgpu_device *adev = hwmgr->adev; <S2SV_StartVul> struct smu7_power_state *smu7_ps = <S2SV_EndVul> <S2SV_StartVul> cast_phw_smu7_power_state(&request_ps->hardware); <S2SV_EndVul> uint32_t sclk; uint32_t mclk; struct PP_Clocks minimum_clocks = {0}; bool disable_mclk_switching; bool disable_mclk_switching_for_frame_lock; bool disable_mclk_switching_for_display; const struct phm_clock_and_voltage_limits *max_limits; uint32_t i; struct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend); struct phm_ppt_v1_information *table_info = (struct phm_ppt_v1_information *)(hwmgr->pptable); int32_t count; int32_t stable_pstate_sclk = 0, stable_pstate_mclk = 0; uint32_t latency; bool latency_allowed = false; data->battery_state = (PP_StateUILabel_Battery == request_ps->classification.ui_label); data->mclk_ignore_signal = false; max_limits = adev->pm.ac_power ? &(hwmgr->dyn_state.max_clock_voltage_on_ac) : &(hwmgr->dyn_state.max_clock_voltage_on_dc); if (!adev->pm.ac_power) { for (i = 0; i < smu7_ps->performance_level_count; i++) { if (smu7_ps->performance_levels[i].memory_clock > max_limits->mclk) smu7_ps->performance_levels[i].memory_clock = max_limits->mclk; if (smu7_ps->performance_levels[i].engine_clock > max_limits->sclk) smu7_ps->performance_levels[i].engine_clock = max_limits->sclk; } } minimum_clocks.engineClock = hwmgr->display_config->min_core_set_clock; minimum_clocks.memoryClock = hwmgr->display_config->min_mem_set_clock; if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_StablePState)) { max_limits = &(hwmgr->dyn_state.max_clock_voltage_on_ac); stable_pstate_sclk = (max_limits->sclk * 75) / 100; for (count = table_info->vdd_dep_on_sclk->count - 1; count >= 0; count--) { if (stable_pstate_sclk >= table_info->vdd_dep_on_sclk->entries[count].clk) { stable_pstate_sclk = table_info->vdd_dep_on_sclk->entries[count].clk; break; } } if (count < 0) stable_pstate_sclk = table_info->vdd_dep_on_sclk->entries[0].clk; stable_pstate_mclk = max_limits->mclk; minimum_clocks.engineClock = stable_pstate_sclk; minimum_clocks.memoryClock = stable_pstate_mclk; } disable_mclk_switching_for_frame_lock = phm_cap_enabled( hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_DisableMclkSwitchingForFrameLock); disable_mclk_switching_for_display = ((1 < hwmgr->display_config->num_display) && !hwmgr->display_config->multi_monitor_in_sync) || (hwmgr->display_config->num_display && smu7_vblank_too_short(hwmgr, hwmgr->display_config->min_vblank_time)); disable_mclk_switching = disable_mclk_switching_for_frame_lock || disable_mclk_switching_for_display; if (hwmgr->display_config->num_display == 0) { if (hwmgr->chip_id >= CHIP_POLARIS10 && hwmgr->chip_id <= CHIP_VEGAM) data->mclk_ignore_signal = true; else disable_mclk_switching = false; } sclk = smu7_ps->performance_levels[0].engine_clock; mclk = smu7_ps->performance_levels[0].memory_clock; if (disable_mclk_switching && (!(hwmgr->chip_id >= CHIP_POLARIS10 && hwmgr->chip_id <= CHIP_VEGAM))) mclk = smu7_ps->performance_levels [smu7_ps->performance_level_count - 1].memory_clock; if (sclk < minimum_clocks.engineClock) sclk = (minimum_clocks.engineClock > max_limits->sclk) ? max_limits->sclk : minimum_clocks.engineClock; if (mclk < minimum_clocks.memoryClock) mclk = (minimum_clocks.memoryClock > max_limits->mclk) ? max_limits->mclk : minimum_clocks.memoryClock; smu7_ps->performance_levels[0].engine_clock = sclk; smu7_ps->performance_levels[0].memory_clock = mclk; smu7_ps->performance_levels[1].engine_clock = (smu7_ps->performance_levels[1].engine_clock >= smu7_ps->performance_levels[0].engine_clock) ? smu7_ps->performance_levels[1].engine_clock : smu7_ps->performance_levels[0].engine_clock; if (disable_mclk_switching) { if (mclk < smu7_ps->performance_levels[1].memory_clock) mclk = smu7_ps->performance_levels[1].memory_clock; if (hwmgr->chip_id >= CHIP_POLARIS10 && hwmgr->chip_id <= CHIP_VEGAM) { if (disable_mclk_switching_for_display) { latency = hwmgr->display_config->dce_tolerable_mclk_in_active_latency; for (i = 0; i < data->mclk_latency_table.count; i++) { if (data->mclk_latency_table.entries[i].latency <= latency) { latency_allowed = true; if ((data->mclk_latency_table.entries[i].frequency >= smu7_ps->performance_levels[0].memory_clock) && (data->mclk_latency_table.entries[i].frequency <= smu7_ps->performance_levels[1].memory_clock)) { mclk = data->mclk_latency_table.entries[i].frequency; break; } } } if ((i >= data->mclk_latency_table.count - 1) && !latency_allowed) { data->mclk_ignore_signal = true; } else { data->mclk_ignore_signal = false; } } if (disable_mclk_switching_for_frame_lock) mclk = smu7_ps->performance_levels[1].memory_clock; } smu7_ps->performance_levels[0].memory_clock = mclk; if (!(hwmgr->chip_id >= CHIP_POLARIS10 && hwmgr->chip_id <= CHIP_VEGAM)) smu7_ps->performance_levels[1].memory_clock = mclk; } else { if (smu7_ps->performance_levels[1].memory_clock < smu7_ps->performance_levels[0].memory_clock) smu7_ps->performance_levels[1].memory_clock = smu7_ps->performance_levels[0].memory_clock; } if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_StablePState)) { for (i = 0; i < smu7_ps->performance_level_count; i++) { smu7_ps->performance_levels[i].engine_clock = stable_pstate_sclk; smu7_ps->performance_levels[i].memory_clock = stable_pstate_mclk; smu7_ps->performance_levels[i].pcie_gen = data->pcie_gen_performance.max; smu7_ps->performance_levels[i].pcie_lane = data->pcie_gen_performance.max; } } return 0; }","- struct smu7_power_state *smu7_ps =
- cast_phw_smu7_power_state(&request_ps->hardware);
+ struct smu7_power_state *smu7_ps;
+ smu7_ps = cast_phw_smu7_power_state(&request_ps->hardware);
+ if (!smu7_ps)
+ return -EINVAL;","static int smu7_apply_state_adjust_rules(struct pp_hwmgr *hwmgr, struct pp_power_state *request_ps, const struct pp_power_state *current_ps) { struct amdgpu_device *adev = hwmgr->adev; struct smu7_power_state *smu7_ps; uint32_t sclk; uint32_t mclk; struct PP_Clocks minimum_clocks = {0}; bool disable_mclk_switching; bool disable_mclk_switching_for_frame_lock; bool disable_mclk_switching_for_display; const struct phm_clock_and_voltage_limits *max_limits; uint32_t i; struct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend); struct phm_ppt_v1_information *table_info = (struct phm_ppt_v1_information *)(hwmgr->pptable); int32_t count; int32_t stable_pstate_sclk = 0, stable_pstate_mclk = 0; uint32_t latency; bool latency_allowed = false; smu7_ps = cast_phw_smu7_power_state(&request_ps->hardware); if (!smu7_ps) return -EINVAL; data->battery_state = (PP_StateUILabel_Battery == request_ps->classification.ui_label); data->mclk_ignore_signal = false; max_limits = adev->pm.ac_power ? &(hwmgr->dyn_state.max_clock_voltage_on_ac) : &(hwmgr->dyn_state.max_clock_voltage_on_dc); if (!adev->pm.ac_power) { for (i = 0; i < smu7_ps->performance_level_count; i++) { if (smu7_ps->performance_levels[i].memory_clock > max_limits->mclk) smu7_ps->performance_levels[i].memory_clock = max_limits->mclk; if (smu7_ps->performance_levels[i].engine_clock > max_limits->sclk) smu7_ps->performance_levels[i].engine_clock = max_limits->sclk; } } minimum_clocks.engineClock = hwmgr->display_config->min_core_set_clock; minimum_clocks.memoryClock = hwmgr->display_config->min_mem_set_clock; if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_StablePState)) { max_limits = &(hwmgr->dyn_state.max_clock_voltage_on_ac); stable_pstate_sclk = (max_limits->sclk * 75) / 100; for (count = table_info->vdd_dep_on_sclk->count - 1; count >= 0; count--) { if (stable_pstate_sclk >= table_info->vdd_dep_on_sclk->entries[count].clk) { stable_pstate_sclk = table_info->vdd_dep_on_sclk->entries[count].clk; break; } } if (count < 0) stable_pstate_sclk = table_info->vdd_dep_on_sclk->entries[0].clk; stable_pstate_mclk = max_limits->mclk; minimum_clocks.engineClock = stable_pstate_sclk; minimum_clocks.memoryClock = stable_pstate_mclk; } disable_mclk_switching_for_frame_lock = phm_cap_enabled( hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_DisableMclkSwitchingForFrameLock); disable_mclk_switching_for_display = ((1 < hwmgr->display_config->num_display) && !hwmgr->display_config->multi_monitor_in_sync) || (hwmgr->display_config->num_display && smu7_vblank_too_short(hwmgr, hwmgr->display_config->min_vblank_time)); disable_mclk_switching = disable_mclk_switching_for_frame_lock || disable_mclk_switching_for_display; if (hwmgr->display_config->num_display == 0) { if (hwmgr->chip_id >= CHIP_POLARIS10 && hwmgr->chip_id <= CHIP_VEGAM) data->mclk_ignore_signal = true; else disable_mclk_switching = false; } sclk = smu7_ps->performance_levels[0].engine_clock; mclk = smu7_ps->performance_levels[0].memory_clock; if (disable_mclk_switching && (!(hwmgr->chip_id >= CHIP_POLARIS10 && hwmgr->chip_id <= CHIP_VEGAM))) mclk = smu7_ps->performance_levels [smu7_ps->performance_level_count - 1].memory_clock; if (sclk < minimum_clocks.engineClock) sclk = (minimum_clocks.engineClock > max_limits->sclk) ? max_limits->sclk : minimum_clocks.engineClock; if (mclk < minimum_clocks.memoryClock) mclk = (minimum_clocks.memoryClock > max_limits->mclk) ? max_limits->mclk : minimum_clocks.memoryClock; smu7_ps->performance_levels[0].engine_clock = sclk; smu7_ps->performance_levels[0].memory_clock = mclk; smu7_ps->performance_levels[1].engine_clock = (smu7_ps->performance_levels[1].engine_clock >= smu7_ps->performance_levels[0].engine_clock) ? smu7_ps->performance_levels[1].engine_clock : smu7_ps->performance_levels[0].engine_clock; if (disable_mclk_switching) { if (mclk < smu7_ps->performance_levels[1].memory_clock) mclk = smu7_ps->performance_levels[1].memory_clock; if (hwmgr->chip_id >= CHIP_POLARIS10 && hwmgr->chip_id <= CHIP_VEGAM) { if (disable_mclk_switching_for_display) { latency = hwmgr->display_config->dce_tolerable_mclk_in_active_latency; for (i = 0; i < data->mclk_latency_table.count; i++) { if (data->mclk_latency_table.entries[i].latency <= latency) { latency_allowed = true; if ((data->mclk_latency_table.entries[i].frequency >= smu7_ps->performance_levels[0].memory_clock) && (data->mclk_latency_table.entries[i].frequency <= smu7_ps->performance_levels[1].memory_clock)) { mclk = data->mclk_latency_table.entries[i].frequency; break; } } } if ((i >= data->mclk_latency_table.count - 1) && !latency_allowed) { data->mclk_ignore_signal = true; } else { data->mclk_ignore_signal = false; } } if (disable_mclk_switching_for_frame_lock) mclk = smu7_ps->performance_levels[1].memory_clock; } smu7_ps->performance_levels[0].memory_clock = mclk; if (!(hwmgr->chip_id >= CHIP_POLARIS10 && hwmgr->chip_id <= CHIP_VEGAM)) smu7_ps->performance_levels[1].memory_clock = mclk; } else { if (smu7_ps->performance_levels[1].memory_clock < smu7_ps->performance_levels[0].memory_clock) smu7_ps->performance_levels[1].memory_clock = smu7_ps->performance_levels[0].memory_clock; } if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_StablePState)) { for (i = 0; i < smu7_ps->performance_level_count; i++) { smu7_ps->performance_levels[i].engine_clock = stable_pstate_sclk; smu7_ps->performance_levels[i].memory_clock = stable_pstate_mclk; smu7_ps->performance_levels[i].pcie_gen = data->pcie_gen_performance.max; smu7_ps->performance_levels[i].pcie_lane = data->pcie_gen_performance.max; } } return 0; }"
1119----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53092/bad/virtio_pci_common.c----vp_find_one_vq_msix,"vp_find_one_vq_msix(struct virtio_device *vdev, int queue_idx, vq_callback_t *callback, const char *name, bool ctx, bool slow_path, int *allocated_vectors, enum vp_vq_vector_policy vector_policy, struct virtio_pci_vq_info **p_info) { struct virtio_pci_device *vp_dev = to_vp_device(vdev); struct virtqueue *vq; u16 msix_vec; int err; if (!callback) msix_vec = VIRTIO_MSI_NO_VECTOR; else if (vector_policy == VP_VQ_VECTOR_POLICY_EACH || (vector_policy == VP_VQ_VECTOR_POLICY_SHARED_SLOW && !slow_path)) msix_vec = (*allocated_vectors)++; else if (vector_policy != VP_VQ_VECTOR_POLICY_EACH && slow_path) msix_vec = VP_MSIX_CONFIG_VECTOR; else msix_vec = VP_MSIX_VQ_VECTOR; vq = vp_setup_vq(vdev, queue_idx, callback, name, ctx, msix_vec, p_info); if (IS_ERR(vq)) return vq; if (vector_policy == VP_VQ_VECTOR_POLICY_SHARED || msix_vec == VIRTIO_MSI_NO_VECTOR || vp_is_slow_path_vector(msix_vec)) return vq; snprintf(vp_dev->msix_names[msix_vec], sizeof(*vp_dev->msix_names), ""%s-%s"", dev_name(&vp_dev->vdev.dev), name); err = request_irq(pci_irq_vector(vp_dev->pci_dev, msix_vec), vring_interrupt, 0, vp_dev->msix_names[msix_vec], vq); if (err) { <S2SV_StartVul> vp_del_vq(vq); <S2SV_EndVul> return ERR_PTR(err); } return vq; }","- vp_del_vq(vq);
+ vp_del_vq(vq, *p_info);
+ }","vp_find_one_vq_msix(struct virtio_device *vdev, int queue_idx, vq_callback_t *callback, const char *name, bool ctx, bool slow_path, int *allocated_vectors, enum vp_vq_vector_policy vector_policy, struct virtio_pci_vq_info **p_info) { struct virtio_pci_device *vp_dev = to_vp_device(vdev); struct virtqueue *vq; u16 msix_vec; int err; if (!callback) msix_vec = VIRTIO_MSI_NO_VECTOR; else if (vector_policy == VP_VQ_VECTOR_POLICY_EACH || (vector_policy == VP_VQ_VECTOR_POLICY_SHARED_SLOW && !slow_path)) msix_vec = (*allocated_vectors)++; else if (vector_policy != VP_VQ_VECTOR_POLICY_EACH && slow_path) msix_vec = VP_MSIX_CONFIG_VECTOR; else msix_vec = VP_MSIX_VQ_VECTOR; vq = vp_setup_vq(vdev, queue_idx, callback, name, ctx, msix_vec, p_info); if (IS_ERR(vq)) return vq; if (vector_policy == VP_VQ_VECTOR_POLICY_SHARED || msix_vec == VIRTIO_MSI_NO_VECTOR || vp_is_slow_path_vector(msix_vec)) return vq; snprintf(vp_dev->msix_names[msix_vec], sizeof(*vp_dev->msix_names), ""%s-%s"", dev_name(&vp_dev->vdev.dev), name); err = request_irq(pci_irq_vector(vp_dev->pci_dev, msix_vec), vring_interrupt, 0, vp_dev->msix_names[msix_vec], vq); if (err) { vp_del_vq(vq, *p_info); return ERR_PTR(err); } return vq; }"
68----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44943/bad/huge_memory.c----*follow_devmap_pud,"struct page *follow_devmap_pud(struct vm_area_struct *vma, unsigned long addr, pud_t *pud, int flags, struct dev_pagemap **pgmap) { unsigned long pfn = pud_pfn(*pud); struct mm_struct *mm = vma->vm_mm; struct page *page; int ret; assert_spin_locked(pud_lockptr(mm, pud)); if (flags & FOLL_WRITE && !pud_write(*pud)) return NULL; if (pud_present(*pud) && pud_devmap(*pud)) ; else return NULL; if (flags & FOLL_TOUCH) touch_pud(vma, addr, pud, flags & FOLL_WRITE); if (!(flags & (FOLL_GET | FOLL_PIN))) return ERR_PTR(-EEXIST); pfn += (addr & ~PUD_MASK) >> PAGE_SHIFT; *pgmap = get_dev_pagemap(pfn, *pgmap); if (!*pgmap) return ERR_PTR(-EFAULT); page = pfn_to_page(pfn); <S2SV_StartVul> ret = try_grab_page(page, flags); <S2SV_EndVul> if (ret) page = ERR_PTR(ret); return page; }","- ret = try_grab_page(page, flags);
+ ret = try_grab_folio(page_folio(page), 1, flags);","struct page *follow_devmap_pud(struct vm_area_struct *vma, unsigned long addr, pud_t *pud, int flags, struct dev_pagemap **pgmap) { unsigned long pfn = pud_pfn(*pud); struct mm_struct *mm = vma->vm_mm; struct page *page; int ret; assert_spin_locked(pud_lockptr(mm, pud)); if (flags & FOLL_WRITE && !pud_write(*pud)) return NULL; if (pud_present(*pud) && pud_devmap(*pud)) ; else return NULL; if (flags & FOLL_TOUCH) touch_pud(vma, addr, pud, flags & FOLL_WRITE); if (!(flags & (FOLL_GET | FOLL_PIN))) return ERR_PTR(-EEXIST); pfn += (addr & ~PUD_MASK) >> PAGE_SHIFT; *pgmap = get_dev_pagemap(pfn, *pgmap); if (!*pgmap) return ERR_PTR(-EFAULT); page = pfn_to_page(pfn); ret = try_grab_folio(page_folio(page), 1, flags); if (ret) page = ERR_PTR(ret); return page; }"
1490----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-57915/bad/u_serial.c----gserial_disconnect,"void gserial_disconnect(struct gserial *gser) { struct gs_port *port = gser->ioport; unsigned long flags; if (!port) return; spin_lock_irqsave(&serial_port_lock, flags); spin_lock(&port->port_lock); gs_console_disconnect(port); port->port_line_coding = gser->port_line_coding; port->port_usb = NULL; gser->ioport = NULL; if (port->port.count > 0) { wake_up_interruptible(&port->drain_wait); if (port->port.tty) tty_hangup(port->port.tty); } port->suspended = false; spin_unlock(&port->port_lock); spin_unlock_irqrestore(&serial_port_lock, flags); <S2SV_StartVul> usb_ep_disable(gser->out); <S2SV_EndVul> <S2SV_StartVul> usb_ep_disable(gser->in); <S2SV_EndVul> spin_lock_irqsave(&port->port_lock, flags); if (port->port.count == 0) kfifo_free(&port->port_write_buf); gs_free_requests(gser->out, &port->read_pool, NULL); gs_free_requests(gser->out, &port->read_queue, NULL); gs_free_requests(gser->in, &port->write_pool, NULL); port->read_allocated = port->read_started = port->write_allocated = port->write_started = 0; spin_unlock_irqrestore(&port->port_lock, flags); }","- usb_ep_disable(gser->out);
- usb_ep_disable(gser->in);
+ usb_ep_disable(gser->out);
+ usb_ep_disable(gser->in);","void gserial_disconnect(struct gserial *gser) { struct gs_port *port = gser->ioport; unsigned long flags; if (!port) return; spin_lock_irqsave(&serial_port_lock, flags); spin_lock(&port->port_lock); gs_console_disconnect(port); port->port_line_coding = gser->port_line_coding; usb_ep_disable(gser->out); usb_ep_disable(gser->in); port->port_usb = NULL; gser->ioport = NULL; if (port->port.count > 0) { wake_up_interruptible(&port->drain_wait); if (port->port.tty) tty_hangup(port->port.tty); } port->suspended = false; spin_unlock(&port->port_lock); spin_unlock_irqrestore(&serial_port_lock, flags); spin_lock_irqsave(&port->port_lock, flags); if (port->port.count == 0) kfifo_free(&port->port_write_buf); gs_free_requests(gser->out, &port->read_pool, NULL); gs_free_requests(gser->out, &port->read_queue, NULL); gs_free_requests(gser->in, &port->write_pool, NULL); port->read_allocated = port->read_started = port->write_allocated = port->write_started = 0; spin_unlock_irqrestore(&port->port_lock, flags); }"
1498----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-57949/bad/irq-gic-v3-its.c----its_irq_set_vcpu_affinity,"static int its_irq_set_vcpu_affinity(struct irq_data *d, void *vcpu_info) { struct its_device *its_dev = irq_data_get_irq_chip_data(d); struct its_cmd_info *info = vcpu_info; if (!is_v4(its_dev->its)) return -EINVAL; <S2SV_StartVul> guard(raw_spinlock_irq)(&its_dev->event_map.vlpi_lock); <S2SV_EndVul> if (!info) return its_vlpi_unmap(d); switch (info->cmd_type) { case MAP_VLPI: return its_vlpi_map(d, info); case GET_VLPI: return its_vlpi_get(d, info); case PROP_UPDATE_VLPI: case PROP_UPDATE_AND_INV_VLPI: return its_vlpi_prop_update(d, info); default: return -EINVAL; } }","- guard(raw_spinlock_irq)(&its_dev->event_map.vlpi_lock);
+ guard(raw_spinlock)(&its_dev->event_map.vlpi_lock);","static int its_irq_set_vcpu_affinity(struct irq_data *d, void *vcpu_info) { struct its_device *its_dev = irq_data_get_irq_chip_data(d); struct its_cmd_info *info = vcpu_info; if (!is_v4(its_dev->its)) return -EINVAL; guard(raw_spinlock)(&its_dev->event_map.vlpi_lock); if (!info) return its_vlpi_unmap(d); switch (info->cmd_type) { case MAP_VLPI: return its_vlpi_map(d, info); case GET_VLPI: return its_vlpi_get(d, info); case PROP_UPDATE_VLPI: case PROP_UPDATE_AND_INV_VLPI: return its_vlpi_prop_update(d, info); default: return -EINVAL; } }"
388----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46849/bad/axg-card.c----axg_card_add_tdm_loopback,"static int axg_card_add_tdm_loopback(struct snd_soc_card *card, int *index) { struct meson_card *priv = snd_soc_card_get_drvdata(card); <S2SV_StartVul> struct snd_soc_dai_link *pad = &card->dai_link[*index]; <S2SV_EndVul> struct snd_soc_dai_link *lb; struct snd_soc_dai_link_component *dlc; int ret; ret = meson_card_reallocate_links(card, card->num_links + 1); if (ret) return ret; lb = &card->dai_link[*index + 1]; lb->name = devm_kasprintf(card->dev, GFP_KERNEL, ""%s-lb"", pad->name); if (!lb->name) return -ENOMEM; dlc = devm_kzalloc(card->dev, 2 * sizeof(*dlc), GFP_KERNEL); if (!dlc) return -ENOMEM; lb->cpus = &dlc[0]; lb->codecs = &dlc[1]; lb->num_cpus = 1; lb->num_codecs = 1; lb->stream_name = lb->name; lb->cpus->of_node = pad->cpus->of_node; lb->cpus->dai_name = ""TDM Loopback""; lb->codecs->name = ""snd-soc-dummy""; lb->codecs->dai_name = ""snd-soc-dummy-dai""; lb->dpcm_capture = 1; lb->no_pcm = 1; lb->ops = &axg_card_tdm_be_ops; lb->init = axg_card_tdm_dai_lb_init; priv->link_data[*index + 1] = priv->link_data[*index]; of_node_get(lb->cpus->of_node); *index += 1; return 0; }","- struct snd_soc_dai_link *pad = &card->dai_link[*index];
+ struct snd_soc_dai_link *pad;
+ pad = &card->dai_link[*index];","static int axg_card_add_tdm_loopback(struct snd_soc_card *card, int *index) { struct meson_card *priv = snd_soc_card_get_drvdata(card); struct snd_soc_dai_link *pad; struct snd_soc_dai_link *lb; struct snd_soc_dai_link_component *dlc; int ret; ret = meson_card_reallocate_links(card, card->num_links + 1); if (ret) return ret; pad = &card->dai_link[*index]; lb = &card->dai_link[*index + 1]; lb->name = devm_kasprintf(card->dev, GFP_KERNEL, ""%s-lb"", pad->name); if (!lb->name) return -ENOMEM; dlc = devm_kzalloc(card->dev, 2 * sizeof(*dlc), GFP_KERNEL); if (!dlc) return -ENOMEM; lb->cpus = &dlc[0]; lb->codecs = &dlc[1]; lb->num_cpus = 1; lb->num_codecs = 1; lb->stream_name = lb->name; lb->cpus->of_node = pad->cpus->of_node; lb->cpus->dai_name = ""TDM Loopback""; lb->codecs->name = ""snd-soc-dummy""; lb->codecs->dai_name = ""snd-soc-dummy-dai""; lb->dpcm_capture = 1; lb->no_pcm = 1; lb->ops = &axg_card_tdm_be_ops; lb->init = axg_card_tdm_dai_lb_init; priv->link_data[*index + 1] = priv->link_data[*index]; of_node_get(lb->cpus->of_node); *index += 1; return 0; }"
649----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49951/bad/mgmt.c----__mgmt_power_off,"void __mgmt_power_off(struct hci_dev *hdev) { struct cmd_lookup match = { NULL, hdev }; <S2SV_StartVul> u8 status, zero_cod[] = { 0, 0, 0 }; <S2SV_EndVul> mgmt_pending_foreach(MGMT_OP_SET_POWERED, hdev, settings_rsp, &match); if (hci_dev_test_flag(hdev, HCI_UNREGISTER)) <S2SV_StartVul> status = MGMT_STATUS_INVALID_INDEX; <S2SV_EndVul> else <S2SV_StartVul> status = MGMT_STATUS_NOT_POWERED; <S2SV_EndVul> <S2SV_StartVul> mgmt_pending_foreach(0, hdev, cmd_complete_rsp, &status); <S2SV_EndVul> if (memcmp(hdev->dev_class, zero_cod, sizeof(zero_cod)) != 0) { mgmt_limited_event(MGMT_EV_CLASS_OF_DEV_CHANGED, hdev, zero_cod, sizeof(zero_cod), HCI_MGMT_DEV_CLASS_EVENTS, NULL); ext_info_changed(hdev, NULL); } new_settings(hdev, match.sk); if (match.sk) sock_put(match.sk); }","- u8 status, zero_cod[] = { 0, 0, 0 };
- status = MGMT_STATUS_INVALID_INDEX;
- status = MGMT_STATUS_NOT_POWERED;
- mgmt_pending_foreach(0, hdev, cmd_complete_rsp, &status);
+ u8 zero_cod[] = { 0, 0, 0 };
+ match.mgmt_status = MGMT_STATUS_INVALID_INDEX;
+ match.mgmt_status = MGMT_STATUS_NOT_POWERED;
+ mgmt_pending_foreach(0, hdev, cmd_complete_rsp, &match);","void __mgmt_power_off(struct hci_dev *hdev) { struct cmd_lookup match = { NULL, hdev }; u8 zero_cod[] = { 0, 0, 0 }; mgmt_pending_foreach(MGMT_OP_SET_POWERED, hdev, settings_rsp, &match); if (hci_dev_test_flag(hdev, HCI_UNREGISTER)) match.mgmt_status = MGMT_STATUS_INVALID_INDEX; else match.mgmt_status = MGMT_STATUS_NOT_POWERED; mgmt_pending_foreach(0, hdev, cmd_complete_rsp, &match); if (memcmp(hdev->dev_class, zero_cod, sizeof(zero_cod)) != 0) { mgmt_limited_event(MGMT_EV_CLASS_OF_DEV_CHANGED, hdev, zero_cod, sizeof(zero_cod), HCI_MGMT_DEV_CLASS_EVENTS, NULL); ext_info_changed(hdev, NULL); } new_settings(hdev, match.sk); if (match.sk) sock_put(match.sk); }"
1512----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2025-21669/bad/virtio_transport_common.c----virtio_transport_recv_pkt,"void virtio_transport_recv_pkt(struct virtio_transport *t, struct sk_buff *skb) { struct virtio_vsock_hdr *hdr = virtio_vsock_hdr(skb); struct sockaddr_vm src, dst; struct vsock_sock *vsk; struct sock *sk; bool space_available; vsock_addr_init(&src, le64_to_cpu(hdr->src_cid), le32_to_cpu(hdr->src_port)); vsock_addr_init(&dst, le64_to_cpu(hdr->dst_cid), le32_to_cpu(hdr->dst_port)); trace_virtio_transport_recv_pkt(src.svm_cid, src.svm_port, dst.svm_cid, dst.svm_port, le32_to_cpu(hdr->len), le16_to_cpu(hdr->type), le16_to_cpu(hdr->op), le32_to_cpu(hdr->flags), le32_to_cpu(hdr->buf_alloc), le32_to_cpu(hdr->fwd_cnt)); if (!virtio_transport_valid_type(le16_to_cpu(hdr->type))) { (void)virtio_transport_reset_no_sock(t, skb); goto free_pkt; } sk = vsock_find_connected_socket(&src, &dst); if (!sk) { sk = vsock_find_bound_socket(&dst); if (!sk) { (void)virtio_transport_reset_no_sock(t, skb); goto free_pkt; } } if (virtio_transport_get_type(sk) != le16_to_cpu(hdr->type)) { (void)virtio_transport_reset_no_sock(t, skb); sock_put(sk); goto free_pkt; } if (!skb_set_owner_sk_safe(skb, sk)) { WARN_ONCE(1, ""receiving vsock socket has sk_refcnt == 0\n""); goto free_pkt; } vsk = vsock_sk(sk); lock_sock(sk); <S2SV_StartVul> if (sock_flag(sk, SOCK_DONE)) { <S2SV_EndVul> (void)virtio_transport_reset_no_sock(t, skb); release_sock(sk); sock_put(sk); goto free_pkt; } space_available = virtio_transport_space_update(sk, skb); if (vsk->local_addr.svm_cid != VMADDR_CID_ANY) vsk->local_addr.svm_cid = dst.svm_cid; if (space_available) sk->sk_write_space(sk); switch (sk->sk_state) { case TCP_LISTEN: virtio_transport_recv_listen(sk, skb, t); kfree_skb(skb); break; case TCP_SYN_SENT: virtio_transport_recv_connecting(sk, skb); kfree_skb(skb); break; case TCP_ESTABLISHED: virtio_transport_recv_connected(sk, skb); break; case TCP_CLOSING: virtio_transport_recv_disconnecting(sk, skb); kfree_skb(skb); break; default: (void)virtio_transport_reset_no_sock(t, skb); kfree_skb(skb); break; } release_sock(sk); sock_put(sk); return; free_pkt: kfree_skb(skb); }","- if (sock_flag(sk, SOCK_DONE)) {
+ if (sock_flag(sk, SOCK_DONE) ||
+ (sk->sk_state != TCP_LISTEN && vsk->transport != &t->transport)) {","void virtio_transport_recv_pkt(struct virtio_transport *t, struct sk_buff *skb) { struct virtio_vsock_hdr *hdr = virtio_vsock_hdr(skb); struct sockaddr_vm src, dst; struct vsock_sock *vsk; struct sock *sk; bool space_available; vsock_addr_init(&src, le64_to_cpu(hdr->src_cid), le32_to_cpu(hdr->src_port)); vsock_addr_init(&dst, le64_to_cpu(hdr->dst_cid), le32_to_cpu(hdr->dst_port)); trace_virtio_transport_recv_pkt(src.svm_cid, src.svm_port, dst.svm_cid, dst.svm_port, le32_to_cpu(hdr->len), le16_to_cpu(hdr->type), le16_to_cpu(hdr->op), le32_to_cpu(hdr->flags), le32_to_cpu(hdr->buf_alloc), le32_to_cpu(hdr->fwd_cnt)); if (!virtio_transport_valid_type(le16_to_cpu(hdr->type))) { (void)virtio_transport_reset_no_sock(t, skb); goto free_pkt; } sk = vsock_find_connected_socket(&src, &dst); if (!sk) { sk = vsock_find_bound_socket(&dst); if (!sk) { (void)virtio_transport_reset_no_sock(t, skb); goto free_pkt; } } if (virtio_transport_get_type(sk) != le16_to_cpu(hdr->type)) { (void)virtio_transport_reset_no_sock(t, skb); sock_put(sk); goto free_pkt; } if (!skb_set_owner_sk_safe(skb, sk)) { WARN_ONCE(1, ""receiving vsock socket has sk_refcnt == 0\n""); goto free_pkt; } vsk = vsock_sk(sk); lock_sock(sk); if (sock_flag(sk, SOCK_DONE) || (sk->sk_state != TCP_LISTEN && vsk->transport != &t->transport)) { (void)virtio_transport_reset_no_sock(t, skb); release_sock(sk); sock_put(sk); goto free_pkt; } space_available = virtio_transport_space_update(sk, skb); if (vsk->local_addr.svm_cid != VMADDR_CID_ANY) vsk->local_addr.svm_cid = dst.svm_cid; if (space_available) sk->sk_write_space(sk); switch (sk->sk_state) { case TCP_LISTEN: virtio_transport_recv_listen(sk, skb, t); kfree_skb(skb); break; case TCP_SYN_SENT: virtio_transport_recv_connecting(sk, skb); kfree_skb(skb); break; case TCP_ESTABLISHED: virtio_transport_recv_connected(sk, skb); break; case TCP_CLOSING: virtio_transport_recv_disconnecting(sk, skb); kfree_skb(skb); break; default: (void)virtio_transport_reset_no_sock(t, skb); kfree_skb(skb); break; } release_sock(sk); sock_put(sk); return; free_pkt: kfree_skb(skb); }"
1033----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50274/bad/idpf_lib.c----idpf_initiate_soft_reset,"int idpf_initiate_soft_reset(struct idpf_vport *vport, enum idpf_vport_reset_cause reset_cause) { struct idpf_netdev_priv *np = netdev_priv(vport->netdev); enum idpf_vport_state current_state = np->state; struct idpf_adapter *adapter = vport->adapter; struct idpf_vport *new_vport; int err; new_vport = kzalloc(sizeof(*vport), GFP_KERNEL); if (!new_vport) return -ENOMEM; <S2SV_StartVul> memcpy(new_vport, vport, offsetof(struct idpf_vport, link_speed_mbps)); <S2SV_EndVul> switch (reset_cause) { case IDPF_SR_Q_CHANGE: err = idpf_vport_adjust_qs(new_vport); if (err) goto free_vport; break; case IDPF_SR_Q_DESC_CHANGE: idpf_vport_calc_num_q_desc(new_vport); break; case IDPF_SR_MTU_CHANGE: case IDPF_SR_RSC_CHANGE: break; default: dev_err(&adapter->pdev->dev, ""Unhandled soft reset cause\n""); err = -EINVAL; goto free_vport; } if (current_state <= __IDPF_VPORT_DOWN) { idpf_send_delete_queues_msg(vport); } else { set_bit(IDPF_VPORT_DEL_QUEUES, vport->flags); idpf_vport_stop(vport); } idpf_deinit_rss(vport); err = idpf_send_add_queues_msg(vport, new_vport->num_txq, new_vport->num_complq, new_vport->num_rxq, new_vport->num_bufq); if (err) goto err_reset; <S2SV_StartVul> memcpy(vport, new_vport, offsetof(struct idpf_vport, link_speed_mbps)); <S2SV_EndVul> if (reset_cause == IDPF_SR_Q_CHANGE) idpf_vport_alloc_vec_indexes(vport); err = idpf_set_real_num_queues(vport); if (err) goto err_open; if (current_state == __IDPF_VPORT_UP) err = idpf_vport_open(vport); kfree(new_vport); return err; err_reset: idpf_send_add_queues_msg(vport, vport->num_txq, vport->num_complq, vport->num_rxq, vport->num_bufq); err_open: if (current_state == __IDPF_VPORT_UP) idpf_vport_open(vport); free_vport: kfree(new_vport); return err; }","- memcpy(new_vport, vport, offsetof(struct idpf_vport, link_speed_mbps));
- memcpy(vport, new_vport, offsetof(struct idpf_vport, link_speed_mbps));
+ memcpy(new_vport, vport, offsetof(struct idpf_vport, link_up));
+ memcpy(vport, new_vport, offsetof(struct idpf_vport, link_up));","int idpf_initiate_soft_reset(struct idpf_vport *vport, enum idpf_vport_reset_cause reset_cause) { struct idpf_netdev_priv *np = netdev_priv(vport->netdev); enum idpf_vport_state current_state = np->state; struct idpf_adapter *adapter = vport->adapter; struct idpf_vport *new_vport; int err; new_vport = kzalloc(sizeof(*vport), GFP_KERNEL); if (!new_vport) return -ENOMEM; memcpy(new_vport, vport, offsetof(struct idpf_vport, link_up)); switch (reset_cause) { case IDPF_SR_Q_CHANGE: err = idpf_vport_adjust_qs(new_vport); if (err) goto free_vport; break; case IDPF_SR_Q_DESC_CHANGE: idpf_vport_calc_num_q_desc(new_vport); break; case IDPF_SR_MTU_CHANGE: case IDPF_SR_RSC_CHANGE: break; default: dev_err(&adapter->pdev->dev, ""Unhandled soft reset cause\n""); err = -EINVAL; goto free_vport; } if (current_state <= __IDPF_VPORT_DOWN) { idpf_send_delete_queues_msg(vport); } else { set_bit(IDPF_VPORT_DEL_QUEUES, vport->flags); idpf_vport_stop(vport); } idpf_deinit_rss(vport); err = idpf_send_add_queues_msg(vport, new_vport->num_txq, new_vport->num_complq, new_vport->num_rxq, new_vport->num_bufq); if (err) goto err_reset; memcpy(vport, new_vport, offsetof(struct idpf_vport, link_up)); if (reset_cause == IDPF_SR_Q_CHANGE) idpf_vport_alloc_vec_indexes(vport); err = idpf_set_real_num_queues(vport); if (err) goto err_open; if (current_state == __IDPF_VPORT_UP) err = idpf_vport_open(vport); kfree(new_vport); return err; err_reset: idpf_send_add_queues_msg(vport, vport->num_txq, vport->num_complq, vport->num_rxq, vport->num_bufq); err_open: if (current_state == __IDPF_VPORT_UP) idpf_vport_open(vport); free_vport: kfree(new_vport); return err; }"
718----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49992/bad/ltdc.c----ltdc_load,"int ltdc_load(struct drm_device *ddev) { struct platform_device *pdev = to_platform_device(ddev->dev); struct ltdc_device *ldev = ddev->dev_private; struct device *dev = ddev->dev; struct device_node *np = dev->of_node; struct drm_bridge *bridge; struct drm_panel *panel; struct drm_crtc *crtc; struct reset_control *rstc; struct resource *res; int irq, i, nb_endpoints; int ret = -ENODEV; DRM_DEBUG_DRIVER(""\n""); nb_endpoints = of_graph_get_endpoint_count(np); if (!nb_endpoints) return -ENODEV; ldev->pixel_clk = devm_clk_get(dev, ""lcd""); if (IS_ERR(ldev->pixel_clk)) { if (PTR_ERR(ldev->pixel_clk) != -EPROBE_DEFER) DRM_ERROR(""Unable to get lcd clock\n""); return PTR_ERR(ldev->pixel_clk); } if (clk_prepare_enable(ldev->pixel_clk)) { DRM_ERROR(""Unable to prepare pixel clock\n""); return -ENODEV; } for (i = 0; i < nb_endpoints; i++) { ret = drm_of_find_panel_or_bridge(np, 0, i, &panel, &bridge); if (ret == -ENODEV) continue; else if (ret) goto err; if (panel) { <S2SV_StartVul> bridge = drm_panel_bridge_add_typed(panel, <S2SV_EndVul> <S2SV_StartVul> DRM_MODE_CONNECTOR_DPI); <S2SV_EndVul> if (IS_ERR(bridge)) { DRM_ERROR(""panel-bridge endpoint %d\n"", i); ret = PTR_ERR(bridge); goto err; } } if (bridge) { ret = ltdc_encoder_init(ddev, bridge); if (ret) { if (ret != -EPROBE_DEFER) DRM_ERROR(""init encoder endpoint %d\n"", i); goto err; } } } rstc = devm_reset_control_get_exclusive(dev, NULL); mutex_init(&ldev->err_lock); if (!IS_ERR(rstc)) { reset_control_assert(rstc); usleep_range(10, 20); reset_control_deassert(rstc); } res = platform_get_resource(pdev, IORESOURCE_MEM, 0); ldev->regs = devm_ioremap_resource(dev, res); if (IS_ERR(ldev->regs)) { DRM_ERROR(""Unable to get ltdc registers\n""); ret = PTR_ERR(ldev->regs); goto err; } ldev->regmap = devm_regmap_init_mmio(&pdev->dev, ldev->regs, &stm32_ltdc_regmap_cfg); if (IS_ERR(ldev->regmap)) { DRM_ERROR(""Unable to regmap ltdc registers\n""); ret = PTR_ERR(ldev->regmap); goto err; } ret = ltdc_get_caps(ddev); if (ret) { DRM_ERROR(""hardware identifier (0x%08x) not supported!\n"", ldev->caps.hw_version); goto err; } if (ldev->caps.fifo_threshold) regmap_clear_bits(ldev->regmap, LTDC_IER, IER_LIE | IER_RRIE | IER_FUWIE | IER_TERRIE); else regmap_clear_bits(ldev->regmap, LTDC_IER, IER_LIE | IER_RRIE | IER_FUWIE | IER_TERRIE | IER_FUEIE); DRM_DEBUG_DRIVER(""ltdc hw version 0x%08x\n"", ldev->caps.hw_version); ldev->transfer_err = 0; ldev->fifo_err = 0; ldev->fifo_warn = 0; ldev->fifo_threshold = FUT_DFT; for (i = 0; i < ldev->caps.nb_irq; i++) { irq = platform_get_irq(pdev, i); if (irq < 0) { ret = irq; goto err; } ret = devm_request_threaded_irq(dev, irq, ltdc_irq, ltdc_irq_thread, IRQF_ONESHOT, dev_name(dev), ddev); if (ret) { DRM_ERROR(""Failed to register LTDC interrupt\n""); goto err; } <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> crtc = devm_kzalloc(dev, sizeof(*crtc), GFP_KERNEL); <S2SV_EndVul> if (!crtc) { DRM_ERROR(""Failed to allocate crtc\n""); <S2SV_StartVul> ret = -ENOMEM; <S2SV_EndVul> goto err; } ret = ltdc_crtc_init(ddev, crtc); if (ret) { DRM_ERROR(""Failed to init crtc\n""); goto err; } ret = drm_vblank_init(ddev, NB_CRTC); if (ret) { DRM_ERROR(""Failed calling drm_vblank_init()\n""); goto err; } clk_disable_unprepare(ldev->pixel_clk); pinctrl_pm_select_sleep_state(ddev->dev); pm_runtime_enable(ddev->dev); return 0; err: <S2SV_StartVul> for (i = 0; i < nb_endpoints; i++) <S2SV_EndVul> <S2SV_StartVul> drm_of_panel_bridge_remove(ddev->dev->of_node, 0, i); <S2SV_EndVul> clk_disable_unprepare(ldev->pixel_clk); <S2SV_StartVul> return ret; <S2SV_EndVul> }","- bridge = drm_panel_bridge_add_typed(panel,
- DRM_MODE_CONNECTOR_DPI);
- }
- crtc = devm_kzalloc(dev, sizeof(*crtc), GFP_KERNEL);
- ret = -ENOMEM;
- for (i = 0; i < nb_endpoints; i++)
- drm_of_panel_bridge_remove(ddev->dev->of_node, 0, i);
- return ret;
+ bridge = drmm_panel_bridge_add(ddev, panel);
+ crtc = drmm_kzalloc(ddev, sizeof(*crtc), GFP_KERNEL);
+ return ret;","int ltdc_load(struct drm_device *ddev) { struct platform_device *pdev = to_platform_device(ddev->dev); struct ltdc_device *ldev = ddev->dev_private; struct device *dev = ddev->dev; struct device_node *np = dev->of_node; struct drm_bridge *bridge; struct drm_panel *panel; struct drm_crtc *crtc; struct reset_control *rstc; struct resource *res; int irq, i, nb_endpoints; int ret = -ENODEV; DRM_DEBUG_DRIVER(""\n""); nb_endpoints = of_graph_get_endpoint_count(np); if (!nb_endpoints) return -ENODEV; ldev->pixel_clk = devm_clk_get(dev, ""lcd""); if (IS_ERR(ldev->pixel_clk)) { if (PTR_ERR(ldev->pixel_clk) != -EPROBE_DEFER) DRM_ERROR(""Unable to get lcd clock\n""); return PTR_ERR(ldev->pixel_clk); } if (clk_prepare_enable(ldev->pixel_clk)) { DRM_ERROR(""Unable to prepare pixel clock\n""); return -ENODEV; } for (i = 0; i < nb_endpoints; i++) { ret = drm_of_find_panel_or_bridge(np, 0, i, &panel, &bridge); if (ret == -ENODEV) continue; else if (ret) goto err; if (panel) { bridge = drmm_panel_bridge_add(ddev, panel); if (IS_ERR(bridge)) { DRM_ERROR(""panel-bridge endpoint %d\n"", i); ret = PTR_ERR(bridge); goto err; } } if (bridge) { ret = ltdc_encoder_init(ddev, bridge); if (ret) { if (ret != -EPROBE_DEFER) DRM_ERROR(""init encoder endpoint %d\n"", i); goto err; } } } rstc = devm_reset_control_get_exclusive(dev, NULL); mutex_init(&ldev->err_lock); if (!IS_ERR(rstc)) { reset_control_assert(rstc); usleep_range(10, 20); reset_control_deassert(rstc); } res = platform_get_resource(pdev, IORESOURCE_MEM, 0); ldev->regs = devm_ioremap_resource(dev, res); if (IS_ERR(ldev->regs)) { DRM_ERROR(""Unable to get ltdc registers\n""); ret = PTR_ERR(ldev->regs); goto err; } ldev->regmap = devm_regmap_init_mmio(&pdev->dev, ldev->regs, &stm32_ltdc_regmap_cfg); if (IS_ERR(ldev->regmap)) { DRM_ERROR(""Unable to regmap ltdc registers\n""); ret = PTR_ERR(ldev->regmap); goto err; } ret = ltdc_get_caps(ddev); if (ret) { DRM_ERROR(""hardware identifier (0x%08x) not supported!\n"", ldev->caps.hw_version); goto err; } if (ldev->caps.fifo_threshold) regmap_clear_bits(ldev->regmap, LTDC_IER, IER_LIE | IER_RRIE | IER_FUWIE | IER_TERRIE); else regmap_clear_bits(ldev->regmap, LTDC_IER, IER_LIE | IER_RRIE | IER_FUWIE | IER_TERRIE | IER_FUEIE); DRM_DEBUG_DRIVER(""ltdc hw version 0x%08x\n"", ldev->caps.hw_version); ldev->transfer_err = 0; ldev->fifo_err = 0; ldev->fifo_warn = 0; ldev->fifo_threshold = FUT_DFT; for (i = 0; i < ldev->caps.nb_irq; i++) { irq = platform_get_irq(pdev, i); if (irq < 0) { ret = irq; goto err; } ret = devm_request_threaded_irq(dev, irq, ltdc_irq, ltdc_irq_thread, IRQF_ONESHOT, dev_name(dev), ddev); if (ret) { DRM_ERROR(""Failed to register LTDC interrupt\n""); goto err; } } crtc = drmm_kzalloc(ddev, sizeof(*crtc), GFP_KERNEL); if (!crtc) { DRM_ERROR(""Failed to allocate crtc\n""); ret = -ENOMEM; goto err; } ret = ltdc_crtc_init(ddev, crtc); if (ret) { DRM_ERROR(""Failed to init crtc\n""); goto err; } ret = drm_vblank_init(ddev, NB_CRTC); if (ret) { DRM_ERROR(""Failed calling drm_vblank_init()\n""); goto err; } clk_disable_unprepare(ldev->pixel_clk); pinctrl_pm_select_sleep_state(ddev->dev); pm_runtime_enable(ddev->dev); return 0; err: clk_disable_unprepare(ldev->pixel_clk); return ret; }"
77----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44951/bad/sc16is7xx.c----sc16is7xx_handle_tx,"static void sc16is7xx_handle_tx(struct uart_port *port) { struct sc16is7xx_port *s = dev_get_drvdata(port->dev); struct tty_port *tport = &port->state->port; unsigned long flags; unsigned int txlen; if (unlikely(port->x_char)) { sc16is7xx_port_write(port, SC16IS7XX_THR_REG, port->x_char); port->icount.tx++; port->x_char = 0; return; } if (kfifo_is_empty(&tport->xmit_fifo) || uart_tx_stopped(port)) { uart_port_lock_irqsave(port, &flags); sc16is7xx_stop_tx(port); uart_port_unlock_irqrestore(port, flags); return; } txlen = sc16is7xx_port_read(port, SC16IS7XX_TXLVL_REG); if (txlen > SC16IS7XX_FIFO_SIZE) { dev_err_ratelimited(port->dev, ""chip reports %d free bytes in TX fifo, but it only has %d"", txlen, SC16IS7XX_FIFO_SIZE); txlen = 0; } <S2SV_StartVul> txlen = uart_fifo_out(port, s->buf, txlen); <S2SV_EndVul> <S2SV_StartVul> sc16is7xx_fifo_write(port, s->buf, txlen); <S2SV_EndVul> uart_port_lock_irqsave(port, &flags); if (kfifo_len(&tport->xmit_fifo) < WAKEUP_CHARS) uart_write_wakeup(port); if (kfifo_is_empty(&tport->xmit_fifo)) sc16is7xx_stop_tx(port); else sc16is7xx_ier_set(port, SC16IS7XX_IER_THRI_BIT); uart_port_unlock_irqrestore(port, flags); }","- txlen = uart_fifo_out(port, s->buf, txlen);
- sc16is7xx_fifo_write(port, s->buf, txlen);
+ txlen = kfifo_out_linear_ptr(&tport->xmit_fifo, &tail, txlen);
+ sc16is7xx_fifo_write(port, tail, txlen);
+ uart_xmit_advance(port, txlen);","static void sc16is7xx_handle_tx(struct uart_port *port) { struct tty_port *tport = &port->state->port; unsigned long flags; unsigned int txlen; unsigned char *tail; if (unlikely(port->x_char)) { sc16is7xx_port_write(port, SC16IS7XX_THR_REG, port->x_char); port->icount.tx++; port->x_char = 0; return; } if (kfifo_is_empty(&tport->xmit_fifo) || uart_tx_stopped(port)) { uart_port_lock_irqsave(port, &flags); sc16is7xx_stop_tx(port); uart_port_unlock_irqrestore(port, flags); return; } txlen = sc16is7xx_port_read(port, SC16IS7XX_TXLVL_REG); if (txlen > SC16IS7XX_FIFO_SIZE) { dev_err_ratelimited(port->dev, ""chip reports %d free bytes in TX fifo, but it only has %d"", txlen, SC16IS7XX_FIFO_SIZE); txlen = 0; } txlen = kfifo_out_linear_ptr(&tport->xmit_fifo, &tail, txlen); sc16is7xx_fifo_write(port, tail, txlen); uart_xmit_advance(port, txlen); uart_port_lock_irqsave(port, &flags); if (kfifo_len(&tport->xmit_fifo) < WAKEUP_CHARS) uart_write_wakeup(port); if (kfifo_is_empty(&tport->xmit_fifo)) sc16is7xx_stop_tx(port); else sc16is7xx_ier_set(port, SC16IS7XX_IER_THRI_BIT); uart_port_unlock_irqrestore(port, flags); }"
546----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49865/bad/xe_vm.c----xe_vm_create_ioctl,"int xe_vm_create_ioctl(struct drm_device *dev, void *data, struct drm_file *file) { struct xe_device *xe = to_xe_device(dev); struct xe_file *xef = to_xe_file(file); struct drm_xe_vm_create *args = data; struct xe_tile *tile; struct xe_vm *vm; u32 id, asid; int err; u32 flags = 0; if (XE_IOCTL_DBG(xe, args->extensions)) return -EINVAL; if (XE_WA(xe_root_mmio_gt(xe), 14016763929)) args->flags |= DRM_XE_VM_CREATE_FLAG_SCRATCH_PAGE; if (XE_IOCTL_DBG(xe, args->flags & DRM_XE_VM_CREATE_FLAG_FAULT_MODE && !xe->info.has_usm)) return -EINVAL; if (XE_IOCTL_DBG(xe, args->reserved[0] || args->reserved[1])) return -EINVAL; if (XE_IOCTL_DBG(xe, args->flags & ~ALL_DRM_XE_VM_CREATE_FLAGS)) return -EINVAL; if (XE_IOCTL_DBG(xe, args->flags & DRM_XE_VM_CREATE_FLAG_SCRATCH_PAGE && args->flags & DRM_XE_VM_CREATE_FLAG_FAULT_MODE)) return -EINVAL; if (XE_IOCTL_DBG(xe, !(args->flags & DRM_XE_VM_CREATE_FLAG_LR_MODE) && args->flags & DRM_XE_VM_CREATE_FLAG_FAULT_MODE)) return -EINVAL; if (XE_IOCTL_DBG(xe, args->flags & DRM_XE_VM_CREATE_FLAG_FAULT_MODE && xe_device_in_non_fault_mode(xe))) return -EINVAL; if (XE_IOCTL_DBG(xe, !(args->flags & DRM_XE_VM_CREATE_FLAG_FAULT_MODE) && xe_device_in_fault_mode(xe))) return -EINVAL; if (XE_IOCTL_DBG(xe, args->extensions)) return -EINVAL; if (args->flags & DRM_XE_VM_CREATE_FLAG_SCRATCH_PAGE) flags |= XE_VM_FLAG_SCRATCH_PAGE; if (args->flags & DRM_XE_VM_CREATE_FLAG_LR_MODE) flags |= XE_VM_FLAG_LR_MODE; if (args->flags & DRM_XE_VM_CREATE_FLAG_FAULT_MODE) flags |= XE_VM_FLAG_FAULT_MODE; vm = xe_vm_create(xe, flags); if (IS_ERR(vm)) return PTR_ERR(vm); <S2SV_StartVul> err = xa_alloc(&xef->vm.xa, &id, vm, xa_limit_32b, GFP_KERNEL); <S2SV_EndVul> <S2SV_StartVul> if (err) <S2SV_EndVul> <S2SV_StartVul> goto err_close_and_put; <S2SV_EndVul> if (xe->info.has_asid) { mutex_lock(&xe->usm.lock); err = xa_alloc_cyclic(&xe->usm.asid_to_vm, &asid, vm, XA_LIMIT(1, XE_MAX_ASID - 1), &xe->usm.next_asid, GFP_KERNEL); mutex_unlock(&xe->usm.lock); if (err < 0) <S2SV_StartVul> goto err_free_id; <S2SV_EndVul> vm->usm.asid = asid; } <S2SV_StartVul> args->vm_id = id; <S2SV_EndVul> vm->xef = xe_file_get(xef); for_each_tile(tile, xe, id) if (vm->pt_root[id]) xe_drm_client_add_bo(vm->xef->client, vm->pt_root[id]->bo); #if IS_ENABLED(CONFIG_DRM_XE_DEBUG_MEM) args->reserved[0] = xe_bo_main_addr(vm->pt_root[0]->bo, XE_PAGE_SIZE); #endif return 0; <S2SV_StartVul> err_free_id: <S2SV_EndVul> <S2SV_StartVul> xa_erase(&xef->vm.xa, id); <S2SV_EndVul> err_close_and_put: xe_vm_close_and_put(vm); return err; }","- err = xa_alloc(&xef->vm.xa, &id, vm, xa_limit_32b, GFP_KERNEL);
- if (err)
- goto err_close_and_put;
- goto err_free_id;
- args->vm_id = id;
- err_free_id:
- xa_erase(&xef->vm.xa, id);
+ goto err_close_and_put;
+ err = xa_alloc(&xef->vm.xa, &id, vm, xa_limit_32b, GFP_KERNEL);
+ if (err)
+ goto err_close_and_put;
+ args->vm_id = id;","int xe_vm_create_ioctl(struct drm_device *dev, void *data, struct drm_file *file) { struct xe_device *xe = to_xe_device(dev); struct xe_file *xef = to_xe_file(file); struct drm_xe_vm_create *args = data; struct xe_tile *tile; struct xe_vm *vm; u32 id, asid; int err; u32 flags = 0; if (XE_IOCTL_DBG(xe, args->extensions)) return -EINVAL; if (XE_WA(xe_root_mmio_gt(xe), 14016763929)) args->flags |= DRM_XE_VM_CREATE_FLAG_SCRATCH_PAGE; if (XE_IOCTL_DBG(xe, args->flags & DRM_XE_VM_CREATE_FLAG_FAULT_MODE && !xe->info.has_usm)) return -EINVAL; if (XE_IOCTL_DBG(xe, args->reserved[0] || args->reserved[1])) return -EINVAL; if (XE_IOCTL_DBG(xe, args->flags & ~ALL_DRM_XE_VM_CREATE_FLAGS)) return -EINVAL; if (XE_IOCTL_DBG(xe, args->flags & DRM_XE_VM_CREATE_FLAG_SCRATCH_PAGE && args->flags & DRM_XE_VM_CREATE_FLAG_FAULT_MODE)) return -EINVAL; if (XE_IOCTL_DBG(xe, !(args->flags & DRM_XE_VM_CREATE_FLAG_LR_MODE) && args->flags & DRM_XE_VM_CREATE_FLAG_FAULT_MODE)) return -EINVAL; if (XE_IOCTL_DBG(xe, args->flags & DRM_XE_VM_CREATE_FLAG_FAULT_MODE && xe_device_in_non_fault_mode(xe))) return -EINVAL; if (XE_IOCTL_DBG(xe, !(args->flags & DRM_XE_VM_CREATE_FLAG_FAULT_MODE) && xe_device_in_fault_mode(xe))) return -EINVAL; if (XE_IOCTL_DBG(xe, args->extensions)) return -EINVAL; if (args->flags & DRM_XE_VM_CREATE_FLAG_SCRATCH_PAGE) flags |= XE_VM_FLAG_SCRATCH_PAGE; if (args->flags & DRM_XE_VM_CREATE_FLAG_LR_MODE) flags |= XE_VM_FLAG_LR_MODE; if (args->flags & DRM_XE_VM_CREATE_FLAG_FAULT_MODE) flags |= XE_VM_FLAG_FAULT_MODE; vm = xe_vm_create(xe, flags); if (IS_ERR(vm)) return PTR_ERR(vm); if (xe->info.has_asid) { mutex_lock(&xe->usm.lock); err = xa_alloc_cyclic(&xe->usm.asid_to_vm, &asid, vm, XA_LIMIT(1, XE_MAX_ASID - 1), &xe->usm.next_asid, GFP_KERNEL); mutex_unlock(&xe->usm.lock); if (err < 0) goto err_close_and_put; vm->usm.asid = asid; } vm->xef = xe_file_get(xef); for_each_tile(tile, xe, id) if (vm->pt_root[id]) xe_drm_client_add_bo(vm->xef->client, vm->pt_root[id]->bo); #if IS_ENABLED(CONFIG_DRM_XE_DEBUG_MEM) args->reserved[0] = xe_bo_main_addr(vm->pt_root[0]->bo, XE_PAGE_SIZE); #endif err = xa_alloc(&xef->vm.xa, &id, vm, xa_limit_32b, GFP_KERNEL); if (err) goto err_close_and_put; args->vm_id = id; return 0; err_close_and_put: xe_vm_close_and_put(vm); return err; }"
57----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44941/bad/extent_cache.c----f2fs_init_read_extent_tree,"void f2fs_init_read_extent_tree(struct inode *inode, struct page *ipage) { struct f2fs_sb_info *sbi = F2FS_I_SB(inode); struct extent_tree_info *eti = &sbi->extent_tree[EX_READ]; struct f2fs_extent *i_ext = &F2FS_INODE(ipage)->i_ext; struct extent_tree *et; struct extent_node *en; struct extent_info ei; if (!__may_extent_tree(inode, EX_READ)) { <S2SV_StartVul> if (i_ext && i_ext->len) { <S2SV_EndVul> f2fs_wait_on_page_writeback(ipage, NODE, true, true); i_ext->len = 0; set_page_dirty(ipage); <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> goto out; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> et = __grab_extent_tree(inode, EX_READ); <S2SV_StartVul> if (!i_ext || !i_ext->len) <S2SV_EndVul> <S2SV_StartVul> goto out; <S2SV_EndVul> get_read_extent_info(&ei, i_ext); write_lock(&et->lock); <S2SV_StartVul> if (atomic_read(&et->node_cnt)) <S2SV_EndVul> <S2SV_StartVul> goto unlock_out; <S2SV_EndVul> en = __attach_extent_node(sbi, et, &ei, NULL, &et->root.rb_root.rb_node, true); if (en) { et->largest = en->ei; et->cached_en = en; spin_lock(&eti->extent_lock); list_add_tail(&en->list, &eti->extent_list); spin_unlock(&eti->extent_lock); <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> unlock_out: <S2SV_EndVul> write_unlock(&et->lock); <S2SV_StartVul> out: <S2SV_EndVul> <S2SV_StartVul> if (!F2FS_I(inode)->extent_tree[EX_READ]) <S2SV_EndVul> <S2SV_StartVul> set_inode_flag(inode, FI_NO_EXTENT); <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul>","- if (i_ext && i_ext->len) {
- }
- goto out;
- }
- if (!i_ext || !i_ext->len)
- goto out;
- if (atomic_read(&et->node_cnt))
- goto unlock_out;
- }
- unlock_out:
- out:
- if (!F2FS_I(inode)->extent_tree[EX_READ])
- set_inode_flag(inode, FI_NO_EXTENT);
- }
+ if (i_ext->len) {
+ }
+ set_inode_flag(inode, FI_NO_EXTENT);
+ return;
+ }
+ get_read_extent_info(&ei, i_ext);
+ if (atomic_read(&et->node_cnt) || !ei.len)
+ goto skip;
+ }
+ skip:
+ if (f2fs_cp_error(sbi)) {
+ et->largest.len = 0;
+ et->largest_updated = true;
+ }
+ }","void f2fs_init_read_extent_tree(struct inode *inode, struct page *ipage) { struct f2fs_sb_info *sbi = F2FS_I_SB(inode); struct extent_tree_info *eti = &sbi->extent_tree[EX_READ]; struct f2fs_extent *i_ext = &F2FS_INODE(ipage)->i_ext; struct extent_tree *et; struct extent_node *en; struct extent_info ei; if (!__may_extent_tree(inode, EX_READ)) { if (i_ext->len) { f2fs_wait_on_page_writeback(ipage, NODE, true, true); i_ext->len = 0; set_page_dirty(ipage); } set_inode_flag(inode, FI_NO_EXTENT); return; } et = __grab_extent_tree(inode, EX_READ); get_read_extent_info(&ei, i_ext); write_lock(&et->lock); if (atomic_read(&et->node_cnt) || !ei.len) goto skip; en = __attach_extent_node(sbi, et, &ei, NULL, &et->root.rb_root.rb_node, true); if (en) { et->largest = en->ei; et->cached_en = en; spin_lock(&eti->extent_lock); list_add_tail(&en->list, &eti->extent_list); spin_unlock(&eti->extent_lock); } skip: if (f2fs_cp_error(sbi)) { et->largest.len = 0; et->largest_updated = true; } write_unlock(&et->lock); }"
385----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46846/bad/spi-rockchip.c----rockchip_spi_resume,static int rockchip_spi_resume(struct device *dev) { int ret; struct spi_controller *ctlr = dev_get_drvdata(dev); <S2SV_StartVul> struct rockchip_spi *rs = spi_controller_get_devdata(ctlr); <S2SV_EndVul> pinctrl_pm_select_default_state(dev); <S2SV_StartVul> ret = clk_prepare_enable(rs->apb_pclk); <S2SV_EndVul> <S2SV_StartVul> if (ret < 0) <S2SV_EndVul> return ret; <S2SV_StartVul> ret = clk_prepare_enable(rs->spiclk); <S2SV_EndVul> <S2SV_StartVul> if (ret < 0) <S2SV_EndVul> <S2SV_StartVul> clk_disable_unprepare(rs->apb_pclk); <S2SV_EndVul> <S2SV_StartVul> ret = spi_controller_resume(ctlr); <S2SV_EndVul> <S2SV_StartVul> if (ret < 0) { <S2SV_EndVul> <S2SV_StartVul> clk_disable_unprepare(rs->spiclk); <S2SV_EndVul> <S2SV_StartVul> clk_disable_unprepare(rs->apb_pclk); <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> return 0; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul>,"- struct rockchip_spi *rs = spi_controller_get_devdata(ctlr);
- ret = clk_prepare_enable(rs->apb_pclk);
- if (ret < 0)
- ret = clk_prepare_enable(rs->spiclk);
- if (ret < 0)
- clk_disable_unprepare(rs->apb_pclk);
- ret = spi_controller_resume(ctlr);
- if (ret < 0) {
- clk_disable_unprepare(rs->spiclk);
- clk_disable_unprepare(rs->apb_pclk);
- }
- return 0;
- }
+ ret = pm_runtime_force_resume(dev);
+ return ret;
+ return spi_controller_resume(ctlr);
+ }",static int rockchip_spi_resume(struct device *dev) { int ret; struct spi_controller *ctlr = dev_get_drvdata(dev); pinctrl_pm_select_default_state(dev); ret = pm_runtime_force_resume(dev); if (ret < 0) return ret; return spi_controller_resume(ctlr); }
971----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50211/bad/inode.c----udf_map_block,"static int udf_map_block(struct inode *inode, struct udf_map_rq *map) { int err; struct udf_inode_info *iinfo = UDF_I(inode); if (WARN_ON_ONCE(iinfo->i_alloc_type == ICBTAG_FLAG_AD_IN_ICB)) return -EFSCORRUPTED; map->oflags = 0; if (!(map->iflags & UDF_MAP_CREATE)) { struct kernel_lb_addr eloc; uint32_t elen; sector_t offset; struct extent_position epos = {}; down_read(&iinfo->i_data_sem); <S2SV_StartVul> if (inode_bmap(inode, map->lblk, &epos, &eloc, &elen, &offset) <S2SV_EndVul> <S2SV_StartVul> == (EXT_RECORDED_ALLOCATED >> 30)) { <S2SV_EndVul> map->pblk = udf_get_lb_pblock(inode->i_sb, &eloc, offset); map->oflags |= UDF_BLK_MAPPED; } up_read(&iinfo->i_data_sem); brelse(epos.bh); <S2SV_StartVul> return 0; <S2SV_EndVul> } down_write(&iinfo->i_data_sem); if (((loff_t)map->lblk) << inode->i_blkbits >= iinfo->i_lenExtents) udf_discard_prealloc(inode); udf_clear_extent_cache(inode); <S2SV_StartVul> err = inode_getblk(inode, map); <S2SV_EndVul> up_write(&iinfo->i_data_sem); <S2SV_StartVul> return err; <S2SV_EndVul> }","- if (inode_bmap(inode, map->lblk, &epos, &eloc, &elen, &offset)
- == (EXT_RECORDED_ALLOCATED >> 30)) {
- return 0;
- err = inode_getblk(inode, map);
- return err;
+ int8_t etype;
+ ret = inode_bmap(inode, map->lblk, &epos, &eloc, &elen, &offset,
+ &etype);
+ if (ret < 0)
+ goto out_read;
+ if (ret > 0 && etype == (EXT_RECORDED_ALLOCATED >> 30)) {
+ ret = 0;
+ out_read:
+ return ret;
+ ret = inode_getblk(inode, map);
+ return ret;","static int udf_map_block(struct inode *inode, struct udf_map_rq *map) { int ret; struct udf_inode_info *iinfo = UDF_I(inode); if (WARN_ON_ONCE(iinfo->i_alloc_type == ICBTAG_FLAG_AD_IN_ICB)) return -EFSCORRUPTED; map->oflags = 0; if (!(map->iflags & UDF_MAP_CREATE)) { struct kernel_lb_addr eloc; uint32_t elen; sector_t offset; struct extent_position epos = {}; int8_t etype; down_read(&iinfo->i_data_sem); ret = inode_bmap(inode, map->lblk, &epos, &eloc, &elen, &offset, &etype); if (ret < 0) goto out_read; if (ret > 0 && etype == (EXT_RECORDED_ALLOCATED >> 30)) { map->pblk = udf_get_lb_pblock(inode->i_sb, &eloc, offset); map->oflags |= UDF_BLK_MAPPED; ret = 0; } out_read: up_read(&iinfo->i_data_sem); brelse(epos.bh); return ret; } down_write(&iinfo->i_data_sem); if (((loff_t)map->lblk) << inode->i_blkbits >= iinfo->i_lenExtents) udf_discard_prealloc(inode); udf_clear_extent_cache(inode); ret = inode_getblk(inode, map); up_write(&iinfo->i_data_sem); return ret; }"
173----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-45026/bad/dasd.c----dasd_ese_needs_format,"static int dasd_ese_needs_format(struct dasd_block *block, struct irb *irb) { struct dasd_device *device = NULL; u8 *sense = NULL; if (!block) return 0; device = block->base; if (!device || !device->discipline->is_ese) return 0; if (!device->discipline->is_ese(device)) return 0; sense = dasd_get_sense(irb); if (!sense) return 0; <S2SV_StartVul> return !!(sense[1] & SNS1_NO_REC_FOUND) || <S2SV_EndVul> <S2SV_StartVul> !!(sense[1] & SNS1_FILE_PROTECTED) || <S2SV_EndVul> <S2SV_StartVul> scsw_cstat(&irb->scsw) == SCHN_STAT_INCORR_LEN; <S2SV_EndVul> }","- return !!(sense[1] & SNS1_NO_REC_FOUND) ||
- !!(sense[1] & SNS1_FILE_PROTECTED) ||
- scsw_cstat(&irb->scsw) == SCHN_STAT_INCORR_LEN;
+ return 0;
+ if (sense[1] & SNS1_NO_REC_FOUND)
+ return 1;
+ if ((sense[1] & SNS1_INV_TRACK_FORMAT) &&
+ scsw_is_tm(&irb->scsw) &&
+ !(sense[2] & SNS2_ENV_DATA_PRESENT))
+ return 1;
+ return 0;","static int dasd_ese_needs_format(struct dasd_block *block, struct irb *irb) { struct dasd_device *device = NULL; u8 *sense = NULL; if (!block) return 0; device = block->base; if (!device || !device->discipline->is_ese) return 0; if (!device->discipline->is_ese(device)) return 0; sense = dasd_get_sense(irb); if (!sense) return 0; if (sense[1] & SNS1_NO_REC_FOUND) return 1; if ((sense[1] & SNS1_INV_TRACK_FORMAT) && scsw_is_tm(&irb->scsw) && !(sense[2] & SNS2_ENV_DATA_PRESENT)) return 1; return 0; }"
1491----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-57916/bad/mchp_pci1xxxx_gpio.c----pci1xxxx_gpio_irq_handler,"static irqreturn_t pci1xxxx_gpio_irq_handler(int irq, void *dev_id) { struct pci1xxxx_gpio *priv = dev_id; struct gpio_chip *gc = &priv->gpio; unsigned long int_status = 0; unsigned long flags; u8 pincount; int bit; u8 gpiobank; spin_lock_irqsave(&priv->lock, flags); pci1xxx_assign_bit(priv->reg_base, PIO_GLOBAL_CONFIG_OFFSET, 16, true); spin_unlock_irqrestore(&priv->lock, flags); for (gpiobank = 0; gpiobank < 3; gpiobank++) { spin_lock_irqsave(&priv->lock, flags); int_status = readl(priv->reg_base + INTR_STATUS_OFFSET(gpiobank)); spin_unlock_irqrestore(&priv->lock, flags); if (gpiobank == 2) pincount = 29; else pincount = 32; for_each_set_bit(bit, &int_status, pincount) { unsigned int irq; spin_lock_irqsave(&priv->lock, flags); writel(BIT(bit), priv->reg_base + INTR_STATUS_OFFSET(gpiobank)); spin_unlock_irqrestore(&priv->lock, flags); irq = irq_find_mapping(gc->irq.domain, (bit + (gpiobank * 32))); <S2SV_StartVul> generic_handle_irq(irq); <S2SV_EndVul> } } spin_lock_irqsave(&priv->lock, flags); pci1xxx_assign_bit(priv->reg_base, PIO_GLOBAL_CONFIG_OFFSET, 16, false); spin_unlock_irqrestore(&priv->lock, flags); return IRQ_HANDLED; }","- generic_handle_irq(irq);
+ handle_nested_irq(irq);","static irqreturn_t pci1xxxx_gpio_irq_handler(int irq, void *dev_id) { struct pci1xxxx_gpio *priv = dev_id; struct gpio_chip *gc = &priv->gpio; unsigned long int_status = 0; unsigned long flags; u8 pincount; int bit; u8 gpiobank; spin_lock_irqsave(&priv->lock, flags); pci1xxx_assign_bit(priv->reg_base, PIO_GLOBAL_CONFIG_OFFSET, 16, true); spin_unlock_irqrestore(&priv->lock, flags); for (gpiobank = 0; gpiobank < 3; gpiobank++) { spin_lock_irqsave(&priv->lock, flags); int_status = readl(priv->reg_base + INTR_STATUS_OFFSET(gpiobank)); spin_unlock_irqrestore(&priv->lock, flags); if (gpiobank == 2) pincount = 29; else pincount = 32; for_each_set_bit(bit, &int_status, pincount) { unsigned int irq; spin_lock_irqsave(&priv->lock, flags); writel(BIT(bit), priv->reg_base + INTR_STATUS_OFFSET(gpiobank)); spin_unlock_irqrestore(&priv->lock, flags); irq = irq_find_mapping(gc->irq.domain, (bit + (gpiobank * 32))); handle_nested_irq(irq); } } spin_lock_irqsave(&priv->lock, flags); pci1xxx_assign_bit(priv->reg_base, PIO_GLOBAL_CONFIG_OFFSET, 16, false); spin_unlock_irqrestore(&priv->lock, flags); return IRQ_HANDLED; }"
232----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46709/bad/vmwgfx_stdu.c----vmw_stdu_bo_cpu_commit,"static void vmw_stdu_bo_cpu_commit(struct vmw_kms_dirty *dirty) { struct vmw_stdu_dirty *ddirty = container_of(dirty, struct vmw_stdu_dirty, base); struct vmw_screen_target_display_unit *stdu = container_of(dirty->unit, typeof(*stdu), base); s32 width, height; s32 src_pitch, dst_pitch; <S2SV_StartVul> struct ttm_buffer_object *src_bo, *dst_bo; <S2SV_EndVul> u32 src_offset, dst_offset; struct vmw_diff_cpy diff = VMW_CPU_BLIT_DIFF_INITIALIZER(stdu->cpp); if (!dirty->num_hits) return; width = ddirty->right - ddirty->left; height = ddirty->bottom - ddirty->top; if (width == 0 || height == 0) return; src_pitch = stdu->display_srf->metadata.base_size.width * stdu->cpp; <S2SV_StartVul> src_bo = &stdu->display_srf->res.guest_memory_bo->tbo; <S2SV_EndVul> src_offset = ddirty->top * src_pitch + ddirty->left * stdu->cpp; dst_pitch = ddirty->pitch; <S2SV_StartVul> dst_bo = &ddirty->buf->tbo; <S2SV_EndVul> dst_offset = ddirty->fb_top * dst_pitch + ddirty->fb_left * stdu->cpp; (void) vmw_bo_cpu_blit(dst_bo, dst_offset, dst_pitch, src_bo, src_offset, src_pitch, width * stdu->cpp, height, &diff); }","- struct ttm_buffer_object *src_bo, *dst_bo;
- src_bo = &stdu->display_srf->res.guest_memory_bo->tbo;
- dst_bo = &ddirty->buf->tbo;
+ struct vmw_bo *src_bo, *dst_bo;
+ src_bo = stdu->display_srf->res.guest_memory_bo;
+ dst_bo = ddirty->buf;","static void vmw_stdu_bo_cpu_commit(struct vmw_kms_dirty *dirty) { struct vmw_stdu_dirty *ddirty = container_of(dirty, struct vmw_stdu_dirty, base); struct vmw_screen_target_display_unit *stdu = container_of(dirty->unit, typeof(*stdu), base); s32 width, height; s32 src_pitch, dst_pitch; struct vmw_bo *src_bo, *dst_bo; u32 src_offset, dst_offset; struct vmw_diff_cpy diff = VMW_CPU_BLIT_DIFF_INITIALIZER(stdu->cpp); if (!dirty->num_hits) return; width = ddirty->right - ddirty->left; height = ddirty->bottom - ddirty->top; if (width == 0 || height == 0) return; src_pitch = stdu->display_srf->metadata.base_size.width * stdu->cpp; src_bo = stdu->display_srf->res.guest_memory_bo; src_offset = ddirty->top * src_pitch + ddirty->left * stdu->cpp; dst_pitch = ddirty->pitch; dst_bo = ddirty->buf; dst_offset = ddirty->fb_top * dst_pitch + ddirty->fb_left * stdu->cpp; (void) vmw_bo_cpu_blit(dst_bo, dst_offset, dst_pitch, src_bo, src_offset, src_pitch, width * stdu->cpp, height, &diff); }"
499----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47738/bad/rate.c----ieee80211_get_tx_rates,"void ieee80211_get_tx_rates(struct ieee80211_vif *vif, struct ieee80211_sta *sta, struct sk_buff *skb, struct ieee80211_tx_rate *dest, int max_rates) { struct ieee80211_sub_if_data *sdata; struct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb); struct ieee80211_supported_band *sband; u32 mask = ~0; rate_control_fill_sta_table(sta, info, dest, max_rates); if (!vif) return; sdata = vif_to_sdata(vif); sband = sdata->local->hw.wiphy->bands[info->band]; if (ieee80211_is_tx_data(skb)) rate_control_apply_mask(sdata, sta, sband, dest, max_rates); <S2SV_StartVul> if (!(info->control.flags & IEEE80211_TX_CTRL_SCAN_TX)) <S2SV_EndVul> mask = sdata->rc_rateidx_mask[info->band]; if (dest[0].idx < 0) __rate_control_send_low(&sdata->local->hw, sband, sta, info, mask); if (sta) rate_fixup_ratelist(vif, sband, info, dest, max_rates); }","- if (!(info->control.flags & IEEE80211_TX_CTRL_SCAN_TX))
+ if (!(info->control.flags & IEEE80211_TX_CTRL_DONT_USE_RATE_MASK))","void ieee80211_get_tx_rates(struct ieee80211_vif *vif, struct ieee80211_sta *sta, struct sk_buff *skb, struct ieee80211_tx_rate *dest, int max_rates) { struct ieee80211_sub_if_data *sdata; struct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb); struct ieee80211_supported_band *sband; u32 mask = ~0; rate_control_fill_sta_table(sta, info, dest, max_rates); if (!vif) return; sdata = vif_to_sdata(vif); sband = sdata->local->hw.wiphy->bands[info->band]; if (ieee80211_is_tx_data(skb)) rate_control_apply_mask(sdata, sta, sband, dest, max_rates); if (!(info->control.flags & IEEE80211_TX_CTRL_DONT_USE_RATE_MASK)) mask = sdata->rc_rateidx_mask[info->band]; if (dest[0].idx < 0) __rate_control_send_low(&sdata->local->hw, sband, sta, info, mask); if (sta) rate_fixup_ratelist(vif, sband, info, dest, max_rates); }"
306----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46775/bad/link_dp_training.c----perform_link_training_with_retries,"bool perform_link_training_with_retries( const struct dc_link_settings *link_setting, bool skip_video_pattern, int attempts, struct pipe_ctx *pipe_ctx, enum signal_type signal, bool do_fallback) { int j; uint8_t delay_between_attempts = LINK_TRAINING_RETRY_DELAY; struct dc_stream_state *stream = pipe_ctx->stream; struct dc_link *link = stream->link; enum dp_panel_mode panel_mode = dp_get_panel_mode(link); enum link_training_result status = LINK_TRAINING_CR_FAIL_LANE0; struct dc_link_settings cur_link_settings = *link_setting; struct dc_link_settings max_link_settings = *link_setting; const struct link_hwss *link_hwss = get_link_hwss(link, &pipe_ctx->link_res); int fail_count = 0; bool is_link_bw_low = false; bool is_link_bw_min = (cur_link_settings.link_rate <= LINK_RATE_LOW) && (cur_link_settings.lane_count <= LANE_COUNT_ONE); dp_trace_commit_lt_init(link); if (link_dp_get_encoding_format(&cur_link_settings) == DP_8b_10b_ENCODING) link_hwss->setup_stream_encoder(pipe_ctx); dp_trace_set_lt_start_timestamp(link, false); j = 0; while (j < attempts && fail_count < (attempts * 10)) { DC_LOG_HW_LINK_TRAINING(""%s: Beginning link(%d) training attempt %u of %d @ rate(%d) x lane(%d) @ spread = %x\n"", __func__, link->link_index, (unsigned int)j + 1, attempts, cur_link_settings.link_rate, cur_link_settings.lane_count, cur_link_settings.link_spread); dp_enable_link_phy( link, &pipe_ctx->link_res, signal, pipe_ctx->clock_source->id, &cur_link_settings); if (stream->sink_patches.dppowerup_delay > 0) { int delay_dp_power_up_in_ms = stream->sink_patches.dppowerup_delay; msleep(delay_dp_power_up_in_ms); } edp_set_panel_assr(link, pipe_ctx, &panel_mode, true); dp_set_panel_mode(link, panel_mode); if (link->aux_access_disabled) { dp_perform_link_training_skip_aux(link, &pipe_ctx->link_res, &cur_link_settings); return true; } else { if (link->ep_type == DISPLAY_ENDPOINT_USB4_DPIA) { status = dpia_perform_link_training( link, &pipe_ctx->link_res, &cur_link_settings, skip_video_pattern); if (status == LINK_TRAINING_SUCCESS && !is_link_bw_low) { dp_set_hw_test_pattern(link, &pipe_ctx->link_res, DP_TEST_PATTERN_VIDEO_MODE, NULL, 0); if (stream->signal == SIGNAL_TYPE_DISPLAY_PORT_MST) { link->verified_link_cap.link_rate = link->cur_link_settings.link_rate; link->verified_link_cap.lane_count = link->cur_link_settings.lane_count; dm_helpers_dp_mst_update_branch_bandwidth(link->ctx, link); } } } else { status = dp_perform_link_training( link, &pipe_ctx->link_res, &cur_link_settings, skip_video_pattern); } dp_trace_lt_total_count_increment(link, false); dp_trace_lt_result_update(link, status, false); dp_trace_set_lt_end_timestamp(link, false); if (status == LINK_TRAINING_SUCCESS && !is_link_bw_low) return true; } fail_count++; dp_trace_lt_fail_count_update(link, fail_count, false); if (link->ep_type == DISPLAY_ENDPOINT_PHY) { if (j == (attempts - 1) || (status == LINK_TRAINING_ABORT)) break; } if (j == (attempts - 1)) { DC_LOG_WARNING( ""%s: Link(%d) training attempt %u of %d failed @ rate(%d) x lane(%d) @ spread = %x : fail reason:(%d)\n"", __func__, link->link_index, (unsigned int)j + 1, attempts, cur_link_settings.link_rate, cur_link_settings.lane_count, cur_link_settings.link_spread, status); } else { DC_LOG_HW_LINK_TRAINING( ""%s: Link(%d) training attempt %u of %d failed @ rate(%d) x lane(%d) @ spread = %x : fail reason:(%d)\n"", __func__, link->link_index, (unsigned int)j + 1, attempts, cur_link_settings.link_rate, cur_link_settings.lane_count, cur_link_settings.link_spread, status); } dp_disable_link_phy(link, &pipe_ctx->link_res, signal); if (status == LINK_TRAINING_ABORT) { enum dc_connection_type type = dc_connection_none; <S2SV_StartVul> link_detect_connection_type(link, &type); <S2SV_EndVul> <S2SV_StartVul> if (type == dc_connection_none) { <S2SV_EndVul> DC_LOG_HW_LINK_TRAINING(""%s: Aborting training because sink unplugged\n"", __func__); break; } } if (!do_fallback || (status == LINK_TRAINING_ABORT) || (status == LINK_TRAINING_SUCCESS && is_link_bw_low) || is_link_bw_min) { j++; cur_link_settings = *link_setting; delay_between_attempts += LINK_TRAINING_RETRY_DELAY; is_link_bw_low = false; is_link_bw_min = (cur_link_settings.link_rate <= LINK_RATE_LOW) && (cur_link_settings.lane_count <= LANE_COUNT_ONE); } else if (do_fallback) { uint32_t req_bw; uint32_t link_bw; enum dc_link_encoding_format link_encoding = DC_LINK_ENCODING_UNSPECIFIED; decide_fallback_link_setting(link, &max_link_settings, &cur_link_settings, status); if (link_dp_get_encoding_format(&cur_link_settings) == DP_8b_10b_ENCODING) link_encoding = DC_LINK_ENCODING_DP_8b_10b; else if (link_dp_get_encoding_format(&cur_link_settings) == DP_128b_132b_ENCODING) link_encoding = DC_LINK_ENCODING_DP_128b_132b; req_bw = dc_bandwidth_in_kbps_from_timing(&stream->timing, link_encoding); link_bw = dp_link_bandwidth_kbps(link, &cur_link_settings); is_link_bw_low = (req_bw > link_bw); is_link_bw_min = ((cur_link_settings.link_rate <= LINK_RATE_LOW) && (cur_link_settings.lane_count <= LANE_COUNT_ONE)); if (is_link_bw_low) { DC_LOG_WARNING( ""%s: Link(%d) bandwidth too low after fallback req_bw(%d) > link_bw(%d)\n"", __func__, link->link_index, req_bw, link_bw); return false; } } msleep(delay_between_attempts); } return false; }","- link_detect_connection_type(link, &type);
- if (type == dc_connection_none) {
+ if (link_detect_connection_type(link, &type) && type == dc_connection_none) {","bool perform_link_training_with_retries( const struct dc_link_settings *link_setting, bool skip_video_pattern, int attempts, struct pipe_ctx *pipe_ctx, enum signal_type signal, bool do_fallback) { int j; uint8_t delay_between_attempts = LINK_TRAINING_RETRY_DELAY; struct dc_stream_state *stream = pipe_ctx->stream; struct dc_link *link = stream->link; enum dp_panel_mode panel_mode = dp_get_panel_mode(link); enum link_training_result status = LINK_TRAINING_CR_FAIL_LANE0; struct dc_link_settings cur_link_settings = *link_setting; struct dc_link_settings max_link_settings = *link_setting; const struct link_hwss *link_hwss = get_link_hwss(link, &pipe_ctx->link_res); int fail_count = 0; bool is_link_bw_low = false; bool is_link_bw_min = (cur_link_settings.link_rate <= LINK_RATE_LOW) && (cur_link_settings.lane_count <= LANE_COUNT_ONE); dp_trace_commit_lt_init(link); if (link_dp_get_encoding_format(&cur_link_settings) == DP_8b_10b_ENCODING) link_hwss->setup_stream_encoder(pipe_ctx); dp_trace_set_lt_start_timestamp(link, false); j = 0; while (j < attempts && fail_count < (attempts * 10)) { DC_LOG_HW_LINK_TRAINING(""%s: Beginning link(%d) training attempt %u of %d @ rate(%d) x lane(%d) @ spread = %x\n"", __func__, link->link_index, (unsigned int)j + 1, attempts, cur_link_settings.link_rate, cur_link_settings.lane_count, cur_link_settings.link_spread); dp_enable_link_phy( link, &pipe_ctx->link_res, signal, pipe_ctx->clock_source->id, &cur_link_settings); if (stream->sink_patches.dppowerup_delay > 0) { int delay_dp_power_up_in_ms = stream->sink_patches.dppowerup_delay; msleep(delay_dp_power_up_in_ms); } edp_set_panel_assr(link, pipe_ctx, &panel_mode, true); dp_set_panel_mode(link, panel_mode); if (link->aux_access_disabled) { dp_perform_link_training_skip_aux(link, &pipe_ctx->link_res, &cur_link_settings); return true; } else { if (link->ep_type == DISPLAY_ENDPOINT_USB4_DPIA) { status = dpia_perform_link_training( link, &pipe_ctx->link_res, &cur_link_settings, skip_video_pattern); if (status == LINK_TRAINING_SUCCESS && !is_link_bw_low) { dp_set_hw_test_pattern(link, &pipe_ctx->link_res, DP_TEST_PATTERN_VIDEO_MODE, NULL, 0); if (stream->signal == SIGNAL_TYPE_DISPLAY_PORT_MST) { link->verified_link_cap.link_rate = link->cur_link_settings.link_rate; link->verified_link_cap.lane_count = link->cur_link_settings.lane_count; dm_helpers_dp_mst_update_branch_bandwidth(link->ctx, link); } } } else { status = dp_perform_link_training( link, &pipe_ctx->link_res, &cur_link_settings, skip_video_pattern); } dp_trace_lt_total_count_increment(link, false); dp_trace_lt_result_update(link, status, false); dp_trace_set_lt_end_timestamp(link, false); if (status == LINK_TRAINING_SUCCESS && !is_link_bw_low) return true; } fail_count++; dp_trace_lt_fail_count_update(link, fail_count, false); if (link->ep_type == DISPLAY_ENDPOINT_PHY) { if (j == (attempts - 1) || (status == LINK_TRAINING_ABORT)) break; } if (j == (attempts - 1)) { DC_LOG_WARNING( ""%s: Link(%d) training attempt %u of %d failed @ rate(%d) x lane(%d) @ spread = %x : fail reason:(%d)\n"", __func__, link->link_index, (unsigned int)j + 1, attempts, cur_link_settings.link_rate, cur_link_settings.lane_count, cur_link_settings.link_spread, status); } else { DC_LOG_HW_LINK_TRAINING( ""%s: Link(%d) training attempt %u of %d failed @ rate(%d) x lane(%d) @ spread = %x : fail reason:(%d)\n"", __func__, link->link_index, (unsigned int)j + 1, attempts, cur_link_settings.link_rate, cur_link_settings.lane_count, cur_link_settings.link_spread, status); } dp_disable_link_phy(link, &pipe_ctx->link_res, signal); if (status == LINK_TRAINING_ABORT) { enum dc_connection_type type = dc_connection_none; if (link_detect_connection_type(link, &type) && type == dc_connection_none) { DC_LOG_HW_LINK_TRAINING(""%s: Aborting training because sink unplugged\n"", __func__); break; } } if (!do_fallback || (status == LINK_TRAINING_ABORT) || (status == LINK_TRAINING_SUCCESS && is_link_bw_low) || is_link_bw_min) { j++; cur_link_settings = *link_setting; delay_between_attempts += LINK_TRAINING_RETRY_DELAY; is_link_bw_low = false; is_link_bw_min = (cur_link_settings.link_rate <= LINK_RATE_LOW) && (cur_link_settings.lane_count <= LANE_COUNT_ONE); } else if (do_fallback) { uint32_t req_bw; uint32_t link_bw; enum dc_link_encoding_format link_encoding = DC_LINK_ENCODING_UNSPECIFIED; decide_fallback_link_setting(link, &max_link_settings, &cur_link_settings, status); if (link_dp_get_encoding_format(&cur_link_settings) == DP_8b_10b_ENCODING) link_encoding = DC_LINK_ENCODING_DP_8b_10b; else if (link_dp_get_encoding_format(&cur_link_settings) == DP_128b_132b_ENCODING) link_encoding = DC_LINK_ENCODING_DP_128b_132b; req_bw = dc_bandwidth_in_kbps_from_timing(&stream->timing, link_encoding); link_bw = dp_link_bandwidth_kbps(link, &cur_link_settings); is_link_bw_low = (req_bw > link_bw); is_link_bw_min = ((cur_link_settings.link_rate <= LINK_RATE_LOW) && (cur_link_settings.lane_count <= LANE_COUNT_ONE)); if (is_link_bw_low) { DC_LOG_WARNING( ""%s: Link(%d) bandwidth too low after fallback req_bw(%d) > link_bw(%d)\n"", __func__, link->link_index, req_bw, link_bw); return false; } } msleep(delay_between_attempts); } return false; }"
799----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50074/bad/procfs.c----do_active_device,"static int do_active_device(const struct ctl_table *table, int write, void *result, size_t *lenp, loff_t *ppos) { struct parport *port = (struct parport *)table->extra1; char buffer[256]; struct pardevice *dev; int len = 0; if (write) return -EACCES; if (*ppos) { *lenp = 0; return 0; } for (dev = port->devices; dev ; dev = dev->next) { if(dev == port->cad) { <S2SV_StartVul> len += snprintf(buffer, sizeof(buffer), ""%s\n"", dev->name); <S2SV_EndVul> } } if(!len) { <S2SV_StartVul> len += snprintf(buffer, sizeof(buffer), ""%s\n"", ""none""); <S2SV_EndVul> } if (len > *lenp) len = *lenp; else *lenp = len; *ppos += len; memcpy(result, buffer, len); return 0; }","- len += snprintf(buffer, sizeof(buffer), ""%s\n"", dev->name);
- len += snprintf(buffer, sizeof(buffer), ""%s\n"", ""none"");
+ len += scnprintf(buffer, sizeof(buffer), ""%s\n"", dev->name);
+ len += scnprintf(buffer, sizeof(buffer), ""%s\n"", ""none"");","static int do_active_device(const struct ctl_table *table, int write, void *result, size_t *lenp, loff_t *ppos) { struct parport *port = (struct parport *)table->extra1; char buffer[256]; struct pardevice *dev; int len = 0; if (write) return -EACCES; if (*ppos) { *lenp = 0; return 0; } for (dev = port->devices; dev ; dev = dev->next) { if(dev == port->cad) { len += scnprintf(buffer, sizeof(buffer), ""%s\n"", dev->name); } } if(!len) { len += scnprintf(buffer, sizeof(buffer), ""%s\n"", ""none""); } if (len > *lenp) len = *lenp; else *lenp = len; *ppos += len; memcpy(result, buffer, len); return 0; }"
731----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50006/bad/migrate.c----ext4_ind_migrate,"int ext4_ind_migrate(struct inode *inode) { struct ext4_extent_header *eh; struct ext4_sb_info *sbi = EXT4_SB(inode->i_sb); struct ext4_super_block *es = sbi->s_es; struct ext4_inode_info *ei = EXT4_I(inode); struct ext4_extent *ex; unsigned int i, len; ext4_lblk_t start, end; ext4_fsblk_t blk; handle_t *handle; int ret, ret2 = 0; int alloc_ctx; if (!ext4_has_feature_extents(inode->i_sb) || (!ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))) return -EINVAL; if (ext4_has_feature_bigalloc(inode->i_sb)) return -EOPNOTSUPP; if (test_opt(inode->i_sb, DELALLOC)) ext4_alloc_da_blocks(inode); alloc_ctx = ext4_writepages_down_write(inode->i_sb); handle = ext4_journal_start(inode, EXT4_HT_MIGRATE, 1); if (IS_ERR(handle)) { ret = PTR_ERR(handle); goto out_unlock; } down_write(&EXT4_I(inode)->i_data_sem); ret = ext4_ext_check_inode(inode); if (ret) goto errout; eh = ext_inode_hdr(inode); ex = EXT_FIRST_EXTENT(eh); if (ext4_blocks_count(es) > EXT4_MAX_BLOCK_FILE_PHYS || eh->eh_depth != 0 || le16_to_cpu(eh->eh_entries) > 1) { ret = -EOPNOTSUPP; goto errout; } if (eh->eh_entries == 0) blk = len = start = end = 0; else { len = le16_to_cpu(ex->ee_len); blk = ext4_ext_pblock(ex); start = le32_to_cpu(ex->ee_block); end = start + len - 1; if (end >= EXT4_NDIR_BLOCKS) { ret = -EOPNOTSUPP; goto errout; } } ext4_clear_inode_flag(inode, EXT4_INODE_EXTENTS); memset(ei->i_data, 0, sizeof(ei->i_data)); for (i = start; i <= end; i++) ei->i_data[i] = cpu_to_le32(blk++); ret2 = ext4_mark_inode_dirty(handle, inode); if (unlikely(ret2 && !ret)) ret = ret2; errout: <S2SV_StartVul> ext4_journal_stop(handle); <S2SV_EndVul> up_write(&EXT4_I(inode)->i_data_sem); out_unlock: ext4_writepages_up_write(inode->i_sb, alloc_ctx); return ret; }","- ext4_journal_stop(handle);
+ ext4_journal_stop(handle);","int ext4_ind_migrate(struct inode *inode) { struct ext4_extent_header *eh; struct ext4_sb_info *sbi = EXT4_SB(inode->i_sb); struct ext4_super_block *es = sbi->s_es; struct ext4_inode_info *ei = EXT4_I(inode); struct ext4_extent *ex; unsigned int i, len; ext4_lblk_t start, end; ext4_fsblk_t blk; handle_t *handle; int ret, ret2 = 0; int alloc_ctx; if (!ext4_has_feature_extents(inode->i_sb) || (!ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))) return -EINVAL; if (ext4_has_feature_bigalloc(inode->i_sb)) return -EOPNOTSUPP; if (test_opt(inode->i_sb, DELALLOC)) ext4_alloc_da_blocks(inode); alloc_ctx = ext4_writepages_down_write(inode->i_sb); handle = ext4_journal_start(inode, EXT4_HT_MIGRATE, 1); if (IS_ERR(handle)) { ret = PTR_ERR(handle); goto out_unlock; } down_write(&EXT4_I(inode)->i_data_sem); ret = ext4_ext_check_inode(inode); if (ret) goto errout; eh = ext_inode_hdr(inode); ex = EXT_FIRST_EXTENT(eh); if (ext4_blocks_count(es) > EXT4_MAX_BLOCK_FILE_PHYS || eh->eh_depth != 0 || le16_to_cpu(eh->eh_entries) > 1) { ret = -EOPNOTSUPP; goto errout; } if (eh->eh_entries == 0) blk = len = start = end = 0; else { len = le16_to_cpu(ex->ee_len); blk = ext4_ext_pblock(ex); start = le32_to_cpu(ex->ee_block); end = start + len - 1; if (end >= EXT4_NDIR_BLOCKS) { ret = -EOPNOTSUPP; goto errout; } } ext4_clear_inode_flag(inode, EXT4_INODE_EXTENTS); memset(ei->i_data, 0, sizeof(ei->i_data)); for (i = start; i <= end; i++) ei->i_data[i] = cpu_to_le32(blk++); ret2 = ext4_mark_inode_dirty(handle, inode); if (unlikely(ret2 && !ret)) ret = ret2; errout: up_write(&EXT4_I(inode)->i_data_sem); ext4_journal_stop(handle); out_unlock: ext4_writepages_up_write(inode->i_sb, alloc_ctx); return ret; }"
891----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50141/bad/prmt.c----acpi_parse_prmt,"acpi_parse_prmt(union acpi_subtable_headers *header, const unsigned long end) { struct acpi_prmt_module_info *module_info; struct acpi_prmt_handler_info *handler_info; struct prm_handler_info *th; struct prm_module_info *tm; u64 mmio_count = 0; u64 cur_handler = 0; u32 module_info_size = 0; u64 mmio_range_size = 0; void *temp_mmio; module_info = (struct acpi_prmt_module_info *) header; module_info_size = struct_size(tm, handlers, module_info->handler_info_count); tm = kmalloc(module_info_size, GFP_KERNEL); guid_copy(&tm->guid, (guid_t *) module_info->module_guid); tm->major_rev = module_info->major_rev; tm->minor_rev = module_info->minor_rev; tm->handler_count = module_info->handler_info_count; tm->updatable = true; if (module_info->mmio_list_pointer) { mmio_count = *(u64 *) memremap(module_info->mmio_list_pointer, 8, MEMREMAP_WB); mmio_range_size = struct_size(tm->mmio_info, addr_ranges, mmio_count); tm->mmio_info = kmalloc(mmio_range_size, GFP_KERNEL); temp_mmio = memremap(module_info->mmio_list_pointer, mmio_range_size, MEMREMAP_WB); memmove(tm->mmio_info, temp_mmio, mmio_range_size); } else { mmio_range_size = struct_size(tm->mmio_info, addr_ranges, mmio_count); tm->mmio_info = kmalloc(mmio_range_size, GFP_KERNEL); tm->mmio_info->mmio_count = 0; } INIT_LIST_HEAD(&tm->module_list); list_add(&tm->module_list, &prm_module_list); handler_info = get_first_handler(module_info); do { th = &tm->handlers[cur_handler]; guid_copy(&th->guid, (guid_t *)handler_info->handler_guid); <S2SV_StartVul> th->handler_addr = (void *)efi_pa_va_lookup(handler_info->handler_address); <S2SV_EndVul> <S2SV_StartVul> th->static_data_buffer_addr = efi_pa_va_lookup(handler_info->static_data_buffer_address); <S2SV_EndVul> <S2SV_StartVul> th->acpi_param_buffer_addr = efi_pa_va_lookup(handler_info->acpi_param_buffer_address); <S2SV_EndVul> } while (++cur_handler < tm->handler_count && (handler_info = get_next_handler(handler_info))); return 0; }","- th->handler_addr = (void *)efi_pa_va_lookup(handler_info->handler_address);
- th->static_data_buffer_addr = efi_pa_va_lookup(handler_info->static_data_buffer_address);
- th->acpi_param_buffer_addr = efi_pa_va_lookup(handler_info->acpi_param_buffer_address);
+ th->handler_addr =
+ (void *)efi_pa_va_lookup(&th->guid, handler_info->handler_address);
+ th->static_data_buffer_addr =
+ efi_pa_va_lookup(&th->guid, handler_info->static_data_buffer_address);
+ th->acpi_param_buffer_addr =
+ efi_pa_va_lookup(&th->guid, handler_info->acpi_param_buffer_address);","acpi_parse_prmt(union acpi_subtable_headers *header, const unsigned long end) { struct acpi_prmt_module_info *module_info; struct acpi_prmt_handler_info *handler_info; struct prm_handler_info *th; struct prm_module_info *tm; u64 mmio_count = 0; u64 cur_handler = 0; u32 module_info_size = 0; u64 mmio_range_size = 0; void *temp_mmio; module_info = (struct acpi_prmt_module_info *) header; module_info_size = struct_size(tm, handlers, module_info->handler_info_count); tm = kmalloc(module_info_size, GFP_KERNEL); guid_copy(&tm->guid, (guid_t *) module_info->module_guid); tm->major_rev = module_info->major_rev; tm->minor_rev = module_info->minor_rev; tm->handler_count = module_info->handler_info_count; tm->updatable = true; if (module_info->mmio_list_pointer) { mmio_count = *(u64 *) memremap(module_info->mmio_list_pointer, 8, MEMREMAP_WB); mmio_range_size = struct_size(tm->mmio_info, addr_ranges, mmio_count); tm->mmio_info = kmalloc(mmio_range_size, GFP_KERNEL); temp_mmio = memremap(module_info->mmio_list_pointer, mmio_range_size, MEMREMAP_WB); memmove(tm->mmio_info, temp_mmio, mmio_range_size); } else { mmio_range_size = struct_size(tm->mmio_info, addr_ranges, mmio_count); tm->mmio_info = kmalloc(mmio_range_size, GFP_KERNEL); tm->mmio_info->mmio_count = 0; } INIT_LIST_HEAD(&tm->module_list); list_add(&tm->module_list, &prm_module_list); handler_info = get_first_handler(module_info); do { th = &tm->handlers[cur_handler]; guid_copy(&th->guid, (guid_t *)handler_info->handler_guid); th->handler_addr = (void *)efi_pa_va_lookup(&th->guid, handler_info->handler_address); th->static_data_buffer_addr = efi_pa_va_lookup(&th->guid, handler_info->static_data_buffer_address); th->acpi_param_buffer_addr = efi_pa_va_lookup(&th->guid, handler_info->acpi_param_buffer_address); } while (++cur_handler < tm->handler_count && (handler_info = get_next_handler(handler_info))); return 0; }"
468----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47709/bad/bcm.c----bcm_notify,"static void bcm_notify(struct bcm_sock *bo, unsigned long msg, struct net_device *dev) { struct sock *sk = &bo->sk; struct bcm_op *op; int notify_enodev = 0; if (!net_eq(dev_net(dev), sock_net(sk))) return; switch (msg) { case NETDEV_UNREGISTER: lock_sock(sk); list_for_each_entry(op, &bo->rx_ops, list) if (op->rx_reg_dev == dev) bcm_rx_unreg(dev, op); if (bo->bound && bo->ifindex == dev->ifindex) { #if IS_ENABLED(CONFIG_PROC_FS) <S2SV_StartVul> if (sock_net(sk)->can.bcmproc_dir && bo->bcm_proc_read) <S2SV_EndVul> remove_proc_entry(bo->procname, sock_net(sk)->can.bcmproc_dir); #endif bo->bound = 0; bo->ifindex = 0; notify_enodev = 1; } release_sock(sk); if (notify_enodev) { sk->sk_err = ENODEV; if (!sock_flag(sk, SOCK_DEAD)) sk_error_report(sk); } break; case NETDEV_DOWN: if (bo->bound && bo->ifindex == dev->ifindex) { sk->sk_err = ENETDOWN; if (!sock_flag(sk, SOCK_DEAD)) sk_error_report(sk); } } }","- if (sock_net(sk)->can.bcmproc_dir && bo->bcm_proc_read)
+ if (sock_net(sk)->can.bcmproc_dir && bo->bcm_proc_read) {
+ bo->bcm_proc_read = NULL;
+ }","static void bcm_notify(struct bcm_sock *bo, unsigned long msg, struct net_device *dev) { struct sock *sk = &bo->sk; struct bcm_op *op; int notify_enodev = 0; if (!net_eq(dev_net(dev), sock_net(sk))) return; switch (msg) { case NETDEV_UNREGISTER: lock_sock(sk); list_for_each_entry(op, &bo->rx_ops, list) if (op->rx_reg_dev == dev) bcm_rx_unreg(dev, op); if (bo->bound && bo->ifindex == dev->ifindex) { #if IS_ENABLED(CONFIG_PROC_FS) if (sock_net(sk)->can.bcmproc_dir && bo->bcm_proc_read) { remove_proc_entry(bo->procname, sock_net(sk)->can.bcmproc_dir); bo->bcm_proc_read = NULL; } #endif bo->bound = 0; bo->ifindex = 0; notify_enodev = 1; } release_sock(sk); if (notify_enodev) { sk->sk_err = ENODEV; if (!sock_flag(sk, SOCK_DEAD)) sk_error_report(sk); } break; case NETDEV_DOWN: if (bo->bound && bo->ifindex == dev->ifindex) { sk->sk_err = ENETDOWN; if (!sock_flag(sk, SOCK_DEAD)) sk_error_report(sk); } } }"
991----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50227/bad/retimer.c----tb_retimer_scan,"int tb_retimer_scan(struct tb_port *port, bool add) { u32 status[TB_MAX_RETIMER_INDEX + 1] = {}; int ret, i, max, last_idx = 0; ret = usb4_port_enumerate_retimers(port); if (ret) return ret; tb_retimer_nvm_authenticate_status(port, status); tb_retimer_set_inbound_sbtx(port); <S2SV_StartVul> for (i = 1; i <= TB_MAX_RETIMER_INDEX; i++) { <S2SV_EndVul> ret = usb4_port_retimer_is_last(port, i); if (ret > 0) last_idx = i; else if (ret < 0) break; } <S2SV_StartVul> max = i; <S2SV_EndVul> ret = 0; for (i = 1; i <= max; i++) { struct tb_retimer *rt; if (usb4_port_retimer_is_cable(port, i)) continue; rt = tb_port_find_retimer(port, i); if (rt) { put_device(&rt->dev); } else if (add) { ret = tb_retimer_add(port, i, status[i], i <= last_idx); if (ret && ret != -EOPNOTSUPP) break; } } tb_retimer_unset_inbound_sbtx(port); return ret; }","- for (i = 1; i <= TB_MAX_RETIMER_INDEX; i++) {
- max = i;
+ for (max = 1, i = 1; i <= TB_MAX_RETIMER_INDEX; i++) {
+ max = i;","int tb_retimer_scan(struct tb_port *port, bool add) { u32 status[TB_MAX_RETIMER_INDEX + 1] = {}; int ret, i, max, last_idx = 0; ret = usb4_port_enumerate_retimers(port); if (ret) return ret; tb_retimer_nvm_authenticate_status(port, status); tb_retimer_set_inbound_sbtx(port); for (max = 1, i = 1; i <= TB_MAX_RETIMER_INDEX; i++) { ret = usb4_port_retimer_is_last(port, i); if (ret > 0) last_idx = i; else if (ret < 0) break; max = i; } ret = 0; for (i = 1; i <= max; i++) { struct tb_retimer *rt; if (usb4_port_retimer_is_cable(port, i)) continue; rt = tb_port_find_retimer(port, i); if (rt) { put_device(&rt->dev); } else if (add) { ret = tb_retimer_add(port, i, status[i], i <= last_idx); if (ret && ret != -EOPNOTSUPP) break; } } tb_retimer_unset_inbound_sbtx(port); return ret; }"
51----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44935/bad/input.c----__sctp_unhash_endpoint,"static void __sctp_unhash_endpoint(struct sctp_endpoint *ep) { struct sock *sk = ep->base.sk; struct sctp_hashbucket *head; ep->hashent = sctp_ep_hashfn(sock_net(sk), ep->base.bind_addr.port); head = &sctp_ep_hashtable[ep->hashent]; if (rcu_access_pointer(sk->sk_reuseport_cb)) reuseport_detach_sock(sk); <S2SV_StartVul> write_lock(&head->lock); <S2SV_EndVul> hlist_del_init(&ep->node); write_unlock(&head->lock); }","- write_lock(&head->lock);
+ write_lock(&head->lock);","static void __sctp_unhash_endpoint(struct sctp_endpoint *ep) { struct sock *sk = ep->base.sk; struct sctp_hashbucket *head; ep->hashent = sctp_ep_hashfn(sock_net(sk), ep->base.bind_addr.port); head = &sctp_ep_hashtable[ep->hashent]; write_lock(&head->lock); if (rcu_access_pointer(sk->sk_reuseport_cb)) reuseport_detach_sock(sk); hlist_del_init(&ep->node); write_unlock(&head->lock); }"
752----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50028/bad/thermal_netlink.c----thermal_genl_cmd_tz_get_temp,"static int thermal_genl_cmd_tz_get_temp(struct param *p) { struct sk_buff *msg = p->msg; <S2SV_StartVul> struct thermal_zone_device *tz; <S2SV_EndVul> int temp, ret, id; if (!p->attrs[THERMAL_GENL_ATTR_TZ_ID]) return -EINVAL; id = nla_get_u32(p->attrs[THERMAL_GENL_ATTR_TZ_ID]); <S2SV_StartVul> tz = thermal_zone_get_by_id(id); <S2SV_EndVul> if (!tz) return -EINVAL; ret = thermal_zone_get_temp(tz, &temp); if (ret) return ret; if (nla_put_u32(msg, THERMAL_GENL_ATTR_TZ_ID, id) || nla_put_u32(msg, THERMAL_GENL_ATTR_TZ_TEMP, temp)) return -EMSGSIZE; return 0; }","- struct thermal_zone_device *tz;
- tz = thermal_zone_get_by_id(id);
+ CLASS(thermal_zone_get_by_id, tz)(id);","static int thermal_genl_cmd_tz_get_temp(struct param *p) { struct sk_buff *msg = p->msg; int temp, ret, id; if (!p->attrs[THERMAL_GENL_ATTR_TZ_ID]) return -EINVAL; id = nla_get_u32(p->attrs[THERMAL_GENL_ATTR_TZ_ID]); CLASS(thermal_zone_get_by_id, tz)(id); if (!tz) return -EINVAL; ret = thermal_zone_get_temp(tz, &temp); if (ret) return ret; if (nla_put_u32(msg, THERMAL_GENL_ATTR_TZ_ID, id) || nla_put_u32(msg, THERMAL_GENL_ATTR_TZ_TEMP, temp)) return -EMSGSIZE; return 0; }"
1091----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53075/bad/cacheinfo.c----populate_cache_leaves,"int populate_cache_leaves(unsigned int cpu) { struct cpu_cacheinfo *this_cpu_ci = get_cpu_cacheinfo(cpu); struct cacheinfo *this_leaf = this_cpu_ci->info_list; <S2SV_StartVul> struct device_node *np = of_cpu_device_node_get(cpu); <S2SV_EndVul> <S2SV_StartVul> struct device_node *prev = NULL; <S2SV_EndVul> int levels = 1, level = 1; if (!acpi_disabled) { int ret, fw_levels, split_levels; ret = acpi_get_cache_info(cpu, &fw_levels, &split_levels); if (ret) return ret; BUG_ON((split_levels > fw_levels) || (split_levels + fw_levels > this_cpu_ci->num_leaves)); for (; level <= this_cpu_ci->num_levels; level++) { if (level <= split_levels) { ci_leaf_init(this_leaf++, CACHE_TYPE_DATA, level); ci_leaf_init(this_leaf++, CACHE_TYPE_INST, level); } else { ci_leaf_init(this_leaf++, CACHE_TYPE_UNIFIED, level); } } return 0; } if (of_property_read_bool(np, ""cache-size"")) ci_leaf_init(this_leaf++, CACHE_TYPE_UNIFIED, level); if (of_property_read_bool(np, ""i-cache-size"")) ci_leaf_init(this_leaf++, CACHE_TYPE_INST, level); if (of_property_read_bool(np, ""d-cache-size"")) ci_leaf_init(this_leaf++, CACHE_TYPE_DATA, level); prev = np; while ((np = of_find_next_cache_node(np))) { of_node_put(prev); prev = np; if (!of_device_is_compatible(np, ""cache"")) break; if (of_property_read_u32(np, ""cache-level"", &level)) break; if (level <= levels) break; if (of_property_read_bool(np, ""cache-size"")) ci_leaf_init(this_leaf++, CACHE_TYPE_UNIFIED, level); if (of_property_read_bool(np, ""i-cache-size"")) ci_leaf_init(this_leaf++, CACHE_TYPE_INST, level); if (of_property_read_bool(np, ""d-cache-size"")) ci_leaf_init(this_leaf++, CACHE_TYPE_DATA, level); levels = level; } of_node_put(np); return 0; }","- struct device_node *np = of_cpu_device_node_get(cpu);
- struct device_node *prev = NULL;
+ struct device_node *np, *prev;
+ np = of_cpu_device_node_get(cpu);
+ if (!np)
+ return -ENOENT;","int populate_cache_leaves(unsigned int cpu) { struct cpu_cacheinfo *this_cpu_ci = get_cpu_cacheinfo(cpu); struct cacheinfo *this_leaf = this_cpu_ci->info_list; struct device_node *np, *prev; int levels = 1, level = 1; if (!acpi_disabled) { int ret, fw_levels, split_levels; ret = acpi_get_cache_info(cpu, &fw_levels, &split_levels); if (ret) return ret; BUG_ON((split_levels > fw_levels) || (split_levels + fw_levels > this_cpu_ci->num_leaves)); for (; level <= this_cpu_ci->num_levels; level++) { if (level <= split_levels) { ci_leaf_init(this_leaf++, CACHE_TYPE_DATA, level); ci_leaf_init(this_leaf++, CACHE_TYPE_INST, level); } else { ci_leaf_init(this_leaf++, CACHE_TYPE_UNIFIED, level); } } return 0; } np = of_cpu_device_node_get(cpu); if (!np) return -ENOENT; if (of_property_read_bool(np, ""cache-size"")) ci_leaf_init(this_leaf++, CACHE_TYPE_UNIFIED, level); if (of_property_read_bool(np, ""i-cache-size"")) ci_leaf_init(this_leaf++, CACHE_TYPE_INST, level); if (of_property_read_bool(np, ""d-cache-size"")) ci_leaf_init(this_leaf++, CACHE_TYPE_DATA, level); prev = np; while ((np = of_find_next_cache_node(np))) { of_node_put(prev); prev = np; if (!of_device_is_compatible(np, ""cache"")) break; if (of_property_read_u32(np, ""cache-level"", &level)) break; if (level <= levels) break; if (of_property_read_bool(np, ""cache-size"")) ci_leaf_init(this_leaf++, CACHE_TYPE_UNIFIED, level); if (of_property_read_bool(np, ""i-cache-size"")) ci_leaf_init(this_leaf++, CACHE_TYPE_INST, level); if (of_property_read_bool(np, ""d-cache-size"")) ci_leaf_init(this_leaf++, CACHE_TYPE_DATA, level); levels = level; } of_node_put(np); return 0; }"
290----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46765/bad/ice_main.c----ice_xdp_setup_prog,"ice_xdp_setup_prog(struct ice_vsi *vsi, struct bpf_prog *prog, struct netlink_ext_ack *extack) { unsigned int frame_size = vsi->netdev->mtu + ICE_ETH_PKT_HDR_PAD; bool if_running = netif_running(vsi->netdev); int ret = 0, xdp_ring_err = 0; if (prog && !prog->aux->xdp_has_frags) { if (frame_size > ice_max_xdp_frame_size(vsi)) { NL_SET_ERR_MSG_MOD(extack, ""MTU is too large for linear frames and XDP prog does not support frags""); return -EOPNOTSUPP; } } <S2SV_StartVul> if (ice_is_xdp_ena_vsi(vsi) == !!prog) { <S2SV_EndVul> ice_vsi_assign_bpf_prog(vsi, prog); return 0; } if (if_running && !test_and_set_bit(ICE_VSI_DOWN, vsi->state)) { ret = ice_down(vsi); if (ret) { NL_SET_ERR_MSG_MOD(extack, ""Preparing device for XDP attach failed""); return ret; } } if (!ice_is_xdp_ena_vsi(vsi) && prog) { xdp_ring_err = ice_vsi_determine_xdp_res(vsi); if (xdp_ring_err) { NL_SET_ERR_MSG_MOD(extack, ""Not enough Tx resources for XDP""); } else { xdp_ring_err = ice_prepare_xdp_rings(vsi, prog, ICE_XDP_CFG_FULL); if (xdp_ring_err) NL_SET_ERR_MSG_MOD(extack, ""Setting up XDP Tx resources failed""); } xdp_features_set_redirect_target(vsi->netdev, true); xdp_ring_err = ice_realloc_zc_buf(vsi, true); if (xdp_ring_err) NL_SET_ERR_MSG_MOD(extack, ""Setting up XDP Rx resources failed""); } else if (ice_is_xdp_ena_vsi(vsi) && !prog) { xdp_features_clear_redirect_target(vsi->netdev); xdp_ring_err = ice_destroy_xdp_rings(vsi, ICE_XDP_CFG_FULL); if (xdp_ring_err) NL_SET_ERR_MSG_MOD(extack, ""Freeing XDP Tx resources failed""); xdp_ring_err = ice_realloc_zc_buf(vsi, false); if (xdp_ring_err) NL_SET_ERR_MSG_MOD(extack, ""Freeing XDP Rx resources failed""); } if (if_running) ret = ice_up(vsi); if (!ret && prog) ice_vsi_rx_napi_schedule(vsi); return (ret || xdp_ring_err) ? -ENOMEM : 0; }","- if (ice_is_xdp_ena_vsi(vsi) == !!prog) {
+ if (ice_is_xdp_ena_vsi(vsi) == !!prog ||
+ test_bit(ICE_VSI_REBUILD_PENDING, vsi->state)) {","ice_xdp_setup_prog(struct ice_vsi *vsi, struct bpf_prog *prog, struct netlink_ext_ack *extack) { unsigned int frame_size = vsi->netdev->mtu + ICE_ETH_PKT_HDR_PAD; bool if_running = netif_running(vsi->netdev); int ret = 0, xdp_ring_err = 0; if (prog && !prog->aux->xdp_has_frags) { if (frame_size > ice_max_xdp_frame_size(vsi)) { NL_SET_ERR_MSG_MOD(extack, ""MTU is too large for linear frames and XDP prog does not support frags""); return -EOPNOTSUPP; } } if (ice_is_xdp_ena_vsi(vsi) == !!prog || test_bit(ICE_VSI_REBUILD_PENDING, vsi->state)) { ice_vsi_assign_bpf_prog(vsi, prog); return 0; } if (if_running && !test_and_set_bit(ICE_VSI_DOWN, vsi->state)) { ret = ice_down(vsi); if (ret) { NL_SET_ERR_MSG_MOD(extack, ""Preparing device for XDP attach failed""); return ret; } } if (!ice_is_xdp_ena_vsi(vsi) && prog) { xdp_ring_err = ice_vsi_determine_xdp_res(vsi); if (xdp_ring_err) { NL_SET_ERR_MSG_MOD(extack, ""Not enough Tx resources for XDP""); } else { xdp_ring_err = ice_prepare_xdp_rings(vsi, prog, ICE_XDP_CFG_FULL); if (xdp_ring_err) NL_SET_ERR_MSG_MOD(extack, ""Setting up XDP Tx resources failed""); } xdp_features_set_redirect_target(vsi->netdev, true); xdp_ring_err = ice_realloc_zc_buf(vsi, true); if (xdp_ring_err) NL_SET_ERR_MSG_MOD(extack, ""Setting up XDP Rx resources failed""); } else if (ice_is_xdp_ena_vsi(vsi) && !prog) { xdp_features_clear_redirect_target(vsi->netdev); xdp_ring_err = ice_destroy_xdp_rings(vsi, ICE_XDP_CFG_FULL); if (xdp_ring_err) NL_SET_ERR_MSG_MOD(extack, ""Freeing XDP Tx resources failed""); xdp_ring_err = ice_realloc_zc_buf(vsi, false); if (xdp_ring_err) NL_SET_ERR_MSG_MOD(extack, ""Freeing XDP Rx resources failed""); } if (if_running) ret = ice_up(vsi); if (!ret && prog) ice_vsi_rx_napi_schedule(vsi); return (ret || xdp_ring_err) ? -ENOMEM : 0; }"
526----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-48881/bad/super.c----cache_set_flush,"static void cache_set_flush(struct closure *cl) { struct cache_set *c = container_of(cl, struct cache_set, caching); struct cache *ca = c->cache; struct btree *b; bch_cache_accounting_destroy(&c->accounting); kobject_put(&c->internal); kobject_del(&c->kobj); if (!IS_ERR_OR_NULL(c->gc_thread)) kthread_stop(c->gc_thread); <S2SV_StartVul> if (!IS_ERR(c->root)) <S2SV_EndVul> list_add(&c->root->list, &c->btree_cache); if (!test_bit(CACHE_SET_IO_DISABLE, &c->flags)) list_for_each_entry(b, &c->btree_cache, list) { mutex_lock(&b->write_lock); if (btree_node_dirty(b)) __bch_btree_node_write(b, NULL); mutex_unlock(&b->write_lock); } if (ca->alloc_thread) kthread_stop(ca->alloc_thread); if (c->journal.cur) { cancel_delayed_work_sync(&c->journal.work); c->journal.work.work.func(&c->journal.work.work); } closure_return(cl); }","- if (!IS_ERR(c->root))
+ if (!IS_ERR_OR_NULL(c->root))","static void cache_set_flush(struct closure *cl) { struct cache_set *c = container_of(cl, struct cache_set, caching); struct cache *ca = c->cache; struct btree *b; bch_cache_accounting_destroy(&c->accounting); kobject_put(&c->internal); kobject_del(&c->kobj); if (!IS_ERR_OR_NULL(c->gc_thread)) kthread_stop(c->gc_thread); if (!IS_ERR_OR_NULL(c->root)) list_add(&c->root->list, &c->btree_cache); if (!test_bit(CACHE_SET_IO_DISABLE, &c->flags)) list_for_each_entry(b, &c->btree_cache, list) { mutex_lock(&b->write_lock); if (btree_node_dirty(b)) __bch_btree_node_write(b, NULL); mutex_unlock(&b->write_lock); } if (ca->alloc_thread) kthread_stop(ca->alloc_thread); if (c->journal.cur) { cancel_delayed_work_sync(&c->journal.work); c->journal.work.work.func(&c->journal.work.work); } closure_return(cl); }"
1146----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53122/bad/protocol.c----mptcp_rcv_space_adjust,"static void mptcp_rcv_space_adjust(struct mptcp_sock *msk, int copied) { struct mptcp_subflow_context *subflow; struct sock *sk = (struct sock *)msk; u8 scaling_ratio = U8_MAX; u32 time, advmss = 1; u64 rtt_us, mstamp; msk_owned_by_me(msk); if (copied <= 0) return; if (!msk->rcvspace_init) mptcp_rcv_space_init(msk, msk->first); msk->rcvq_space.copied += copied; mstamp = div_u64(tcp_clock_ns(), NSEC_PER_USEC); time = tcp_stamp_us_delta(mstamp, msk->rcvq_space.time); rtt_us = msk->rcvq_space.rtt_us; if (rtt_us && time < (rtt_us >> 3)) return; rtt_us = 0; mptcp_for_each_subflow(msk, subflow) { const struct tcp_sock *tp; u64 sf_rtt_us; u32 sf_advmss; tp = tcp_sk(mptcp_subflow_tcp_sock(subflow)); sf_rtt_us = READ_ONCE(tp->rcv_rtt_est.rtt_us); sf_advmss = READ_ONCE(tp->advmss); rtt_us = max(sf_rtt_us, rtt_us); advmss = max(sf_advmss, advmss); scaling_ratio = min(tp->scaling_ratio, scaling_ratio); } msk->rcvq_space.rtt_us = rtt_us; msk->scaling_ratio = scaling_ratio; if (time < (rtt_us >> 3) || rtt_us == 0) return; if (msk->rcvq_space.copied <= msk->rcvq_space.space) goto new_measure; if (READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_moderate_rcvbuf) && !(sk->sk_userlocks & SOCK_RCVBUF_LOCK)) { u64 rcvwin, grow; int rcvbuf; rcvwin = ((u64)msk->rcvq_space.copied << 1) + 16 * advmss; grow = rcvwin * (msk->rcvq_space.copied - msk->rcvq_space.space); do_div(grow, msk->rcvq_space.space); rcvwin += (grow << 1); rcvbuf = min_t(u64, __tcp_space_from_win(scaling_ratio, rcvwin), READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_rmem[2])); if (rcvbuf > sk->sk_rcvbuf) { u32 window_clamp; window_clamp = __tcp_win_from_space(scaling_ratio, rcvbuf); WRITE_ONCE(sk->sk_rcvbuf, rcvbuf); mptcp_for_each_subflow(msk, subflow) { struct sock *ssk; bool slow; ssk = mptcp_subflow_tcp_sock(subflow); slow = lock_sock_fast(ssk); WRITE_ONCE(ssk->sk_rcvbuf, rcvbuf); WRITE_ONCE(tcp_sk(ssk)->window_clamp, window_clamp); <S2SV_StartVul> tcp_cleanup_rbuf(ssk, 1); <S2SV_EndVul> unlock_sock_fast(ssk, slow); } } } msk->rcvq_space.space = msk->rcvq_space.copied; new_measure: msk->rcvq_space.copied = 0; msk->rcvq_space.time = mstamp; }","- tcp_cleanup_rbuf(ssk, 1);
+ if (tcp_can_send_ack(ssk))
+ tcp_cleanup_rbuf(ssk, 1);","static void mptcp_rcv_space_adjust(struct mptcp_sock *msk, int copied) { struct mptcp_subflow_context *subflow; struct sock *sk = (struct sock *)msk; u8 scaling_ratio = U8_MAX; u32 time, advmss = 1; u64 rtt_us, mstamp; msk_owned_by_me(msk); if (copied <= 0) return; if (!msk->rcvspace_init) mptcp_rcv_space_init(msk, msk->first); msk->rcvq_space.copied += copied; mstamp = div_u64(tcp_clock_ns(), NSEC_PER_USEC); time = tcp_stamp_us_delta(mstamp, msk->rcvq_space.time); rtt_us = msk->rcvq_space.rtt_us; if (rtt_us && time < (rtt_us >> 3)) return; rtt_us = 0; mptcp_for_each_subflow(msk, subflow) { const struct tcp_sock *tp; u64 sf_rtt_us; u32 sf_advmss; tp = tcp_sk(mptcp_subflow_tcp_sock(subflow)); sf_rtt_us = READ_ONCE(tp->rcv_rtt_est.rtt_us); sf_advmss = READ_ONCE(tp->advmss); rtt_us = max(sf_rtt_us, rtt_us); advmss = max(sf_advmss, advmss); scaling_ratio = min(tp->scaling_ratio, scaling_ratio); } msk->rcvq_space.rtt_us = rtt_us; msk->scaling_ratio = scaling_ratio; if (time < (rtt_us >> 3) || rtt_us == 0) return; if (msk->rcvq_space.copied <= msk->rcvq_space.space) goto new_measure; if (READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_moderate_rcvbuf) && !(sk->sk_userlocks & SOCK_RCVBUF_LOCK)) { u64 rcvwin, grow; int rcvbuf; rcvwin = ((u64)msk->rcvq_space.copied << 1) + 16 * advmss; grow = rcvwin * (msk->rcvq_space.copied - msk->rcvq_space.space); do_div(grow, msk->rcvq_space.space); rcvwin += (grow << 1); rcvbuf = min_t(u64, __tcp_space_from_win(scaling_ratio, rcvwin), READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_rmem[2])); if (rcvbuf > sk->sk_rcvbuf) { u32 window_clamp; window_clamp = __tcp_win_from_space(scaling_ratio, rcvbuf); WRITE_ONCE(sk->sk_rcvbuf, rcvbuf); mptcp_for_each_subflow(msk, subflow) { struct sock *ssk; bool slow; ssk = mptcp_subflow_tcp_sock(subflow); slow = lock_sock_fast(ssk); WRITE_ONCE(ssk->sk_rcvbuf, rcvbuf); WRITE_ONCE(tcp_sk(ssk)->window_clamp, window_clamp); if (tcp_can_send_ack(ssk)) tcp_cleanup_rbuf(ssk, 1); unlock_sock_fast(ssk, slow); } } } msk->rcvq_space.space = msk->rcvq_space.copied; new_measure: msk->rcvq_space.copied = 0; msk->rcvq_space.time = mstamp; }"
816----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50086/bad/user_session.c----ksmbd_expire_session,"static void ksmbd_expire_session(struct ksmbd_conn *conn) { unsigned long id; struct ksmbd_session *sess; down_write(&conn->session_lock); xa_for_each(&conn->sessions, id, sess) { <S2SV_StartVul> if (sess->state != SMB2_SESSION_VALID || <S2SV_EndVul> <S2SV_StartVul> time_after(jiffies, <S2SV_EndVul> <S2SV_StartVul> sess->last_active + SMB2_SESSION_TIMEOUT)) { <S2SV_EndVul> xa_erase(&conn->sessions, sess->id); hash_del(&sess->hlist); ksmbd_session_destroy(sess); continue; } } up_write(&conn->session_lock); }","- if (sess->state != SMB2_SESSION_VALID ||
- time_after(jiffies,
- sess->last_active + SMB2_SESSION_TIMEOUT)) {
+ if (atomic_read(&sess->refcnt) == 0 &&
+ (sess->state != SMB2_SESSION_VALID ||
+ time_after(jiffies,
+ sess->last_active + SMB2_SESSION_TIMEOUT))) {","static void ksmbd_expire_session(struct ksmbd_conn *conn) { unsigned long id; struct ksmbd_session *sess; down_write(&conn->session_lock); xa_for_each(&conn->sessions, id, sess) { if (atomic_read(&sess->refcnt) == 0 && (sess->state != SMB2_SESSION_VALID || time_after(jiffies, sess->last_active + SMB2_SESSION_TIMEOUT))) { xa_erase(&conn->sessions, sess->id); hash_del(&sess->hlist); ksmbd_session_destroy(sess); continue; } } up_write(&conn->session_lock); }"
1005----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50242/bad/file.c----ntfs_file_release,"static int ntfs_file_release(struct inode *inode, struct file *file) { struct ntfs_inode *ni = ntfs_i(inode); struct ntfs_sb_info *sbi = ni->mi.sbi; int err = 0; <S2SV_StartVul> if (sbi->options->prealloc && ((file->f_mode & FMODE_WRITE) && <S2SV_EndVul> <S2SV_StartVul> atomic_read(&inode->i_writecount) == 1)) { <S2SV_EndVul> ni_lock(ni); down_write(&ni->file.run_lock); err = attr_set_size(ni, ATTR_DATA, NULL, 0, &ni->file.run, inode->i_size, &ni->i_valid, false, NULL); up_write(&ni->file.run_lock); ni_unlock(ni); } return err; }","- if (sbi->options->prealloc && ((file->f_mode & FMODE_WRITE) &&
- atomic_read(&inode->i_writecount) == 1)) {
+ if (sbi->options->prealloc &&
+ ((file->f_mode & FMODE_WRITE) &&
+ atomic_read(&inode->i_writecount) == 1)
+ && inode->i_ino != MFT_REC_MFT) {","static int ntfs_file_release(struct inode *inode, struct file *file) { struct ntfs_inode *ni = ntfs_i(inode); struct ntfs_sb_info *sbi = ni->mi.sbi; int err = 0; if (sbi->options->prealloc && ((file->f_mode & FMODE_WRITE) && atomic_read(&inode->i_writecount) == 1) && inode->i_ino != MFT_REC_MFT) { ni_lock(ni); down_write(&ni->file.run_lock); err = attr_set_size(ni, ATTR_DATA, NULL, 0, &ni->file.run, inode->i_size, &ni->i_valid, false, NULL); up_write(&ni->file.run_lock); ni_unlock(ni); } return err; }"
1351----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56672/bad/blk-cgroup.c----blkcg_unpin_online,void blkcg_unpin_online(struct cgroup_subsys_state *blkcg_css) { struct blkcg *blkcg = css_to_blkcg(blkcg_css); do { if (!refcount_dec_and_test(&blkcg->online_pin)) break; blkcg_destroy_blkgs(blkcg); <S2SV_StartVul> blkcg = blkcg_parent(blkcg); <S2SV_EndVul> } while (blkcg); },"- blkcg = blkcg_parent(blkcg);
+ struct blkcg *parent;
+ parent = blkcg_parent(blkcg);
+ blkcg = parent;",void blkcg_unpin_online(struct cgroup_subsys_state *blkcg_css) { struct blkcg *blkcg = css_to_blkcg(blkcg_css); do { struct blkcg *parent; if (!refcount_dec_and_test(&blkcg->online_pin)) break; parent = blkcg_parent(blkcg); blkcg_destroy_blkgs(blkcg); blkcg = parent; } while (blkcg); }
988----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50226/bad/region.c----cxl_region_invalidate_memregion,"static int cxl_region_invalidate_memregion(struct cxl_region *cxlr) { if (!cpu_cache_has_invalidate_memregion()) { if (IS_ENABLED(CONFIG_CXL_REGION_INVALIDATION_TEST)) { dev_warn_once( &cxlr->dev, ""Bypassing cpu_cache_invalidate_memregion() for testing!\n""); <S2SV_StartVul> return 0; <S2SV_EndVul> } else { <S2SV_StartVul> dev_err(&cxlr->dev, <S2SV_EndVul> <S2SV_StartVul> ""Failed to synchronize CPU cache state\n""); <S2SV_EndVul> return -ENXIO; } } cpu_cache_invalidate_memregion(IORES_DESC_CXL); return 0; }","- return 0;
- dev_err(&cxlr->dev,
- ""Failed to synchronize CPU cache state\n"");
+ dev_WARN(&cxlr->dev,
+ ""Failed to synchronize CPU cache state\n"");","static int cxl_region_invalidate_memregion(struct cxl_region *cxlr) { if (!cpu_cache_has_invalidate_memregion()) { if (IS_ENABLED(CONFIG_CXL_REGION_INVALIDATION_TEST)) { dev_warn_once( &cxlr->dev, ""Bypassing cpu_cache_invalidate_memregion() for testing!\n""); return 0; } else { dev_WARN(&cxlr->dev, ""Failed to synchronize CPU cache state\n""); return -ENXIO; } } cpu_cache_invalidate_memregion(IORES_DESC_CXL); return 0; }"
286----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46763/bad/fou_core.c----*fou_gro_receive,"static struct sk_buff *fou_gro_receive(struct sock *sk, struct list_head *head, struct sk_buff *skb) { const struct net_offload __rcu **offloads; <S2SV_StartVul> u8 proto = fou_from_sock(sk)->protocol; <S2SV_EndVul> const struct net_offload *ops; struct sk_buff *pp = NULL; NAPI_GRO_CB(skb)->encap_mark = 0; NAPI_GRO_CB(skb)->is_fou = 1; offloads = NAPI_GRO_CB(skb)->is_ipv6 ? inet6_offloads : inet_offloads; ops = rcu_dereference(offloads[proto]); if (!ops || !ops->callbacks.gro_receive) goto out; pp = call_gro_receive(ops->callbacks.gro_receive, head, skb); out: return pp; }","- u8 proto = fou_from_sock(sk)->protocol;
+ struct fou *fou = fou_from_sock(sk);
+ u8 proto;
+ if (!fou)
+ goto out;
+ proto = fou->protocol;","static struct sk_buff *fou_gro_receive(struct sock *sk, struct list_head *head, struct sk_buff *skb) { const struct net_offload __rcu **offloads; struct fou *fou = fou_from_sock(sk); const struct net_offload *ops; struct sk_buff *pp = NULL; u8 proto; if (!fou) goto out; proto = fou->protocol; NAPI_GRO_CB(skb)->encap_mark = 0; NAPI_GRO_CB(skb)->is_fou = 1; offloads = NAPI_GRO_CB(skb)->is_ipv6 ? inet6_offloads : inet_offloads; ops = rcu_dereference(offloads[proto]); if (!ops || !ops->callbacks.gro_receive) goto out; pp = call_gro_receive(ops->callbacks.gro_receive, head, skb); out: return pp; }"
222----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46701/bad/libfs.c----*offset_iterate_dir,"static void *offset_iterate_dir(struct inode *inode, struct dir_context *ctx) { struct offset_ctx *octx = inode->i_op->get_offset_ctx(inode); struct dentry *dentry; while (true) { dentry = offset_find_next(octx, ctx->pos); if (!dentry) <S2SV_StartVul> return ERR_PTR(-ENOENT); <S2SV_EndVul> if (!offset_dir_emit(ctx, dentry)) { dput(dentry); <S2SV_StartVul> break; <S2SV_EndVul> } ctx->pos = dentry2offset(dentry) + 1; dput(dentry); } <S2SV_StartVul> return NULL; <S2SV_EndVul> }","- return ERR_PTR(-ENOENT);
- break;
- return NULL;","#include <linux/blkdev.h> #include <linux/export.h> #include <linux/pagemap.h> #include <linux/slab.h> #include <linux/cred.h> #include <linux/mount.h> #include <linux/vfs.h> #include <linux/quotaops.h> #include <linux/mutex.h> #include <linux/namei.h> #include <linux/exportfs.h> #include <linux/iversion.h> #include <linux/writeback.h> #include <linux/buffer_head.h> #include <linux/fs_context.h> #include <linux/pseudo_fs.h> #include <linux/fsnotify.h> #include <linux/unicode.h> #include <linux/fscrypt.h> #include <linux/pidfs.h> #include <linux/uaccess.h> #include ""internal.h"" int simple_getattr(struct mnt_idmap *idmap, const struct path *path, struct kstat *stat, u32 request_mask, unsigned int query_flags) { struct inode *inode = d_inode(path->dentry); generic_fillattr(&nop_mnt_idmap, request_mask, inode, stat); stat->blocks = inode->i_mapping->nrpages << (PAGE_SHIFT - 9); return 0; }"
195----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46680/bad/btnxpuart.c----nxp_serdev_remove,"static void nxp_serdev_remove(struct serdev_device *serdev) { struct btnxpuart_dev *nxpdev = serdev_device_get_drvdata(serdev); struct hci_dev *hdev = nxpdev->hdev; if (is_fw_downloading(nxpdev)) { set_bit(BTNXPUART_FW_DOWNLOAD_ABORT, &nxpdev->tx_state); clear_bit(BTNXPUART_FW_DOWNLOADING, &nxpdev->tx_state); wake_up_interruptible(&nxpdev->check_boot_sign_wait_q); wake_up_interruptible(&nxpdev->fw_dnld_done_wait_q); } else { if (nxpdev->current_baudrate != nxpdev->fw_init_baudrate) { nxpdev->new_baudrate = nxpdev->fw_init_baudrate; nxp_set_baudrate_cmd(hdev, NULL); } <S2SV_StartVul> ps_cancel_timer(nxpdev); <S2SV_EndVul> } hci_unregister_dev(hdev); hci_free_dev(hdev); }","- ps_cancel_timer(nxpdev);
+ }
+ }
+ ps_cleanup(nxpdev);
+ }","static void nxp_serdev_remove(struct serdev_device *serdev) { struct btnxpuart_dev *nxpdev = serdev_device_get_drvdata(serdev); struct hci_dev *hdev = nxpdev->hdev; if (is_fw_downloading(nxpdev)) { set_bit(BTNXPUART_FW_DOWNLOAD_ABORT, &nxpdev->tx_state); clear_bit(BTNXPUART_FW_DOWNLOADING, &nxpdev->tx_state); wake_up_interruptible(&nxpdev->check_boot_sign_wait_q); wake_up_interruptible(&nxpdev->fw_dnld_done_wait_q); } else { if (nxpdev->current_baudrate != nxpdev->fw_init_baudrate) { nxpdev->new_baudrate = nxpdev->fw_init_baudrate; nxp_set_baudrate_cmd(hdev, NULL); } } ps_cleanup(nxpdev); hci_unregister_dev(hdev); hci_free_dev(hdev); }"
187----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46678/bad/bond_main.c----bond_ipsec_add_sa,"static int bond_ipsec_add_sa(struct xfrm_state *xs, struct netlink_ext_ack *extack) { struct net_device *bond_dev = xs->xso.dev; struct net_device *real_dev; struct bond_ipsec *ipsec; struct bonding *bond; struct slave *slave; int err; if (!bond_dev) return -EINVAL; rcu_read_lock(); bond = netdev_priv(bond_dev); <S2SV_StartVul> slave = rcu_dereference(bond->curr_active_slave); <S2SV_EndVul> <S2SV_StartVul> if (!slave) { <S2SV_EndVul> <S2SV_StartVul> rcu_read_unlock(); <S2SV_EndVul> <S2SV_StartVul> return -ENODEV; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> real_dev = slave->dev; <S2SV_EndVul> if (!real_dev->xfrmdev_ops || !real_dev->xfrmdev_ops->xdo_dev_state_add || netif_is_bond_master(real_dev)) { NL_SET_ERR_MSG_MOD(extack, ""Slave does not support ipsec offload""); <S2SV_StartVul> rcu_read_unlock(); <S2SV_EndVul> <S2SV_StartVul> return -EINVAL; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> ipsec = kmalloc(sizeof(*ipsec), GFP_ATOMIC); <S2SV_EndVul> if (!ipsec) { <S2SV_StartVul> rcu_read_unlock(); <S2SV_EndVul> <S2SV_StartVul> return -ENOMEM; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> xs->xso.real_dev = real_dev; err = real_dev->xfrmdev_ops->xdo_dev_state_add(xs, extack); if (!err) { ipsec->xs = xs; INIT_LIST_HEAD(&ipsec->list); <S2SV_StartVul> spin_lock_bh(&bond->ipsec_lock); <S2SV_EndVul> list_add(&ipsec->list, &bond->ipsec_list); <S2SV_StartVul> spin_unlock_bh(&bond->ipsec_lock); <S2SV_EndVul> } else { kfree(ipsec); <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> rcu_read_unlock(); <S2SV_EndVul> return err; <S2SV_StartVul> } <S2SV_EndVul>","- slave = rcu_dereference(bond->curr_active_slave);
- if (!slave) {
- rcu_read_unlock();
- return -ENODEV;
- }
- real_dev = slave->dev;
- rcu_read_unlock();
- return -EINVAL;
- }
- ipsec = kmalloc(sizeof(*ipsec), GFP_ATOMIC);
- rcu_read_unlock();
- return -ENOMEM;
- }
- spin_lock_bh(&bond->ipsec_lock);
- spin_unlock_bh(&bond->ipsec_lock);
- }
- rcu_read_unlock();
- }
+ netdevice_tracker tracker;
+ real_dev = slave ? slave->dev : NULL;
+ netdev_hold(real_dev, &tracker, GFP_ATOMIC);
+ rcu_read_unlock();
+ if (!real_dev) {
+ err = -ENODEV;
+ goto out;
+ err = -EINVAL;
+ goto out;
+ ipsec = kmalloc(sizeof(*ipsec), GFP_KERNEL);
+ err = -ENOMEM;
+ goto out;
+ mutex_lock(&bond->ipsec_lock);
+ mutex_unlock(&bond->ipsec_lock);
+ out:
+ netdev_put(real_dev, &tracker);","static int bond_ipsec_add_sa(struct xfrm_state *xs, struct netlink_ext_ack *extack) { struct net_device *bond_dev = xs->xso.dev; struct net_device *real_dev; netdevice_tracker tracker; struct bond_ipsec *ipsec; struct bonding *bond; struct slave *slave; int err; if (!bond_dev) return -EINVAL; rcu_read_lock(); bond = netdev_priv(bond_dev); slave = rcu_dereference(bond->curr_active_slave); real_dev = slave ? slave->dev : NULL; netdev_hold(real_dev, &tracker, GFP_ATOMIC); rcu_read_unlock(); if (!real_dev) { err = -ENODEV; goto out; } if (!real_dev->xfrmdev_ops || !real_dev->xfrmdev_ops->xdo_dev_state_add || netif_is_bond_master(real_dev)) { NL_SET_ERR_MSG_MOD(extack, ""Slave does not support ipsec offload""); err = -EINVAL; goto out; } ipsec = kmalloc(sizeof(*ipsec), GFP_KERNEL); if (!ipsec) { err = -ENOMEM; goto out; } xs->xso.real_dev = real_dev; err = real_dev->xfrmdev_ops->xdo_dev_state_add(xs, extack); if (!err) { ipsec->xs = xs; INIT_LIST_HEAD(&ipsec->list); mutex_lock(&bond->ipsec_lock); list_add(&ipsec->list, &bond->ipsec_list); mutex_unlock(&bond->ipsec_lock); } else { kfree(ipsec); } out: netdev_put(real_dev, &tracker); return err; }"
1262----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56544/bad/udmabuf.c----release_udmabuf,"static void release_udmabuf(struct dma_buf *buf) { struct udmabuf *ubuf = buf->priv; struct device *dev = ubuf->device->this_device; if (ubuf->sg) put_sg_table(dev, ubuf->sg, DMA_BIDIRECTIONAL); unpin_all_folios(&ubuf->unpin_list); <S2SV_StartVul> kfree(ubuf->offsets); <S2SV_EndVul> <S2SV_StartVul> kfree(ubuf->folios); <S2SV_EndVul> kfree(ubuf); }","- kfree(ubuf->offsets);
- kfree(ubuf->folios);
+ kvfree(ubuf->offsets);
+ kvfree(ubuf->folios);","static void release_udmabuf(struct dma_buf *buf) { struct udmabuf *ubuf = buf->priv; struct device *dev = ubuf->device->this_device; if (ubuf->sg) put_sg_table(dev, ubuf->sg, DMA_BIDIRECTIONAL); unpin_all_folios(&ubuf->unpin_list); kvfree(ubuf->offsets); kvfree(ubuf->folios); kfree(ubuf); }"
312----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46780/bad/sysfs.c----nilfs_dev_device_size_show,"ssize_t nilfs_dev_device_size_show(struct nilfs_dev_attr *attr, struct the_nilfs *nilfs, char *buf) { <S2SV_StartVul> struct nilfs_super_block **sbp = nilfs->ns_sbp; <S2SV_EndVul> <S2SV_StartVul> u64 dev_size = le64_to_cpu(sbp[0]->s_dev_size); <S2SV_EndVul> return sysfs_emit(buf, ""%llu\n"", dev_size); }","- struct nilfs_super_block **sbp = nilfs->ns_sbp;
- u64 dev_size = le64_to_cpu(sbp[0]->s_dev_size);
+ struct nilfs_super_block *raw_sb;
+ u64 dev_size;
+ down_read(&nilfs->ns_sem);
+ raw_sb = nilfs->ns_sbp[0];
+ dev_size = le64_to_cpu(raw_sb->s_dev_size);
+ up_read(&nilfs->ns_sem);","ssize_t nilfs_dev_device_size_show(struct nilfs_dev_attr *attr, struct the_nilfs *nilfs, char *buf) { struct nilfs_super_block *raw_sb; u64 dev_size; down_read(&nilfs->ns_sem); raw_sb = nilfs->ns_sbp[0]; dev_size = le64_to_cpu(raw_sb->s_dev_size); up_read(&nilfs->ns_sem); return sysfs_emit(buf, ""%llu\n"", dev_size); }"
194----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46680/bad/btnxpuart.c----btnxpuart_close,"static int btnxpuart_close(struct hci_dev *hdev) { struct btnxpuart_dev *nxpdev = hci_get_drvdata(hdev); <S2SV_StartVul> ps_wakeup(nxpdev); <S2SV_EndVul> serdev_device_close(nxpdev->serdev); skb_queue_purge(&nxpdev->txq); kfree_skb(nxpdev->rx_skb); nxpdev->rx_skb = NULL; clear_bit(BTNXPUART_SERDEV_OPEN, &nxpdev->tx_state); return 0; }",- ps_wakeup(nxpdev);,"static int btnxpuart_close(struct hci_dev *hdev) { struct btnxpuart_dev *nxpdev = hci_get_drvdata(hdev); serdev_device_close(nxpdev->serdev); skb_queue_purge(&nxpdev->txq); kfree_skb(nxpdev->rx_skb); nxpdev->rx_skb = NULL; clear_bit(BTNXPUART_SERDEV_OPEN, &nxpdev->tx_state); return 0; }"
1053----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50296/bad/hnae3.c----hnae3_unregister_ae_algo_prepare,"void hnae3_unregister_ae_algo_prepare(struct hnae3_ae_algo *ae_algo) { const struct pci_device_id *pci_id; struct hnae3_ae_dev *ae_dev; if (!ae_algo) return; list_for_each_entry(ae_dev, &hnae3_ae_dev_list, node) { if (!hnae3_get_bit(ae_dev->flag, HNAE3_DEV_INITED_B)) continue; pci_id = pci_match_id(ae_algo->pdev_id_table, ae_dev->pdev); if (!pci_id) continue; <S2SV_StartVul> if (IS_ENABLED(CONFIG_PCI_IOV)) <S2SV_EndVul> pci_disable_sriov(ae_dev->pdev); } }","- if (IS_ENABLED(CONFIG_PCI_IOV))
+ if (IS_ENABLED(CONFIG_PCI_IOV)) {
+ device_lock(&ae_dev->pdev->dev);
+ device_unlock(&ae_dev->pdev->dev);
+ }
+ }
+ }","void hnae3_unregister_ae_algo_prepare(struct hnae3_ae_algo *ae_algo) { const struct pci_device_id *pci_id; struct hnae3_ae_dev *ae_dev; if (!ae_algo) return; list_for_each_entry(ae_dev, &hnae3_ae_dev_list, node) { if (!hnae3_get_bit(ae_dev->flag, HNAE3_DEV_INITED_B)) continue; pci_id = pci_match_id(ae_algo->pdev_id_table, ae_dev->pdev); if (!pci_id) continue; if (IS_ENABLED(CONFIG_PCI_IOV)) { device_lock(&ae_dev->pdev->dev); pci_disable_sriov(ae_dev->pdev); device_unlock(&ae_dev->pdev->dev); } } }"
237----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46712/bad/vmwgfx_surface.c----vmw_dumb_create,"int vmw_dumb_create(struct drm_file *file_priv, struct drm_device *dev, struct drm_mode_create_dumb *args) { struct vmw_private *dev_priv = vmw_priv(dev); struct ttm_object_file *tfile = vmw_fpriv(file_priv)->tfile; struct vmw_bo *vbo = NULL; struct vmw_resource *res = NULL; union drm_vmw_gb_surface_create_ext_arg arg = { 0 }; struct drm_vmw_gb_surface_create_ext_req *req = &arg.req; int ret; struct drm_vmw_size drm_size = { .width = args->width, .height = args->height, .depth = 1, }; SVGA3dSurfaceFormat format = vmw_format_bpp_to_svga(dev_priv, args->bpp); const struct SVGA3dSurfaceDesc *desc = vmw_surface_get_desc(format); SVGA3dSurfaceAllFlags flags = SVGA3D_SURFACE_HINT_TEXTURE | SVGA3D_SURFACE_HINT_RENDERTARGET | SVGA3D_SURFACE_SCREENTARGET | SVGA3D_SURFACE_BIND_SHADER_RESOURCE | SVGA3D_SURFACE_BIND_RENDER_TARGET; <S2SV_StartVul> if (!dev_priv->has_mob) { <S2SV_EndVul> int cpp = DIV_ROUND_UP(args->bpp, 8); switch (cpp) { case 1: case 2: case 4: break; default: return -EINVAL; } args->pitch = args->width * cpp; args->size = ALIGN(args->pitch * args->height, PAGE_SIZE); ret = vmw_gem_object_create_with_handle(dev_priv, file_priv, args->size, &args->handle, &vbo); drm_gem_object_put(&vbo->tbo.base); return ret; } req->version = drm_vmw_gb_surface_v1; req->multisample_pattern = SVGA3D_MS_PATTERN_NONE; req->quality_level = SVGA3D_MS_QUALITY_NONE; req->buffer_byte_stride = 0; req->must_be_zero = 0; req->base.svga3d_flags = SVGA3D_FLAGS_LOWER_32(flags); req->svga3d_flags_upper_32_bits = SVGA3D_FLAGS_UPPER_32(flags); req->base.format = (uint32_t)format; req->base.drm_surface_flags = drm_vmw_surface_flag_scanout; req->base.drm_surface_flags |= drm_vmw_surface_flag_shareable; req->base.drm_surface_flags |= drm_vmw_surface_flag_create_buffer; req->base.drm_surface_flags |= drm_vmw_surface_flag_coherent; req->base.base_size.width = args->width; req->base.base_size.height = args->height; req->base.base_size.depth = 1; req->base.array_size = 0; req->base.mip_levels = 1; req->base.multisample_count = 0; req->base.buffer_handle = SVGA3D_INVALID_ID; req->base.autogen_filter = SVGA3D_TEX_FILTER_NONE; ret = vmw_gb_surface_define_ext_ioctl(dev, &arg, file_priv); if (ret) { drm_warn(dev, ""Unable to create a dumb buffer\n""); return ret; } args->handle = arg.rep.buffer_handle; args->size = arg.rep.buffer_size; args->pitch = vmw_surface_calculate_pitch(desc, &drm_size); ret = vmw_user_resource_lookup_handle(dev_priv, tfile, arg.rep.handle, user_surface_converter, &res); if (ret) { drm_err(dev, ""Created resource handle doesn't exist!\n""); goto err; } vbo = res->guest_memory_bo; vbo->is_dumb = true; vbo->dumb_surface = vmw_res_to_srf(res); err: if (res) vmw_resource_unreference(&res); if (ret) ttm_ref_object_base_unref(tfile, arg.rep.handle); return ret; }","- if (!dev_priv->has_mob) {
+ if (!dev_priv->has_mob || !vmw_supports_3d(dev_priv)) {","int vmw_dumb_create(struct drm_file *file_priv, struct drm_device *dev, struct drm_mode_create_dumb *args) { struct vmw_private *dev_priv = vmw_priv(dev); struct ttm_object_file *tfile = vmw_fpriv(file_priv)->tfile; struct vmw_bo *vbo = NULL; struct vmw_resource *res = NULL; union drm_vmw_gb_surface_create_ext_arg arg = { 0 }; struct drm_vmw_gb_surface_create_ext_req *req = &arg.req; int ret; struct drm_vmw_size drm_size = { .width = args->width, .height = args->height, .depth = 1, }; SVGA3dSurfaceFormat format = vmw_format_bpp_to_svga(dev_priv, args->bpp); const struct SVGA3dSurfaceDesc *desc = vmw_surface_get_desc(format); SVGA3dSurfaceAllFlags flags = SVGA3D_SURFACE_HINT_TEXTURE | SVGA3D_SURFACE_HINT_RENDERTARGET | SVGA3D_SURFACE_SCREENTARGET | SVGA3D_SURFACE_BIND_SHADER_RESOURCE | SVGA3D_SURFACE_BIND_RENDER_TARGET; if (!dev_priv->has_mob || !vmw_supports_3d(dev_priv)) { int cpp = DIV_ROUND_UP(args->bpp, 8); switch (cpp) { case 1: case 2: case 4: break; default: return -EINVAL; } args->pitch = args->width * cpp; args->size = ALIGN(args->pitch * args->height, PAGE_SIZE); ret = vmw_gem_object_create_with_handle(dev_priv, file_priv, args->size, &args->handle, &vbo); drm_gem_object_put(&vbo->tbo.base); return ret; } req->version = drm_vmw_gb_surface_v1; req->multisample_pattern = SVGA3D_MS_PATTERN_NONE; req->quality_level = SVGA3D_MS_QUALITY_NONE; req->buffer_byte_stride = 0; req->must_be_zero = 0; req->base.svga3d_flags = SVGA3D_FLAGS_LOWER_32(flags); req->svga3d_flags_upper_32_bits = SVGA3D_FLAGS_UPPER_32(flags); req->base.format = (uint32_t)format; req->base.drm_surface_flags = drm_vmw_surface_flag_scanout; req->base.drm_surface_flags |= drm_vmw_surface_flag_shareable; req->base.drm_surface_flags |= drm_vmw_surface_flag_create_buffer; req->base.drm_surface_flags |= drm_vmw_surface_flag_coherent; req->base.base_size.width = args->width; req->base.base_size.height = args->height; req->base.base_size.depth = 1; req->base.array_size = 0; req->base.mip_levels = 1; req->base.multisample_count = 0; req->base.buffer_handle = SVGA3D_INVALID_ID; req->base.autogen_filter = SVGA3D_TEX_FILTER_NONE; ret = vmw_gb_surface_define_ext_ioctl(dev, &arg, file_priv); if (ret) { drm_warn(dev, ""Unable to create a dumb buffer\n""); return ret; } args->handle = arg.rep.buffer_handle; args->size = arg.rep.buffer_size; args->pitch = vmw_surface_calculate_pitch(desc, &drm_size); ret = vmw_user_resource_lookup_handle(dev_priv, tfile, arg.rep.handle, user_surface_converter, &res); if (ret) { drm_err(dev, ""Created resource handle doesn't exist!\n""); goto err; } vbo = res->guest_memory_bo; vbo->is_dumb = true; vbo->dumb_surface = vmw_res_to_srf(res); err: if (res) vmw_resource_unreference(&res); if (ret) ttm_ref_object_base_unref(tfile, arg.rep.handle); return ret; }"
224----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46703/bad/8250_omap.c----omap8250_suspend,"static int omap8250_suspend(struct device *dev) { struct omap8250_priv *priv = dev_get_drvdata(dev); struct uart_8250_port *up = serial8250_get_port(priv->line); <S2SV_StartVul> struct generic_pm_domain *genpd = pd_to_genpd(dev->pm_domain); <S2SV_EndVul> int err = 0; serial8250_suspend_port(priv->line); err = pm_runtime_resume_and_get(dev); if (err) return err; if (!device_may_wakeup(dev)) priv->wer = 0; serial_out(up, UART_OMAP_WER, priv->wer); <S2SV_StartVul> if (uart_console(&up->port)) { <S2SV_EndVul> <S2SV_StartVul> if (console_suspend_enabled) <S2SV_EndVul> <S2SV_StartVul> err = pm_runtime_force_suspend(dev); <S2SV_EndVul> <S2SV_StartVul> else { <S2SV_EndVul> <S2SV_StartVul> genpd_flags_console = genpd->flags; <S2SV_EndVul> <S2SV_StartVul> genpd->flags |= GENPD_FLAG_ALWAYS_ON; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> flush_work(&priv->qos_work); <S2SV_StartVul> return err; <S2SV_EndVul> }","- struct generic_pm_domain *genpd = pd_to_genpd(dev->pm_domain);
- if (uart_console(&up->port)) {
- if (console_suspend_enabled)
- err = pm_runtime_force_suspend(dev);
- else {
- genpd_flags_console = genpd->flags;
- genpd->flags |= GENPD_FLAG_ALWAYS_ON;
- }
- }
- return err;
+ if (uart_console(&up->port) && console_suspend_enabled)
+ err = pm_runtime_force_suspend(dev);
+ return err;","static int omap8250_suspend(struct device *dev) { struct omap8250_priv *priv = dev_get_drvdata(dev); struct uart_8250_port *up = serial8250_get_port(priv->line); int err = 0; serial8250_suspend_port(priv->line); err = pm_runtime_resume_and_get(dev); if (err) return err; if (!device_may_wakeup(dev)) priv->wer = 0; serial_out(up, UART_OMAP_WER, priv->wer); if (uart_console(&up->port) && console_suspend_enabled) err = pm_runtime_force_suspend(dev); flush_work(&priv->qos_work); return err; }"
1156----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53131/bad/page.c----*__nilfs_get_folio_block,"static struct buffer_head *__nilfs_get_folio_block(struct folio *folio, unsigned long block, pgoff_t index, int blkbits, unsigned long b_state) { unsigned long first_block; struct buffer_head *bh = folio_buffers(folio); if (!bh) bh = create_empty_buffers(folio, 1 << blkbits, b_state); first_block = (unsigned long)index << (PAGE_SHIFT - blkbits); bh = get_nth_bh(bh, block - first_block); <S2SV_StartVul> touch_buffer(bh); <S2SV_EndVul> wait_on_buffer(bh); return bh; }",- touch_buffer(bh);,"static struct buffer_head *__nilfs_get_folio_block(struct folio *folio, unsigned long block, pgoff_t index, int blkbits, unsigned long b_state) { unsigned long first_block; struct buffer_head *bh = folio_buffers(folio); if (!bh) bh = create_empty_buffers(folio, 1 << blkbits, b_state); first_block = (unsigned long)index << (PAGE_SHIFT - blkbits); bh = get_nth_bh(bh, block - first_block); wait_on_buffer(bh); return bh; }"
157----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-45013/bad/core.c----nvme_stop_ctrl,void nvme_stop_ctrl(struct nvme_ctrl *ctrl) { nvme_mpath_stop(ctrl); nvme_auth_stop(ctrl); <S2SV_StartVul> nvme_stop_keep_alive(ctrl); <S2SV_EndVul> nvme_stop_failfast_work(ctrl); flush_work(&ctrl->async_event_work); cancel_work_sync(&ctrl->fw_act_work); if (ctrl->ops->stop_ctrl) ctrl->ops->stop_ctrl(ctrl); },- nvme_stop_keep_alive(ctrl);,void nvme_stop_ctrl(struct nvme_ctrl *ctrl) { nvme_mpath_stop(ctrl); nvme_auth_stop(ctrl); nvme_stop_failfast_work(ctrl); flush_work(&ctrl->async_event_work); cancel_work_sync(&ctrl->fw_act_work); if (ctrl->ops->stop_ctrl) ctrl->ops->stop_ctrl(ctrl); }
1379----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56712/bad/udmabuf.c----udmabuf_create,"static long udmabuf_create(struct miscdevice *device, struct udmabuf_create_list *head, struct udmabuf_create_item *list) { unsigned long max_nr_folios = 0; struct folio **folios = NULL; pgoff_t pgcnt = 0, pglimit; struct udmabuf *ubuf; long ret = -EINVAL; u32 i, flags; ubuf = kzalloc(sizeof(*ubuf), GFP_KERNEL); if (!ubuf) return -ENOMEM; pglimit = (size_limit_mb * 1024 * 1024) >> PAGE_SHIFT; for (i = 0; i < head->count; i++) { pgoff_t subpgcnt; if (!PAGE_ALIGNED(list[i].offset)) goto err_noinit; if (!PAGE_ALIGNED(list[i].size)) goto err_noinit; subpgcnt = list[i].size >> PAGE_SHIFT; pgcnt += subpgcnt; if (pgcnt > pglimit) goto err_noinit; max_nr_folios = max_t(unsigned long, subpgcnt, max_nr_folios); } if (!pgcnt) goto err_noinit; ret = init_udmabuf(ubuf, pgcnt); if (ret) goto err; folios = kvmalloc_array(max_nr_folios, sizeof(*folios), GFP_KERNEL); if (!folios) { ret = -ENOMEM; goto err; } for (i = 0; i < head->count; i++) { struct file *memfd = fget(list[i].memfd); if (!memfd) { ret = -EBADFD; goto err; } inode_lock_shared(file_inode(memfd)); ret = check_memfd_seals(memfd); if (ret) goto out_unlock; ret = udmabuf_pin_folios(ubuf, memfd, list[i].offset, list[i].size, folios); out_unlock: inode_unlock_shared(file_inode(memfd)); fput(memfd); if (ret) goto err; } flags = head->flags & UDMABUF_FLAGS_CLOEXEC ? O_CLOEXEC : 0; <S2SV_StartVul> ret = export_udmabuf(ubuf, device, flags); <S2SV_EndVul> <S2SV_StartVul> if (ret < 0) <S2SV_EndVul> goto err; kvfree(folios); return ret; err: deinit_udmabuf(ubuf); err_noinit: kfree(ubuf); kvfree(folios); return ret; }","- ret = export_udmabuf(ubuf, device, flags);
- if (ret < 0)
+ struct dma_buf *dmabuf;
+ dmabuf = export_udmabuf(ubuf, device);
+ if (IS_ERR(dmabuf)) {
+ ret = PTR_ERR(dmabuf);
+ }
+ ret = dma_buf_fd(dmabuf, flags);
+ if (ret < 0)
+ dma_buf_put(dmabuf);","static long udmabuf_create(struct miscdevice *device, struct udmabuf_create_list *head, struct udmabuf_create_item *list) { unsigned long max_nr_folios = 0; struct folio **folios = NULL; pgoff_t pgcnt = 0, pglimit; struct udmabuf *ubuf; struct dma_buf *dmabuf; long ret = -EINVAL; u32 i, flags; ubuf = kzalloc(sizeof(*ubuf), GFP_KERNEL); if (!ubuf) return -ENOMEM; pglimit = (size_limit_mb * 1024 * 1024) >> PAGE_SHIFT; for (i = 0; i < head->count; i++) { pgoff_t subpgcnt; if (!PAGE_ALIGNED(list[i].offset)) goto err_noinit; if (!PAGE_ALIGNED(list[i].size)) goto err_noinit; subpgcnt = list[i].size >> PAGE_SHIFT; pgcnt += subpgcnt; if (pgcnt > pglimit) goto err_noinit; max_nr_folios = max_t(unsigned long, subpgcnt, max_nr_folios); } if (!pgcnt) goto err_noinit; ret = init_udmabuf(ubuf, pgcnt); if (ret) goto err; folios = kvmalloc_array(max_nr_folios, sizeof(*folios), GFP_KERNEL); if (!folios) { ret = -ENOMEM; goto err; } for (i = 0; i < head->count; i++) { struct file *memfd = fget(list[i].memfd); if (!memfd) { ret = -EBADFD; goto err; } inode_lock_shared(file_inode(memfd)); ret = check_memfd_seals(memfd); if (ret) goto out_unlock; ret = udmabuf_pin_folios(ubuf, memfd, list[i].offset, list[i].size, folios); out_unlock: inode_unlock_shared(file_inode(memfd)); fput(memfd); if (ret) goto err; } flags = head->flags & UDMABUF_FLAGS_CLOEXEC ? O_CLOEXEC : 0; dmabuf = export_udmabuf(ubuf, device); if (IS_ERR(dmabuf)) { ret = PTR_ERR(dmabuf); goto err; } ret = dma_buf_fd(dmabuf, flags); if (ret < 0) dma_buf_put(dmabuf); kvfree(folios); return ret; err: deinit_udmabuf(ubuf); err_noinit: kfree(ubuf); kvfree(folios); return ret; }"
939----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50187/bad/vc4_perfmon.c----vc4_perfmon_close_file,"void vc4_perfmon_close_file(struct vc4_file *vc4file) { struct vc4_dev *vc4 = vc4file->dev; if (WARN_ON_ONCE(vc4->is_vc5)) return; mutex_lock(&vc4file->perfmon.lock); <S2SV_StartVul> idr_for_each(&vc4file->perfmon.idr, vc4_perfmon_idr_del, NULL); <S2SV_EndVul> idr_destroy(&vc4file->perfmon.idr); mutex_unlock(&vc4file->perfmon.lock); mutex_destroy(&vc4file->perfmon.lock); }","- idr_for_each(&vc4file->perfmon.idr, vc4_perfmon_idr_del, NULL);
+ idr_for_each(&vc4file->perfmon.idr, vc4_perfmon_idr_del, vc4);","void vc4_perfmon_close_file(struct vc4_file *vc4file) { struct vc4_dev *vc4 = vc4file->dev; if (WARN_ON_ONCE(vc4->is_vc5)) return; mutex_lock(&vc4file->perfmon.lock); idr_for_each(&vc4file->perfmon.idr, vc4_perfmon_idr_del, vc4); idr_destroy(&vc4file->perfmon.idr); mutex_unlock(&vc4file->perfmon.lock); mutex_destroy(&vc4file->perfmon.lock); }"
117----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44974/bad/pm_netlink.c----select_signal_address,"select_signal_address(struct pm_nl_pernet *pernet, const struct mptcp_sock *msk) { struct mptcp_pm_addr_entry *entry, *ret = NULL; rcu_read_lock(); list_for_each_entry_rcu(entry, &pernet->local_addr_list, list) { if (!test_bit(entry->addr.id, msk->pm.id_avail_bitmap)) continue; if (!(entry->flags & MPTCP_PM_ADDR_FLAG_SIGNAL)) continue; <S2SV_StartVul> ret = entry; <S2SV_EndVul> <S2SV_StartVul> break; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> rcu_read_unlock(); <S2SV_StartVul> return ret; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul>","- ret = entry;
- break;
- }
- return ret;
- }
+ static struct genl_family mptcp_genl_family;
+ struct mptcp_pm_add_entry {","#define pr_fmt(fmt) ""MPTCP: "" fmt #include <linux/inet.h> #include <linux/kernel.h> #include <net/tcp.h> #include <net/inet_common.h> #include <net/netns/generic.h> #include <net/mptcp.h> #include <net/genetlink.h> #include <uapi/linux/mptcp.h> #include ""protocol.h"" #include ""mib.h"" static struct genl_family mptcp_genl_family; static int pm_nl_pernet_id; struct mptcp_pm_add_entry { struct list_head list; struct mptcp_addr_info addr; u8 retrans_times; struct timer_list add_timer; struct mptcp_sock *sock; };"
1483----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-57908/bad/kmx61.c----kmx61_trigger_handler,"static irqreturn_t kmx61_trigger_handler(int irq, void *p) { struct iio_poll_func *pf = p; struct iio_dev *indio_dev = pf->indio_dev; struct kmx61_data *data = kmx61_get_data(indio_dev); int bit, ret, i = 0; u8 base; <S2SV_StartVul> s16 buffer[8]; <S2SV_EndVul> if (indio_dev == data->acc_indio_dev) base = KMX61_ACC_XOUT_L; else base = KMX61_MAG_XOUT_L; mutex_lock(&data->lock); for_each_set_bit(bit, indio_dev->active_scan_mask, indio_dev->masklength) { ret = kmx61_read_measurement(data, base, bit); if (ret < 0) { mutex_unlock(&data->lock); goto err; } buffer[i++] = ret; } mutex_unlock(&data->lock); iio_push_to_buffers(indio_dev, buffer); err: iio_trigger_notify_done(indio_dev->trig); return IRQ_HANDLED; }","- s16 buffer[8];
+ s16 buffer[8] = { };","static irqreturn_t kmx61_trigger_handler(int irq, void *p) { struct iio_poll_func *pf = p; struct iio_dev *indio_dev = pf->indio_dev; struct kmx61_data *data = kmx61_get_data(indio_dev); int bit, ret, i = 0; u8 base; s16 buffer[8] = { }; if (indio_dev == data->acc_indio_dev) base = KMX61_ACC_XOUT_L; else base = KMX61_MAG_XOUT_L; mutex_lock(&data->lock); for_each_set_bit(bit, indio_dev->active_scan_mask, indio_dev->masklength) { ret = kmx61_read_measurement(data, base, bit); if (ret < 0) { mutex_unlock(&data->lock); goto err; } buffer[i++] = ret; } mutex_unlock(&data->lock); iio_push_to_buffers(indio_dev, buffer); err: iio_trigger_notify_done(indio_dev->trig); return IRQ_HANDLED; }"
806----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50081/bad/blk-mq.c----blk_mq_init_allocated_queue,"int blk_mq_init_allocated_queue(struct blk_mq_tag_set *set, struct request_queue *q) { q->mq_ops = set->ops; if (blk_mq_alloc_ctxs(q)) goto err_exit; blk_mq_sysfs_init(q); INIT_LIST_HEAD(&q->unused_hctx_list); spin_lock_init(&q->unused_hctx_lock); xa_init(&q->hctx_table); blk_mq_realloc_hw_ctxs(set, q); if (!q->nr_hw_queues) goto err_hctxs; INIT_WORK(&q->timeout_work, blk_mq_timeout_work); blk_queue_rq_timeout(q, set->timeout ? set->timeout : 30 * HZ); <S2SV_StartVul> q->tag_set = set; <S2SV_EndVul> q->queue_flags |= QUEUE_FLAG_MQ_DEFAULT; INIT_DELAYED_WORK(&q->requeue_work, blk_mq_requeue_work); INIT_LIST_HEAD(&q->flush_list); INIT_LIST_HEAD(&q->requeue_list); spin_lock_init(&q->requeue_lock); q->nr_requests = set->queue_depth; blk_mq_init_cpu_queues(q, set->nr_hw_queues); blk_mq_add_queue_tag_set(set, q); blk_mq_map_swqueue(q); return 0; err_hctxs: blk_mq_release(q); err_exit: q->mq_ops = NULL; return -ENOMEM; }","- q->tag_set = set;
+ q->tag_set = set;","int blk_mq_init_allocated_queue(struct blk_mq_tag_set *set, struct request_queue *q) { q->mq_ops = set->ops; q->tag_set = set; if (blk_mq_alloc_ctxs(q)) goto err_exit; blk_mq_sysfs_init(q); INIT_LIST_HEAD(&q->unused_hctx_list); spin_lock_init(&q->unused_hctx_lock); xa_init(&q->hctx_table); blk_mq_realloc_hw_ctxs(set, q); if (!q->nr_hw_queues) goto err_hctxs; INIT_WORK(&q->timeout_work, blk_mq_timeout_work); blk_queue_rq_timeout(q, set->timeout ? set->timeout : 30 * HZ); q->queue_flags |= QUEUE_FLAG_MQ_DEFAULT; INIT_DELAYED_WORK(&q->requeue_work, blk_mq_requeue_work); INIT_LIST_HEAD(&q->flush_list); INIT_LIST_HEAD(&q->requeue_list); spin_lock_init(&q->requeue_lock); q->nr_requests = set->queue_depth; blk_mq_init_cpu_queues(q, set->nr_hw_queues); blk_mq_add_queue_tag_set(set, q); blk_mq_map_swqueue(q); return 0; err_hctxs: blk_mq_release(q); err_exit: q->mq_ops = NULL; return -ENOMEM; }"
81----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44953/bad/ufshcd.c----ufshcd_update_rtc,"static void ufshcd_update_rtc(struct ufs_hba *hba) { struct timespec64 ts64; int err; u32 val; ktime_get_real_ts64(&ts64); if (ts64.tv_sec < hba->dev_info.rtc_time_baseline) { dev_warn_once(hba->dev, ""%s: Current time precedes previous setting!\n"", __func__); return; } val = ts64.tv_sec - hba->dev_info.rtc_time_baseline; <S2SV_StartVul> ufshcd_rpm_get_sync(hba); <S2SV_EndVul> err = ufshcd_query_attr(hba, UPIU_QUERY_OPCODE_WRITE_ATTR, QUERY_ATTR_IDN_SECONDS_PASSED, 0, 0, &val); ufshcd_rpm_put_sync(hba); if (err) dev_err(hba->dev, ""%s: Failed to update rtc %d\n"", __func__, err); else if (hba->dev_info.rtc_type == UFS_RTC_RELATIVE) hba->dev_info.rtc_time_baseline = ts64.tv_sec; }","- ufshcd_rpm_get_sync(hba);
+ if (ufshcd_rpm_get_if_active(hba) <= 0)
+ return;","static void ufshcd_update_rtc(struct ufs_hba *hba) { struct timespec64 ts64; int err; u32 val; ktime_get_real_ts64(&ts64); if (ts64.tv_sec < hba->dev_info.rtc_time_baseline) { dev_warn_once(hba->dev, ""%s: Current time precedes previous setting!\n"", __func__); return; } val = ts64.tv_sec - hba->dev_info.rtc_time_baseline; if (ufshcd_rpm_get_if_active(hba) <= 0) return; err = ufshcd_query_attr(hba, UPIU_QUERY_OPCODE_WRITE_ATTR, QUERY_ATTR_IDN_SECONDS_PASSED, 0, 0, &val); ufshcd_rpm_put_sync(hba); if (err) dev_err(hba->dev, ""%s: Failed to update rtc %d\n"", __func__, err); else if (hba->dev_info.rtc_type == UFS_RTC_RELATIVE) hba->dev_info.rtc_time_baseline = ts64.tv_sec; }"
1072----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53056/bad/mtk_crtc.c----mtk_crtc_destroy,"static void mtk_crtc_destroy(struct drm_crtc *crtc) { struct mtk_crtc *mtk_crtc = to_mtk_crtc(crtc); int i; mtk_mutex_put(mtk_crtc->mutex); #if IS_REACHABLE(CONFIG_MTK_CMDQ) <S2SV_StartVul> cmdq_pkt_destroy(&mtk_crtc->cmdq_client, &mtk_crtc->cmdq_handle); <S2SV_EndVul> if (mtk_crtc->cmdq_client.chan) { mbox_free_channel(mtk_crtc->cmdq_client.chan); mtk_crtc->cmdq_client.chan = NULL; } #endif for (i = 0; i < mtk_crtc->ddp_comp_nr; i++) { struct mtk_ddp_comp *comp; comp = mtk_crtc->ddp_comp[i]; mtk_ddp_comp_unregister_vblank_cb(comp); } drm_crtc_cleanup(crtc); }","- cmdq_pkt_destroy(&mtk_crtc->cmdq_client, &mtk_crtc->cmdq_handle);
+ cmdq_pkt_destroy(&mtk_crtc->cmdq_client, &mtk_crtc->cmdq_handle);","static void mtk_crtc_destroy(struct drm_crtc *crtc) { struct mtk_crtc *mtk_crtc = to_mtk_crtc(crtc); int i; mtk_mutex_put(mtk_crtc->mutex); #if IS_REACHABLE(CONFIG_MTK_CMDQ) if (mtk_crtc->cmdq_client.chan) { cmdq_pkt_destroy(&mtk_crtc->cmdq_client, &mtk_crtc->cmdq_handle); mbox_free_channel(mtk_crtc->cmdq_client.chan); mtk_crtc->cmdq_client.chan = NULL; } #endif for (i = 0; i < mtk_crtc->ddp_comp_nr; i++) { struct mtk_ddp_comp *comp; comp = mtk_crtc->ddp_comp[i]; mtk_ddp_comp_unregister_vblank_cb(comp); } drm_crtc_cleanup(crtc); }"
462----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47703/bad/btf.c----btf_ctx_access,"bool btf_ctx_access(int off, int size, enum bpf_access_type type, const struct bpf_prog *prog, struct bpf_insn_access_aux *info) { const struct btf_type *t = prog->aux->attach_func_proto; struct bpf_prog *tgt_prog = prog->aux->dst_prog; struct btf *btf = bpf_prog_get_target_btf(prog); const char *tname = prog->aux->attach_func_name; struct bpf_verifier_log *log = info->log; const struct btf_param *args; const char *tag_value; u32 nr_args, arg; int i, ret; if (off % 8) { bpf_log(log, ""func '%s' offset %d is not multiple of 8\n"", tname, off); return false; } arg = get_ctx_arg_idx(btf, t, off); args = (const struct btf_param *)(t + 1); nr_args = t ? btf_type_vlen(t) : MAX_BPF_FUNC_REG_ARGS; if (prog->aux->attach_btf_trace) { args++; nr_args--; } if (arg > nr_args) { bpf_log(log, ""func '%s' doesn't have %d-th argument\n"", tname, arg + 1); return false; } if (arg == nr_args) { switch (prog->expected_attach_type) { <S2SV_StartVul> case BPF_LSM_CGROUP: <S2SV_EndVul> case BPF_LSM_MAC: case BPF_TRACE_FEXIT: if (!t) return true; t = btf_type_by_id(btf, t->type); break; case BPF_MODIFY_RETURN: if (!t) return false; t = btf_type_skip_modifiers(btf, t->type, NULL); if (!btf_type_is_small_int(t)) { bpf_log(log, ""ret type %s not allowed for fmod_ret\n"", btf_type_str(t)); return false; } break; default: bpf_log(log, ""func '%s' doesn't have %d-th argument\n"", tname, arg + 1); return false; } } else { if (!t) return true; t = btf_type_by_id(btf, args[arg].type); } while (btf_type_is_modifier(t)) t = btf_type_by_id(btf, t->type); if (btf_type_is_small_int(t) || btf_is_any_enum(t) || __btf_type_is_struct(t)) return true; if (!btf_type_is_ptr(t)) { bpf_log(log, ""func '%s' arg%d '%s' has type %s. Only pointer access is allowed\n"", tname, arg, __btf_name_by_offset(btf, t->name_off), btf_type_str(t)); return false; } for (i = 0; i < prog->aux->ctx_arg_info_size; i++) { const struct bpf_ctx_arg_aux *ctx_arg_info = &prog->aux->ctx_arg_info[i]; u32 type, flag; type = base_type(ctx_arg_info->reg_type); flag = type_flag(ctx_arg_info->reg_type); if (ctx_arg_info->offset == off && type == PTR_TO_BUF && (flag & PTR_MAYBE_NULL)) { info->reg_type = ctx_arg_info->reg_type; return true; } } if (t->type == 0) return true; if (is_int_ptr(btf, t)) return true; for (i = 0; i < prog->aux->ctx_arg_info_size; i++) { const struct bpf_ctx_arg_aux *ctx_arg_info = &prog->aux->ctx_arg_info[i]; if (ctx_arg_info->offset == off) { if (!ctx_arg_info->btf_id) { bpf_log(log,""invalid btf_id for context argument offset %u\n"", off); return false; } info->reg_type = ctx_arg_info->reg_type; info->btf = ctx_arg_info->btf ? : btf_vmlinux; info->btf_id = ctx_arg_info->btf_id; return true; } } info->reg_type = PTR_TO_BTF_ID; if (prog_args_trusted(prog)) info->reg_type |= PTR_TRUSTED; if (tgt_prog) { enum bpf_prog_type tgt_type; if (tgt_prog->type == BPF_PROG_TYPE_EXT) tgt_type = tgt_prog->aux->saved_dst_prog_type; else tgt_type = tgt_prog->type; ret = btf_translate_to_vmlinux(log, btf, t, tgt_type, arg); if (ret > 0) { info->btf = btf_vmlinux; info->btf_id = ret; return true; } else { return false; } } info->btf = btf; info->btf_id = t->type; t = btf_type_by_id(btf, t->type); if (btf_type_is_type_tag(t)) { tag_value = __btf_name_by_offset(btf, t->name_off); if (strcmp(tag_value, ""user"") == 0) info->reg_type |= MEM_USER; if (strcmp(tag_value, ""percpu"") == 0) info->reg_type |= MEM_PERCPU; } while (btf_type_is_modifier(t)) { info->btf_id = t->type; t = btf_type_by_id(btf, t->type); } if (!btf_type_is_struct(t)) { bpf_log(log, ""func '%s' arg%d type %s is not a struct\n"", tname, arg, btf_type_str(t)); return false; } bpf_log(log, ""func '%s' arg%d has btf_id %d type %s '%s'\n"", tname, arg, info->btf_id, btf_type_str(t), __btf_name_by_offset(btf, t->name_off)); return true; }","- case BPF_LSM_CGROUP:
+ info->is_retval = true;
+ fallthrough;
+ case BPF_LSM_CGROUP:","bool btf_ctx_access(int off, int size, enum bpf_access_type type, const struct bpf_prog *prog, struct bpf_insn_access_aux *info) { const struct btf_type *t = prog->aux->attach_func_proto; struct bpf_prog *tgt_prog = prog->aux->dst_prog; struct btf *btf = bpf_prog_get_target_btf(prog); const char *tname = prog->aux->attach_func_name; struct bpf_verifier_log *log = info->log; const struct btf_param *args; const char *tag_value; u32 nr_args, arg; int i, ret; if (off % 8) { bpf_log(log, ""func '%s' offset %d is not multiple of 8\n"", tname, off); return false; } arg = get_ctx_arg_idx(btf, t, off); args = (const struct btf_param *)(t + 1); nr_args = t ? btf_type_vlen(t) : MAX_BPF_FUNC_REG_ARGS; if (prog->aux->attach_btf_trace) { args++; nr_args--; } if (arg > nr_args) { bpf_log(log, ""func '%s' doesn't have %d-th argument\n"", tname, arg + 1); return false; } if (arg == nr_args) { switch (prog->expected_attach_type) { case BPF_LSM_MAC: info->is_retval = true; fallthrough; case BPF_LSM_CGROUP: case BPF_TRACE_FEXIT: if (!t) return true; t = btf_type_by_id(btf, t->type); break; case BPF_MODIFY_RETURN: if (!t) return false; t = btf_type_skip_modifiers(btf, t->type, NULL); if (!btf_type_is_small_int(t)) { bpf_log(log, ""ret type %s not allowed for fmod_ret\n"", btf_type_str(t)); return false; } break; default: bpf_log(log, ""func '%s' doesn't have %d-th argument\n"", tname, arg + 1); return false; } } else { if (!t) return true; t = btf_type_by_id(btf, args[arg].type); } while (btf_type_is_modifier(t)) t = btf_type_by_id(btf, t->type); if (btf_type_is_small_int(t) || btf_is_any_enum(t) || __btf_type_is_struct(t)) return true; if (!btf_type_is_ptr(t)) { bpf_log(log, ""func '%s' arg%d '%s' has type %s. Only pointer access is allowed\n"", tname, arg, __btf_name_by_offset(btf, t->name_off), btf_type_str(t)); return false; } for (i = 0; i < prog->aux->ctx_arg_info_size; i++) { const struct bpf_ctx_arg_aux *ctx_arg_info = &prog->aux->ctx_arg_info[i]; u32 type, flag; type = base_type(ctx_arg_info->reg_type); flag = type_flag(ctx_arg_info->reg_type); if (ctx_arg_info->offset == off && type == PTR_TO_BUF && (flag & PTR_MAYBE_NULL)) { info->reg_type = ctx_arg_info->reg_type; return true; } } if (t->type == 0) return true; if (is_int_ptr(btf, t)) return true; for (i = 0; i < prog->aux->ctx_arg_info_size; i++) { const struct bpf_ctx_arg_aux *ctx_arg_info = &prog->aux->ctx_arg_info[i]; if (ctx_arg_info->offset == off) { if (!ctx_arg_info->btf_id) { bpf_log(log,""invalid btf_id for context argument offset %u\n"", off); return false; } info->reg_type = ctx_arg_info->reg_type; info->btf = ctx_arg_info->btf ? : btf_vmlinux; info->btf_id = ctx_arg_info->btf_id; return true; } } info->reg_type = PTR_TO_BTF_ID; if (prog_args_trusted(prog)) info->reg_type |= PTR_TRUSTED; if (tgt_prog) { enum bpf_prog_type tgt_type; if (tgt_prog->type == BPF_PROG_TYPE_EXT) tgt_type = tgt_prog->aux->saved_dst_prog_type; else tgt_type = tgt_prog->type; ret = btf_translate_to_vmlinux(log, btf, t, tgt_type, arg); if (ret > 0) { info->btf = btf_vmlinux; info->btf_id = ret; return true; } else { return false; } } info->btf = btf; info->btf_id = t->type; t = btf_type_by_id(btf, t->type); if (btf_type_is_type_tag(t)) { tag_value = __btf_name_by_offset(btf, t->name_off); if (strcmp(tag_value, ""user"") == 0) info->reg_type |= MEM_USER; if (strcmp(tag_value, ""percpu"") == 0) info->reg_type |= MEM_PERCPU; } while (btf_type_is_modifier(t)) { info->btf_id = t->type; t = btf_type_by_id(btf, t->type); } if (!btf_type_is_struct(t)) { bpf_log(log, ""func '%s' arg%d type %s is not a struct\n"", tname, arg, btf_type_str(t)); return false; } bpf_log(log, ""func '%s' arg%d has btf_id %d type %s '%s'\n"", tname, arg, info->btf_id, btf_type_str(t), __btf_name_by_offset(btf, t->name_off)); return true; }"
303----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46773/bad/amdgpu_dm.c----dm_update_mst_vcpi_slots_for_dsc,"static int dm_update_mst_vcpi_slots_for_dsc(struct drm_atomic_state *state, struct dc_state *dc_state, struct dsc_mst_fairness_vars *vars) { struct dc_stream_state *stream = NULL; struct drm_connector *connector; struct drm_connector_state *new_con_state; struct amdgpu_dm_connector *aconnector; struct dm_connector_state *dm_conn_state; int i, j, ret; int vcpi, pbn_div, pbn = 0, slot_num = 0; for_each_new_connector_in_state(state, connector, new_con_state, i) { if (connector->connector_type == DRM_MODE_CONNECTOR_WRITEBACK) continue; aconnector = to_amdgpu_dm_connector(connector); if (!aconnector->mst_output_port) continue; if (!new_con_state || !new_con_state->crtc) continue; dm_conn_state = to_dm_connector_state(new_con_state); for (j = 0; j < dc_state->stream_count; j++) { stream = dc_state->streams[j]; if (!stream) continue; if ((struct amdgpu_dm_connector *)stream->dm_stream_context == aconnector) break; stream = NULL; } if (!stream) continue; pbn_div = dm_mst_get_pbn_divider(stream->link); for (j = 0; j < dc_state->stream_count; j++) { if (vars[j].aconnector == aconnector) { pbn = vars[j].pbn; break; } } <S2SV_StartVul> if (j == dc_state->stream_count) <S2SV_EndVul> continue; slot_num = DIV_ROUND_UP(pbn, pbn_div); if (stream->timing.flags.DSC != 1) { dm_conn_state->pbn = pbn; dm_conn_state->vcpi_slots = slot_num; ret = drm_dp_mst_atomic_enable_dsc(state, aconnector->mst_output_port, dm_conn_state->pbn, false); if (ret < 0) return ret; continue; } vcpi = drm_dp_mst_atomic_enable_dsc(state, aconnector->mst_output_port, pbn, true); if (vcpi < 0) return vcpi; dm_conn_state->pbn = pbn; dm_conn_state->vcpi_slots = vcpi; } return 0; }","- if (j == dc_state->stream_count)
+ if (j == dc_state->stream_count || pbn_div == 0)","static int dm_update_mst_vcpi_slots_for_dsc(struct drm_atomic_state *state, struct dc_state *dc_state, struct dsc_mst_fairness_vars *vars) { struct dc_stream_state *stream = NULL; struct drm_connector *connector; struct drm_connector_state *new_con_state; struct amdgpu_dm_connector *aconnector; struct dm_connector_state *dm_conn_state; int i, j, ret; int vcpi, pbn_div, pbn = 0, slot_num = 0; for_each_new_connector_in_state(state, connector, new_con_state, i) { if (connector->connector_type == DRM_MODE_CONNECTOR_WRITEBACK) continue; aconnector = to_amdgpu_dm_connector(connector); if (!aconnector->mst_output_port) continue; if (!new_con_state || !new_con_state->crtc) continue; dm_conn_state = to_dm_connector_state(new_con_state); for (j = 0; j < dc_state->stream_count; j++) { stream = dc_state->streams[j]; if (!stream) continue; if ((struct amdgpu_dm_connector *)stream->dm_stream_context == aconnector) break; stream = NULL; } if (!stream) continue; pbn_div = dm_mst_get_pbn_divider(stream->link); for (j = 0; j < dc_state->stream_count; j++) { if (vars[j].aconnector == aconnector) { pbn = vars[j].pbn; break; } } if (j == dc_state->stream_count || pbn_div == 0) continue; slot_num = DIV_ROUND_UP(pbn, pbn_div); if (stream->timing.flags.DSC != 1) { dm_conn_state->pbn = pbn; dm_conn_state->vcpi_slots = slot_num; ret = drm_dp_mst_atomic_enable_dsc(state, aconnector->mst_output_port, dm_conn_state->pbn, false); if (ret < 0) return ret; continue; } vcpi = drm_dp_mst_atomic_enable_dsc(state, aconnector->mst_output_port, pbn, true); if (vcpi < 0) return vcpi; dm_conn_state->pbn = pbn; dm_conn_state->vcpi_slots = vcpi; } return 0; }"
501----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47738/bad/tx.c----ieee80211_tx_h_rate_ctrl,"ieee80211_tx_h_rate_ctrl(struct ieee80211_tx_data *tx) { struct ieee80211_tx_info *info = IEEE80211_SKB_CB(tx->skb); struct ieee80211_hdr *hdr = (void *)tx->skb->data; struct ieee80211_supported_band *sband; u32 len; struct ieee80211_tx_rate_control txrc; struct ieee80211_sta_rates *ratetbl = NULL; bool encap = info->flags & IEEE80211_TX_CTL_HW_80211_ENCAP; bool assoc = false; memset(&txrc, 0, sizeof(txrc)); sband = tx->local->hw.wiphy->bands[info->band]; len = min_t(u32, tx->skb->len + FCS_LEN, tx->local->hw.wiphy->frag_threshold); txrc.hw = &tx->local->hw; txrc.sband = sband; txrc.bss_conf = &tx->sdata->vif.bss_conf; txrc.skb = tx->skb; txrc.reported_rate.idx = -1; <S2SV_StartVul> if (unlikely(info->control.flags & IEEE80211_TX_CTRL_SCAN_TX)) { <S2SV_EndVul> txrc.rate_idx_mask = ~0; } else { txrc.rate_idx_mask = tx->sdata->rc_rateidx_mask[info->band]; if (tx->sdata->rc_has_mcs_mask[info->band]) txrc.rate_idx_mcs_mask = tx->sdata->rc_rateidx_mcs_mask[info->band]; } txrc.bss = (tx->sdata->vif.type == NL80211_IFTYPE_AP || tx->sdata->vif.type == NL80211_IFTYPE_MESH_POINT || tx->sdata->vif.type == NL80211_IFTYPE_ADHOC || tx->sdata->vif.type == NL80211_IFTYPE_OCB); if (len > tx->local->hw.wiphy->rts_threshold) { txrc.rts = true; } info->control.use_rts = txrc.rts; info->control.use_cts_prot = tx->sdata->vif.bss_conf.use_cts_prot; if (tx->sdata->vif.bss_conf.use_short_preamble && (ieee80211_is_tx_data(tx->skb) || (tx->sta && test_sta_flag(tx->sta, WLAN_STA_SHORT_PREAMBLE)))) txrc.short_preamble = true; info->control.short_preamble = txrc.short_preamble; if (info->control.flags & IEEE80211_TX_CTRL_RATE_INJECT) return TX_CONTINUE; if (tx->sta) assoc = test_sta_flag(tx->sta, WLAN_STA_ASSOC); if (WARN(test_bit(SCAN_SW_SCANNING, &tx->local->scanning) && assoc && !rate_usable_index_exists(sband, &tx->sta->sta), ""%s: Dropped data frame as no usable bitrate found while "" ""scanning and associated. Target station: "" ""%pM on %d GHz band\n"", tx->sdata->name, encap ? ((struct ethhdr *)hdr)->h_dest : hdr->addr1, info->band ? 5 : 2)) return TX_DROP; rate_control_get_rate(tx->sdata, tx->sta, &txrc); if (tx->sta && !info->control.skip_table) ratetbl = rcu_dereference(tx->sta->sta.rates); if (unlikely(info->control.rates[0].idx < 0)) { if (ratetbl) { struct ieee80211_tx_rate rate = { .idx = ratetbl->rate[0].idx, .flags = ratetbl->rate[0].flags, .count = ratetbl->rate[0].count }; if (ratetbl->rate[0].idx < 0) return TX_DROP; tx->rate = rate; } else { return TX_DROP; } } else { tx->rate = info->control.rates[0]; } if (txrc.reported_rate.idx < 0) { txrc.reported_rate = tx->rate; if (tx->sta && ieee80211_is_tx_data(tx->skb)) tx->sta->deflink.tx_stats.last_rate = txrc.reported_rate; } else if (tx->sta) tx->sta->deflink.tx_stats.last_rate = txrc.reported_rate; if (ratetbl) return TX_CONTINUE; if (unlikely(!info->control.rates[0].count)) info->control.rates[0].count = 1; if (WARN_ON_ONCE((info->control.rates[0].count > 1) && (info->flags & IEEE80211_TX_CTL_NO_ACK))) info->control.rates[0].count = 1; return TX_CONTINUE; }","- if (unlikely(info->control.flags & IEEE80211_TX_CTRL_SCAN_TX)) {
+ if (unlikely(info->control.flags & IEEE80211_TX_CTRL_DONT_USE_RATE_MASK)) {","ieee80211_tx_h_rate_ctrl(struct ieee80211_tx_data *tx) { struct ieee80211_tx_info *info = IEEE80211_SKB_CB(tx->skb); struct ieee80211_hdr *hdr = (void *)tx->skb->data; struct ieee80211_supported_band *sband; u32 len; struct ieee80211_tx_rate_control txrc; struct ieee80211_sta_rates *ratetbl = NULL; bool encap = info->flags & IEEE80211_TX_CTL_HW_80211_ENCAP; bool assoc = false; memset(&txrc, 0, sizeof(txrc)); sband = tx->local->hw.wiphy->bands[info->band]; len = min_t(u32, tx->skb->len + FCS_LEN, tx->local->hw.wiphy->frag_threshold); txrc.hw = &tx->local->hw; txrc.sband = sband; txrc.bss_conf = &tx->sdata->vif.bss_conf; txrc.skb = tx->skb; txrc.reported_rate.idx = -1; if (unlikely(info->control.flags & IEEE80211_TX_CTRL_DONT_USE_RATE_MASK)) { txrc.rate_idx_mask = ~0; } else { txrc.rate_idx_mask = tx->sdata->rc_rateidx_mask[info->band]; if (tx->sdata->rc_has_mcs_mask[info->band]) txrc.rate_idx_mcs_mask = tx->sdata->rc_rateidx_mcs_mask[info->band]; } txrc.bss = (tx->sdata->vif.type == NL80211_IFTYPE_AP || tx->sdata->vif.type == NL80211_IFTYPE_MESH_POINT || tx->sdata->vif.type == NL80211_IFTYPE_ADHOC || tx->sdata->vif.type == NL80211_IFTYPE_OCB); if (len > tx->local->hw.wiphy->rts_threshold) { txrc.rts = true; } info->control.use_rts = txrc.rts; info->control.use_cts_prot = tx->sdata->vif.bss_conf.use_cts_prot; if (tx->sdata->vif.bss_conf.use_short_preamble && (ieee80211_is_tx_data(tx->skb) || (tx->sta && test_sta_flag(tx->sta, WLAN_STA_SHORT_PREAMBLE)))) txrc.short_preamble = true; info->control.short_preamble = txrc.short_preamble; if (info->control.flags & IEEE80211_TX_CTRL_RATE_INJECT) return TX_CONTINUE; if (tx->sta) assoc = test_sta_flag(tx->sta, WLAN_STA_ASSOC); if (WARN(test_bit(SCAN_SW_SCANNING, &tx->local->scanning) && assoc && !rate_usable_index_exists(sband, &tx->sta->sta), ""%s: Dropped data frame as no usable bitrate found while "" ""scanning and associated. Target station: "" ""%pM on %d GHz band\n"", tx->sdata->name, encap ? ((struct ethhdr *)hdr)->h_dest : hdr->addr1, info->band ? 5 : 2)) return TX_DROP; rate_control_get_rate(tx->sdata, tx->sta, &txrc); if (tx->sta && !info->control.skip_table) ratetbl = rcu_dereference(tx->sta->sta.rates); if (unlikely(info->control.rates[0].idx < 0)) { if (ratetbl) { struct ieee80211_tx_rate rate = { .idx = ratetbl->rate[0].idx, .flags = ratetbl->rate[0].flags, .count = ratetbl->rate[0].count }; if (ratetbl->rate[0].idx < 0) return TX_DROP; tx->rate = rate; } else { return TX_DROP; } } else { tx->rate = info->control.rates[0]; } if (txrc.reported_rate.idx < 0) { txrc.reported_rate = tx->rate; if (tx->sta && ieee80211_is_tx_data(tx->skb)) tx->sta->deflink.tx_stats.last_rate = txrc.reported_rate; } else if (tx->sta) tx->sta->deflink.tx_stats.last_rate = txrc.reported_rate; if (ratetbl) return TX_CONTINUE; if (unlikely(!info->control.rates[0].count)) info->control.rates[0].count = 1; if (WARN_ON_ONCE((info->control.rates[0].count > 1) && (info->flags & IEEE80211_TX_CTL_NO_ACK))) info->control.rates[0].count = 1; return TX_CONTINUE; }"
1166----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53140/bad/af_netlink.c----netlink_sock_destruct,"static void netlink_sock_destruct(struct sock *sk) { struct netlink_sock *nlk = nlk_sk(sk); if (nlk->cb_running) { if (nlk->cb.done) nlk->cb.done(&nlk->cb); module_put(nlk->cb.module); kfree_skb(nlk->cb.skb); } skb_queue_purge(&sk->sk_receive_queue); if (!sock_flag(sk, SOCK_DEAD)) { printk(KERN_ERR ""Freeing alive netlink socket %p\n"", sk); return; } WARN_ON(atomic_read(&sk->sk_rmem_alloc)); WARN_ON(refcount_read(&sk->sk_wmem_alloc)); WARN_ON(nlk_sk(sk)->groups); <S2SV_StartVul> } <S2SV_EndVul>","- }
+ }","static void netlink_sock_destruct(struct sock *sk) { skb_queue_purge(&sk->sk_receive_queue); if (!sock_flag(sk, SOCK_DEAD)) { printk(KERN_ERR ""Freeing alive netlink socket %p\n"", sk); return; } WARN_ON(atomic_read(&sk->sk_rmem_alloc)); WARN_ON(refcount_read(&sk->sk_wmem_alloc)); WARN_ON(nlk_sk(sk)->groups); }"
474----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47718/bad/main.c----rtw_wait_firmware_completion,static int rtw_wait_firmware_completion(struct rtw_dev *rtwdev) { const struct rtw_chip_info *chip = rtwdev->chip; struct rtw_fw_state *fw; fw = &rtwdev->fw; wait_for_completion(&fw->completion); if (!fw->firmware) <S2SV_StartVul> return -EINVAL; <S2SV_EndVul> if (chip->wow_fw_name) { fw = &rtwdev->wow_fw; wait_for_completion(&fw->completion); if (!fw->firmware) <S2SV_StartVul> return -EINVAL; <S2SV_EndVul> } <S2SV_StartVul> return 0; <S2SV_EndVul> },"- return -EINVAL;
- return -EINVAL;
- return 0;
+ int ret = 0;
+ ret = -EINVAL;
+ ret = -EINVAL;
+ return ret;",static int rtw_wait_firmware_completion(struct rtw_dev *rtwdev) { const struct rtw_chip_info *chip = rtwdev->chip; struct rtw_fw_state *fw; int ret = 0; fw = &rtwdev->fw; wait_for_completion(&fw->completion); if (!fw->firmware) ret = -EINVAL; if (chip->wow_fw_name) { fw = &rtwdev->wow_fw; wait_for_completion(&fw->completion); if (!fw->firmware) ret = -EINVAL; } return ret; }
206----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46690/bad/nfs4state.c----nfsd4_deleg_getattr_conflict,"nfsd4_deleg_getattr_conflict(struct svc_rqst *rqstp, struct dentry *dentry, bool *modified, u64 *size) { __be32 status; struct nfsd_net *nn = net_generic(SVC_NET(rqstp), nfsd_net_id); struct file_lock_context *ctx; struct file_lease *fl; struct iattr attrs; struct nfs4_cb_fattr *ncf; struct inode *inode = d_inode(dentry); *modified = false; ctx = locks_inode_context(inode); if (!ctx) return 0; spin_lock(&ctx->flc_lock); for_each_file_lock(fl, &ctx->flc_lease) { unsigned char type = fl->c.flc_type; if (fl->c.flc_flags == FL_LAYOUT) continue; if (fl->fl_lmops != &nfsd_lease_mng_ops) { if (type == F_RDLCK) break; <S2SV_StartVul> goto break_lease; <S2SV_EndVul> } if (type == F_WRLCK) { struct nfs4_delegation *dp = fl->c.flc_owner; if (dp->dl_recall.cb_clp == *(rqstp->rq_lease_breaker)) { spin_unlock(&ctx->flc_lock); return 0; } <S2SV_StartVul> break_lease: <S2SV_EndVul> nfsd_stats_wdeleg_getattr_inc(nn); dp = fl->c.flc_owner; refcount_inc(&dp->dl_stid.sc_count); ncf = &dp->dl_cb_fattr; nfs4_cb_getattr(&dp->dl_cb_fattr); spin_unlock(&ctx->flc_lock); wait_on_bit_timeout(&ncf->ncf_cb_flags, CB_GETATTR_BUSY, TASK_INTERRUPTIBLE, NFSD_CB_GETATTR_TIMEOUT); if (ncf->ncf_cb_status) { status = nfserrno(nfsd_open_break_lease(inode, NFSD_MAY_READ)); if (status != nfserr_jukebox || !nfsd_wait_for_delegreturn(rqstp, inode)) { nfs4_put_stid(&dp->dl_stid); return status; } } if (!ncf->ncf_file_modified && (ncf->ncf_initial_cinfo != ncf->ncf_cb_change || ncf->ncf_cur_fsize != ncf->ncf_cb_fsize)) ncf->ncf_file_modified = true; if (ncf->ncf_file_modified) { int err; attrs.ia_mtime = attrs.ia_ctime = current_time(inode); attrs.ia_valid = ATTR_MTIME | ATTR_CTIME | ATTR_DELEG; inode_lock(inode); err = notify_change(&nop_mnt_idmap, dentry, &attrs, NULL); inode_unlock(inode); if (err) { nfs4_put_stid(&dp->dl_stid); return nfserrno(err); } ncf->ncf_cur_fsize = ncf->ncf_cb_fsize; *size = ncf->ncf_cur_fsize; *modified = true; } nfs4_put_stid(&dp->dl_stid); return 0; } break; } spin_unlock(&ctx->flc_lock); return 0; }","- goto break_lease;
- break_lease:
+ nfsd_stats_wdeleg_getattr_inc(nn);
+ spin_unlock(&ctx->flc_lock);
+ status = nfserrno(nfsd_open_break_lease(inode, NFSD_MAY_READ));
+ if (status != nfserr_jukebox ||
+ !nfsd_wait_for_delegreturn(rqstp, inode))
+ return status;
+ return 0;
+ return 0;
+ nfsd_stats_wdeleg_getattr_inc(nn);","nfsd4_deleg_getattr_conflict(struct svc_rqst *rqstp, struct dentry *dentry, bool *modified, u64 *size) { __be32 status; struct nfsd_net *nn = net_generic(SVC_NET(rqstp), nfsd_net_id); struct file_lock_context *ctx; struct file_lease *fl; struct iattr attrs; struct nfs4_cb_fattr *ncf; struct inode *inode = d_inode(dentry); *modified = false; ctx = locks_inode_context(inode); if (!ctx) return 0; spin_lock(&ctx->flc_lock); for_each_file_lock(fl, &ctx->flc_lease) { unsigned char type = fl->c.flc_type; if (fl->c.flc_flags == FL_LAYOUT) continue; if (fl->fl_lmops != &nfsd_lease_mng_ops) { if (type == F_RDLCK) break; nfsd_stats_wdeleg_getattr_inc(nn); spin_unlock(&ctx->flc_lock); status = nfserrno(nfsd_open_break_lease(inode, NFSD_MAY_READ)); if (status != nfserr_jukebox || !nfsd_wait_for_delegreturn(rqstp, inode)) return status; return 0; } if (type == F_WRLCK) { struct nfs4_delegation *dp = fl->c.flc_owner; if (dp->dl_recall.cb_clp == *(rqstp->rq_lease_breaker)) { spin_unlock(&ctx->flc_lock); return 0; } nfsd_stats_wdeleg_getattr_inc(nn); dp = fl->c.flc_owner; refcount_inc(&dp->dl_stid.sc_count); ncf = &dp->dl_cb_fattr; nfs4_cb_getattr(&dp->dl_cb_fattr); spin_unlock(&ctx->flc_lock); wait_on_bit_timeout(&ncf->ncf_cb_flags, CB_GETATTR_BUSY, TASK_INTERRUPTIBLE, NFSD_CB_GETATTR_TIMEOUT); if (ncf->ncf_cb_status) { status = nfserrno(nfsd_open_break_lease(inode, NFSD_MAY_READ)); if (status != nfserr_jukebox || !nfsd_wait_for_delegreturn(rqstp, inode)) { nfs4_put_stid(&dp->dl_stid); return status; } } if (!ncf->ncf_file_modified && (ncf->ncf_initial_cinfo != ncf->ncf_cb_change || ncf->ncf_cur_fsize != ncf->ncf_cb_fsize)) ncf->ncf_file_modified = true; if (ncf->ncf_file_modified) { int err; attrs.ia_mtime = attrs.ia_ctime = current_time(inode); attrs.ia_valid = ATTR_MTIME | ATTR_CTIME | ATTR_DELEG; inode_lock(inode); err = notify_change(&nop_mnt_idmap, dentry, &attrs, NULL); inode_unlock(inode); if (err) { nfs4_put_stid(&dp->dl_stid); return nfserrno(err); } ncf->ncf_cur_fsize = ncf->ncf_cb_fsize; *size = ncf->ncf_cur_fsize; *modified = true; } nfs4_put_stid(&dp->dl_stid); return 0; } break; } spin_unlock(&ctx->flc_lock); return 0; }"
787----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50063/bad/core.c----bpf_prog_map_compatible,"bool bpf_prog_map_compatible(struct bpf_map *map, const struct bpf_prog *fp) { enum bpf_prog_type prog_type = resolve_prog_type(fp); bool ret; if (fp->kprobe_override) return false; <S2SV_StartVul> if (bpf_prog_is_dev_bound(fp->aux)) <S2SV_EndVul> return false; spin_lock(&map->owner.lock); if (!map->owner.type) { map->owner.type = prog_type; map->owner.jited = fp->jited; <S2SV_StartVul> map->owner.xdp_has_frags = fp->aux->xdp_has_frags; <S2SV_EndVul> ret = true; } else { ret = map->owner.type == prog_type && map->owner.jited == fp->jited && <S2SV_StartVul> map->owner.xdp_has_frags == fp->aux->xdp_has_frags; <S2SV_EndVul> } spin_unlock(&map->owner.lock); return ret; }","- if (bpf_prog_is_dev_bound(fp->aux))
- map->owner.xdp_has_frags = fp->aux->xdp_has_frags;
- map->owner.xdp_has_frags == fp->aux->xdp_has_frags;
+ struct bpf_prog_aux *aux = fp->aux;
+ if (bpf_prog_is_dev_bound(aux))
+ map->owner.xdp_has_frags = aux->xdp_has_frags;
+ map->owner.attach_func_proto = aux->attach_func_proto;
+ map->owner.xdp_has_frags == aux->xdp_has_frags;
+ if (ret &&
+ map->owner.attach_func_proto != aux->attach_func_proto) {
+ switch (prog_type) {
+ case BPF_PROG_TYPE_TRACING:
+ case BPF_PROG_TYPE_LSM:
+ case BPF_PROG_TYPE_EXT:
+ case BPF_PROG_TYPE_STRUCT_OPS:
+ ret = false;
+ break;
+ default:
+ break;
+ }
+ }
+ }","bool bpf_prog_map_compatible(struct bpf_map *map, const struct bpf_prog *fp) { enum bpf_prog_type prog_type = resolve_prog_type(fp); bool ret; struct bpf_prog_aux *aux = fp->aux; if (fp->kprobe_override) return false; if (bpf_prog_is_dev_bound(aux)) return false; spin_lock(&map->owner.lock); if (!map->owner.type) { map->owner.type = prog_type; map->owner.jited = fp->jited; map->owner.xdp_has_frags = aux->xdp_has_frags; map->owner.attach_func_proto = aux->attach_func_proto; ret = true; } else { ret = map->owner.type == prog_type && map->owner.jited == fp->jited && map->owner.xdp_has_frags == aux->xdp_has_frags; if (ret && map->owner.attach_func_proto != aux->attach_func_proto) { switch (prog_type) { case BPF_PROG_TYPE_TRACING: case BPF_PROG_TYPE_LSM: case BPF_PROG_TYPE_EXT: case BPF_PROG_TYPE_STRUCT_OPS: ret = false; break; default: break; } } } spin_unlock(&map->owner.lock); return ret; }"
1074----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53058/bad/stmmac_main.c----stmmac_tso_xmit,"static netdev_tx_t stmmac_tso_xmit(struct sk_buff *skb, struct net_device *dev) { struct dma_desc *desc, *first, *mss_desc = NULL; struct stmmac_priv *priv = netdev_priv(dev); int nfrags = skb_shinfo(skb)->nr_frags; u32 queue = skb_get_queue_mapping(skb); unsigned int first_entry, tx_packets; int tmp_pay_len = 0, first_tx; struct stmmac_tx_queue *tx_q; bool has_vlan, set_ic; u8 proto_hdr_len, hdr; u32 pay_len, mss; dma_addr_t des; int i; tx_q = &priv->dma_conf.tx_queue[queue]; first_tx = tx_q->cur_tx; if (skb_shinfo(skb)->gso_type & SKB_GSO_UDP_L4) { proto_hdr_len = skb_transport_offset(skb) + sizeof(struct udphdr); hdr = sizeof(struct udphdr); } else { proto_hdr_len = skb_tcp_all_headers(skb); hdr = tcp_hdrlen(skb); } if (unlikely(stmmac_tx_avail(priv, queue) < (((skb->len - proto_hdr_len) / TSO_MAX_BUFF_SIZE + 1)))) { if (!netif_tx_queue_stopped(netdev_get_tx_queue(dev, queue))) { netif_tx_stop_queue(netdev_get_tx_queue(priv->dev, queue)); netdev_err(priv->dev, ""%s: Tx Ring full when queue awake\n"", __func__); } return NETDEV_TX_BUSY; } pay_len = skb_headlen(skb) - proto_hdr_len; mss = skb_shinfo(skb)->gso_size; if (mss != tx_q->mss) { if (tx_q->tbs & STMMAC_TBS_AVAIL) mss_desc = &tx_q->dma_entx[tx_q->cur_tx].basic; else mss_desc = &tx_q->dma_tx[tx_q->cur_tx]; stmmac_set_mss(priv, mss_desc, mss); tx_q->mss = mss; tx_q->cur_tx = STMMAC_GET_ENTRY(tx_q->cur_tx, priv->dma_conf.dma_tx_size); WARN_ON(tx_q->tx_skbuff[tx_q->cur_tx]); } if (netif_msg_tx_queued(priv)) { pr_info(""%s: hdrlen %d, hdr_len %d, pay_len %d, mss %d\n"", __func__, hdr, proto_hdr_len, pay_len, mss); pr_info(""\tskb->len %d, skb->data_len %d\n"", skb->len, skb->data_len); } has_vlan = stmmac_vlan_insert(priv, skb, tx_q); first_entry = tx_q->cur_tx; WARN_ON(tx_q->tx_skbuff[first_entry]); if (tx_q->tbs & STMMAC_TBS_AVAIL) desc = &tx_q->dma_entx[first_entry].basic; else desc = &tx_q->dma_tx[first_entry]; first = desc; if (has_vlan) stmmac_set_desc_vlan(priv, first, STMMAC_VLAN_INSERT); des = dma_map_single(priv->device, skb->data, skb_headlen(skb), DMA_TO_DEVICE); if (dma_mapping_error(priv->device, des)) goto dma_map_err; <S2SV_StartVul> tx_q->tx_skbuff_dma[first_entry].buf = des; <S2SV_EndVul> <S2SV_StartVul> tx_q->tx_skbuff_dma[first_entry].len = skb_headlen(skb); <S2SV_EndVul> <S2SV_StartVul> tx_q->tx_skbuff_dma[first_entry].map_as_page = false; <S2SV_EndVul> <S2SV_StartVul> tx_q->tx_skbuff_dma[first_entry].buf_type = STMMAC_TXBUF_T_SKB; <S2SV_EndVul> if (priv->dma_cap.addr64 <= 32) { first->des0 = cpu_to_le32(des); if (pay_len) first->des1 = cpu_to_le32(des + proto_hdr_len); tmp_pay_len = pay_len - TSO_MAX_BUFF_SIZE; } else { stmmac_set_desc_addr(priv, first, des); tmp_pay_len = pay_len; des += proto_hdr_len; pay_len = 0; } stmmac_tso_allocator(priv, des, tmp_pay_len, (nfrags == 0), queue); for (i = 0; i < nfrags; i++) { const skb_frag_t *frag = &skb_shinfo(skb)->frags[i]; des = skb_frag_dma_map(priv->device, frag, 0, skb_frag_size(frag), DMA_TO_DEVICE); if (dma_mapping_error(priv->device, des)) goto dma_map_err; stmmac_tso_allocator(priv, des, skb_frag_size(frag), (i == nfrags - 1), queue); tx_q->tx_skbuff_dma[tx_q->cur_tx].buf = des; tx_q->tx_skbuff_dma[tx_q->cur_tx].len = skb_frag_size(frag); tx_q->tx_skbuff_dma[tx_q->cur_tx].map_as_page = true; tx_q->tx_skbuff_dma[tx_q->cur_tx].buf_type = STMMAC_TXBUF_T_SKB; } tx_q->tx_skbuff_dma[tx_q->cur_tx].last_segment = true; tx_q->tx_skbuff[tx_q->cur_tx] = skb; tx_q->tx_skbuff_dma[tx_q->cur_tx].buf_type = STMMAC_TXBUF_T_SKB; tx_packets = (tx_q->cur_tx + 1) - first_tx; tx_q->tx_count_frames += tx_packets; if ((skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP) && priv->hwts_tx_en) set_ic = true; else if (!priv->tx_coal_frames[queue]) set_ic = false; else if (tx_packets > priv->tx_coal_frames[queue]) set_ic = true; else if ((tx_q->tx_count_frames % priv->tx_coal_frames[queue]) < tx_packets) set_ic = true; else set_ic = false; if (set_ic) { if (tx_q->tbs & STMMAC_TBS_AVAIL) desc = &tx_q->dma_entx[tx_q->cur_tx].basic; else desc = &tx_q->dma_tx[tx_q->cur_tx]; tx_q->tx_count_frames = 0; stmmac_set_tx_ic(priv, desc); priv->xstats.tx_set_ic_bit++; } tx_q->cur_tx = STMMAC_GET_ENTRY(tx_q->cur_tx, priv->dma_conf.dma_tx_size); if (unlikely(stmmac_tx_avail(priv, queue) <= (MAX_SKB_FRAGS + 1))) { netif_dbg(priv, hw, priv->dev, ""%s: stop transmitted packets\n"", __func__); netif_tx_stop_queue(netdev_get_tx_queue(priv->dev, queue)); } dev->stats.tx_bytes += skb->len; priv->xstats.tx_tso_frames++; priv->xstats.tx_tso_nfrags += nfrags; if (priv->sarc_type) stmmac_set_desc_sarc(priv, first, priv->sarc_type); skb_tx_timestamp(skb); if (unlikely((skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP) && priv->hwts_tx_en)) { skb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS; stmmac_enable_tx_timestamp(priv, first); } stmmac_prepare_tso_tx_desc(priv, first, 1, proto_hdr_len, pay_len, 1, tx_q->tx_skbuff_dma[first_entry].last_segment, hdr / 4, (skb->len - proto_hdr_len)); if (mss_desc) { dma_wmb(); stmmac_set_tx_owner(priv, mss_desc); } if (netif_msg_pktdata(priv)) { pr_info(""%s: curr=%d dirty=%d f=%d, e=%d, f_p=%p, nfrags %d\n"", __func__, tx_q->cur_tx, tx_q->dirty_tx, first_entry, tx_q->cur_tx, first, nfrags); pr_info("">>> frame to be transmitted: ""); print_pkt(skb->data, skb_headlen(skb)); } netdev_tx_sent_queue(netdev_get_tx_queue(dev, queue), skb->len); stmmac_flush_tx_descriptors(priv, queue); stmmac_tx_timer_arm(priv, queue); return NETDEV_TX_OK; dma_map_err: dev_err(priv->device, ""Tx dma map failed\n""); dev_kfree_skb(skb); priv->dev->stats.tx_dropped++; return NETDEV_TX_OK; }","- tx_q->tx_skbuff_dma[first_entry].buf = des;
- tx_q->tx_skbuff_dma[first_entry].len = skb_headlen(skb);
- tx_q->tx_skbuff_dma[first_entry].map_as_page = false;
- tx_q->tx_skbuff_dma[first_entry].buf_type = STMMAC_TXBUF_T_SKB;
+ tx_q->tx_skbuff_dma[tx_q->cur_tx].buf = des;
+ tx_q->tx_skbuff_dma[tx_q->cur_tx].len = skb_headlen(skb);
+ tx_q->tx_skbuff_dma[tx_q->cur_tx].map_as_page = false;
+ tx_q->tx_skbuff_dma[tx_q->cur_tx].buf_type = STMMAC_TXBUF_T_SKB;","static netdev_tx_t stmmac_tso_xmit(struct sk_buff *skb, struct net_device *dev) { struct dma_desc *desc, *first, *mss_desc = NULL; struct stmmac_priv *priv = netdev_priv(dev); int nfrags = skb_shinfo(skb)->nr_frags; u32 queue = skb_get_queue_mapping(skb); unsigned int first_entry, tx_packets; int tmp_pay_len = 0, first_tx; struct stmmac_tx_queue *tx_q; bool has_vlan, set_ic; u8 proto_hdr_len, hdr; u32 pay_len, mss; dma_addr_t des; int i; tx_q = &priv->dma_conf.tx_queue[queue]; first_tx = tx_q->cur_tx; if (skb_shinfo(skb)->gso_type & SKB_GSO_UDP_L4) { proto_hdr_len = skb_transport_offset(skb) + sizeof(struct udphdr); hdr = sizeof(struct udphdr); } else { proto_hdr_len = skb_tcp_all_headers(skb); hdr = tcp_hdrlen(skb); } if (unlikely(stmmac_tx_avail(priv, queue) < (((skb->len - proto_hdr_len) / TSO_MAX_BUFF_SIZE + 1)))) { if (!netif_tx_queue_stopped(netdev_get_tx_queue(dev, queue))) { netif_tx_stop_queue(netdev_get_tx_queue(priv->dev, queue)); netdev_err(priv->dev, ""%s: Tx Ring full when queue awake\n"", __func__); } return NETDEV_TX_BUSY; } pay_len = skb_headlen(skb) - proto_hdr_len; mss = skb_shinfo(skb)->gso_size; if (mss != tx_q->mss) { if (tx_q->tbs & STMMAC_TBS_AVAIL) mss_desc = &tx_q->dma_entx[tx_q->cur_tx].basic; else mss_desc = &tx_q->dma_tx[tx_q->cur_tx]; stmmac_set_mss(priv, mss_desc, mss); tx_q->mss = mss; tx_q->cur_tx = STMMAC_GET_ENTRY(tx_q->cur_tx, priv->dma_conf.dma_tx_size); WARN_ON(tx_q->tx_skbuff[tx_q->cur_tx]); } if (netif_msg_tx_queued(priv)) { pr_info(""%s: hdrlen %d, hdr_len %d, pay_len %d, mss %d\n"", __func__, hdr, proto_hdr_len, pay_len, mss); pr_info(""\tskb->len %d, skb->data_len %d\n"", skb->len, skb->data_len); } has_vlan = stmmac_vlan_insert(priv, skb, tx_q); first_entry = tx_q->cur_tx; WARN_ON(tx_q->tx_skbuff[first_entry]); if (tx_q->tbs & STMMAC_TBS_AVAIL) desc = &tx_q->dma_entx[first_entry].basic; else desc = &tx_q->dma_tx[first_entry]; first = desc; if (has_vlan) stmmac_set_desc_vlan(priv, first, STMMAC_VLAN_INSERT); des = dma_map_single(priv->device, skb->data, skb_headlen(skb), DMA_TO_DEVICE); if (dma_mapping_error(priv->device, des)) goto dma_map_err; if (priv->dma_cap.addr64 <= 32) { first->des0 = cpu_to_le32(des); if (pay_len) first->des1 = cpu_to_le32(des + proto_hdr_len); tmp_pay_len = pay_len - TSO_MAX_BUFF_SIZE; } else { stmmac_set_desc_addr(priv, first, des); tmp_pay_len = pay_len; des += proto_hdr_len; pay_len = 0; } stmmac_tso_allocator(priv, des, tmp_pay_len, (nfrags == 0), queue); tx_q->tx_skbuff_dma[tx_q->cur_tx].buf = des; tx_q->tx_skbuff_dma[tx_q->cur_tx].len = skb_headlen(skb); tx_q->tx_skbuff_dma[tx_q->cur_tx].map_as_page = false; tx_q->tx_skbuff_dma[tx_q->cur_tx].buf_type = STMMAC_TXBUF_T_SKB; for (i = 0; i < nfrags; i++) { const skb_frag_t *frag = &skb_shinfo(skb)->frags[i]; des = skb_frag_dma_map(priv->device, frag, 0, skb_frag_size(frag), DMA_TO_DEVICE); if (dma_mapping_error(priv->device, des)) goto dma_map_err; stmmac_tso_allocator(priv, des, skb_frag_size(frag), (i == nfrags - 1), queue); tx_q->tx_skbuff_dma[tx_q->cur_tx].buf = des; tx_q->tx_skbuff_dma[tx_q->cur_tx].len = skb_frag_size(frag); tx_q->tx_skbuff_dma[tx_q->cur_tx].map_as_page = true; tx_q->tx_skbuff_dma[tx_q->cur_tx].buf_type = STMMAC_TXBUF_T_SKB; } tx_q->tx_skbuff_dma[tx_q->cur_tx].last_segment = true; tx_q->tx_skbuff[tx_q->cur_tx] = skb; tx_q->tx_skbuff_dma[tx_q->cur_tx].buf_type = STMMAC_TXBUF_T_SKB; tx_packets = (tx_q->cur_tx + 1) - first_tx; tx_q->tx_count_frames += tx_packets; if ((skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP) && priv->hwts_tx_en) set_ic = true; else if (!priv->tx_coal_frames[queue]) set_ic = false; else if (tx_packets > priv->tx_coal_frames[queue]) set_ic = true; else if ((tx_q->tx_count_frames % priv->tx_coal_frames[queue]) < tx_packets) set_ic = true; else set_ic = false; if (set_ic) { if (tx_q->tbs & STMMAC_TBS_AVAIL) desc = &tx_q->dma_entx[tx_q->cur_tx].basic; else desc = &tx_q->dma_tx[tx_q->cur_tx]; tx_q->tx_count_frames = 0; stmmac_set_tx_ic(priv, desc); priv->xstats.tx_set_ic_bit++; } tx_q->cur_tx = STMMAC_GET_ENTRY(tx_q->cur_tx, priv->dma_conf.dma_tx_size); if (unlikely(stmmac_tx_avail(priv, queue) <= (MAX_SKB_FRAGS + 1))) { netif_dbg(priv, hw, priv->dev, ""%s: stop transmitted packets\n"", __func__); netif_tx_stop_queue(netdev_get_tx_queue(priv->dev, queue)); } dev->stats.tx_bytes += skb->len; priv->xstats.tx_tso_frames++; priv->xstats.tx_tso_nfrags += nfrags; if (priv->sarc_type) stmmac_set_desc_sarc(priv, first, priv->sarc_type); skb_tx_timestamp(skb); if (unlikely((skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP) && priv->hwts_tx_en)) { skb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS; stmmac_enable_tx_timestamp(priv, first); } stmmac_prepare_tso_tx_desc(priv, first, 1, proto_hdr_len, pay_len, 1, tx_q->tx_skbuff_dma[first_entry].last_segment, hdr / 4, (skb->len - proto_hdr_len)); if (mss_desc) { dma_wmb(); stmmac_set_tx_owner(priv, mss_desc); } if (netif_msg_pktdata(priv)) { pr_info(""%s: curr=%d dirty=%d f=%d, e=%d, f_p=%p, nfrags %d\n"", __func__, tx_q->cur_tx, tx_q->dirty_tx, first_entry, tx_q->cur_tx, first, nfrags); pr_info("">>> frame to be transmitted: ""); print_pkt(skb->data, skb_headlen(skb)); } netdev_tx_sent_queue(netdev_get_tx_queue(dev, queue), skb->len); stmmac_flush_tx_descriptors(priv, queue); stmmac_tx_timer_arm(priv, queue); return NETDEV_TX_OK; dma_map_err: dev_err(priv->device, ""Tx dma map failed\n""); dev_kfree_skb(skb); priv->dev->stats.tx_dropped++; return NETDEV_TX_OK; }"
15----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-43891/bad/trace_events_trigger.c----event_trigger_regex_write,"static ssize_t event_trigger_regex_write(struct file *file, const char __user *ubuf, size_t cnt, loff_t *ppos) { struct trace_event_file *event_file; ssize_t ret; char *buf; if (!cnt) return 0; if (cnt >= PAGE_SIZE) return -EINVAL; buf = memdup_user_nul(ubuf, cnt); if (IS_ERR(buf)) return PTR_ERR(buf); strim(buf); mutex_lock(&event_mutex); <S2SV_StartVul> event_file = event_file_data(file); <S2SV_EndVul> if (unlikely(!event_file)) { mutex_unlock(&event_mutex); kfree(buf); return -ENODEV; } ret = trigger_process_regex(event_file, buf); mutex_unlock(&event_mutex); kfree(buf); if (ret < 0) goto out; *ppos += cnt; ret = cnt; out: return ret; }","- event_file = event_file_data(file);
+ event_file = event_file_file(file);","static ssize_t event_trigger_regex_write(struct file *file, const char __user *ubuf, size_t cnt, loff_t *ppos) { struct trace_event_file *event_file; ssize_t ret; char *buf; if (!cnt) return 0; if (cnt >= PAGE_SIZE) return -EINVAL; buf = memdup_user_nul(ubuf, cnt); if (IS_ERR(buf)) return PTR_ERR(buf); strim(buf); mutex_lock(&event_mutex); event_file = event_file_file(file); if (unlikely(!event_file)) { mutex_unlock(&event_mutex); kfree(buf); return -ENODEV; } ret = trigger_process_regex(event_file, buf); mutex_unlock(&event_mutex); kfree(buf); if (ret < 0) goto out; *ppos += cnt; ret = cnt; out: return ret; }"
626----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49932/bad/relocation.c----relocate_one_folio,"static int relocate_one_folio(struct reloc_control *rc, struct file_ra_state *ra, int *cluster_nr, unsigned long index) { const struct file_extent_cluster *cluster = &rc->cluster; struct inode *inode = rc->data_inode; struct btrfs_fs_info *fs_info = inode_to_fs_info(inode); u64 offset = BTRFS_I(inode)->reloc_block_group_start; const unsigned long last_index = (cluster->end - offset) >> PAGE_SHIFT; gfp_t mask = btrfs_alloc_write_mask(inode->i_mapping); struct folio *folio; u64 folio_start; u64 folio_end; u64 cur; int ret; ASSERT(index <= last_index); folio = filemap_lock_folio(inode->i_mapping, index); if (IS_ERR(folio)) { <S2SV_StartVul> page_cache_sync_readahead(inode->i_mapping, ra, NULL, <S2SV_EndVul> <S2SV_StartVul> index, last_index + 1 - index); <S2SV_EndVul> folio = __filemap_get_folio(inode->i_mapping, index, <S2SV_StartVul> FGP_LOCK | FGP_ACCESSED | FGP_CREAT, mask); <S2SV_EndVul> if (IS_ERR(folio)) return PTR_ERR(folio); } WARN_ON(folio_order(folio)); <S2SV_StartVul> if (folio_test_readahead(folio)) <S2SV_EndVul> page_cache_async_readahead(inode->i_mapping, ra, NULL, folio, last_index + 1 - index); if (!folio_test_uptodate(folio)) { btrfs_read_folio(NULL, folio); folio_lock(folio); if (!folio_test_uptodate(folio)) { ret = -EIO; goto release_folio; } } ret = set_folio_extent_mapped(folio); if (ret < 0) goto release_folio; folio_start = folio_pos(folio); folio_end = folio_start + PAGE_SIZE - 1; cur = max(folio_start, cluster->boundary[*cluster_nr] - offset); while (cur <= folio_end) { struct extent_state *cached_state = NULL; u64 extent_start = cluster->boundary[*cluster_nr] - offset; u64 extent_end = get_cluster_boundary_end(cluster, *cluster_nr) - offset; u64 clamped_start = max(folio_start, extent_start); u64 clamped_end = min(folio_end, extent_end); u32 clamped_len = clamped_end + 1 - clamped_start; ret = btrfs_delalloc_reserve_metadata(BTRFS_I(inode), clamped_len, clamped_len, false); if (ret) goto release_folio; lock_extent(&BTRFS_I(inode)->io_tree, clamped_start, clamped_end, &cached_state); ret = btrfs_set_extent_delalloc(BTRFS_I(inode), clamped_start, clamped_end, 0, &cached_state); if (ret) { clear_extent_bit(&BTRFS_I(inode)->io_tree, clamped_start, clamped_end, EXTENT_LOCKED | EXTENT_BOUNDARY, &cached_state); btrfs_delalloc_release_metadata(BTRFS_I(inode), clamped_len, true); btrfs_delalloc_release_extents(BTRFS_I(inode), clamped_len); goto release_folio; } btrfs_folio_set_dirty(fs_info, folio, clamped_start, clamped_len); if (in_range(cluster->boundary[*cluster_nr] - offset, folio_start, PAGE_SIZE)) { u64 boundary_start = cluster->boundary[*cluster_nr] - offset; u64 boundary_end = boundary_start + fs_info->sectorsize - 1; set_extent_bit(&BTRFS_I(inode)->io_tree, boundary_start, boundary_end, EXTENT_BOUNDARY, NULL); } unlock_extent(&BTRFS_I(inode)->io_tree, clamped_start, clamped_end, &cached_state); btrfs_delalloc_release_extents(BTRFS_I(inode), clamped_len); cur += clamped_len; if (cur >= extent_end) { (*cluster_nr)++; if (*cluster_nr >= cluster->nr) break; } } folio_unlock(folio); folio_put(folio); balance_dirty_pages_ratelimited(inode->i_mapping); btrfs_throttle(fs_info); if (btrfs_should_cancel_balance(fs_info)) ret = -ECANCELED; return ret; release_folio: folio_unlock(folio); folio_put(folio); return ret; }","- page_cache_sync_readahead(inode->i_mapping, ra, NULL,
- index, last_index + 1 - index);
- FGP_LOCK | FGP_ACCESSED | FGP_CREAT, mask);
- if (folio_test_readahead(folio))
+ const bool use_rst = btrfs_need_stripe_tree_update(fs_info, rc->block_group->flags);
+ if (!use_rst)
+ page_cache_sync_readahead(inode->i_mapping, ra, NULL,
+ index, last_index + 1 - index);
+ FGP_LOCK | FGP_ACCESSED | FGP_CREAT,
+ mask);
+ if (folio_test_readahead(folio) && !use_rst)","static int relocate_one_folio(struct reloc_control *rc, struct file_ra_state *ra, int *cluster_nr, unsigned long index) { const struct file_extent_cluster *cluster = &rc->cluster; struct inode *inode = rc->data_inode; struct btrfs_fs_info *fs_info = inode_to_fs_info(inode); u64 offset = BTRFS_I(inode)->reloc_block_group_start; const unsigned long last_index = (cluster->end - offset) >> PAGE_SHIFT; gfp_t mask = btrfs_alloc_write_mask(inode->i_mapping); struct folio *folio; u64 folio_start; u64 folio_end; u64 cur; int ret; const bool use_rst = btrfs_need_stripe_tree_update(fs_info, rc->block_group->flags); ASSERT(index <= last_index); folio = filemap_lock_folio(inode->i_mapping, index); if (IS_ERR(folio)) { if (!use_rst) page_cache_sync_readahead(inode->i_mapping, ra, NULL, index, last_index + 1 - index); folio = __filemap_get_folio(inode->i_mapping, index, FGP_LOCK | FGP_ACCESSED | FGP_CREAT, mask); if (IS_ERR(folio)) return PTR_ERR(folio); } WARN_ON(folio_order(folio)); if (folio_test_readahead(folio) && !use_rst) page_cache_async_readahead(inode->i_mapping, ra, NULL, folio, last_index + 1 - index); if (!folio_test_uptodate(folio)) { btrfs_read_folio(NULL, folio); folio_lock(folio); if (!folio_test_uptodate(folio)) { ret = -EIO; goto release_folio; } } ret = set_folio_extent_mapped(folio); if (ret < 0) goto release_folio; folio_start = folio_pos(folio); folio_end = folio_start + PAGE_SIZE - 1; cur = max(folio_start, cluster->boundary[*cluster_nr] - offset); while (cur <= folio_end) { struct extent_state *cached_state = NULL; u64 extent_start = cluster->boundary[*cluster_nr] - offset; u64 extent_end = get_cluster_boundary_end(cluster, *cluster_nr) - offset; u64 clamped_start = max(folio_start, extent_start); u64 clamped_end = min(folio_end, extent_end); u32 clamped_len = clamped_end + 1 - clamped_start; ret = btrfs_delalloc_reserve_metadata(BTRFS_I(inode), clamped_len, clamped_len, false); if (ret) goto release_folio; lock_extent(&BTRFS_I(inode)->io_tree, clamped_start, clamped_end, &cached_state); ret = btrfs_set_extent_delalloc(BTRFS_I(inode), clamped_start, clamped_end, 0, &cached_state); if (ret) { clear_extent_bit(&BTRFS_I(inode)->io_tree, clamped_start, clamped_end, EXTENT_LOCKED | EXTENT_BOUNDARY, &cached_state); btrfs_delalloc_release_metadata(BTRFS_I(inode), clamped_len, true); btrfs_delalloc_release_extents(BTRFS_I(inode), clamped_len); goto release_folio; } btrfs_folio_set_dirty(fs_info, folio, clamped_start, clamped_len); if (in_range(cluster->boundary[*cluster_nr] - offset, folio_start, PAGE_SIZE)) { u64 boundary_start = cluster->boundary[*cluster_nr] - offset; u64 boundary_end = boundary_start + fs_info->sectorsize - 1; set_extent_bit(&BTRFS_I(inode)->io_tree, boundary_start, boundary_end, EXTENT_BOUNDARY, NULL); } unlock_extent(&BTRFS_I(inode)->io_tree, clamped_start, clamped_end, &cached_state); btrfs_delalloc_release_extents(BTRFS_I(inode), clamped_len); cur += clamped_len; if (cur >= extent_end) { (*cluster_nr)++; if (*cluster_nr >= cluster->nr) break; } } folio_unlock(folio); folio_put(folio); balance_dirty_pages_ratelimited(inode->i_mapping); btrfs_throttle(fs_info); if (btrfs_should_cancel_balance(fs_info)) ret = -ECANCELED; return ret; release_folio: folio_unlock(folio); folio_put(folio); return ret; }"
268----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46750/bad/pci.c----pci_bus_trylock,"static int pci_bus_trylock(struct pci_bus *bus) { struct pci_dev *dev; list_for_each_entry(dev, &bus->devices, bus_list) { <S2SV_StartVul> if (!pci_dev_trylock(dev)) <S2SV_EndVul> <S2SV_StartVul> goto unlock; <S2SV_EndVul> if (dev->subordinate) { <S2SV_StartVul> if (!pci_bus_trylock(dev->subordinate)) { <S2SV_EndVul> <S2SV_StartVul> pci_dev_unlock(dev); <S2SV_EndVul> <S2SV_StartVul> goto unlock; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> return 1; unlock: list_for_each_entry_continue_reverse(dev, &bus->devices, bus_list) { if (dev->subordinate) pci_bus_unlock(dev->subordinate); <S2SV_StartVul> pci_dev_unlock(dev); <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> return 0; <S2SV_StartVul> } <S2SV_EndVul>","- if (!pci_dev_trylock(dev))
- goto unlock;
- if (!pci_bus_trylock(dev->subordinate)) {
- pci_dev_unlock(dev);
- goto unlock;
- }
- }
- }
- pci_dev_unlock(dev);
- }
- }
+ if (!pci_dev_trylock(bus->self))
+ return 0;
+ if (!pci_bus_trylock(dev->subordinate))
+ goto unlock;
+ } else if (!pci_dev_trylock(dev))
+ goto unlock;
+ else
+ pci_dev_unlock(dev);
+ pci_dev_unlock(bus->self);
+ return 0;","static int pci_bus_trylock(struct pci_bus *bus) { struct pci_dev *dev; if (!pci_dev_trylock(bus->self)) return 0; list_for_each_entry(dev, &bus->devices, bus_list) { if (dev->subordinate) { if (!pci_bus_trylock(dev->subordinate)) goto unlock; } else if (!pci_dev_trylock(dev)) goto unlock; } return 1; unlock: list_for_each_entry_continue_reverse(dev, &bus->devices, bus_list) { if (dev->subordinate) pci_bus_unlock(dev->subordinate); else pci_dev_unlock(dev); } pci_dev_unlock(bus->self); return 0; }"
1399----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56724/bad/intel_bxtwc_tmu.c----bxt_wcove_tmu_probe,"static int bxt_wcove_tmu_probe(struct platform_device *pdev) { struct intel_soc_pmic *pmic = dev_get_drvdata(pdev->dev.parent); <S2SV_StartVul> struct regmap_irq_chip_data *regmap_irq_chip; <S2SV_EndVul> struct wcove_tmu *wctmu; <S2SV_StartVul> int ret, virq, irq; <S2SV_EndVul> wctmu = devm_kzalloc(&pdev->dev, sizeof(*wctmu), GFP_KERNEL); if (!wctmu) return -ENOMEM; wctmu->dev = &pdev->dev; wctmu->regmap = pmic->regmap; <S2SV_StartVul> irq = platform_get_irq(pdev, 0); <S2SV_EndVul> <S2SV_StartVul> if (irq < 0) <S2SV_EndVul> <S2SV_StartVul> return irq; <S2SV_EndVul> <S2SV_StartVul> regmap_irq_chip = pmic->irq_chip_data_tmu; <S2SV_EndVul> <S2SV_StartVul> virq = regmap_irq_get_virq(regmap_irq_chip, irq); <S2SV_EndVul> <S2SV_StartVul> if (virq < 0) { <S2SV_EndVul> <S2SV_StartVul> dev_err(&pdev->dev, <S2SV_EndVul> <S2SV_StartVul> ""failed to get virtual interrupt=%d\n"", irq); <S2SV_EndVul> <S2SV_StartVul> return virq; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> ret = devm_request_threaded_irq(&pdev->dev, virq, <S2SV_EndVul> NULL, bxt_wcove_tmu_irq_handler, IRQF_ONESHOT, ""bxt_wcove_tmu"", wctmu); if (ret) { dev_err(&pdev->dev, ""request irq failed: %d,virq: %d\n"", <S2SV_StartVul> ret, virq); <S2SV_EndVul> return ret; <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> wctmu->irq = virq; <S2SV_EndVul> regmap_update_bits(wctmu->regmap, BXTWC_MTMUIRQ_REG, BXTWC_TMU_ALRM_MASK, 0); platform_set_drvdata(pdev, wctmu); return 0; }","- struct regmap_irq_chip_data *regmap_irq_chip;
- int ret, virq, irq;
- irq = platform_get_irq(pdev, 0);
- if (irq < 0)
- return irq;
- regmap_irq_chip = pmic->irq_chip_data_tmu;
- virq = regmap_irq_get_virq(regmap_irq_chip, irq);
- if (virq < 0) {
- dev_err(&pdev->dev,
- ""failed to get virtual interrupt=%d\n"", irq);
- return virq;
- }
- ret = devm_request_threaded_irq(&pdev->dev, virq,
- ret, virq);
- }
- wctmu->irq = virq;
+ int ret;
+ wctmu->irq = platform_get_irq(pdev, 0);
+ if (wctmu->irq < 0)
+ return wctmu->irq;
+ ret = devm_request_threaded_irq(&pdev->dev, wctmu->irq,
+ ret, wctmu->irq);","static int bxt_wcove_tmu_probe(struct platform_device *pdev) { struct intel_soc_pmic *pmic = dev_get_drvdata(pdev->dev.parent); struct wcove_tmu *wctmu; int ret; wctmu = devm_kzalloc(&pdev->dev, sizeof(*wctmu), GFP_KERNEL); if (!wctmu) return -ENOMEM; wctmu->dev = &pdev->dev; wctmu->regmap = pmic->regmap; wctmu->irq = platform_get_irq(pdev, 0); if (wctmu->irq < 0) return wctmu->irq; ret = devm_request_threaded_irq(&pdev->dev, wctmu->irq, NULL, bxt_wcove_tmu_irq_handler, IRQF_ONESHOT, ""bxt_wcove_tmu"", wctmu); if (ret) { dev_err(&pdev->dev, ""request irq failed: %d,virq: %d\n"", ret, wctmu->irq); return ret; } regmap_update_bits(wctmu->regmap, BXTWC_MTMUIRQ_REG, BXTWC_TMU_ALRM_MASK, 0); platform_set_drvdata(pdev, wctmu); return 0; }"
64----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44943/bad/gup.c----*follow_page_pte,"static struct page *follow_page_pte(struct vm_area_struct *vma, unsigned long address, pmd_t *pmd, unsigned int flags, struct dev_pagemap **pgmap) { struct mm_struct *mm = vma->vm_mm; struct page *page; spinlock_t *ptl; pte_t *ptep, pte; int ret; if (WARN_ON_ONCE((flags & (FOLL_PIN | FOLL_GET)) == (FOLL_PIN | FOLL_GET))) return ERR_PTR(-EINVAL); ptep = pte_offset_map_lock(mm, pmd, address, &ptl); if (!ptep) return no_page_table(vma, flags); pte = ptep_get(ptep); if (!pte_present(pte)) goto no_page; if (pte_protnone(pte) && !gup_can_follow_protnone(vma, flags)) goto no_page; page = vm_normal_page(vma, address, pte); if ((flags & FOLL_WRITE) && !can_follow_write_pte(pte, page, vma, flags)) { page = NULL; goto out; } if (!page && pte_devmap(pte) && (flags & (FOLL_GET | FOLL_PIN))) { *pgmap = get_dev_pagemap(pte_pfn(pte), *pgmap); if (*pgmap) page = pte_page(pte); else goto no_page; } else if (unlikely(!page)) { if (flags & FOLL_DUMP) { page = ERR_PTR(-EFAULT); goto out; } if (is_zero_pfn(pte_pfn(pte))) { page = pte_page(pte); } else { ret = follow_pfn_pte(vma, address, ptep, flags); page = ERR_PTR(ret); goto out; } } if (!pte_write(pte) && gup_must_unshare(vma, flags, page)) { page = ERR_PTR(-EMLINK); goto out; } VM_BUG_ON_PAGE((flags & FOLL_PIN) && PageAnon(page) && !PageAnonExclusive(page), page); <S2SV_StartVul> ret = try_grab_page(page, flags); <S2SV_EndVul> if (unlikely(ret)) { page = ERR_PTR(ret); goto out; } if (flags & FOLL_PIN) { ret = arch_make_page_accessible(page); if (ret) { unpin_user_page(page); page = ERR_PTR(ret); goto out; } } if (flags & FOLL_TOUCH) { if ((flags & FOLL_WRITE) && !pte_dirty(pte) && !PageDirty(page)) set_page_dirty(page); mark_page_accessed(page); } out: pte_unmap_unlock(ptep, ptl); return page; no_page: pte_unmap_unlock(ptep, ptl); if (!pte_none(pte)) return NULL; return no_page_table(vma, flags); }","- ret = try_grab_page(page, flags);
+ ret = try_grab_folio(page_folio(page), 1, flags);","static struct page *follow_page_pte(struct vm_area_struct *vma, unsigned long address, pmd_t *pmd, unsigned int flags, struct dev_pagemap **pgmap) { struct mm_struct *mm = vma->vm_mm; struct page *page; spinlock_t *ptl; pte_t *ptep, pte; int ret; if (WARN_ON_ONCE((flags & (FOLL_PIN | FOLL_GET)) == (FOLL_PIN | FOLL_GET))) return ERR_PTR(-EINVAL); ptep = pte_offset_map_lock(mm, pmd, address, &ptl); if (!ptep) return no_page_table(vma, flags); pte = ptep_get(ptep); if (!pte_present(pte)) goto no_page; if (pte_protnone(pte) && !gup_can_follow_protnone(vma, flags)) goto no_page; page = vm_normal_page(vma, address, pte); if ((flags & FOLL_WRITE) && !can_follow_write_pte(pte, page, vma, flags)) { page = NULL; goto out; } if (!page && pte_devmap(pte) && (flags & (FOLL_GET | FOLL_PIN))) { *pgmap = get_dev_pagemap(pte_pfn(pte), *pgmap); if (*pgmap) page = pte_page(pte); else goto no_page; } else if (unlikely(!page)) { if (flags & FOLL_DUMP) { page = ERR_PTR(-EFAULT); goto out; } if (is_zero_pfn(pte_pfn(pte))) { page = pte_page(pte); } else { ret = follow_pfn_pte(vma, address, ptep, flags); page = ERR_PTR(ret); goto out; } } if (!pte_write(pte) && gup_must_unshare(vma, flags, page)) { page = ERR_PTR(-EMLINK); goto out; } VM_BUG_ON_PAGE((flags & FOLL_PIN) && PageAnon(page) && !PageAnonExclusive(page), page); ret = try_grab_folio(page_folio(page), 1, flags); if (unlikely(ret)) { page = ERR_PTR(ret); goto out; } if (flags & FOLL_PIN) { ret = arch_make_page_accessible(page); if (ret) { unpin_user_page(page); page = ERR_PTR(ret); goto out; } } if (flags & FOLL_TOUCH) { if ((flags & FOLL_WRITE) && !pte_dirty(pte) && !PageDirty(page)) set_page_dirty(page); mark_page_accessed(page); } out: pte_unmap_unlock(ptep, ptl); return page; no_page: pte_unmap_unlock(ptep, ptl); if (!pte_none(pte)) return NULL; return no_page_table(vma, flags); }"
1090----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53074/bad/mac80211.c----iwl_mvm_mac_remove_interface,"static void iwl_mvm_mac_remove_interface(struct ieee80211_hw *hw, struct ieee80211_vif *vif) { struct iwl_mvm *mvm = IWL_MAC80211_GET_MVM(hw); struct iwl_mvm_vif *mvmvif = iwl_mvm_vif_from_mac80211(vif); struct iwl_probe_resp_data *probe_data; iwl_mvm_prepare_mac_removal(mvm, vif); if (!(vif->type == NL80211_IFTYPE_AP || vif->type == NL80211_IFTYPE_ADHOC)) iwl_mvm_tcm_rm_vif(mvm, vif); mutex_lock(&mvm->mutex); if (vif == mvm->csme_vif) { iwl_mei_set_netdev(NULL); mvm->csme_vif = NULL; } probe_data = rcu_dereference_protected(mvmvif->deflink.probe_resp_data, lockdep_is_held(&mvm->mutex)); RCU_INIT_POINTER(mvmvif->deflink.probe_resp_data, NULL); if (probe_data) kfree_rcu(probe_data, rcu_head); if (mvm->bf_allowed_vif == mvmvif) { mvm->bf_allowed_vif = NULL; vif->driver_flags &= ~(IEEE80211_VIF_BEACON_FILTER | IEEE80211_VIF_SUPPORTS_CQM_RSSI); } if (vif->bss_conf.ftm_responder) memset(&mvm->ftm_resp_stats, 0, sizeof(mvm->ftm_resp_stats)); iwl_mvm_vif_dbgfs_rm_link(mvm, vif); if (vif->type == NL80211_IFTYPE_AP || vif->type == NL80211_IFTYPE_ADHOC) { #ifdef CONFIG_NL80211_TESTMODE if (vif == mvm->noa_vif) { mvm->noa_vif = NULL; mvm->noa_duration = 0; } #endif goto out; } iwl_mvm_power_update_mac(mvm); if (vif->type == NL80211_IFTYPE_P2P_DEVICE) { if (mvmvif->deflink.phy_ctxt) { iwl_mvm_phy_ctxt_unref(mvm, mvmvif->deflink.phy_ctxt); mvmvif->deflink.phy_ctxt = NULL; } mvm->p2p_device_vif = NULL; } <S2SV_StartVul> iwl_mvm_unset_link_mapping(mvm, vif, &vif->bss_conf); <S2SV_EndVul> iwl_mvm_mac_ctxt_remove(mvm, vif); RCU_INIT_POINTER(mvm->vif_id_to_mac[mvmvif->id], NULL); if (vif->type == NL80211_IFTYPE_MONITOR) mvm->monitor_on = false; out: if (vif->type == NL80211_IFTYPE_AP || vif->type == NL80211_IFTYPE_ADHOC) { iwl_mvm_dealloc_int_sta(mvm, &mvmvif->deflink.mcast_sta); iwl_mvm_dealloc_bcast_sta(mvm, vif); } mutex_unlock(&mvm->mutex); }","- iwl_mvm_unset_link_mapping(mvm, vif, &vif->bss_conf);
+ iwl_mvm_unset_link_mapping(mvm, vif, &vif->bss_conf);","static void iwl_mvm_mac_remove_interface(struct ieee80211_hw *hw, struct ieee80211_vif *vif) { struct iwl_mvm *mvm = IWL_MAC80211_GET_MVM(hw); struct iwl_mvm_vif *mvmvif = iwl_mvm_vif_from_mac80211(vif); struct iwl_probe_resp_data *probe_data; iwl_mvm_prepare_mac_removal(mvm, vif); if (!(vif->type == NL80211_IFTYPE_AP || vif->type == NL80211_IFTYPE_ADHOC)) iwl_mvm_tcm_rm_vif(mvm, vif); mutex_lock(&mvm->mutex); if (vif == mvm->csme_vif) { iwl_mei_set_netdev(NULL); mvm->csme_vif = NULL; } probe_data = rcu_dereference_protected(mvmvif->deflink.probe_resp_data, lockdep_is_held(&mvm->mutex)); RCU_INIT_POINTER(mvmvif->deflink.probe_resp_data, NULL); if (probe_data) kfree_rcu(probe_data, rcu_head); if (mvm->bf_allowed_vif == mvmvif) { mvm->bf_allowed_vif = NULL; vif->driver_flags &= ~(IEEE80211_VIF_BEACON_FILTER | IEEE80211_VIF_SUPPORTS_CQM_RSSI); } if (vif->bss_conf.ftm_responder) memset(&mvm->ftm_resp_stats, 0, sizeof(mvm->ftm_resp_stats)); iwl_mvm_vif_dbgfs_rm_link(mvm, vif); if (vif->type == NL80211_IFTYPE_AP || vif->type == NL80211_IFTYPE_ADHOC) { #ifdef CONFIG_NL80211_TESTMODE if (vif == mvm->noa_vif) { mvm->noa_vif = NULL; mvm->noa_duration = 0; } #endif goto out; } iwl_mvm_power_update_mac(mvm); if (vif->type == NL80211_IFTYPE_P2P_DEVICE) { if (mvmvif->deflink.phy_ctxt) { iwl_mvm_phy_ctxt_unref(mvm, mvmvif->deflink.phy_ctxt); mvmvif->deflink.phy_ctxt = NULL; } mvm->p2p_device_vif = NULL; } iwl_mvm_mac_ctxt_remove(mvm, vif); RCU_INIT_POINTER(mvm->vif_id_to_mac[mvmvif->id], NULL); if (vif->type == NL80211_IFTYPE_MONITOR) mvm->monitor_on = false; out: iwl_mvm_unset_link_mapping(mvm, vif, &vif->bss_conf); if (vif->type == NL80211_IFTYPE_AP || vif->type == NL80211_IFTYPE_ADHOC) { iwl_mvm_dealloc_int_sta(mvm, &mvmvif->deflink.mcast_sta); iwl_mvm_dealloc_bcast_sta(mvm, vif); } mutex_unlock(&mvm->mutex); }"
853----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50110/bad/xfrm_user.c----copy_to_user_auth,"static int copy_to_user_auth(struct xfrm_algo_auth *auth, struct sk_buff *skb) { struct xfrm_algo *algo; struct xfrm_algo_auth *ap; struct nlattr *nla; bool redact_secret = xfrm_redact(); nla = nla_reserve(skb, XFRMA_ALG_AUTH, sizeof(*algo) + (auth->alg_key_len + 7) / 8); if (!nla) return -EMSGSIZE; algo = nla_data(nla); strncpy(algo->alg_name, auth->alg_name, sizeof(algo->alg_name)); if (redact_secret && auth->alg_key_len) memset(algo->alg_key, 0, (auth->alg_key_len + 7) / 8); else memcpy(algo->alg_key, auth->alg_key, (auth->alg_key_len + 7) / 8); algo->alg_key_len = auth->alg_key_len; nla = nla_reserve(skb, XFRMA_ALG_AUTH_TRUNC, xfrm_alg_auth_len(auth)); if (!nla) return -EMSGSIZE; ap = nla_data(nla); <S2SV_StartVul> memcpy(ap, auth, sizeof(struct xfrm_algo_auth)); <S2SV_EndVul> if (redact_secret && auth->alg_key_len) memset(ap->alg_key, 0, (auth->alg_key_len + 7) / 8); else memcpy(ap->alg_key, auth->alg_key, (auth->alg_key_len + 7) / 8); return 0; }","- memcpy(ap, auth, sizeof(struct xfrm_algo_auth));
+ strscpy_pad(ap->alg_name, auth->alg_name, sizeof(ap->alg_name));
+ ap->alg_key_len = auth->alg_key_len;
+ ap->alg_trunc_len = auth->alg_trunc_len;","static int copy_to_user_auth(struct xfrm_algo_auth *auth, struct sk_buff *skb) { struct xfrm_algo *algo; struct xfrm_algo_auth *ap; struct nlattr *nla; bool redact_secret = xfrm_redact(); nla = nla_reserve(skb, XFRMA_ALG_AUTH, sizeof(*algo) + (auth->alg_key_len + 7) / 8); if (!nla) return -EMSGSIZE; algo = nla_data(nla); strncpy(algo->alg_name, auth->alg_name, sizeof(algo->alg_name)); if (redact_secret && auth->alg_key_len) memset(algo->alg_key, 0, (auth->alg_key_len + 7) / 8); else memcpy(algo->alg_key, auth->alg_key, (auth->alg_key_len + 7) / 8); algo->alg_key_len = auth->alg_key_len; nla = nla_reserve(skb, XFRMA_ALG_AUTH_TRUNC, xfrm_alg_auth_len(auth)); if (!nla) return -EMSGSIZE; ap = nla_data(nla); strscpy_pad(ap->alg_name, auth->alg_name, sizeof(ap->alg_name)); ap->alg_key_len = auth->alg_key_len; ap->alg_trunc_len = auth->alg_trunc_len; if (redact_secret && auth->alg_key_len) memset(ap->alg_key, 0, (auth->alg_key_len + 7) / 8); else memcpy(ap->alg_key, auth->alg_key, (auth->alg_key_len + 7) / 8); return 0; }"
1511----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2025-21667/bad/buffered-io.c----iomap_write_delalloc_scan,"static int iomap_write_delalloc_scan(struct inode *inode, loff_t *punch_start_byte, loff_t start_byte, loff_t end_byte, iomap_punch_t punch) { while (start_byte < end_byte) { struct folio *folio; int ret; folio = filemap_lock_folio(inode->i_mapping, start_byte >> PAGE_SHIFT); if (IS_ERR(folio)) { start_byte = ALIGN_DOWN(start_byte, PAGE_SIZE) + PAGE_SIZE; continue; } ret = iomap_write_delalloc_punch(inode, folio, punch_start_byte, start_byte, end_byte, punch); if (ret) { folio_unlock(folio); folio_put(folio); return ret; } <S2SV_StartVul> start_byte = folio_next_index(folio) << PAGE_SHIFT; <S2SV_EndVul> folio_unlock(folio); folio_put(folio); } return 0; }","- start_byte = folio_next_index(folio) << PAGE_SHIFT;
+ start_byte = folio_pos(folio) + folio_size(folio);","static int iomap_write_delalloc_scan(struct inode *inode, loff_t *punch_start_byte, loff_t start_byte, loff_t end_byte, iomap_punch_t punch) { while (start_byte < end_byte) { struct folio *folio; int ret; folio = filemap_lock_folio(inode->i_mapping, start_byte >> PAGE_SHIFT); if (IS_ERR(folio)) { start_byte = ALIGN_DOWN(start_byte, PAGE_SIZE) + PAGE_SIZE; continue; } ret = iomap_write_delalloc_punch(inode, folio, punch_start_byte, start_byte, end_byte, punch); if (ret) { folio_unlock(folio); folio_put(folio); return ret; } start_byte = folio_pos(folio) + folio_size(folio); folio_unlock(folio); folio_put(folio); } return 0; }"
642----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49944/bad/socket.c----sctp_listen_start,"static int sctp_listen_start(struct sock *sk, int backlog) { struct sctp_sock *sp = sctp_sk(sk); struct sctp_endpoint *ep = sp->ep; struct crypto_shash *tfm = NULL; char alg[32]; if (!sp->hmac && sp->sctp_hmac_alg) { sprintf(alg, ""hmac(%s)"", sp->sctp_hmac_alg); tfm = crypto_alloc_shash(alg, 0, 0); if (IS_ERR(tfm)) { net_info_ratelimited(""failed to load transform for %s: %ld\n"", sp->sctp_hmac_alg, PTR_ERR(tfm)); return -ENOSYS; } sctp_sk(sk)->hmac = tfm; } inet_sk_set_state(sk, SCTP_SS_LISTENING); if (!ep->base.bind_addr.port) { <S2SV_StartVul> if (sctp_autobind(sk)) <S2SV_EndVul> return -EAGAIN; } else { if (sctp_get_port(sk, inet_sk(sk)->inet_num)) { inet_sk_set_state(sk, SCTP_SS_CLOSED); return -EADDRINUSE; } } WRITE_ONCE(sk->sk_max_ack_backlog, backlog); return sctp_hash_endpoint(ep); }","- if (sctp_autobind(sk))
+ if (sctp_autobind(sk)) {
+ inet_sk_set_state(sk, SCTP_SS_CLOSED);
+ }
+ inet_sk_set_state(sk, SCTP_SS_CLOSED);","static int sctp_listen_start(struct sock *sk, int backlog) { struct sctp_sock *sp = sctp_sk(sk); struct sctp_endpoint *ep = sp->ep; struct crypto_shash *tfm = NULL; char alg[32]; if (!sp->hmac && sp->sctp_hmac_alg) { sprintf(alg, ""hmac(%s)"", sp->sctp_hmac_alg); tfm = crypto_alloc_shash(alg, 0, 0); if (IS_ERR(tfm)) { net_info_ratelimited(""failed to load transform for %s: %ld\n"", sp->sctp_hmac_alg, PTR_ERR(tfm)); return -ENOSYS; } sctp_sk(sk)->hmac = tfm; } inet_sk_set_state(sk, SCTP_SS_LISTENING); if (!ep->base.bind_addr.port) { if (sctp_autobind(sk)) { inet_sk_set_state(sk, SCTP_SS_CLOSED); return -EAGAIN; } } else { if (sctp_get_port(sk, inet_sk(sk)->inet_num)) { inet_sk_set_state(sk, SCTP_SS_CLOSED); return -EADDRINUSE; } } WRITE_ONCE(sk->sk_max_ack_backlog, backlog); return sctp_hash_endpoint(ep); }"
402----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46860/bad/main.c----mt7921_ipv6_addr_change,"static void mt7921_ipv6_addr_change(struct ieee80211_hw *hw, struct ieee80211_vif *vif, struct inet6_dev *idev) { struct mt792x_vif *mvif = (struct mt792x_vif *)vif->drv_priv; <S2SV_StartVul> struct mt792x_dev *dev = mvif->phy->dev; <S2SV_EndVul> struct inet6_ifaddr *ifa; struct in6_addr ns_addrs[IEEE80211_BSS_ARP_ADDR_LIST_LEN]; struct sk_buff *skb; u8 i, idx = 0; struct { struct { u8 bss_idx; u8 pad[3]; } __packed hdr; struct mt76_connac_arpns_tlv arpns; } req_hdr = { .hdr = { .bss_idx = mvif->mt76.idx, }, .arpns = { .tag = cpu_to_le16(UNI_OFFLOAD_OFFLOAD_ND), .mode = 2, .option = 1, }, }; read_lock_bh(&idev->lock); list_for_each_entry(ifa, &idev->addr_list, if_list) { if (ifa->flags & IFA_F_TENTATIVE) continue; ns_addrs[idx] = ifa->addr; if (++idx >= IEEE80211_BSS_ARP_ADDR_LIST_LEN) break; } read_unlock_bh(&idev->lock); if (!idx) return; req_hdr.arpns.ips_num = idx; req_hdr.arpns.len = cpu_to_le16(sizeof(struct mt76_connac_arpns_tlv) + idx * sizeof(struct in6_addr)); skb = __mt76_mcu_msg_alloc(&dev->mt76, &req_hdr, sizeof(req_hdr) + idx * sizeof(struct in6_addr), sizeof(req_hdr), GFP_ATOMIC); if (!skb) return; for (i = 0; i < idx; i++) skb_put_data(skb, &ns_addrs[i].in6_u, sizeof(struct in6_addr)); skb_queue_tail(&dev->ipv6_ns_list, skb); ieee80211_queue_work(dev->mt76.hw, &dev->ipv6_ns_work); }","- struct mt792x_dev *dev = mvif->phy->dev;
+ struct mt792x_dev *dev = mt792x_hw_dev(hw);","static void mt7921_ipv6_addr_change(struct ieee80211_hw *hw, struct ieee80211_vif *vif, struct inet6_dev *idev) { struct mt792x_vif *mvif = (struct mt792x_vif *)vif->drv_priv; struct mt792x_dev *dev = mt792x_hw_dev(hw); struct inet6_ifaddr *ifa; struct in6_addr ns_addrs[IEEE80211_BSS_ARP_ADDR_LIST_LEN]; struct sk_buff *skb; u8 i, idx = 0; struct { struct { u8 bss_idx; u8 pad[3]; } __packed hdr; struct mt76_connac_arpns_tlv arpns; } req_hdr = { .hdr = { .bss_idx = mvif->mt76.idx, }, .arpns = { .tag = cpu_to_le16(UNI_OFFLOAD_OFFLOAD_ND), .mode = 2, .option = 1, }, }; read_lock_bh(&idev->lock); list_for_each_entry(ifa, &idev->addr_list, if_list) { if (ifa->flags & IFA_F_TENTATIVE) continue; ns_addrs[idx] = ifa->addr; if (++idx >= IEEE80211_BSS_ARP_ADDR_LIST_LEN) break; } read_unlock_bh(&idev->lock); if (!idx) return; req_hdr.arpns.ips_num = idx; req_hdr.arpns.len = cpu_to_le16(sizeof(struct mt76_connac_arpns_tlv) + idx * sizeof(struct in6_addr)); skb = __mt76_mcu_msg_alloc(&dev->mt76, &req_hdr, sizeof(req_hdr) + idx * sizeof(struct in6_addr), sizeof(req_hdr), GFP_ATOMIC); if (!skb) return; for (i = 0; i < idx; i++) skb_put_data(skb, &ns_addrs[i].in6_u, sizeof(struct in6_addr)); skb_queue_tail(&dev->ipv6_ns_list, skb); ieee80211_queue_work(dev->mt76.hw, &dev->ipv6_ns_work); }"
1427----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56756/bad/pci.c----nvme_free_host_mem,"static void nvme_free_host_mem(struct nvme_dev *dev) { int i; for (i = 0; i < dev->nr_host_mem_descs; i++) { struct nvme_host_mem_buf_desc *desc = &dev->host_mem_descs[i]; size_t size = le32_to_cpu(desc->size) * NVME_CTRL_PAGE_SIZE; dma_free_attrs(dev->dev, size, dev->host_mem_desc_bufs[i], le64_to_cpu(desc->addr), DMA_ATTR_NO_KERNEL_MAPPING | DMA_ATTR_NO_WARN); } kfree(dev->host_mem_desc_bufs); dev->host_mem_desc_bufs = NULL; <S2SV_StartVul> dma_free_coherent(dev->dev, <S2SV_EndVul> <S2SV_StartVul> dev->nr_host_mem_descs * sizeof(*dev->host_mem_descs), <S2SV_EndVul> dev->host_mem_descs, dev->host_mem_descs_dma); dev->host_mem_descs = NULL; dev->nr_host_mem_descs = 0; }","- dma_free_coherent(dev->dev,
- dev->nr_host_mem_descs * sizeof(*dev->host_mem_descs),
+ dma_free_coherent(dev->dev, dev->host_mem_descs_size,
+ dev->host_mem_descs_size = 0;","static void nvme_free_host_mem(struct nvme_dev *dev) { int i; for (i = 0; i < dev->nr_host_mem_descs; i++) { struct nvme_host_mem_buf_desc *desc = &dev->host_mem_descs[i]; size_t size = le32_to_cpu(desc->size) * NVME_CTRL_PAGE_SIZE; dma_free_attrs(dev->dev, size, dev->host_mem_desc_bufs[i], le64_to_cpu(desc->addr), DMA_ATTR_NO_KERNEL_MAPPING | DMA_ATTR_NO_WARN); } kfree(dev->host_mem_desc_bufs); dev->host_mem_desc_bufs = NULL; dma_free_coherent(dev->dev, dev->host_mem_descs_size, dev->host_mem_descs, dev->host_mem_descs_dma); dev->host_mem_descs = NULL; dev->host_mem_descs_size = 0; dev->nr_host_mem_descs = 0; }"
137----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44996/bad/af_vsock.c----vsock_connectible_recvmsg,"vsock_connectible_recvmsg(struct socket *sock, struct msghdr *msg, size_t len, int flags) { struct sock *sk; struct vsock_sock *vsk; const struct vsock_transport *transport; #ifdef CONFIG_BPF_SYSCALL const struct proto *prot; #endif int err; sk = sock->sk; if (unlikely(flags & MSG_ERRQUEUE)) return sock_recv_errqueue(sk, msg, len, SOL_VSOCK, VSOCK_RECVERR); vsk = vsock_sk(sk); err = 0; lock_sock(sk); transport = vsk->transport; if (!transport || sk->sk_state != TCP_ESTABLISHED) { if (sock_flag(sk, SOCK_DONE)) err = 0; else err = -ENOTCONN; goto out; } if (flags & MSG_OOB) { err = -EOPNOTSUPP; goto out; } if (sk->sk_shutdown & RCV_SHUTDOWN) { err = 0; goto out; } if (!len) { err = 0; goto out; <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> #ifdef CONFIG_BPF_SYSCALL <S2SV_EndVul> <S2SV_StartVul> prot = READ_ONCE(sk->sk_prot); <S2SV_EndVul> <S2SV_StartVul> if (prot != &vsock_proto) { <S2SV_EndVul> <S2SV_StartVul> release_sock(sk); <S2SV_EndVul> <S2SV_StartVul> return prot->recvmsg(sk, msg, len, flags, NULL); <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> #endif <S2SV_EndVul> if (sk->sk_type == SOCK_STREAM) err = __vsock_stream_recvmsg(sk, msg, len, flags); else err = __vsock_seqpacket_recvmsg(sk, msg, len, flags); out: release_sock(sk); return err; <S2SV_StartVul> } <S2SV_EndVul>","- }
- #ifdef CONFIG_BPF_SYSCALL
- prot = READ_ONCE(sk->sk_prot);
- if (prot != &vsock_proto) {
- release_sock(sk);
- return prot->recvmsg(sk, msg, len, flags, NULL);
- }
- #endif
- }","vsock_connectible_recvmsg(struct socket *sock, struct msghdr *msg, size_t len, int flags) { #ifdef CONFIG_BPF_SYSCALL struct sock *sk = sock->sk; const struct proto *prot; prot = READ_ONCE(sk->sk_prot); if (prot != &vsock_proto) return prot->recvmsg(sk, msg, len, flags, NULL); #endif return __vsock_connectible_recvmsg(sock, msg, len, flags); }"
91----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44960/bad/core.c----usb_ep_enable,"int usb_ep_enable(struct usb_ep *ep) { int ret = 0; if (ep->enabled) goto out; <S2SV_StartVul> if (usb_endpoint_maxp(ep->desc) == 0) { <S2SV_EndVul> ret = -EINVAL; goto out; } ret = ep->ops->enable(ep, ep->desc); if (ret) goto out; ep->enabled = true; out: trace_usb_ep_enable(ep, ret); return ret; }","- if (usb_endpoint_maxp(ep->desc) == 0) {
+ if (!ep->desc || usb_endpoint_maxp(ep->desc) == 0) {
+ WARN_ONCE(1, ""%s: ep%d (%s) has %s\n"", __func__, ep->address, ep->name,
+ (!ep->desc) ? ""NULL descriptor"" : ""maxpacket 0"");","int usb_ep_enable(struct usb_ep *ep) { int ret = 0; if (ep->enabled) goto out; if (!ep->desc || usb_endpoint_maxp(ep->desc) == 0) { WARN_ONCE(1, ""%s: ep%d (%s) has %s\n"", __func__, ep->address, ep->name, (!ep->desc) ? ""NULL descriptor"" : ""maxpacket 0""); ret = -EINVAL; goto out; } ret = ep->ops->enable(ep, ep->desc); if (ret) goto out; ep->enabled = true; out: trace_usb_ep_enable(ep, ret); return ret; }"
390----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46851/bad/dcn10_hwseq.c----dcn10_set_drr,"void dcn10_set_drr(struct pipe_ctx **pipe_ctx, int num_pipes, struct dc_crtc_timing_adjust adjust) { int i = 0; struct drr_params params = {0}; unsigned int event_triggers = 0x800; unsigned int num_frames = 2; params.vertical_total_max = adjust.v_total_max; params.vertical_total_min = adjust.v_total_min; params.vertical_total_mid = adjust.v_total_mid; params.vertical_total_mid_frame_num = adjust.v_total_mid_frame_num; for (i = 0; i < num_pipes; i++) { <S2SV_StartVul> if ((pipe_ctx[i]->stream_res.tg != NULL) && pipe_ctx[i]->stream_res.tg->funcs) { <S2SV_EndVul> <S2SV_StartVul> if (pipe_ctx[i]->stream_res.tg->funcs->set_drr) <S2SV_EndVul> <S2SV_StartVul> pipe_ctx[i]->stream_res.tg->funcs->set_drr( <S2SV_EndVul> <S2SV_StartVul> pipe_ctx[i]->stream_res.tg, &params); <S2SV_EndVul> if (adjust.v_total_max != 0 && adjust.v_total_min != 0) <S2SV_StartVul> if (pipe_ctx[i]->stream_res.tg->funcs->set_static_screen_control) <S2SV_EndVul> <S2SV_StartVul> pipe_ctx[i]->stream_res.tg->funcs->set_static_screen_control( <S2SV_EndVul> <S2SV_StartVul> pipe_ctx[i]->stream_res.tg, <S2SV_EndVul> <S2SV_StartVul> event_triggers, num_frames); <S2SV_EndVul> } } }","- if ((pipe_ctx[i]->stream_res.tg != NULL) && pipe_ctx[i]->stream_res.tg->funcs) {
- if (pipe_ctx[i]->stream_res.tg->funcs->set_drr)
- pipe_ctx[i]->stream_res.tg->funcs->set_drr(
- pipe_ctx[i]->stream_res.tg, &params);
- if (pipe_ctx[i]->stream_res.tg->funcs->set_static_screen_control)
- pipe_ctx[i]->stream_res.tg->funcs->set_static_screen_control(
- pipe_ctx[i]->stream_res.tg,
- event_triggers, num_frames);
+ struct timing_generator *tg = pipe_ctx[i]->stream_res.tg;
+ if ((tg != NULL) && tg->funcs) {
+ if (tg->funcs->set_drr)
+ tg->funcs->set_drr(tg, &params);
+ if (tg->funcs->set_static_screen_control)
+ tg->funcs->set_static_screen_control(
+ tg, event_triggers, num_frames);","void dcn10_set_drr(struct pipe_ctx **pipe_ctx, int num_pipes, struct dc_crtc_timing_adjust adjust) { int i = 0; struct drr_params params = {0}; unsigned int event_triggers = 0x800; unsigned int num_frames = 2; params.vertical_total_max = adjust.v_total_max; params.vertical_total_min = adjust.v_total_min; params.vertical_total_mid = adjust.v_total_mid; params.vertical_total_mid_frame_num = adjust.v_total_mid_frame_num; for (i = 0; i < num_pipes; i++) { struct timing_generator *tg = pipe_ctx[i]->stream_res.tg; if ((tg != NULL) && tg->funcs) { if (tg->funcs->set_drr) tg->funcs->set_drr(tg, &params); if (adjust.v_total_max != 0 && adjust.v_total_min != 0) if (tg->funcs->set_static_screen_control) tg->funcs->set_static_screen_control( tg, event_triggers, num_frames); } } }"
541----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49862/bad/intel_rapl_common.c----*get_rpi,"static struct rapl_primitive_info *get_rpi(struct rapl_package *rp, int prim) { struct rapl_primitive_info *rpi = rp->priv->rpi; <S2SV_StartVul> if (prim < 0 || prim > NR_RAPL_PRIMITIVES || !rpi) <S2SV_EndVul> return NULL; return &rpi[prim]; }","- if (prim < 0 || prim > NR_RAPL_PRIMITIVES || !rpi)
+ if (prim < 0 || prim >= NR_RAPL_PRIMITIVES || !rpi)","static struct rapl_primitive_info *get_rpi(struct rapl_package *rp, int prim) { struct rapl_primitive_info *rpi = rp->priv->rpi; if (prim < 0 || prim >= NR_RAPL_PRIMITIVES || !rpi) return NULL; return &rpi[prim]; }"
3----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-43890/bad/tracing_map.c----*tracing_map_create,"struct tracing_map *tracing_map_create(unsigned int map_bits, unsigned int key_size, const struct tracing_map_ops *ops, void *private_data) { struct tracing_map *map; unsigned int i; if (map_bits < TRACING_MAP_BITS_MIN || map_bits > TRACING_MAP_BITS_MAX) return ERR_PTR(-EINVAL); map = kzalloc(sizeof(*map), GFP_KERNEL); if (!map) return ERR_PTR(-ENOMEM); map->map_bits = map_bits; map->max_elts = (1 << map_bits); <S2SV_StartVul> atomic_set(&map->next_elt, -1); <S2SV_EndVul> map->map_size = (1 << (map_bits + 1)); map->ops = ops; map->private_data = private_data; map->map = tracing_map_array_alloc(map->map_size, sizeof(struct tracing_map_entry)); if (!map->map) goto free; map->key_size = key_size; for (i = 0; i < TRACING_MAP_KEYS_MAX; i++) map->key_idx[i] = -1; out: return map; free: tracing_map_destroy(map); map = ERR_PTR(-ENOMEM); goto out; }","- atomic_set(&map->next_elt, -1);
+ atomic_set(&map->next_elt, 0);","struct tracing_map *tracing_map_create(unsigned int map_bits, unsigned int key_size, const struct tracing_map_ops *ops, void *private_data) { struct tracing_map *map; unsigned int i; if (map_bits < TRACING_MAP_BITS_MIN || map_bits > TRACING_MAP_BITS_MAX) return ERR_PTR(-EINVAL); map = kzalloc(sizeof(*map), GFP_KERNEL); if (!map) return ERR_PTR(-ENOMEM); map->map_bits = map_bits; map->max_elts = (1 << map_bits); atomic_set(&map->next_elt, 0); map->map_size = (1 << (map_bits + 1)); map->ops = ops; map->private_data = private_data; map->map = tracing_map_array_alloc(map->map_size, sizeof(struct tracing_map_entry)); if (!map->map) goto free; map->key_size = key_size; for (i = 0; i < TRACING_MAP_KEYS_MAX; i++) map->key_idx[i] = -1; out: return map; free: tracing_map_destroy(map); map = ERR_PTR(-ENOMEM); goto out; }"
379----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46840/bad/extent-tree.c----walk_up_proc,"static noinline int walk_up_proc(struct btrfs_trans_handle *trans, struct btrfs_root *root, struct btrfs_path *path, struct walk_control *wc) { struct btrfs_fs_info *fs_info = root->fs_info; int ret; int level = wc->level; struct extent_buffer *eb = path->nodes[level]; u64 parent = 0; if (wc->stage == UPDATE_BACKREF) { BUG_ON(wc->shared_level < level); if (level < wc->shared_level) goto out; ret = find_next_key(path, level + 1, &wc->update_progress); if (ret > 0) wc->update_ref = 0; wc->stage = DROP_REFERENCE; wc->shared_level = -1; path->slots[level] = 0; if (!path->locks[level]) { BUG_ON(level == 0); btrfs_tree_lock(eb); path->locks[level] = BTRFS_WRITE_LOCK; ret = btrfs_lookup_extent_info(trans, fs_info, eb->start, level, 1, &wc->refs[level], &wc->flags[level]); if (ret < 0) { btrfs_tree_unlock_rw(eb, path->locks[level]); path->locks[level] = 0; return ret; } <S2SV_StartVul> BUG_ON(wc->refs[level] == 0); <S2SV_EndVul> if (wc->refs[level] == 1) { btrfs_tree_unlock_rw(eb, path->locks[level]); path->locks[level] = 0; return 1; } } } BUG_ON(wc->refs[level] > 1 && !path->locks[level]); if (wc->refs[level] == 1) { if (level == 0) { if (wc->flags[level] & BTRFS_BLOCK_FLAG_FULL_BACKREF) ret = btrfs_dec_ref(trans, root, eb, 1); else ret = btrfs_dec_ref(trans, root, eb, 0); BUG_ON(ret); if (is_fstree(root->root_key.objectid)) { ret = btrfs_qgroup_trace_leaf_items(trans, eb); if (ret) { btrfs_err_rl(fs_info, ""error %d accounting leaf items, quota is out of sync, rescan required"", ret); } } } if (!path->locks[level] && btrfs_header_generation(eb) == trans->transid) { btrfs_tree_lock(eb); path->locks[level] = BTRFS_WRITE_LOCK; } btrfs_clean_tree_block(eb); } if (eb == root->node) { if (wc->flags[level] & BTRFS_BLOCK_FLAG_FULL_BACKREF) parent = eb->start; else if (root->root_key.objectid != btrfs_header_owner(eb)) goto owner_mismatch; } else { if (wc->flags[level + 1] & BTRFS_BLOCK_FLAG_FULL_BACKREF) parent = path->nodes[level + 1]->start; else if (root->root_key.objectid != btrfs_header_owner(path->nodes[level + 1])) goto owner_mismatch; } btrfs_free_tree_block(trans, btrfs_root_id(root), eb, parent, wc->refs[level] == 1); out: wc->refs[level] = 0; wc->flags[level] = 0; return 0; owner_mismatch: btrfs_err_rl(fs_info, ""unexpected tree owner, have %llu expect %llu"", btrfs_header_owner(eb), root->root_key.objectid); return -EUCLEAN; }","- BUG_ON(wc->refs[level] == 0);
+ }
+ if (unlikely(wc->refs[level] == 0)) {
+ btrfs_tree_unlock_rw(eb, path->locks[level]);
+ btrfs_err(fs_info, ""bytenr %llu has 0 references, expect > 0"",
+ eb->start);
+ return -EUCLEAN;
+ }
+ btrfs_tree_unlock_rw(eb, path->locks[level]);","static noinline int walk_up_proc(struct btrfs_trans_handle *trans, struct btrfs_root *root, struct btrfs_path *path, struct walk_control *wc) { struct btrfs_fs_info *fs_info = root->fs_info; int ret; int level = wc->level; struct extent_buffer *eb = path->nodes[level]; u64 parent = 0; if (wc->stage == UPDATE_BACKREF) { BUG_ON(wc->shared_level < level); if (level < wc->shared_level) goto out; ret = find_next_key(path, level + 1, &wc->update_progress); if (ret > 0) wc->update_ref = 0; wc->stage = DROP_REFERENCE; wc->shared_level = -1; path->slots[level] = 0; if (!path->locks[level]) { BUG_ON(level == 0); btrfs_tree_lock(eb); path->locks[level] = BTRFS_WRITE_LOCK; ret = btrfs_lookup_extent_info(trans, fs_info, eb->start, level, 1, &wc->refs[level], &wc->flags[level]); if (ret < 0) { btrfs_tree_unlock_rw(eb, path->locks[level]); path->locks[level] = 0; return ret; } if (unlikely(wc->refs[level] == 0)) { btrfs_tree_unlock_rw(eb, path->locks[level]); btrfs_err(fs_info, ""bytenr %llu has 0 references, expect > 0"", eb->start); return -EUCLEAN; } if (wc->refs[level] == 1) { btrfs_tree_unlock_rw(eb, path->locks[level]); path->locks[level] = 0; return 1; } } } BUG_ON(wc->refs[level] > 1 && !path->locks[level]); if (wc->refs[level] == 1) { if (level == 0) { if (wc->flags[level] & BTRFS_BLOCK_FLAG_FULL_BACKREF) ret = btrfs_dec_ref(trans, root, eb, 1); else ret = btrfs_dec_ref(trans, root, eb, 0); BUG_ON(ret); if (is_fstree(root->root_key.objectid)) { ret = btrfs_qgroup_trace_leaf_items(trans, eb); if (ret) { btrfs_err_rl(fs_info, ""error %d accounting leaf items, quota is out of sync, rescan required"", ret); } } } if (!path->locks[level] && btrfs_header_generation(eb) == trans->transid) { btrfs_tree_lock(eb); path->locks[level] = BTRFS_WRITE_LOCK; } btrfs_clean_tree_block(eb); } if (eb == root->node) { if (wc->flags[level] & BTRFS_BLOCK_FLAG_FULL_BACKREF) parent = eb->start; else if (root->root_key.objectid != btrfs_header_owner(eb)) goto owner_mismatch; } else { if (wc->flags[level + 1] & BTRFS_BLOCK_FLAG_FULL_BACKREF) parent = path->nodes[level + 1]->start; else if (root->root_key.objectid != btrfs_header_owner(path->nodes[level + 1])) goto owner_mismatch; } btrfs_free_tree_block(trans, btrfs_root_id(root), eb, parent, wc->refs[level] == 1); out: wc->refs[level] = 0; wc->flags[level] = 0; return 0; owner_mismatch: btrfs_err_rl(fs_info, ""unexpected tree owner, have %llu expect %llu"", btrfs_header_owner(eb), root->root_key.objectid); return -EUCLEAN; }"
687----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49982/bad/aoecmd.c----resend,"resend(struct aoedev *d, struct frame *f) { struct sk_buff *skb; struct sk_buff_head queue; struct aoe_hdr *h; struct aoetgt *t; char buf[128]; u32 n; t = f->t; n = newtag(d); skb = f->skb; if (ifrotate(t) == NULL) { pr_info(""aoe: resend: no interfaces to rotate to.\n""); ktcomplete(f, NULL); return; } h = (struct aoe_hdr *) skb_mac_header(skb); if (!(f->flags & FFL_PROBE)) { snprintf(buf, sizeof(buf), ""%15s e%ld.%d oldtag=%08x@%08lx newtag=%08x s=%pm d=%pm nout=%d\n"", ""retransmit"", d->aoemajor, d->aoeminor, f->tag, jiffies, n, h->src, h->dst, t->nout); aoechr_error(buf); } f->tag = n; fhash(f); h->tag = cpu_to_be32(n); memcpy(h->dst, t->addr, sizeof h->dst); memcpy(h->src, t->ifp->nd->dev_addr, sizeof h->src); skb->dev = t->ifp->nd; skb = skb_clone(skb, GFP_ATOMIC); <S2SV_StartVul> if (skb == NULL) <S2SV_EndVul> return; f->sent = ktime_get(); __skb_queue_head_init(&queue); __skb_queue_tail(&queue, skb); aoenet_xmit(&queue); }","- if (skb == NULL)
+ dev_hold(t->ifp->nd);
+ if (skb == NULL) {
+ dev_put(t->ifp->nd);
+ }","resend(struct aoedev *d, struct frame *f) { struct sk_buff *skb; struct sk_buff_head queue; struct aoe_hdr *h; struct aoetgt *t; char buf[128]; u32 n; t = f->t; n = newtag(d); skb = f->skb; if (ifrotate(t) == NULL) { pr_info(""aoe: resend: no interfaces to rotate to.\n""); ktcomplete(f, NULL); return; } h = (struct aoe_hdr *) skb_mac_header(skb); if (!(f->flags & FFL_PROBE)) { snprintf(buf, sizeof(buf), ""%15s e%ld.%d oldtag=%08x@%08lx newtag=%08x s=%pm d=%pm nout=%d\n"", ""retransmit"", d->aoemajor, d->aoeminor, f->tag, jiffies, n, h->src, h->dst, t->nout); aoechr_error(buf); } f->tag = n; fhash(f); h->tag = cpu_to_be32(n); memcpy(h->dst, t->addr, sizeof h->dst); memcpy(h->src, t->ifp->nd->dev_addr, sizeof h->src); dev_hold(t->ifp->nd); skb->dev = t->ifp->nd; skb = skb_clone(skb, GFP_ATOMIC); if (skb == NULL) { dev_put(t->ifp->nd); return; } f->sent = ktime_get(); __skb_queue_head_init(&queue); __skb_queue_tail(&queue, skb); aoenet_xmit(&queue); }"
412----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47141/bad/pinmux.c----*pin_free,"static const char *pin_free(struct pinctrl_dev *pctldev, int pin, struct pinctrl_gpio_range *gpio_range) { const struct pinmux_ops *ops = pctldev->desc->pmxops; struct pin_desc *desc; const char *owner; desc = pin_desc_get(pctldev, pin); if (desc == NULL) { dev_err(pctldev->dev, ""pin is not registered so it cannot be freed\n""); return NULL; <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> if (!gpio_range) { <S2SV_EndVul> <S2SV_StartVul> if (WARN_ON(!desc->mux_usecount)) <S2SV_EndVul> <S2SV_StartVul> return NULL; <S2SV_EndVul> <S2SV_StartVul> desc->mux_usecount--; <S2SV_EndVul> <S2SV_StartVul> if (desc->mux_usecount) <S2SV_EndVul> <S2SV_StartVul> return NULL; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> if (gpio_range && ops->gpio_disable_free) ops->gpio_disable_free(pctldev, gpio_range, pin); else if (ops->free) ops->free(pctldev, pin); <S2SV_StartVul> if (gpio_range) { <S2SV_EndVul> <S2SV_StartVul> owner = desc->gpio_owner; <S2SV_EndVul> <S2SV_StartVul> desc->gpio_owner = NULL; <S2SV_EndVul> <S2SV_StartVul> } else { <S2SV_EndVul> <S2SV_StartVul> owner = desc->mux_owner; <S2SV_EndVul> <S2SV_StartVul> desc->mux_owner = NULL; <S2SV_EndVul> <S2SV_StartVul> desc->mux_setting = NULL; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> module_put(pctldev->owner); return owner; }","- }
- if (!gpio_range) {
- if (WARN_ON(!desc->mux_usecount))
- return NULL;
- desc->mux_usecount--;
- if (desc->mux_usecount)
- return NULL;
- }
- if (gpio_range) {
- owner = desc->gpio_owner;
- desc->gpio_owner = NULL;
- } else {
- owner = desc->mux_owner;
- desc->mux_owner = NULL;
- desc->mux_setting = NULL;
- }
+ }
+ scoped_guard(mutex, &desc->mux_lock) {
+ if (!gpio_range) {
+ if (WARN_ON(!desc->mux_usecount))
+ return NULL;
+ desc->mux_usecount--;
+ if (desc->mux_usecount)
+ return NULL;
+ }
+ }
+ scoped_guard(mutex, &desc->mux_lock) {
+ if (gpio_range) {
+ owner = desc->gpio_owner;
+ desc->gpio_owner = NULL;
+ } else {
+ owner = desc->mux_owner;
+ desc->mux_owner = NULL;
+ desc->mux_setting = NULL;
+ }
+ }","static const char *pin_free(struct pinctrl_dev *pctldev, int pin, struct pinctrl_gpio_range *gpio_range) { const struct pinmux_ops *ops = pctldev->desc->pmxops; struct pin_desc *desc; const char *owner; desc = pin_desc_get(pctldev, pin); if (desc == NULL) { dev_err(pctldev->dev, ""pin is not registered so it cannot be freed\n""); return NULL; } scoped_guard(mutex, &desc->mux_lock) { if (!gpio_range) { if (WARN_ON(!desc->mux_usecount)) return NULL; desc->mux_usecount--; if (desc->mux_usecount) return NULL; } } if (gpio_range && ops->gpio_disable_free) ops->gpio_disable_free(pctldev, gpio_range, pin); else if (ops->free) ops->free(pctldev, pin); scoped_guard(mutex, &desc->mux_lock) { if (gpio_range) { owner = desc->gpio_owner; desc->gpio_owner = NULL; } else { owner = desc->mux_owner; desc->mux_owner = NULL; desc->mux_setting = NULL; } } module_put(pctldev->owner); return owner; }"
940----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50188/bad/dp83869.c----dp83869_configure_fiber,"static int dp83869_configure_fiber(struct phy_device *phydev, struct dp83869_private *dp83869) { int bmcr; int ret; linkmode_and(phydev->advertising, phydev->advertising, phydev->supported); linkmode_set_bit(ETHTOOL_LINK_MODE_FIBRE_BIT, phydev->supported); <S2SV_StartVul> linkmode_set_bit(ADVERTISED_FIBRE, phydev->advertising); <S2SV_EndVul> if (dp83869->mode == DP83869_RGMII_1000_BASE) { linkmode_set_bit(ETHTOOL_LINK_MODE_1000baseX_Full_BIT, phydev->supported); } else { linkmode_set_bit(ETHTOOL_LINK_MODE_100baseFX_Full_BIT, phydev->supported); linkmode_set_bit(ETHTOOL_LINK_MODE_100baseFX_Half_BIT, phydev->supported); bmcr = phy_read(phydev, MII_BMCR); if (bmcr < 0) return bmcr; phydev->autoneg = AUTONEG_DISABLE; linkmode_clear_bit(ETHTOOL_LINK_MODE_Autoneg_BIT, phydev->supported); linkmode_clear_bit(ETHTOOL_LINK_MODE_Autoneg_BIT, phydev->advertising); if (bmcr & BMCR_ANENABLE) { ret = phy_modify(phydev, MII_BMCR, BMCR_ANENABLE, 0); if (ret < 0) return ret; } } linkmode_or(phydev->advertising, phydev->advertising, phydev->supported); return 0; }","- linkmode_set_bit(ADVERTISED_FIBRE, phydev->advertising);","static int dp83869_configure_fiber(struct phy_device *phydev, struct dp83869_private *dp83869) { int bmcr; int ret; linkmode_and(phydev->advertising, phydev->advertising, phydev->supported); linkmode_set_bit(ETHTOOL_LINK_MODE_FIBRE_BIT, phydev->supported); if (dp83869->mode == DP83869_RGMII_1000_BASE) { linkmode_set_bit(ETHTOOL_LINK_MODE_1000baseX_Full_BIT, phydev->supported); } else { linkmode_set_bit(ETHTOOL_LINK_MODE_100baseFX_Full_BIT, phydev->supported); linkmode_set_bit(ETHTOOL_LINK_MODE_100baseFX_Half_BIT, phydev->supported); bmcr = phy_read(phydev, MII_BMCR); if (bmcr < 0) return bmcr; phydev->autoneg = AUTONEG_DISABLE; linkmode_clear_bit(ETHTOOL_LINK_MODE_Autoneg_BIT, phydev->supported); linkmode_clear_bit(ETHTOOL_LINK_MODE_Autoneg_BIT, phydev->advertising); if (bmcr & BMCR_ANENABLE) { ret = phy_modify(phydev, MII_BMCR, BMCR_ANENABLE, 0); if (ret < 0) return ret; } } linkmode_or(phydev->advertising, phydev->advertising, phydev->supported); return 0; }"
455----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47698/bad/rtl2832.c----rtl2832_pid_filter,"static int rtl2832_pid_filter(struct dvb_frontend *fe, u8 index, u16 pid, int onoff) { struct rtl2832_dev *dev = fe->demodulator_priv; struct i2c_client *client = dev->client; int ret; u8 buf[4]; dev_dbg(&client->dev, ""index=%d pid=%04x onoff=%d slave_ts=%d\n"", index, pid, onoff, dev->slave_ts); if (pid > 0x1fff || index > 32) return 0; if (onoff) set_bit(index, &dev->filters); else clear_bit(index, &dev->filters); buf[0] = (dev->filters >> 0) & 0xff; buf[1] = (dev->filters >> 8) & 0xff; buf[2] = (dev->filters >> 16) & 0xff; buf[3] = (dev->filters >> 24) & 0xff; if (dev->slave_ts) ret = regmap_bulk_write(dev->regmap, 0x022, buf, 4); else ret = regmap_bulk_write(dev->regmap, 0x062, buf, 4); if (ret) goto err; buf[0] = (pid >> 8) & 0xff; buf[1] = (pid >> 0) & 0xff; if (dev->slave_ts) ret = regmap_bulk_write(dev->regmap, 0x026 + 2 * index, buf, 2); else <S2SV_StartVul> ret = regmap_bulk_write(dev->regmap, 0x066 + 2 * index, buf, 2); <S2SV_EndVul> if (ret) goto err; return 0; err: dev_dbg(&client->dev, ""failed=%d\n"", ret); return ret; }","- ret = regmap_bulk_write(dev->regmap, 0x066 + 2 * index, buf, 2);
+ ret = regmap_bulk_write(dev->regmap, 0x066 + 2 * index, buf, 2);","static int rtl2832_pid_filter(struct dvb_frontend *fe, u8 index, u16 pid, int onoff) { struct rtl2832_dev *dev = fe->demodulator_priv; struct i2c_client *client = dev->client; int ret; u8 buf[4]; dev_dbg(&client->dev, ""index=%d pid=%04x onoff=%d slave_ts=%d\n"", index, pid, onoff, dev->slave_ts); if (pid > 0x1fff || index >= 32) return 0; if (onoff) set_bit(index, &dev->filters); else clear_bit(index, &dev->filters); buf[0] = (dev->filters >> 0) & 0xff; buf[1] = (dev->filters >> 8) & 0xff; buf[2] = (dev->filters >> 16) & 0xff; buf[3] = (dev->filters >> 24) & 0xff; if (dev->slave_ts) ret = regmap_bulk_write(dev->regmap, 0x022, buf, 4); else ret = regmap_bulk_write(dev->regmap, 0x062, buf, 4); if (ret) goto err; buf[0] = (pid >> 8) & 0xff; buf[1] = (pid >> 0) & 0xff; if (dev->slave_ts) ret = regmap_bulk_write(dev->regmap, 0x026 + 2 * index, buf, 2); else ret = regmap_bulk_write(dev->regmap, 0x066 + 2 * index, buf, 2); if (ret) goto err; return 0; err: dev_dbg(&client->dev, ""failed=%d\n"", ret); return ret; }"
1474----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-57890/bad/uverbs_cmd.c----ib_uverbs_unmarshall_recv,"ib_uverbs_unmarshall_recv(struct uverbs_req_iter *iter, u32 wr_count, u32 wqe_size, u32 sge_count) { struct ib_uverbs_recv_wr *user_wr; struct ib_recv_wr *wr = NULL, *last, *next; int sg_ind; int i; int ret; const struct ib_sge __user *sgls; const void __user *wqes; if (wqe_size < sizeof(struct ib_uverbs_recv_wr)) return ERR_PTR(-EINVAL); <S2SV_StartVul> wqes = uverbs_request_next_ptr(iter, wqe_size * wr_count); <S2SV_EndVul> if (IS_ERR(wqes)) return ERR_CAST(wqes); <S2SV_StartVul> sgls = uverbs_request_next_ptr( <S2SV_EndVul> <S2SV_StartVul> iter, sge_count * sizeof(struct ib_uverbs_sge)); <S2SV_EndVul> if (IS_ERR(sgls)) return ERR_CAST(sgls); ret = uverbs_request_finish(iter); if (ret) return ERR_PTR(ret); user_wr = kmalloc(wqe_size, GFP_KERNEL); if (!user_wr) return ERR_PTR(-ENOMEM); sg_ind = 0; last = NULL; for (i = 0; i < wr_count; ++i) { if (copy_from_user(user_wr, wqes + i * wqe_size, wqe_size)) { ret = -EFAULT; goto err; } if (user_wr->num_sge + sg_ind > sge_count) { ret = -EINVAL; goto err; } if (user_wr->num_sge >= (U32_MAX - ALIGN(sizeof(*next), sizeof(struct ib_sge))) / sizeof(struct ib_sge)) { ret = -EINVAL; goto err; } next = kmalloc(ALIGN(sizeof(*next), sizeof(struct ib_sge)) + user_wr->num_sge * sizeof(struct ib_sge), GFP_KERNEL); if (!next) { ret = -ENOMEM; goto err; } if (!last) wr = next; else last->next = next; last = next; next->next = NULL; next->wr_id = user_wr->wr_id; next->num_sge = user_wr->num_sge; if (next->num_sge) { next->sg_list = (void *)next + ALIGN(sizeof(*next), sizeof(struct ib_sge)); if (copy_from_user(next->sg_list, sgls + sg_ind, next->num_sge * sizeof(struct ib_sge))) { ret = -EFAULT; goto err; } sg_ind += next->num_sge; } else next->sg_list = NULL; } kfree(user_wr); return wr; err: kfree(user_wr); while (wr) { next = wr->next; kfree(wr); wr = next; } return ERR_PTR(ret); }","- wqes = uverbs_request_next_ptr(iter, wqe_size * wr_count);
- sgls = uverbs_request_next_ptr(
- iter, sge_count * sizeof(struct ib_uverbs_sge));
+ wqes = uverbs_request_next_ptr(iter, size_mul(wqe_size, wr_count));
+ sgls = uverbs_request_next_ptr(iter, size_mul(sge_count,
+ sizeof(struct ib_uverbs_sge)));","ib_uverbs_unmarshall_recv(struct uverbs_req_iter *iter, u32 wr_count, u32 wqe_size, u32 sge_count) { struct ib_uverbs_recv_wr *user_wr; struct ib_recv_wr *wr = NULL, *last, *next; int sg_ind; int i; int ret; const struct ib_sge __user *sgls; const void __user *wqes; if (wqe_size < sizeof(struct ib_uverbs_recv_wr)) return ERR_PTR(-EINVAL); wqes = uverbs_request_next_ptr(iter, size_mul(wqe_size, wr_count)); if (IS_ERR(wqes)) return ERR_CAST(wqes); sgls = uverbs_request_next_ptr(iter, size_mul(sge_count, sizeof(struct ib_uverbs_sge))); if (IS_ERR(sgls)) return ERR_CAST(sgls); ret = uverbs_request_finish(iter); if (ret) return ERR_PTR(ret); user_wr = kmalloc(wqe_size, GFP_KERNEL); if (!user_wr) return ERR_PTR(-ENOMEM); sg_ind = 0; last = NULL; for (i = 0; i < wr_count; ++i) { if (copy_from_user(user_wr, wqes + i * wqe_size, wqe_size)) { ret = -EFAULT; goto err; } if (user_wr->num_sge + sg_ind > sge_count) { ret = -EINVAL; goto err; } if (user_wr->num_sge >= (U32_MAX - ALIGN(sizeof(*next), sizeof(struct ib_sge))) / sizeof(struct ib_sge)) { ret = -EINVAL; goto err; } next = kmalloc(ALIGN(sizeof(*next), sizeof(struct ib_sge)) + user_wr->num_sge * sizeof(struct ib_sge), GFP_KERNEL); if (!next) { ret = -ENOMEM; goto err; } if (!last) wr = next; else last->next = next; last = next; next->next = NULL; next->wr_id = user_wr->wr_id; next->num_sge = user_wr->num_sge; if (next->num_sge) { next->sg_list = (void *)next + ALIGN(sizeof(*next), sizeof(struct ib_sge)); if (copy_from_user(next->sg_list, sgls + sg_ind, next->num_sge * sizeof(struct ib_sge))) { ret = -EFAULT; goto err; } sg_ind += next->num_sge; } else next->sg_list = NULL; } kfree(user_wr); return wr; err: kfree(user_wr); while (wr) { next = wr->next; kfree(wr); wr = next; } return ERR_PTR(ret); }"
543----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49864/bad/io_thread.c----rxrpc_io_thread,"int rxrpc_io_thread(void *data) { struct rxrpc_connection *conn; struct sk_buff_head rx_queue; struct rxrpc_local *local = data; struct rxrpc_call *call; struct sk_buff *skb; #ifdef CONFIG_AF_RXRPC_INJECT_RX_DELAY ktime_t now; #endif bool should_stop; complete(&local->io_thread_ready); skb_queue_head_init(&rx_queue); set_user_nice(current, MIN_NICE); for (;;) { rxrpc_inc_stat(local->rxnet, stat_io_loop); conn = list_first_entry_or_null(&local->conn_attend_q, struct rxrpc_connection, attend_link); if (conn) { spin_lock_bh(&local->lock); list_del_init(&conn->attend_link); spin_unlock_bh(&local->lock); rxrpc_input_conn_event(conn, NULL); rxrpc_put_connection(conn, rxrpc_conn_put_poke); continue; } if (test_and_clear_bit(RXRPC_CLIENT_CONN_REAP_TIMER, &local->client_conn_flags)) rxrpc_discard_expired_client_conns(local); if ((call = list_first_entry_or_null(&local->call_attend_q, struct rxrpc_call, attend_link))) { spin_lock_bh(&local->lock); list_del_init(&call->attend_link); spin_unlock_bh(&local->lock); trace_rxrpc_call_poked(call); rxrpc_input_call_event(call, NULL); rxrpc_put_call(call, rxrpc_call_put_poke); continue; } if (!list_empty(&local->new_client_calls)) rxrpc_connect_client_calls(local); if ((skb = __skb_dequeue(&rx_queue))) { struct rxrpc_skb_priv *sp = rxrpc_skb(skb); switch (skb->mark) { case RXRPC_SKB_MARK_PACKET: skb->priority = 0; if (!rxrpc_input_packet(local, &skb)) rxrpc_reject_packet(local, skb); trace_rxrpc_rx_done(skb->mark, skb->priority); rxrpc_free_skb(skb, rxrpc_skb_put_input); break; case RXRPC_SKB_MARK_ERROR: rxrpc_input_error(local, skb); rxrpc_free_skb(skb, rxrpc_skb_put_error_report); break; case RXRPC_SKB_MARK_SERVICE_CONN_SECURED: rxrpc_input_conn_event(sp->conn, skb); rxrpc_put_connection(sp->conn, rxrpc_conn_put_poke); rxrpc_free_skb(skb, rxrpc_skb_put_conn_secured); break; default: WARN_ON_ONCE(1); rxrpc_free_skb(skb, rxrpc_skb_put_unknown); break; } continue; } #ifdef CONFIG_AF_RXRPC_INJECT_RX_DELAY now = ktime_get_real(); while ((skb = skb_peek(&local->rx_delay_queue))) { if (ktime_before(now, skb->tstamp)) break; skb = skb_dequeue(&local->rx_delay_queue); skb_queue_tail(&local->rx_queue, skb); } #endif if (!skb_queue_empty(&local->rx_queue)) { spin_lock_irq(&local->rx_queue.lock); skb_queue_splice_tail_init(&local->rx_queue, &rx_queue); spin_unlock_irq(&local->rx_queue.lock); continue; } set_current_state(TASK_INTERRUPTIBLE); should_stop = kthread_should_stop(); if (!skb_queue_empty(&local->rx_queue) || !list_empty(&local->call_attend_q) || !list_empty(&local->conn_attend_q) || !list_empty(&local->new_client_calls) || test_bit(RXRPC_CLIENT_CONN_REAP_TIMER, &local->client_conn_flags)) { __set_current_state(TASK_RUNNING); continue; } if (should_stop) break; #ifdef CONFIG_AF_RXRPC_INJECT_RX_DELAY skb = skb_peek(&local->rx_delay_queue); if (skb) { unsigned long timeout; ktime_t tstamp = skb->tstamp; ktime_t now = ktime_get_real(); s64 delay_ns = ktime_to_ns(ktime_sub(tstamp, now)); if (delay_ns <= 0) { __set_current_state(TASK_RUNNING); continue; } timeout = nsecs_to_jiffies(delay_ns); timeout = max(timeout, 1UL); schedule_timeout(timeout); __set_current_state(TASK_RUNNING); continue; } #endif schedule(); } __set_current_state(TASK_RUNNING); rxrpc_see_local(local, rxrpc_local_stop); rxrpc_destroy_local(local); <S2SV_StartVul> local->io_thread = NULL; <S2SV_EndVul> rxrpc_see_local(local, rxrpc_local_stopped); return 0; }","- local->io_thread = NULL;
+ WRITE_ONCE(local->io_thread, NULL);
+ return 0;
+ }","int rxrpc_io_thread(void *data) { struct rxrpc_connection *conn; struct sk_buff_head rx_queue; struct rxrpc_local *local = data; struct rxrpc_call *call; struct sk_buff *skb; #ifdef CONFIG_AF_RXRPC_INJECT_RX_DELAY ktime_t now; #endif bool should_stop; complete(&local->io_thread_ready); skb_queue_head_init(&rx_queue); set_user_nice(current, MIN_NICE); for (;;) { rxrpc_inc_stat(local->rxnet, stat_io_loop); conn = list_first_entry_or_null(&local->conn_attend_q, struct rxrpc_connection, attend_link); if (conn) { spin_lock_bh(&local->lock); list_del_init(&conn->attend_link); spin_unlock_bh(&local->lock); rxrpc_input_conn_event(conn, NULL); rxrpc_put_connection(conn, rxrpc_conn_put_poke); continue; } if (test_and_clear_bit(RXRPC_CLIENT_CONN_REAP_TIMER, &local->client_conn_flags)) rxrpc_discard_expired_client_conns(local); if ((call = list_first_entry_or_null(&local->call_attend_q, struct rxrpc_call, attend_link))) { spin_lock_bh(&local->lock); list_del_init(&call->attend_link); spin_unlock_bh(&local->lock); trace_rxrpc_call_poked(call); rxrpc_input_call_event(call, NULL); rxrpc_put_call(call, rxrpc_call_put_poke); continue; } if (!list_empty(&local->new_client_calls)) rxrpc_connect_client_calls(local); if ((skb = __skb_dequeue(&rx_queue))) { struct rxrpc_skb_priv *sp = rxrpc_skb(skb); switch (skb->mark) { case RXRPC_SKB_MARK_PACKET: skb->priority = 0; if (!rxrpc_input_packet(local, &skb)) rxrpc_reject_packet(local, skb); trace_rxrpc_rx_done(skb->mark, skb->priority); rxrpc_free_skb(skb, rxrpc_skb_put_input); break; case RXRPC_SKB_MARK_ERROR: rxrpc_input_error(local, skb); rxrpc_free_skb(skb, rxrpc_skb_put_error_report); break; case RXRPC_SKB_MARK_SERVICE_CONN_SECURED: rxrpc_input_conn_event(sp->conn, skb); rxrpc_put_connection(sp->conn, rxrpc_conn_put_poke); rxrpc_free_skb(skb, rxrpc_skb_put_conn_secured); break; default: WARN_ON_ONCE(1); rxrpc_free_skb(skb, rxrpc_skb_put_unknown); break; } continue; } #ifdef CONFIG_AF_RXRPC_INJECT_RX_DELAY now = ktime_get_real(); while ((skb = skb_peek(&local->rx_delay_queue))) { if (ktime_before(now, skb->tstamp)) break; skb = skb_dequeue(&local->rx_delay_queue); skb_queue_tail(&local->rx_queue, skb); } #endif if (!skb_queue_empty(&local->rx_queue)) { spin_lock_irq(&local->rx_queue.lock); skb_queue_splice_tail_init(&local->rx_queue, &rx_queue); spin_unlock_irq(&local->rx_queue.lock); continue; } set_current_state(TASK_INTERRUPTIBLE); should_stop = kthread_should_stop(); if (!skb_queue_empty(&local->rx_queue) || !list_empty(&local->call_attend_q) || !list_empty(&local->conn_attend_q) || !list_empty(&local->new_client_calls) || test_bit(RXRPC_CLIENT_CONN_REAP_TIMER, &local->client_conn_flags)) { __set_current_state(TASK_RUNNING); continue; } if (should_stop) break; #ifdef CONFIG_AF_RXRPC_INJECT_RX_DELAY skb = skb_peek(&local->rx_delay_queue); if (skb) { unsigned long timeout; ktime_t tstamp = skb->tstamp; ktime_t now = ktime_get_real(); s64 delay_ns = ktime_to_ns(ktime_sub(tstamp, now)); if (delay_ns <= 0) { __set_current_state(TASK_RUNNING); continue; } timeout = nsecs_to_jiffies(delay_ns); timeout = max(timeout, 1UL); schedule_timeout(timeout); __set_current_state(TASK_RUNNING); continue; } #endif schedule(); } __set_current_state(TASK_RUNNING); rxrpc_see_local(local, rxrpc_local_stop); rxrpc_destroy_local(local); WRITE_ONCE(local->io_thread, NULL); rxrpc_see_local(local, rxrpc_local_stopped); return 0; }"
1313----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56615/bad/devmap.c----dev_map_hash_delete_elem,"static int dev_map_hash_delete_elem(struct bpf_map *map, void *key) { struct bpf_dtab *dtab = container_of(map, struct bpf_dtab, map); struct bpf_dtab_netdev *old_dev; <S2SV_StartVul> int k = *(u32 *)key; <S2SV_EndVul> unsigned long flags; int ret = -ENOENT; spin_lock_irqsave(&dtab->index_lock, flags); old_dev = __dev_map_hash_lookup_elem(map, k); if (old_dev) { dtab->items--; hlist_del_init_rcu(&old_dev->index_hlist); call_rcu(&old_dev->rcu, __dev_map_entry_free); ret = 0; } spin_unlock_irqrestore(&dtab->index_lock, flags); return ret; }","- int k = *(u32 *)key;
+ u32 k = *(u32 *)key;","static int dev_map_hash_delete_elem(struct bpf_map *map, void *key) { struct bpf_dtab *dtab = container_of(map, struct bpf_dtab, map); struct bpf_dtab_netdev *old_dev; u32 k = *(u32 *)key; unsigned long flags; int ret = -ENOENT; spin_lock_irqsave(&dtab->index_lock, flags); old_dev = __dev_map_hash_lookup_elem(map, k); if (old_dev) { dtab->items--; hlist_del_init_rcu(&old_dev->index_hlist); call_rcu(&old_dev->rcu, __dev_map_entry_free); ret = 0; } spin_unlock_irqrestore(&dtab->index_lock, flags); return ret; }"
493----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47734/bad/bond_main.c----bond_xdp_get_xmit_slave,"bond_xdp_get_xmit_slave(struct net_device *bond_dev, struct xdp_buff *xdp) { struct bonding *bond = netdev_priv(bond_dev); struct slave *slave; switch (BOND_MODE(bond)) { case BOND_MODE_ROUNDROBIN: slave = bond_xdp_xmit_roundrobin_slave_get(bond, xdp); break; case BOND_MODE_ACTIVEBACKUP: slave = bond_xmit_activebackup_slave_get(bond); break; case BOND_MODE_8023AD: case BOND_MODE_XOR: slave = bond_xdp_xmit_3ad_xor_slave_get(bond, xdp); break; default: <S2SV_StartVul> netdev_err(bond_dev, ""Unknown bonding mode %d for xdp xmit\n"", BOND_MODE(bond)); <S2SV_EndVul> <S2SV_StartVul> WARN_ON_ONCE(1); <S2SV_EndVul> return NULL; } if (slave) return slave->dev; return NULL; }","- netdev_err(bond_dev, ""Unknown bonding mode %d for xdp xmit\n"", BOND_MODE(bond));
- WARN_ON_ONCE(1);
+ if (net_ratelimit())
+ netdev_err(bond_dev, ""Unknown bonding mode %d for xdp xmit\n"",
+ BOND_MODE(bond));","bond_xdp_get_xmit_slave(struct net_device *bond_dev, struct xdp_buff *xdp) { struct bonding *bond = netdev_priv(bond_dev); struct slave *slave; switch (BOND_MODE(bond)) { case BOND_MODE_ROUNDROBIN: slave = bond_xdp_xmit_roundrobin_slave_get(bond, xdp); break; case BOND_MODE_ACTIVEBACKUP: slave = bond_xmit_activebackup_slave_get(bond); break; case BOND_MODE_8023AD: case BOND_MODE_XOR: slave = bond_xdp_xmit_3ad_xor_slave_get(bond, xdp); break; default: if (net_ratelimit()) netdev_err(bond_dev, ""Unknown bonding mode %d for xdp xmit\n"", BOND_MODE(bond)); return NULL; } if (slave) return slave->dev; return NULL; }"
770----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50047/bad/smb2ops.c----decrypt_raw_data,"decrypt_raw_data(struct TCP_Server_Info *server, char *buf, unsigned int buf_data_size, struct iov_iter *iter, bool is_offloaded) { <S2SV_StartVul> struct kvec iov[2]; <S2SV_EndVul> struct smb_rqst rqst = {NULL}; size_t iter_size = 0; int rc; iov[0].iov_base = buf; iov[0].iov_len = sizeof(struct smb2_transform_hdr); iov[1].iov_base = buf + sizeof(struct smb2_transform_hdr); iov[1].iov_len = buf_data_size; rqst.rq_iov = iov; rqst.rq_nvec = 2; if (iter) { rqst.rq_iter = *iter; iter_size = iov_iter_count(iter); <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> rc = crypt_message(server, 1, &rqst, 0); <S2SV_EndVul> cifs_dbg(FYI, ""Decrypt message returned %d\n"", rc); if (rc) <S2SV_StartVul> return rc; <S2SV_EndVul> memmove(buf, iov[1].iov_base, buf_data_size); if (!is_offloaded) server->total_read = buf_data_size + iter_size; return rc; }","- struct kvec iov[2];
- }
- rc = crypt_message(server, 1, &rqst, 0);
- return rc;
+ struct crypto_aead *tfm;
+ struct kvec iov[2];
+ }
+ if (is_offloaded) {
+ if ((server->cipher_type == SMB2_ENCRYPTION_AES128_GCM) ||
+ (server->cipher_type == SMB2_ENCRYPTION_AES256_GCM))
+ tfm = crypto_alloc_aead(""gcm(aes)"", 0, 0);
+ else
+ tfm = crypto_alloc_aead(""ccm(aes)"", 0, 0);
+ if (IS_ERR(tfm)) {
+ rc = PTR_ERR(tfm);
+ cifs_server_dbg(VFS, ""%s: Failed alloc decrypt TFM, rc=%d\n"", __func__, rc);
+ return rc;
+ }
+ } else {
+ if (unlikely(!server->secmech.dec))
+ return -EIO;
+ tfm = server->secmech.dec;
+ }
+ rc = crypt_message(server, 1, &rqst, 0, tfm);
+ if (is_offloaded)
+ crypto_free_aead(tfm);
+ return rc;","decrypt_raw_data(struct TCP_Server_Info *server, char *buf, unsigned int buf_data_size, struct iov_iter *iter, bool is_offloaded) { struct crypto_aead *tfm; struct smb_rqst rqst = {NULL}; struct kvec iov[2]; size_t iter_size = 0; int rc; iov[0].iov_base = buf; iov[0].iov_len = sizeof(struct smb2_transform_hdr); iov[1].iov_base = buf + sizeof(struct smb2_transform_hdr); iov[1].iov_len = buf_data_size; rqst.rq_iov = iov; rqst.rq_nvec = 2; if (iter) { rqst.rq_iter = *iter; iter_size = iov_iter_count(iter); } if (is_offloaded) { if ((server->cipher_type == SMB2_ENCRYPTION_AES128_GCM) || (server->cipher_type == SMB2_ENCRYPTION_AES256_GCM)) tfm = crypto_alloc_aead(""gcm(aes)"", 0, 0); else tfm = crypto_alloc_aead(""ccm(aes)"", 0, 0); if (IS_ERR(tfm)) { rc = PTR_ERR(tfm); cifs_server_dbg(VFS, ""%s: Failed alloc decrypt TFM, rc=%d\n"", __func__, rc); return rc; } } else { if (unlikely(!server->secmech.dec)) return -EIO; tfm = server->secmech.dec; } rc = crypt_message(server, 1, &rqst, 0, tfm); cifs_dbg(FYI, ""Decrypt message returned %d\n"", rc); if (is_offloaded) crypto_free_aead(tfm); if (rc) return rc; memmove(buf, iov[1].iov_base, buf_data_size); if (!is_offloaded) server->total_read = buf_data_size + iter_size; return rc; }"
1444----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56770/bad/sch_netem.c----*netem_dequeue,"static struct sk_buff *netem_dequeue(struct Qdisc *sch) { struct netem_sched_data *q = qdisc_priv(sch); struct sk_buff *skb; tfifo_dequeue: skb = __qdisc_dequeue_head(&sch->q); if (skb) { <S2SV_StartVul> qdisc_qstats_backlog_dec(sch, skb); <S2SV_EndVul> deliver: qdisc_bstats_update(sch, skb); return skb; } skb = netem_peek(q); if (skb) { u64 time_to_send; u64 now = ktime_get_ns(); time_to_send = netem_skb_cb(skb)->time_to_send; if (q->slot.slot_next && q->slot.slot_next < time_to_send) get_slot_next(q, now); if (time_to_send <= now && q->slot.slot_next <= now) { netem_erase_head(q, skb); <S2SV_StartVul> sch->q.qlen--; <S2SV_EndVul> <S2SV_StartVul> qdisc_qstats_backlog_dec(sch, skb); <S2SV_EndVul> skb->next = NULL; skb->prev = NULL; skb->dev = qdisc_dev(sch); if (q->slot.slot_next) { q->slot.packets_left--; q->slot.bytes_left -= qdisc_pkt_len(skb); if (q->slot.packets_left <= 0 || q->slot.bytes_left <= 0) get_slot_next(q, now); } if (q->qdisc) { unsigned int pkt_len = qdisc_pkt_len(skb); struct sk_buff *to_free = NULL; int err; err = qdisc_enqueue(skb, q->qdisc, &to_free); kfree_skb_list(to_free); if (err != NET_XMIT_SUCCESS) { if (net_xmit_drop_count(err)) qdisc_qstats_drop(sch); qdisc_tree_reduce_backlog(sch, 1, pkt_len); } goto tfifo_dequeue; } goto deliver; } if (q->qdisc) { skb = q->qdisc->ops->dequeue(q->qdisc); <S2SV_StartVul> if (skb) <S2SV_EndVul> goto deliver; } qdisc_watchdog_schedule_ns(&q->watchdog, max(time_to_send, q->slot.slot_next)); } if (q->qdisc) { skb = q->qdisc->ops->dequeue(q->qdisc); <S2SV_StartVul> if (skb) <S2SV_EndVul> goto deliver; } return NULL; }","- qdisc_qstats_backlog_dec(sch, skb);
- sch->q.qlen--;
- qdisc_qstats_backlog_dec(sch, skb);
- if (skb)
- if (skb)
+ if (skb) {
+ qdisc_qstats_backlog_dec(sch, skb);
+ }
+ q->t_len--;
+ sch->qstats.backlog -= pkt_len;
+ sch->q.qlen--;
+ }
+ }
+ sch->q.qlen--;
+ }
+ if (skb) {
+ sch->q.qlen--;
+ }
+ }
+ if (skb) {
+ sch->q.qlen--;
+ }
+ }
+ }","static struct sk_buff *netem_dequeue(struct Qdisc *sch) { struct netem_sched_data *q = qdisc_priv(sch); struct sk_buff *skb; tfifo_dequeue: skb = __qdisc_dequeue_head(&sch->q); if (skb) { deliver: qdisc_qstats_backlog_dec(sch, skb); qdisc_bstats_update(sch, skb); return skb; } skb = netem_peek(q); if (skb) { u64 time_to_send; u64 now = ktime_get_ns(); time_to_send = netem_skb_cb(skb)->time_to_send; if (q->slot.slot_next && q->slot.slot_next < time_to_send) get_slot_next(q, now); if (time_to_send <= now && q->slot.slot_next <= now) { netem_erase_head(q, skb); q->t_len--; skb->next = NULL; skb->prev = NULL; skb->dev = qdisc_dev(sch); if (q->slot.slot_next) { q->slot.packets_left--; q->slot.bytes_left -= qdisc_pkt_len(skb); if (q->slot.packets_left <= 0 || q->slot.bytes_left <= 0) get_slot_next(q, now); } if (q->qdisc) { unsigned int pkt_len = qdisc_pkt_len(skb); struct sk_buff *to_free = NULL; int err; err = qdisc_enqueue(skb, q->qdisc, &to_free); kfree_skb_list(to_free); if (err != NET_XMIT_SUCCESS) { if (net_xmit_drop_count(err)) qdisc_qstats_drop(sch); qdisc_tree_reduce_backlog(sch, 1, pkt_len); sch->qstats.backlog -= pkt_len; sch->q.qlen--; } goto tfifo_dequeue; } sch->q.qlen--; goto deliver; } if (q->qdisc) { skb = q->qdisc->ops->dequeue(q->qdisc); if (skb) { sch->q.qlen--; goto deliver; } } qdisc_watchdog_schedule_ns(&q->watchdog, max(time_to_send, q->slot.slot_next)); } if (q->qdisc) { skb = q->qdisc->ops->dequeue(q->qdisc); if (skb) { sch->q.qlen--; goto deliver; } } return NULL; }"
502----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47739/bad/padata.c----padata_do_serial,"void padata_do_serial(struct padata_priv *padata) { struct parallel_data *pd = padata->pd; int hashed_cpu = padata_cpu_hash(pd, padata->seq_nr); struct padata_list *reorder = per_cpu_ptr(pd->reorder_list, hashed_cpu); struct padata_priv *cur; struct list_head *pos; spin_lock(&reorder->lock); list_for_each_prev(pos, &reorder->list) { cur = list_entry(pos, struct padata_priv, list); <S2SV_StartVul> if (cur->seq_nr < padata->seq_nr) <S2SV_EndVul> break; } list_add(&padata->list, pos); spin_unlock(&reorder->lock); smp_mb(); padata_reorder(pd); }","- if (cur->seq_nr < padata->seq_nr)
+ if ((signed int)(cur->seq_nr - padata->seq_nr) < 0)","void padata_do_serial(struct padata_priv *padata) { struct parallel_data *pd = padata->pd; int hashed_cpu = padata_cpu_hash(pd, padata->seq_nr); struct padata_list *reorder = per_cpu_ptr(pd->reorder_list, hashed_cpu); struct padata_priv *cur; struct list_head *pos; spin_lock(&reorder->lock); list_for_each_prev(pos, &reorder->list) { cur = list_entry(pos, struct padata_priv, list); if ((signed int)(cur->seq_nr - padata->seq_nr) < 0) break; } list_add(&padata->list, pos); spin_unlock(&reorder->lock); smp_mb(); padata_reorder(pd); }"
856----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50115/bad/nested.c----nested_svm_get_tdp_pdptr,"static u64 nested_svm_get_tdp_pdptr(struct kvm_vcpu *vcpu, int index) { struct vcpu_svm *svm = to_svm(vcpu); u64 cr3 = svm->nested.ctl.nested_cr3; u64 pdpte; int ret; ret = kvm_vcpu_read_guest_page(vcpu, gpa_to_gfn(cr3), &pdpte, <S2SV_StartVul> offset_in_page(cr3) + index * 8, 8); <S2SV_EndVul> if (ret) return 0; return pdpte; }","- offset_in_page(cr3) + index * 8, 8);
+ (cr3 & GENMASK(11, 5)) + index * 8, 8);","static u64 nested_svm_get_tdp_pdptr(struct kvm_vcpu *vcpu, int index) { struct vcpu_svm *svm = to_svm(vcpu); u64 cr3 = svm->nested.ctl.nested_cr3; u64 pdpte; int ret; ret = kvm_vcpu_read_guest_page(vcpu, gpa_to_gfn(cr3), &pdpte, (cr3 & GENMASK(11, 5)) + index * 8, 8); if (ret) return 0; return pdpte; }"
377----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46840/bad/extent-tree.c----reada_walk_down,"static noinline void reada_walk_down(struct btrfs_trans_handle *trans, struct btrfs_root *root, struct walk_control *wc, struct btrfs_path *path) { struct btrfs_fs_info *fs_info = root->fs_info; u64 bytenr; u64 generation; u64 refs; u64 flags; u32 nritems; struct btrfs_key key; struct extent_buffer *eb; int ret; int slot; int nread = 0; if (path->slots[wc->level] < wc->reada_slot) { wc->reada_count = wc->reada_count * 2 / 3; wc->reada_count = max(wc->reada_count, 2); } else { wc->reada_count = wc->reada_count * 3 / 2; wc->reada_count = min_t(int, wc->reada_count, BTRFS_NODEPTRS_PER_BLOCK(fs_info)); } eb = path->nodes[wc->level]; nritems = btrfs_header_nritems(eb); for (slot = path->slots[wc->level]; slot < nritems; slot++) { if (nread >= wc->reada_count) break; cond_resched(); bytenr = btrfs_node_blockptr(eb, slot); generation = btrfs_node_ptr_generation(eb, slot); if (slot == path->slots[wc->level]) goto reada; if (wc->stage == UPDATE_BACKREF && generation <= root->root_key.offset) continue; ret = btrfs_lookup_extent_info(trans, fs_info, bytenr, wc->level - 1, 1, &refs, &flags); if (ret < 0) continue; <S2SV_StartVul> BUG_ON(refs == 0); <S2SV_EndVul> if (wc->stage == DROP_REFERENCE) { if (refs == 1) goto reada; if (wc->level == 1 && (flags & BTRFS_BLOCK_FLAG_FULL_BACKREF)) continue; if (!wc->update_ref || generation <= root->root_key.offset) continue; btrfs_node_key_to_cpu(eb, &key, slot); ret = btrfs_comp_cpu_keys(&key, &wc->update_progress); if (ret < 0) continue; } else { if (wc->level == 1 && (flags & BTRFS_BLOCK_FLAG_FULL_BACKREF)) continue; } reada: btrfs_readahead_node_child(eb, slot); nread++; } wc->reada_slot = slot; }","- BUG_ON(refs == 0);
+ continue;
+ if (refs == 0)
+ continue;","static noinline void reada_walk_down(struct btrfs_trans_handle *trans, struct btrfs_root *root, struct walk_control *wc, struct btrfs_path *path) { struct btrfs_fs_info *fs_info = root->fs_info; u64 bytenr; u64 generation; u64 refs; u64 flags; u32 nritems; struct btrfs_key key; struct extent_buffer *eb; int ret; int slot; int nread = 0; if (path->slots[wc->level] < wc->reada_slot) { wc->reada_count = wc->reada_count * 2 / 3; wc->reada_count = max(wc->reada_count, 2); } else { wc->reada_count = wc->reada_count * 3 / 2; wc->reada_count = min_t(int, wc->reada_count, BTRFS_NODEPTRS_PER_BLOCK(fs_info)); } eb = path->nodes[wc->level]; nritems = btrfs_header_nritems(eb); for (slot = path->slots[wc->level]; slot < nritems; slot++) { if (nread >= wc->reada_count) break; cond_resched(); bytenr = btrfs_node_blockptr(eb, slot); generation = btrfs_node_ptr_generation(eb, slot); if (slot == path->slots[wc->level]) goto reada; if (wc->stage == UPDATE_BACKREF && generation <= root->root_key.offset) continue; ret = btrfs_lookup_extent_info(trans, fs_info, bytenr, wc->level - 1, 1, &refs, &flags); if (ret < 0) continue; if (refs == 0) continue; if (wc->stage == DROP_REFERENCE) { if (refs == 1) goto reada; if (wc->level == 1 && (flags & BTRFS_BLOCK_FLAG_FULL_BACKREF)) continue; if (!wc->update_ref || generation <= root->root_key.offset) continue; btrfs_node_key_to_cpu(eb, &key, slot); ret = btrfs_comp_cpu_keys(&key, &wc->update_progress); if (ret < 0) continue; } else { if (wc->level == 1 && (flags & BTRFS_BLOCK_FLAG_FULL_BACKREF)) continue; } reada: btrfs_readahead_node_child(eb, slot); nread++; } wc->reada_slot = slot; }"
657----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49958/bad/refcounttree.c----__ocfs2_reflink,"static int __ocfs2_reflink(struct dentry *old_dentry, struct buffer_head *old_bh, struct inode *new_inode, bool preserve) { int ret; struct inode *inode = d_inode(old_dentry); struct buffer_head *new_bh = NULL; <S2SV_StartVul> if (OCFS2_I(inode)->ip_flags & OCFS2_INODE_SYSTEM_FILE) { <S2SV_EndVul> ret = -EINVAL; mlog_errno(ret); goto out; } ret = filemap_fdatawrite(inode->i_mapping); if (ret) { mlog_errno(ret); goto out; } ret = ocfs2_attach_refcount_tree(inode, old_bh); if (ret) { mlog_errno(ret); goto out; } inode_lock_nested(new_inode, I_MUTEX_CHILD); ret = ocfs2_inode_lock_nested(new_inode, &new_bh, 1, OI_LS_REFLINK_TARGET); if (ret) { mlog_errno(ret); goto out_unlock; } ret = ocfs2_create_reflink_node(inode, old_bh, new_inode, new_bh, preserve); if (ret) { mlog_errno(ret); goto inode_unlock; } <S2SV_StartVul> if (OCFS2_I(inode)->ip_dyn_features & OCFS2_HAS_XATTR_FL) { <S2SV_EndVul> ret = ocfs2_reflink_xattrs(inode, old_bh, new_inode, new_bh, preserve); if (ret) { mlog_errno(ret); goto inode_unlock; } } ret = ocfs2_complete_reflink(inode, old_bh, new_inode, new_bh, preserve); if (ret) mlog_errno(ret); inode_unlock: ocfs2_inode_unlock(new_inode, 1); brelse(new_bh); out_unlock: inode_unlock(new_inode); out: if (!ret) { ret = filemap_fdatawait(inode->i_mapping); if (ret) mlog_errno(ret); } return ret; }","- if (OCFS2_I(inode)->ip_flags & OCFS2_INODE_SYSTEM_FILE) {
- if (OCFS2_I(inode)->ip_dyn_features & OCFS2_HAS_XATTR_FL) {
+ struct ocfs2_inode_info *oi = OCFS2_I(inode);
+ if (oi->ip_flags & OCFS2_INODE_SYSTEM_FILE) {
+ }
+ if ((oi->ip_dyn_features & OCFS2_HAS_XATTR_FL) &&
+ (oi->ip_dyn_features & OCFS2_INLINE_XATTR_FL)) {
+ struct ocfs2_inode_info *new_oi = OCFS2_I(new_inode);
+ if (!(new_oi->ip_dyn_features & OCFS2_INLINE_DATA_FL) &&
+ !(ocfs2_inode_is_fast_symlink(new_inode))) {
+ struct ocfs2_dinode *new_di = (struct ocfs2_dinode *)new_bh->b_data;
+ struct ocfs2_dinode *old_di = (struct ocfs2_dinode *)old_bh->b_data;
+ struct ocfs2_extent_list *el = &new_di->id2.i_list;
+ int inline_size = le16_to_cpu(old_di->i_xattr_inline_size);
+ le16_add_cpu(&el->l_count, -(inline_size /
+ sizeof(struct ocfs2_extent_rec)));
+ }
+ }
+ }
+ if (oi->ip_dyn_features & OCFS2_HAS_XATTR_FL) {","static int __ocfs2_reflink(struct dentry *old_dentry, struct buffer_head *old_bh, struct inode *new_inode, bool preserve) { int ret; struct inode *inode = d_inode(old_dentry); struct buffer_head *new_bh = NULL; struct ocfs2_inode_info *oi = OCFS2_I(inode); if (oi->ip_flags & OCFS2_INODE_SYSTEM_FILE) { ret = -EINVAL; mlog_errno(ret); goto out; } ret = filemap_fdatawrite(inode->i_mapping); if (ret) { mlog_errno(ret); goto out; } ret = ocfs2_attach_refcount_tree(inode, old_bh); if (ret) { mlog_errno(ret); goto out; } inode_lock_nested(new_inode, I_MUTEX_CHILD); ret = ocfs2_inode_lock_nested(new_inode, &new_bh, 1, OI_LS_REFLINK_TARGET); if (ret) { mlog_errno(ret); goto out_unlock; } if ((oi->ip_dyn_features & OCFS2_HAS_XATTR_FL) && (oi->ip_dyn_features & OCFS2_INLINE_XATTR_FL)) { struct ocfs2_inode_info *new_oi = OCFS2_I(new_inode); if (!(new_oi->ip_dyn_features & OCFS2_INLINE_DATA_FL) && !(ocfs2_inode_is_fast_symlink(new_inode))) { struct ocfs2_dinode *new_di = (struct ocfs2_dinode *)new_bh->b_data; struct ocfs2_dinode *old_di = (struct ocfs2_dinode *)old_bh->b_data; struct ocfs2_extent_list *el = &new_di->id2.i_list; int inline_size = le16_to_cpu(old_di->i_xattr_inline_size); le16_add_cpu(&el->l_count, -(inline_size / sizeof(struct ocfs2_extent_rec))); } } ret = ocfs2_create_reflink_node(inode, old_bh, new_inode, new_bh, preserve); if (ret) { mlog_errno(ret); goto inode_unlock; } if (oi->ip_dyn_features & OCFS2_HAS_XATTR_FL) { ret = ocfs2_reflink_xattrs(inode, old_bh, new_inode, new_bh, preserve); if (ret) { mlog_errno(ret); goto inode_unlock; } } ret = ocfs2_complete_reflink(inode, old_bh, new_inode, new_bh, preserve); if (ret) mlog_errno(ret); inode_unlock: ocfs2_inode_unlock(new_inode, 1); brelse(new_bh); out_unlock: inode_unlock(new_inode); out: if (!ret) { ret = filemap_fdatawait(inode->i_mapping); if (ret) mlog_errno(ret); } return ret; }"
518----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47753/bad/vdec_vp8_req_if.c----vdec_vp8_slice_decode,"static int vdec_vp8_slice_decode(void *h_vdec, struct mtk_vcodec_mem *bs, struct vdec_fb *fb, bool *res_chg) { struct vdec_vp8_slice_inst *inst = h_vdec; struct vdec_vpu_inst *vpu = &inst->vpu; struct mtk_video_dec_buf *src_buf_info, *dst_buf_info; unsigned int data; u64 y_fb_dma, c_fb_dma; int err, timeout; *res_chg = false; if (!bs) return vpu_dec_reset(vpu); src_buf_info = container_of(bs, struct mtk_video_dec_buf, bs_buffer); fb = inst->ctx->dev->vdec_pdata->get_cap_buffer(inst->ctx); <S2SV_StartVul> dst_buf_info = container_of(fb, struct mtk_video_dec_buf, frame_buffer); <S2SV_EndVul> <S2SV_StartVul> y_fb_dma = fb ? (u64)fb->base_y.dma_addr : 0; <S2SV_EndVul> if (inst->ctx->q_data[MTK_Q_DATA_DST].fmt->num_planes == 1) c_fb_dma = y_fb_dma + inst->ctx->picinfo.buf_w * inst->ctx->picinfo.buf_h; else <S2SV_StartVul> c_fb_dma = fb ? (u64)fb->base_c.dma_addr : 0; <S2SV_EndVul> inst->vsi->dec.bs_dma = (u64)bs->dma_addr; inst->vsi->dec.bs_sz = bs->size; inst->vsi->dec.cur_y_fb_dma = y_fb_dma; inst->vsi->dec.cur_c_fb_dma = c_fb_dma; mtk_vdec_debug(inst->ctx, ""frame[%d] bs(%zu 0x%llx) y/c(0x%llx 0x%llx)"", inst->ctx->decoded_frame_cnt, bs->size, (u64)bs->dma_addr, y_fb_dma, c_fb_dma); v4l2_m2m_buf_copy_metadata(&src_buf_info->m2m_buf.vb, &dst_buf_info->m2m_buf.vb, true); err = vdec_vp8_slice_get_decode_parameters(inst); if (err) goto error; err = vpu_dec_start(vpu, &data, 1); if (err) { mtk_vdec_debug(inst->ctx, ""vp8 dec start err!""); goto error; } if (inst->vsi->dec.resolution_changed) { mtk_vdec_debug(inst->ctx, ""- resolution_changed -""); *res_chg = true; return 0; } timeout = mtk_vcodec_wait_for_done_ctx(inst->ctx, MTK_INST_IRQ_RECEIVED, 50, MTK_VDEC_CORE); err = vpu_dec_end(vpu); if (err || timeout) mtk_vdec_debug(inst->ctx, ""vp8 dec error timeout:%d err: %d pic_%d"", timeout, err, inst->ctx->decoded_frame_cnt); mtk_vdec_debug(inst->ctx, ""pic[%d] crc: 0x%x 0x%x 0x%x 0x%x 0x%x 0x%x 0x%x 0x%x"", inst->ctx->decoded_frame_cnt, inst->vsi->dec.crc[0], inst->vsi->dec.crc[1], inst->vsi->dec.crc[2], inst->vsi->dec.crc[3], inst->vsi->dec.crc[4], inst->vsi->dec.crc[5], inst->vsi->dec.crc[6], inst->vsi->dec.crc[7]); inst->ctx->decoded_frame_cnt++; error: return err; }","- dst_buf_info = container_of(fb, struct mtk_video_dec_buf, frame_buffer);
- y_fb_dma = fb ? (u64)fb->base_y.dma_addr : 0;
- c_fb_dma = fb ? (u64)fb->base_c.dma_addr : 0;
+ if (!fb) {
+ mtk_vdec_err(inst->ctx, ""fb buffer is NULL"");
+ return -ENOMEM;
+ }
+ dst_buf_info = container_of(fb, struct mtk_video_dec_buf, frame_buffer);
+ y_fb_dma = fb->base_y.dma_addr;
+ c_fb_dma = fb->base_c.dma_addr;","static int vdec_vp8_slice_decode(void *h_vdec, struct mtk_vcodec_mem *bs, struct vdec_fb *fb, bool *res_chg) { struct vdec_vp8_slice_inst *inst = h_vdec; struct vdec_vpu_inst *vpu = &inst->vpu; struct mtk_video_dec_buf *src_buf_info, *dst_buf_info; unsigned int data; u64 y_fb_dma, c_fb_dma; int err, timeout; *res_chg = false; if (!bs) return vpu_dec_reset(vpu); src_buf_info = container_of(bs, struct mtk_video_dec_buf, bs_buffer); fb = inst->ctx->dev->vdec_pdata->get_cap_buffer(inst->ctx); if (!fb) { mtk_vdec_err(inst->ctx, ""fb buffer is NULL""); return -ENOMEM; } dst_buf_info = container_of(fb, struct mtk_video_dec_buf, frame_buffer); y_fb_dma = fb->base_y.dma_addr; if (inst->ctx->q_data[MTK_Q_DATA_DST].fmt->num_planes == 1) c_fb_dma = y_fb_dma + inst->ctx->picinfo.buf_w * inst->ctx->picinfo.buf_h; else c_fb_dma = fb->base_c.dma_addr; inst->vsi->dec.bs_dma = (u64)bs->dma_addr; inst->vsi->dec.bs_sz = bs->size; inst->vsi->dec.cur_y_fb_dma = y_fb_dma; inst->vsi->dec.cur_c_fb_dma = c_fb_dma; mtk_vdec_debug(inst->ctx, ""frame[%d] bs(%zu 0x%llx) y/c(0x%llx 0x%llx)"", inst->ctx->decoded_frame_cnt, bs->size, (u64)bs->dma_addr, y_fb_dma, c_fb_dma); v4l2_m2m_buf_copy_metadata(&src_buf_info->m2m_buf.vb, &dst_buf_info->m2m_buf.vb, true); err = vdec_vp8_slice_get_decode_parameters(inst); if (err) goto error; err = vpu_dec_start(vpu, &data, 1); if (err) { mtk_vdec_debug(inst->ctx, ""vp8 dec start err!""); goto error; } if (inst->vsi->dec.resolution_changed) { mtk_vdec_debug(inst->ctx, ""- resolution_changed -""); *res_chg = true; return 0; } timeout = mtk_vcodec_wait_for_done_ctx(inst->ctx, MTK_INST_IRQ_RECEIVED, 50, MTK_VDEC_CORE); err = vpu_dec_end(vpu); if (err || timeout) mtk_vdec_debug(inst->ctx, ""vp8 dec error timeout:%d err: %d pic_%d"", timeout, err, inst->ctx->decoded_frame_cnt); mtk_vdec_debug(inst->ctx, ""pic[%d] crc: 0x%x 0x%x 0x%x 0x%x 0x%x 0x%x 0x%x 0x%x"", inst->ctx->decoded_frame_cnt, inst->vsi->dec.crc[0], inst->vsi->dec.crc[1], inst->vsi->dec.crc[2], inst->vsi->dec.crc[3], inst->vsi->dec.crc[4], inst->vsi->dec.crc[5], inst->vsi->dec.crc[6], inst->vsi->dec.crc[7]); inst->ctx->decoded_frame_cnt++; error: return err; }"
1075----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53059/bad/fw.c----iwl_mvm_send_recovery_cmd,"void iwl_mvm_send_recovery_cmd(struct iwl_mvm *mvm, u32 flags) { u32 error_log_size = mvm->fw->ucode_capa.error_log_size; int ret; <S2SV_StartVul> u32 resp; <S2SV_EndVul> struct iwl_fw_error_recovery_cmd recovery_cmd = { .flags = cpu_to_le32(flags), .buf_size = 0, }; struct iwl_host_cmd host_cmd = { .id = WIDE_ID(SYSTEM_GROUP, FW_ERROR_RECOVERY_CMD), <S2SV_StartVul> .flags = CMD_WANT_SKB, <S2SV_EndVul> .data = {&recovery_cmd, }, .len = {sizeof(recovery_cmd), }, }; if (!error_log_size) return; if (flags & ERROR_RECOVERY_UPDATE_DB) { if (!mvm->error_recovery_buf) return; host_cmd.data[1] = mvm->error_recovery_buf; host_cmd.len[1] = error_log_size; host_cmd.dataflags[1] = IWL_HCMD_DFL_NOCOPY; recovery_cmd.buf_size = cpu_to_le32(error_log_size); } <S2SV_StartVul> ret = iwl_mvm_send_cmd(mvm, &host_cmd); <S2SV_EndVul> kfree(mvm->error_recovery_buf); mvm->error_recovery_buf = NULL; if (ret) { IWL_ERR(mvm, ""Failed to send recovery cmd %d\n"", ret); return; } if (flags & ERROR_RECOVERY_UPDATE_DB) { <S2SV_StartVul> resp = le32_to_cpu(*(__le32 *)host_cmd.resp_pkt->data); <S2SV_EndVul> <S2SV_StartVul> if (resp) { <S2SV_EndVul> IWL_ERR(mvm, ""Failed to send recovery cmd blob was invalid %d\n"", <S2SV_StartVul> resp); <S2SV_EndVul> ieee80211_iterate_interfaces(mvm->hw, 0, iwl_mvm_disconnect_iterator, mvm); } } }","- u32 resp;
- .flags = CMD_WANT_SKB,
- ret = iwl_mvm_send_cmd(mvm, &host_cmd);
- resp = le32_to_cpu(*(__le32 *)host_cmd.resp_pkt->data);
- if (resp) {
- resp);
+ u32 status = 0;
+ ret = iwl_mvm_send_cmd_status(mvm, &host_cmd, &status);
+ if (status) {
+ status);","void iwl_mvm_send_recovery_cmd(struct iwl_mvm *mvm, u32 flags) { u32 error_log_size = mvm->fw->ucode_capa.error_log_size; u32 status = 0; int ret; struct iwl_fw_error_recovery_cmd recovery_cmd = { .flags = cpu_to_le32(flags), .buf_size = 0, }; struct iwl_host_cmd host_cmd = { .id = WIDE_ID(SYSTEM_GROUP, FW_ERROR_RECOVERY_CMD), .data = {&recovery_cmd, }, .len = {sizeof(recovery_cmd), }, }; if (!error_log_size) return; if (flags & ERROR_RECOVERY_UPDATE_DB) { if (!mvm->error_recovery_buf) return; host_cmd.data[1] = mvm->error_recovery_buf; host_cmd.len[1] = error_log_size; host_cmd.dataflags[1] = IWL_HCMD_DFL_NOCOPY; recovery_cmd.buf_size = cpu_to_le32(error_log_size); } ret = iwl_mvm_send_cmd_status(mvm, &host_cmd, &status); kfree(mvm->error_recovery_buf); mvm->error_recovery_buf = NULL; if (ret) { IWL_ERR(mvm, ""Failed to send recovery cmd %d\n"", ret); return; } if (flags & ERROR_RECOVERY_UPDATE_DB) { if (status) { IWL_ERR(mvm, ""Failed to send recovery cmd blob was invalid %d\n"", status); ieee80211_iterate_interfaces(mvm->hw, 0, iwl_mvm_disconnect_iterator, mvm); } } }"
877----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50132/bad/trace_fprobe.c----__trace_fprobe_create,"static int __trace_fprobe_create(int argc, const char *argv[]) { struct trace_fprobe *tf = NULL; int i, len, new_argc = 0, ret = 0; bool is_return = false; char *symbol = NULL; const char *event = NULL, *group = FPROBE_EVENT_SYSTEM; const char **new_argv = NULL; int maxactive = 0; char buf[MAX_EVENT_NAME_LEN]; char gbuf[MAX_EVENT_NAME_LEN]; char sbuf[KSYM_NAME_LEN]; char abuf[MAX_BTF_ARGS_LEN]; char *dbuf = NULL; bool is_tracepoint = false; struct tracepoint *tpoint = NULL; struct traceprobe_parse_context ctx = { .flags = TPARG_FL_KERNEL | TPARG_FL_FPROBE, }; if ((argv[0][0] != 'f' && argv[0][0] != 't') || argc < 2) return -ECANCELED; if (argv[0][0] == 't') { is_tracepoint = true; group = TRACEPOINT_EVENT_SYSTEM; } trace_probe_log_init(""trace_fprobe"", argc, argv); event = strchr(&argv[0][1], ':'); if (event) event++; if (isdigit(argv[0][1])) { if (event) len = event - &argv[0][1] - 1; else len = strlen(&argv[0][1]); if (len > MAX_EVENT_NAME_LEN - 1) { trace_probe_log_err(1, BAD_MAXACT); goto parse_error; } memcpy(buf, &argv[0][1], len); buf[len] = '\0'; ret = kstrtouint(buf, 0, &maxactive); if (ret || !maxactive) { trace_probe_log_err(1, BAD_MAXACT); goto parse_error; } if (maxactive > RETHOOK_MAXACTIVE_MAX) { trace_probe_log_err(1, MAXACT_TOO_BIG); goto parse_error; } } trace_probe_log_set_index(1); ret = parse_symbol_and_return(argc, argv, &symbol, &is_return, is_tracepoint); if (ret < 0) goto parse_error; if (!is_return && maxactive) { trace_probe_log_set_index(0); trace_probe_log_err(1, BAD_MAXACT_TYPE); goto parse_error; } trace_probe_log_set_index(0); if (event) { ret = traceprobe_parse_event_name(&event, &group, gbuf, event - argv[0]); if (ret) goto parse_error; } if (!event) { if (is_tracepoint) snprintf(buf, MAX_EVENT_NAME_LEN, ""%s%s"", isdigit(*symbol) ? ""_"" : """", symbol); else snprintf(buf, MAX_EVENT_NAME_LEN, ""%s__%s"", symbol, is_return ? ""exit"" : ""entry""); sanitize_event_name(buf); event = buf; } if (is_return) ctx.flags |= TPARG_FL_RETURN; else ctx.flags |= TPARG_FL_FENTRY; if (is_tracepoint) { ctx.flags |= TPARG_FL_TPOINT; tpoint = find_tracepoint(symbol); if (!tpoint) { trace_probe_log_set_index(1); trace_probe_log_err(0, NO_TRACEPOINT); goto parse_error; } ctx.funcname = kallsyms_lookup( (unsigned long)tpoint->probestub, NULL, NULL, NULL, sbuf); } else ctx.funcname = symbol; argc -= 2; argv += 2; new_argv = traceprobe_expand_meta_args(argc, argv, &new_argc, abuf, MAX_BTF_ARGS_LEN, &ctx); if (IS_ERR(new_argv)) { ret = PTR_ERR(new_argv); new_argv = NULL; goto out; } if (new_argv) { argc = new_argc; argv = new_argv; } ret = traceprobe_expand_dentry_args(argc, argv, &dbuf); if (ret) goto out; tf = alloc_trace_fprobe(group, event, symbol, tpoint, maxactive, argc, is_return); if (IS_ERR(tf)) { ret = PTR_ERR(tf); WARN_ON_ONCE(ret != -ENOMEM); goto out; } if (is_tracepoint) tf->mod = __module_text_address( (unsigned long)tf->tpoint->probestub); <S2SV_StartVul> for (i = 0; i < argc && i < MAX_TRACE_ARGS; i++) { <S2SV_EndVul> trace_probe_log_set_index(i + 2); ctx.offset = 0; ret = traceprobe_parse_probe_arg(&tf->tp, i, argv[i], &ctx); if (ret) goto error; } if (is_return && tf->tp.entry_arg) { tf->fp.entry_handler = trace_fprobe_entry_handler; tf->fp.entry_data_size = traceprobe_get_entry_data_size(&tf->tp); } ret = traceprobe_set_print_fmt(&tf->tp, is_return ? PROBE_PRINT_RETURN : PROBE_PRINT_NORMAL); if (ret < 0) goto error; ret = register_trace_fprobe(tf); if (ret) { trace_probe_log_set_index(1); if (ret == -EILSEQ) trace_probe_log_err(0, BAD_INSN_BNDRY); else if (ret == -ENOENT) trace_probe_log_err(0, BAD_PROBE_ADDR); else if (ret != -ENOMEM && ret != -EEXIST) trace_probe_log_err(0, FAIL_REG_PROBE); goto error; } out: traceprobe_finish_parse(&ctx); trace_probe_log_clear(); kfree(new_argv); kfree(symbol); kfree(dbuf); return ret; parse_error: ret = -EINVAL; error: free_trace_fprobe(tf); goto out; }","- for (i = 0; i < argc && i < MAX_TRACE_ARGS; i++) {
+ }
+ if (argc > MAX_TRACE_ARGS) {
+ ret = -E2BIG;
+ goto out;
+ }
+ for (i = 0; i < argc; i++) {","static int __trace_fprobe_create(int argc, const char *argv[]) { struct trace_fprobe *tf = NULL; int i, len, new_argc = 0, ret = 0; bool is_return = false; char *symbol = NULL; const char *event = NULL, *group = FPROBE_EVENT_SYSTEM; const char **new_argv = NULL; int maxactive = 0; char buf[MAX_EVENT_NAME_LEN]; char gbuf[MAX_EVENT_NAME_LEN]; char sbuf[KSYM_NAME_LEN]; char abuf[MAX_BTF_ARGS_LEN]; char *dbuf = NULL; bool is_tracepoint = false; struct tracepoint *tpoint = NULL; struct traceprobe_parse_context ctx = { .flags = TPARG_FL_KERNEL | TPARG_FL_FPROBE, }; if ((argv[0][0] != 'f' && argv[0][0] != 't') || argc < 2) return -ECANCELED; if (argv[0][0] == 't') { is_tracepoint = true; group = TRACEPOINT_EVENT_SYSTEM; } trace_probe_log_init(""trace_fprobe"", argc, argv); event = strchr(&argv[0][1], ':'); if (event) event++; if (isdigit(argv[0][1])) { if (event) len = event - &argv[0][1] - 1; else len = strlen(&argv[0][1]); if (len > MAX_EVENT_NAME_LEN - 1) { trace_probe_log_err(1, BAD_MAXACT); goto parse_error; } memcpy(buf, &argv[0][1], len); buf[len] = '\0'; ret = kstrtouint(buf, 0, &maxactive); if (ret || !maxactive) { trace_probe_log_err(1, BAD_MAXACT); goto parse_error; } if (maxactive > RETHOOK_MAXACTIVE_MAX) { trace_probe_log_err(1, MAXACT_TOO_BIG); goto parse_error; } } trace_probe_log_set_index(1); ret = parse_symbol_and_return(argc, argv, &symbol, &is_return, is_tracepoint); if (ret < 0) goto parse_error; if (!is_return && maxactive) { trace_probe_log_set_index(0); trace_probe_log_err(1, BAD_MAXACT_TYPE); goto parse_error; } trace_probe_log_set_index(0); if (event) { ret = traceprobe_parse_event_name(&event, &group, gbuf, event - argv[0]); if (ret) goto parse_error; } if (!event) { if (is_tracepoint) snprintf(buf, MAX_EVENT_NAME_LEN, ""%s%s"", isdigit(*symbol) ? ""_"" : """", symbol); else snprintf(buf, MAX_EVENT_NAME_LEN, ""%s__%s"", symbol, is_return ? ""exit"" : ""entry""); sanitize_event_name(buf); event = buf; } if (is_return) ctx.flags |= TPARG_FL_RETURN; else ctx.flags |= TPARG_FL_FENTRY; if (is_tracepoint) { ctx.flags |= TPARG_FL_TPOINT; tpoint = find_tracepoint(symbol); if (!tpoint) { trace_probe_log_set_index(1); trace_probe_log_err(0, NO_TRACEPOINT); goto parse_error; } ctx.funcname = kallsyms_lookup( (unsigned long)tpoint->probestub, NULL, NULL, NULL, sbuf); } else ctx.funcname = symbol; argc -= 2; argv += 2; new_argv = traceprobe_expand_meta_args(argc, argv, &new_argc, abuf, MAX_BTF_ARGS_LEN, &ctx); if (IS_ERR(new_argv)) { ret = PTR_ERR(new_argv); new_argv = NULL; goto out; } if (new_argv) { argc = new_argc; argv = new_argv; } if (argc > MAX_TRACE_ARGS) { ret = -E2BIG; goto out; } ret = traceprobe_expand_dentry_args(argc, argv, &dbuf); if (ret) goto out; tf = alloc_trace_fprobe(group, event, symbol, tpoint, maxactive, argc, is_return); if (IS_ERR(tf)) { ret = PTR_ERR(tf); WARN_ON_ONCE(ret != -ENOMEM); goto out; } if (is_tracepoint) tf->mod = __module_text_address( (unsigned long)tf->tpoint->probestub); for (i = 0; i < argc; i++) { trace_probe_log_set_index(i + 2); ctx.offset = 0; ret = traceprobe_parse_probe_arg(&tf->tp, i, argv[i], &ctx); if (ret) goto error; } if (is_return && tf->tp.entry_arg) { tf->fp.entry_handler = trace_fprobe_entry_handler; tf->fp.entry_data_size = traceprobe_get_entry_data_size(&tf->tp); } ret = traceprobe_set_print_fmt(&tf->tp, is_return ? PROBE_PRINT_RETURN : PROBE_PRINT_NORMAL); if (ret < 0) goto error; ret = register_trace_fprobe(tf); if (ret) { trace_probe_log_set_index(1); if (ret == -EILSEQ) trace_probe_log_err(0, BAD_INSN_BNDRY); else if (ret == -ENOENT) trace_probe_log_err(0, BAD_PROBE_ADDR); else if (ret != -ENOMEM && ret != -EEXIST) trace_probe_log_err(0, FAIL_REG_PROBE); goto error; } out: traceprobe_finish_parse(&ctx); trace_probe_log_clear(); kfree(new_argv); kfree(symbol); kfree(dbuf); return ret; parse_error: ret = -EINVAL; error: free_trace_fprobe(tf); goto out; }"
60----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44943/bad/gup.c----gup_pte_range,"static int gup_pte_range(pmd_t pmd, pmd_t *pmdp, unsigned long addr, unsigned long end, unsigned int flags, struct page **pages, int *nr) { struct dev_pagemap *pgmap = NULL; int nr_start = *nr, ret = 0; pte_t *ptep, *ptem; ptem = ptep = pte_offset_map(&pmd, addr); if (!ptep) return 0; do { pte_t pte = ptep_get_lockless(ptep); struct page *page; struct folio *folio; if (pte_protnone(pte)) goto pte_unmap; if (!pte_access_permitted(pte, flags & FOLL_WRITE)) goto pte_unmap; if (pte_devmap(pte)) { if (unlikely(flags & FOLL_LONGTERM)) goto pte_unmap; pgmap = get_dev_pagemap(pte_pfn(pte), pgmap); if (unlikely(!pgmap)) { undo_dev_pagemap(nr, nr_start, flags, pages); goto pte_unmap; } } else if (pte_special(pte)) goto pte_unmap; VM_BUG_ON(!pfn_valid(pte_pfn(pte))); page = pte_page(pte); <S2SV_StartVul> folio = try_grab_folio(page, 1, flags); <S2SV_EndVul> <S2SV_StartVul> if (!folio) <S2SV_EndVul> goto pte_unmap; if (unlikely(folio_is_secretmem(folio))) { gup_put_folio(folio, 1, flags); goto pte_unmap; } if (unlikely(pmd_val(pmd) != pmd_val(*pmdp)) || unlikely(pte_val(pte) != pte_val(ptep_get(ptep)))) { gup_put_folio(folio, 1, flags); goto pte_unmap; } if (!folio_fast_pin_allowed(folio, flags)) { gup_put_folio(folio, 1, flags); goto pte_unmap; } if (!pte_write(pte) && gup_must_unshare(NULL, flags, page)) { gup_put_folio(folio, 1, flags); goto pte_unmap; } if (flags & FOLL_PIN) { ret = arch_make_page_accessible(page); if (ret) { gup_put_folio(folio, 1, flags); goto pte_unmap; } } folio_set_referenced(folio); pages[*nr] = page; (*nr)++; } while (ptep++, addr += PAGE_SIZE, addr != end); ret = 1; pte_unmap: if (pgmap) put_dev_pagemap(pgmap); pte_unmap(ptem); return ret; }","- folio = try_grab_folio(page, 1, flags);
- if (!folio)
+ folio = try_grab_folio_fast(page, 1, flags);
+ if (!folio)","static int gup_pte_range(pmd_t pmd, pmd_t *pmdp, unsigned long addr, unsigned long end, unsigned int flags, struct page **pages, int *nr) { struct dev_pagemap *pgmap = NULL; int nr_start = *nr, ret = 0; pte_t *ptep, *ptem; ptem = ptep = pte_offset_map(&pmd, addr); if (!ptep) return 0; do { pte_t pte = ptep_get_lockless(ptep); struct page *page; struct folio *folio; if (pte_protnone(pte)) goto pte_unmap; if (!pte_access_permitted(pte, flags & FOLL_WRITE)) goto pte_unmap; if (pte_devmap(pte)) { if (unlikely(flags & FOLL_LONGTERM)) goto pte_unmap; pgmap = get_dev_pagemap(pte_pfn(pte), pgmap); if (unlikely(!pgmap)) { undo_dev_pagemap(nr, nr_start, flags, pages); goto pte_unmap; } } else if (pte_special(pte)) goto pte_unmap; VM_BUG_ON(!pfn_valid(pte_pfn(pte))); page = pte_page(pte); folio = try_grab_folio_fast(page, 1, flags); if (!folio) goto pte_unmap; if (unlikely(folio_is_secretmem(folio))) { gup_put_folio(folio, 1, flags); goto pte_unmap; } if (unlikely(pmd_val(pmd) != pmd_val(*pmdp)) || unlikely(pte_val(pte) != pte_val(ptep_get(ptep)))) { gup_put_folio(folio, 1, flags); goto pte_unmap; } if (!folio_fast_pin_allowed(folio, flags)) { gup_put_folio(folio, 1, flags); goto pte_unmap; } if (!pte_write(pte) && gup_must_unshare(NULL, flags, page)) { gup_put_folio(folio, 1, flags); goto pte_unmap; } if (flags & FOLL_PIN) { ret = arch_make_page_accessible(page); if (ret) { gup_put_folio(folio, 1, flags); goto pte_unmap; } } folio_set_referenced(folio); pages[*nr] = page; (*nr)++; } while (ptep++, addr += PAGE_SIZE, addr != end); ret = 1; pte_unmap: if (pgmap) put_dev_pagemap(pgmap); pte_unmap(ptem); return ret; }"
1098----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53079/bad/memcontrol.c----uncharge_folio,"static void uncharge_folio(struct folio *folio, struct uncharge_gather *ug) { long nr_pages; struct mem_cgroup *memcg; struct obj_cgroup *objcg; VM_BUG_ON_FOLIO(folio_test_lru(folio), folio); <S2SV_StartVul> VM_BUG_ON_FOLIO(folio_order(folio) > 1 && <S2SV_EndVul> <S2SV_StartVul> !folio_test_hugetlb(folio) && <S2SV_EndVul> <S2SV_StartVul> !list_empty(&folio->_deferred_list), folio); <S2SV_EndVul> if (folio_memcg_kmem(folio)) { objcg = __folio_objcg(folio); memcg = get_mem_cgroup_from_objcg(objcg); } else { memcg = __folio_memcg(folio); } if (!memcg) return; if (ug->memcg != memcg) { if (ug->memcg) { uncharge_batch(ug); uncharge_gather_clear(ug); } ug->memcg = memcg; ug->nid = folio_nid(folio); css_get(&memcg->css); } nr_pages = folio_nr_pages(folio); if (folio_memcg_kmem(folio)) { ug->nr_memory += nr_pages; ug->nr_kmem += nr_pages; folio->memcg_data = 0; obj_cgroup_put(objcg); } else { if (!mem_cgroup_is_root(memcg)) ug->nr_memory += nr_pages; ug->pgpgout++; folio->memcg_data = 0; } css_put(&memcg->css); }","- VM_BUG_ON_FOLIO(folio_order(folio) > 1 &&
- !folio_test_hugetlb(folio) &&
- !list_empty(&folio->_deferred_list), folio);
+ WARN_ON_ONCE(folio_unqueue_deferred_split(folio));
+ }","static void uncharge_folio(struct folio *folio, struct uncharge_gather *ug) { long nr_pages; struct mem_cgroup *memcg; struct obj_cgroup *objcg; VM_BUG_ON_FOLIO(folio_test_lru(folio), folio); if (folio_memcg_kmem(folio)) { objcg = __folio_objcg(folio); memcg = get_mem_cgroup_from_objcg(objcg); } else { memcg = __folio_memcg(folio); } if (!memcg) return; if (ug->memcg != memcg) { if (ug->memcg) { uncharge_batch(ug); uncharge_gather_clear(ug); } ug->memcg = memcg; ug->nid = folio_nid(folio); css_get(&memcg->css); } nr_pages = folio_nr_pages(folio); if (folio_memcg_kmem(folio)) { ug->nr_memory += nr_pages; ug->nr_kmem += nr_pages; folio->memcg_data = 0; obj_cgroup_put(objcg); } else { if (!mem_cgroup_is_root(memcg)) ug->nr_memory += nr_pages; ug->pgpgout++; WARN_ON_ONCE(folio_unqueue_deferred_split(folio)); folio->memcg_data = 0; } css_put(&memcg->css); }"
602----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49920/bad/dcn32_resource_helpers.c----dcn32_is_center_timing,bool dcn32_is_center_timing(struct pipe_ctx *pipe) { bool is_center_timing = false; if (pipe->stream) { if (pipe->stream->timing.v_addressable != pipe->stream->dst.height || pipe->stream->timing.v_addressable != pipe->stream->src.height) { <S2SV_StartVul> is_center_timing = true; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> if (pipe->plane_state) { <S2SV_EndVul> <S2SV_StartVul> if (pipe->stream->timing.v_addressable != pipe->plane_state->dst_rect.height && <S2SV_EndVul> <S2SV_StartVul> pipe->stream->timing.v_addressable != pipe->plane_state->src_rect.height) { <S2SV_EndVul> <S2SV_StartVul> is_center_timing = true; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> return is_center_timing; },"- is_center_timing = true;
- }
- }
- if (pipe->plane_state) {
- if (pipe->stream->timing.v_addressable != pipe->plane_state->dst_rect.height &&
- pipe->stream->timing.v_addressable != pipe->plane_state->src_rect.height) {
- is_center_timing = true;
- }
- }
+ is_center_timing = true;
+ }
+ if (pipe->plane_state) {
+ if (pipe->stream->timing.v_addressable != pipe->plane_state->dst_rect.height &&
+ pipe->stream->timing.v_addressable != pipe->plane_state->src_rect.height) {
+ is_center_timing = true;
+ }
+ }
+ }",bool dcn32_is_center_timing(struct pipe_ctx *pipe) { bool is_center_timing = false; if (pipe->stream) { if (pipe->stream->timing.v_addressable != pipe->stream->dst.height || pipe->stream->timing.v_addressable != pipe->stream->src.height) { is_center_timing = true; } if (pipe->plane_state) { if (pipe->stream->timing.v_addressable != pipe->plane_state->dst_rect.height && pipe->stream->timing.v_addressable != pipe->plane_state->src_rect.height) { is_center_timing = true; } } } return is_center_timing; }
344----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46804/bad/hdcp_ddc.c----write,"static enum mod_hdcp_status write(struct mod_hdcp *hdcp, enum mod_hdcp_ddc_message_id msg_id, uint8_t *buf, uint32_t buf_len) { bool success = true; uint32_t cur_size = 0; uint32_t data_offset = 0; <S2SV_StartVul> if (msg_id == MOD_HDCP_MESSAGE_ID_INVALID) { <S2SV_EndVul> return MOD_HDCP_STATUS_DDC_FAILURE; <S2SV_StartVul> } <S2SV_EndVul> if (is_dp_hdcp(hdcp)) { while (buf_len > 0) { cur_size = MIN(buf_len, HDCP_MAX_AUX_TRANSACTION_SIZE); success = hdcp->config.ddc.funcs.write_dpcd( hdcp->config.ddc.handle, hdcp_dpcd_addrs[msg_id] + data_offset, buf + data_offset, cur_size); if (!success) break; buf_len -= cur_size; data_offset += cur_size; <S2SV_StartVul> } <S2SV_EndVul> } else { hdcp->buf[0] = hdcp_i2c_offsets[msg_id]; memmove(&hdcp->buf[1], buf, buf_len); success = hdcp->config.ddc.funcs.write_i2c( hdcp->config.ddc.handle, HDCP_I2C_ADDR, hdcp->buf, (uint32_t)(buf_len+1)); } return success ? MOD_HDCP_STATUS_SUCCESS : MOD_HDCP_STATUS_DDC_FAILURE; }","- if (msg_id == MOD_HDCP_MESSAGE_ID_INVALID) {
- }
- }
+ if (msg_id == MOD_HDCP_MESSAGE_ID_INVALID ||
+ msg_id >= MOD_HDCP_MESSAGE_ID_MAX)
+ return MOD_HDCP_STATUS_DDC_FAILURE;
+ int num_dpcd_addrs = sizeof(hdcp_dpcd_addrs) /
+ sizeof(hdcp_dpcd_addrs[0]);
+ if (msg_id >= num_dpcd_addrs)
+ return MOD_HDCP_STATUS_DDC_FAILURE;
+ int num_i2c_offsets = sizeof(hdcp_i2c_offsets) /
+ sizeof(hdcp_i2c_offsets[0]);
+ if (msg_id >= num_i2c_offsets)
+ return MOD_HDCP_STATUS_DDC_FAILURE;","static enum mod_hdcp_status write(struct mod_hdcp *hdcp, enum mod_hdcp_ddc_message_id msg_id, uint8_t *buf, uint32_t buf_len) { bool success = true; uint32_t cur_size = 0; uint32_t data_offset = 0; if (msg_id == MOD_HDCP_MESSAGE_ID_INVALID || msg_id >= MOD_HDCP_MESSAGE_ID_MAX) return MOD_HDCP_STATUS_DDC_FAILURE; if (is_dp_hdcp(hdcp)) { int num_dpcd_addrs = sizeof(hdcp_dpcd_addrs) / sizeof(hdcp_dpcd_addrs[0]); if (msg_id >= num_dpcd_addrs) return MOD_HDCP_STATUS_DDC_FAILURE; while (buf_len > 0) { cur_size = MIN(buf_len, HDCP_MAX_AUX_TRANSACTION_SIZE); success = hdcp->config.ddc.funcs.write_dpcd( hdcp->config.ddc.handle, hdcp_dpcd_addrs[msg_id] + data_offset, buf + data_offset, cur_size); if (!success) break; buf_len -= cur_size; data_offset += cur_size; } } else { int num_i2c_offsets = sizeof(hdcp_i2c_offsets) / sizeof(hdcp_i2c_offsets[0]); if (msg_id >= num_i2c_offsets) return MOD_HDCP_STATUS_DDC_FAILURE; hdcp->buf[0] = hdcp_i2c_offsets[msg_id]; memmove(&hdcp->buf[1], buf, buf_len); success = hdcp->config.ddc.funcs.write_i2c( hdcp->config.ddc.handle, HDCP_I2C_ADDR, hdcp->buf, (uint32_t)(buf_len+1)); } return success ? MOD_HDCP_STATUS_SUCCESS : MOD_HDCP_STATUS_DDC_FAILURE; }"
1378----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56712/bad/udmabuf.c----export_udmabuf,"static int export_udmabuf(struct udmabuf *ubuf, struct miscdevice *device, u32 flags) { DEFINE_DMA_BUF_EXPORT_INFO(exp_info); struct dma_buf *buf; ubuf->device = device; exp_info.ops = &udmabuf_ops; exp_info.size = ubuf->pagecount << PAGE_SHIFT; exp_info.priv = ubuf; exp_info.flags = O_RDWR; <S2SV_StartVul> buf = dma_buf_export(&exp_info); <S2SV_EndVul> <S2SV_StartVul> if (IS_ERR(buf)) <S2SV_EndVul> <S2SV_StartVul> return PTR_ERR(buf); <S2SV_EndVul> <S2SV_StartVul> return dma_buf_fd(buf, flags); <S2SV_EndVul> }","- buf = dma_buf_export(&exp_info);
- if (IS_ERR(buf))
- return PTR_ERR(buf);
- return dma_buf_fd(buf, flags);
+ #include <linux/shmem_fs.h>
+ #include <linux/hugetlb.h>","#include <linux/cred.h> #include <linux/device.h> #include <linux/dma-buf.h> #include <linux/dma-resv.h> #include <linux/highmem.h> #include <linux/init.h> #include <linux/kernel.h> #include <linux/memfd.h> #include <linux/miscdevice.h> #include <linux/module.h> #include <linux/shmem_fs.h> #include <linux/hugetlb.h> #include <linux/slab.h> #include <linux/udmabuf.h> #include <linux/vmalloc.h> #include <linux/iosys-map.h> static int list_limit = 1024; module_param(list_limit, int, 0644); MODULE_PARM_DESC(list_limit, ""udmabuf_create_list->count limit. Default is 1024.""); static int size_limit_mb = 64; module_param(size_limit_mb, int, 0644); MODULE_PARM_DESC(size_limit_mb, ""Max size of a dmabuf, in megabytes. Default is 64.""); struct udmabuf { pgoff_t pagecount; struct folio **folios; pgoff_t nr_pinned; struct folio **pinned_folios; struct sg_table *sg; struct miscdevice *device; pgoff_t *offsets; };"
516----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47751/bad/pcie-kirin.c----kirin_pcie_parse_port,"static int kirin_pcie_parse_port(struct kirin_pcie *pcie, struct platform_device *pdev, struct device_node *node) { struct device *dev = &pdev->dev; struct device_node *parent, *child; int ret, slot, i; char name[32]; for_each_available_child_of_node(node, parent) { for_each_available_child_of_node(parent, child) { i = pcie->num_slots; pcie->gpio_id_reset[i] = of_get_named_gpio(child, ""reset-gpios"", 0); if (pcie->gpio_id_reset[i] < 0) continue; <S2SV_StartVul> pcie->num_slots++; <S2SV_EndVul> <S2SV_StartVul> if (pcie->num_slots > MAX_PCI_SLOTS) { <S2SV_EndVul> dev_err(dev, ""Too many PCI slots!\n""); ret = -EINVAL; goto put_node; } ret = of_pci_get_devfn(child); if (ret < 0) { dev_err(dev, ""failed to parse devfn: %d\n"", ret); goto put_node; } slot = PCI_SLOT(ret); sprintf(name, ""pcie_perst_%d"", slot); pcie->reset_names[i] = devm_kstrdup_const(dev, name, GFP_KERNEL); if (!pcie->reset_names[i]) { ret = -ENOMEM; goto put_node; } } } return 0; put_node: of_node_put(child); of_node_put(parent); return ret; }","- pcie->num_slots++;
- if (pcie->num_slots > MAX_PCI_SLOTS) {
+ if (pcie->num_slots + 1 >= MAX_PCI_SLOTS) {
+ pcie->num_slots++;","static int kirin_pcie_parse_port(struct kirin_pcie *pcie, struct platform_device *pdev, struct device_node *node) { struct device *dev = &pdev->dev; struct device_node *parent, *child; int ret, slot, i; char name[32]; for_each_available_child_of_node(node, parent) { for_each_available_child_of_node(parent, child) { i = pcie->num_slots; pcie->gpio_id_reset[i] = of_get_named_gpio(child, ""reset-gpios"", 0); if (pcie->gpio_id_reset[i] < 0) continue; if (pcie->num_slots + 1 >= MAX_PCI_SLOTS) { dev_err(dev, ""Too many PCI slots!\n""); ret = -EINVAL; goto put_node; } pcie->num_slots++; ret = of_pci_get_devfn(child); if (ret < 0) { dev_err(dev, ""failed to parse devfn: %d\n"", ret); goto put_node; } slot = PCI_SLOT(ret); sprintf(name, ""pcie_perst_%d"", slot); pcie->reset_names[i] = devm_kstrdup_const(dev, name, GFP_KERNEL); if (!pcie->reset_names[i]) { ret = -ENOMEM; goto put_node; } } } return 0; put_node: of_node_put(child); of_node_put(parent); return ret; }"
951----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50194/bad/uprobes.c----arch_uprobe_skip_sstep,"bool arch_uprobe_skip_sstep(struct arch_uprobe *auprobe, struct pt_regs *regs) { probe_opcode_t insn; unsigned long addr; if (!auprobe->simulate) return false; <S2SV_StartVul> insn = *(probe_opcode_t *)(&auprobe->insn[0]); <S2SV_EndVul> addr = instruction_pointer(regs); if (auprobe->api.handler) auprobe->api.handler(insn, addr, regs); return true; }","- insn = *(probe_opcode_t *)(&auprobe->insn[0]);
+ insn = le32_to_cpu(auprobe->insn);","bool arch_uprobe_skip_sstep(struct arch_uprobe *auprobe, struct pt_regs *regs) { probe_opcode_t insn; unsigned long addr; if (!auprobe->simulate) return false; insn = le32_to_cpu(auprobe->insn); addr = instruction_pointer(regs); if (auprobe->api.handler) auprobe->api.handler(insn, addr, regs); return true; }"
403----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46864/bad/mshyperv.c----hv_machine_shutdown,static void hv_machine_shutdown(void) { if (kexec_in_progress && hv_kexec_handler) hv_kexec_handler(); <S2SV_StartVul> if (kexec_in_progress && hyperv_init_cpuhp > 0) <S2SV_EndVul> <S2SV_StartVul> cpuhp_remove_state(hyperv_init_cpuhp); <S2SV_EndVul> native_machine_shutdown(); if (kexec_in_progress) hyperv_cleanup(); },"- if (kexec_in_progress && hyperv_init_cpuhp > 0)
- cpuhp_remove_state(hyperv_init_cpuhp);
+ if (kexec_in_progress)
+ cpuhp_remove_state(CPUHP_AP_HYPERV_ONLINE);",static void hv_machine_shutdown(void) { if (kexec_in_progress && hv_kexec_handler) hv_kexec_handler(); if (kexec_in_progress) cpuhp_remove_state(CPUHP_AP_HYPERV_ONLINE); native_machine_shutdown(); if (kexec_in_progress) hyperv_cleanup(); }
399----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46858/bad/pm_netlink.c----remove_anno_list_by_saddr,"static bool remove_anno_list_by_saddr(struct mptcp_sock *msk, const struct mptcp_addr_info *addr) { struct mptcp_pm_add_entry *entry; entry = mptcp_pm_del_add_timer(msk, addr, false); if (entry) { <S2SV_StartVul> list_del(&entry->list); <S2SV_EndVul> kfree(entry); return true; } return false; }","- list_del(&entry->list);
+ }","static bool remove_anno_list_by_saddr(struct mptcp_sock *msk, const struct mptcp_addr_info *addr) { struct mptcp_pm_add_entry *entry; entry = mptcp_pm_del_add_timer(msk, addr, false); if (entry) { kfree(entry); return true; } return false; }"
431----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47670/bad/xattr.c----ocfs2_xattr_ibody_find,"static int ocfs2_xattr_ibody_find(struct inode *inode, int name_index, const char *name, struct ocfs2_xattr_search *xs) { struct ocfs2_inode_info *oi = OCFS2_I(inode); struct ocfs2_dinode *di = (struct ocfs2_dinode *)xs->inode_bh->b_data; int ret; int has_space = 0; if (inode->i_sb->s_blocksize == OCFS2_MIN_BLOCKSIZE) return 0; if (!(oi->ip_dyn_features & OCFS2_INLINE_XATTR_FL)) { down_read(&oi->ip_alloc_sem); has_space = ocfs2_xattr_has_space_inline(inode, di); up_read(&oi->ip_alloc_sem); if (!has_space) return 0; } xs->xattr_bh = xs->inode_bh; xs->end = (void *)di + inode->i_sb->s_blocksize; if (oi->ip_dyn_features & OCFS2_INLINE_XATTR_FL) xs->header = (struct ocfs2_xattr_header *) (xs->end - le16_to_cpu(di->i_xattr_inline_size)); else xs->header = (struct ocfs2_xattr_header *) (xs->end - OCFS2_SB(inode->i_sb)->s_xattr_inline_size); xs->base = (void *)xs->header; xs->here = xs->header->xh_entries; if (oi->ip_dyn_features & OCFS2_INLINE_XATTR_FL) { <S2SV_StartVul> ret = ocfs2_xattr_find_entry(name_index, name, xs); <S2SV_EndVul> if (ret && ret != -ENODATA) return ret; xs->not_found = ret; } return 0; }","- ret = ocfs2_xattr_find_entry(name_index, name, xs);
+ ret = ocfs2_xattr_find_entry(inode, name_index, name, xs);","static int ocfs2_xattr_ibody_find(struct inode *inode, int name_index, const char *name, struct ocfs2_xattr_search *xs) { struct ocfs2_inode_info *oi = OCFS2_I(inode); struct ocfs2_dinode *di = (struct ocfs2_dinode *)xs->inode_bh->b_data; int ret; int has_space = 0; if (inode->i_sb->s_blocksize == OCFS2_MIN_BLOCKSIZE) return 0; if (!(oi->ip_dyn_features & OCFS2_INLINE_XATTR_FL)) { down_read(&oi->ip_alloc_sem); has_space = ocfs2_xattr_has_space_inline(inode, di); up_read(&oi->ip_alloc_sem); if (!has_space) return 0; } xs->xattr_bh = xs->inode_bh; xs->end = (void *)di + inode->i_sb->s_blocksize; if (oi->ip_dyn_features & OCFS2_INLINE_XATTR_FL) xs->header = (struct ocfs2_xattr_header *) (xs->end - le16_to_cpu(di->i_xattr_inline_size)); else xs->header = (struct ocfs2_xattr_header *) (xs->end - OCFS2_SB(inode->i_sb)->s_xattr_inline_size); xs->base = (void *)xs->header; xs->here = xs->header->xh_entries; if (oi->ip_dyn_features & OCFS2_INLINE_XATTR_FL) { ret = ocfs2_xattr_find_entry(inode, name_index, name, xs); if (ret && ret != -ENODATA) return ret; xs->not_found = ret; } return 0; }"
122----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44980/bad/xe_display.c----xe_display_init_noirq,"int xe_display_init_noirq(struct xe_device *xe) { int err; if (!xe->info.enable_display) return 0; intel_display_driver_early_probe(xe); intel_opregion_setup(xe); intel_dram_detect(xe); intel_bw_init_hw(xe); intel_display_device_info_runtime_init(xe); err = intel_display_driver_probe_noirq(xe); <S2SV_StartVul> if (err) <S2SV_EndVul> return err; return devm_add_action_or_reset(xe->drm.dev, xe_display_fini_noirq, xe); }","- if (err)
+ if (err) {
+ intel_opregion_cleanup(xe);
+ }
+ }","int xe_display_init_noirq(struct xe_device *xe) { int err; if (!xe->info.enable_display) return 0; intel_display_driver_early_probe(xe); intel_opregion_setup(xe); intel_dram_detect(xe); intel_bw_init_hw(xe); intel_display_device_info_runtime_init(xe); err = intel_display_driver_probe_noirq(xe); if (err) { intel_opregion_cleanup(xe); return err; } return devm_add_action_or_reset(xe->drm.dev, xe_display_fini_noirq, xe); }"
1355----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56682/bad/irq-riscv-aplic-main.c----aplic_probe,"static int aplic_probe(struct platform_device *pdev) { struct device *dev = &pdev->dev; bool msi_mode = false; void __iomem *regs; int rc; regs = devm_platform_ioremap_resource(pdev, 0); if (IS_ERR(regs)) { dev_err(dev, ""failed map MMIO registers\n""); return PTR_ERR(regs); } if (is_of_node(dev->fwnode)) msi_mode = of_property_present(to_of_node(dev->fwnode), ""msi-parent""); else msi_mode = imsic_acpi_get_fwnode(NULL) ? 1 : 0; if (msi_mode) rc = aplic_msi_setup(dev, regs); else rc = aplic_direct_setup(dev, regs); if (rc) <S2SV_StartVul> dev_err(dev, ""failed to setup APLIC in %s mode\n"", msi_mode ? ""MSI"" : ""direct""); <S2SV_EndVul> #ifdef CONFIG_ACPI if (!acpi_disabled) acpi_dev_clear_dependencies(ACPI_COMPANION(dev)); #endif return rc; }","- dev_err(dev, ""failed to setup APLIC in %s mode\n"", msi_mode ? ""MSI"" : ""direct"");
+ dev_err_probe(dev, rc, ""failed to setup APLIC in %s mode\n"",
+ msi_mode ? ""MSI"" : ""direct"");","static int aplic_probe(struct platform_device *pdev) { struct device *dev = &pdev->dev; bool msi_mode = false; void __iomem *regs; int rc; regs = devm_platform_ioremap_resource(pdev, 0); if (IS_ERR(regs)) { dev_err(dev, ""failed map MMIO registers\n""); return PTR_ERR(regs); } if (is_of_node(dev->fwnode)) msi_mode = of_property_present(to_of_node(dev->fwnode), ""msi-parent""); else msi_mode = imsic_acpi_get_fwnode(NULL) ? 1 : 0; if (msi_mode) rc = aplic_msi_setup(dev, regs); else rc = aplic_direct_setup(dev, regs); if (rc) dev_err_probe(dev, rc, ""failed to setup APLIC in %s mode\n"", msi_mode ? ""MSI"" : ""direct""); #ifdef CONFIG_ACPI if (!acpi_disabled) acpi_dev_clear_dependencies(ACPI_COMPANION(dev)); #endif return rc; }"
838----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50100/bad/dummy_hcd.c----dummy_bus_resume,"static int dummy_bus_resume(struct usb_hcd *hcd) { struct dummy_hcd *dum_hcd = hcd_to_dummy_hcd(hcd); int rc = 0; dev_dbg(&hcd->self.root_hub->dev, ""%s\n"", __func__); spin_lock_irq(&dum_hcd->dum->lock); if (!HCD_HW_ACCESSIBLE(hcd)) { rc = -ESHUTDOWN; } else { dum_hcd->rh_state = DUMMY_RH_RUNNING; set_link_state(dum_hcd); <S2SV_StartVul> if (!list_empty(&dum_hcd->urbp_list)) <S2SV_EndVul> hrtimer_start(&dum_hcd->timer, ns_to_ktime(0), HRTIMER_MODE_REL_SOFT); hcd->state = HC_STATE_RUNNING; } spin_unlock_irq(&dum_hcd->dum->lock); return rc; }","- if (!list_empty(&dum_hcd->urbp_list))
+ if (!list_empty(&dum_hcd->urbp_list)) {
+ dum_hcd->timer_pending = 1;
+ }
+ }","static int dummy_bus_resume(struct usb_hcd *hcd) { struct dummy_hcd *dum_hcd = hcd_to_dummy_hcd(hcd); int rc = 0; dev_dbg(&hcd->self.root_hub->dev, ""%s\n"", __func__); spin_lock_irq(&dum_hcd->dum->lock); if (!HCD_HW_ACCESSIBLE(hcd)) { rc = -ESHUTDOWN; } else { dum_hcd->rh_state = DUMMY_RH_RUNNING; set_link_state(dum_hcd); if (!list_empty(&dum_hcd->urbp_list)) { dum_hcd->timer_pending = 1; hrtimer_start(&dum_hcd->timer, ns_to_ktime(0), HRTIMER_MODE_REL_SOFT); } hcd->state = HC_STATE_RUNNING; } spin_unlock_irq(&dum_hcd->dum->lock); return rc; }"
958----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50202/bad/namei.c----nilfs_do_unlink,"static int nilfs_do_unlink(struct inode *dir, struct dentry *dentry) { struct inode *inode; struct nilfs_dir_entry *de; struct page *page; int err; <S2SV_StartVul> err = -ENOENT; <S2SV_EndVul> de = nilfs_find_entry(dir, &dentry->d_name, &page); <S2SV_StartVul> if (!de) <S2SV_EndVul> goto out; inode = d_inode(dentry); err = -EIO; if (le64_to_cpu(de->inode) != inode->i_ino) goto out; if (!inode->i_nlink) { nilfs_warn(inode->i_sb, ""deleting nonexistent file (ino=%lu), %d"", inode->i_ino, inode->i_nlink); set_nlink(inode, 1); } err = nilfs_delete_entry(de, page); if (err) goto out; inode_set_ctime_to_ts(inode, inode_get_ctime(dir)); drop_nlink(inode); err = 0; out: return err; }","- err = -ENOENT;
- if (!de)
+ if (IS_ERR(de)) {
+ err = PTR_ERR(de);
+ }","static int nilfs_do_unlink(struct inode *dir, struct dentry *dentry) { struct inode *inode; struct nilfs_dir_entry *de; struct page *page; int err; de = nilfs_find_entry(dir, &dentry->d_name, &page); if (IS_ERR(de)) { err = PTR_ERR(de); goto out; } inode = d_inode(dentry); err = -EIO; if (le64_to_cpu(de->inode) != inode->i_ino) goto out; if (!inode->i_nlink) { nilfs_warn(inode->i_sb, ""deleting nonexistent file (ino=%lu), %d"", inode->i_ino, inode->i_nlink); set_nlink(inode, 1); } err = nilfs_delete_entry(de, page); if (err) goto out; inode_set_ctime_to_ts(inode, inode_get_ctime(dir)); drop_nlink(inode); err = 0; out: return err; }"
707----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49989/bad/link_factory.c----link_destruct,static void link_destruct(struct dc_link *link) { int i; if (link->hpd_gpio) { dal_gpio_destroy_irq(&link->hpd_gpio); link->hpd_gpio = NULL; } if (link->ddc) link_destroy_ddc_service(&link->ddc); if (link->panel_cntl) link->panel_cntl->funcs->destroy(&link->panel_cntl); <S2SV_StartVul> if (link->link_enc) { <S2SV_EndVul> if (link->link_id.id != CONNECTOR_ID_VIRTUAL) { link->dc->res_pool->link_encoders[link->eng_id - ENGINE_ID_DIGA] = NULL; link->dc->res_pool->dig_link_enc_count--; } link->link_enc->funcs->destroy(&link->link_enc); } if (link->local_sink) dc_sink_release(link->local_sink); for (i = 0; i < link->sink_count; ++i) dc_sink_release(link->remote_sinks[i]); },"- if (link->link_enc) {
+ if (link->link_enc && !link->is_dig_mapping_flexible) {",static void link_destruct(struct dc_link *link) { int i; if (link->hpd_gpio) { dal_gpio_destroy_irq(&link->hpd_gpio); link->hpd_gpio = NULL; } if (link->ddc) link_destroy_ddc_service(&link->ddc); if (link->panel_cntl) link->panel_cntl->funcs->destroy(&link->panel_cntl); if (link->link_enc && !link->is_dig_mapping_flexible) { if (link->link_id.id != CONNECTOR_ID_VIRTUAL) { link->dc->res_pool->link_encoders[link->eng_id - ENGINE_ID_DIGA] = NULL; link->dc->res_pool->dig_link_enc_count--; } link->link_enc->funcs->destroy(&link->link_enc); } if (link->local_sink) dc_sink_release(link->local_sink); for (i = 0; i < link->sink_count; ++i) dc_sink_release(link->remote_sinks[i]); }
950----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50192/bad/irq-gic-v3-its.c----its_vpe_init,"static int its_vpe_init(struct its_vpe *vpe) { struct page *vpt_page; int vpe_id; vpe_id = its_vpe_id_alloc(); if (vpe_id < 0) return vpe_id; vpt_page = its_allocate_pending_table(GFP_KERNEL); if (!vpt_page) { its_vpe_id_free(vpe_id); return -ENOMEM; } if (!its_alloc_vpe_table(vpe_id)) { its_vpe_id_free(vpe_id); its_free_pending_table(vpt_page); return -ENOMEM; } raw_spin_lock_init(&vpe->vpe_lock); vpe->vpe_id = vpe_id; vpe->vpt_page = vpt_page; <S2SV_StartVul> if (gic_rdists->has_rvpeid) <S2SV_EndVul> <S2SV_StartVul> atomic_set(&vpe->vmapp_count, 0); <S2SV_EndVul> <S2SV_StartVul> else <S2SV_EndVul> vpe->vpe_proxy_event = -1; return 0; }","- if (gic_rdists->has_rvpeid)
- atomic_set(&vpe->vmapp_count, 0);
- else
+ atomic_set(&vpe->vmapp_count, 0);
+ if (!gic_rdists->has_rvpeid)","static int its_vpe_init(struct its_vpe *vpe) { struct page *vpt_page; int vpe_id; vpe_id = its_vpe_id_alloc(); if (vpe_id < 0) return vpe_id; vpt_page = its_allocate_pending_table(GFP_KERNEL); if (!vpt_page) { its_vpe_id_free(vpe_id); return -ENOMEM; } if (!its_alloc_vpe_table(vpe_id)) { its_vpe_id_free(vpe_id); its_free_pending_table(vpt_page); return -ENOMEM; } raw_spin_lock_init(&vpe->vpe_lock); vpe->vpe_id = vpe_id; vpe->vpt_page = vpt_page; atomic_set(&vpe->vmapp_count, 0); if (!gic_rdists->has_rvpeid) vpe->vpe_proxy_event = -1; return 0; }"
921----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50165/bad/inode.c----bpf_parse_param,"static int bpf_parse_param(struct fs_context *fc, struct fs_parameter *param) { struct bpf_mount_opts *opts = fc->s_fs_info; struct fs_parse_result result; kuid_t uid; kgid_t gid; int opt, err; opt = fs_parse(fc, bpf_fs_parameters, param, &result); if (opt < 0) { if (opt == -ENOPARAM) { opt = vfs_parse_fs_param_source(fc, param); if (opt != -ENOPARAM) return opt; return 0; } if (opt < 0) return opt; } switch (opt) { case OPT_UID: uid = make_kuid(current_user_ns(), result.uint_32); if (!uid_valid(uid)) goto bad_value; if (!kuid_has_mapping(fc->user_ns, uid)) goto bad_value; opts->uid = uid; break; case OPT_GID: gid = make_kgid(current_user_ns(), result.uint_32); if (!gid_valid(gid)) goto bad_value; if (!kgid_has_mapping(fc->user_ns, gid)) goto bad_value; opts->gid = gid; break; case OPT_MODE: opts->mode = result.uint_32 & S_IALLUGO; break; case OPT_DELEGATE_CMDS: case OPT_DELEGATE_MAPS: case OPT_DELEGATE_PROGS: case OPT_DELEGATE_ATTACHS: { struct bpffs_btf_enums info; const struct btf_type *enum_t; const char *enum_pfx; u64 *delegate_msk, msk = 0; <S2SV_StartVul> char *p; <S2SV_EndVul> int val; (void)find_bpffs_btf_enums(&info); switch (opt) { case OPT_DELEGATE_CMDS: delegate_msk = &opts->delegate_cmds; enum_t = info.cmd_t; enum_pfx = ""BPF_""; break; case OPT_DELEGATE_MAPS: delegate_msk = &opts->delegate_maps; enum_t = info.map_t; enum_pfx = ""BPF_MAP_TYPE_""; break; case OPT_DELEGATE_PROGS: delegate_msk = &opts->delegate_progs; enum_t = info.prog_t; enum_pfx = ""BPF_PROG_TYPE_""; break; case OPT_DELEGATE_ATTACHS: delegate_msk = &opts->delegate_attachs; enum_t = info.attach_t; enum_pfx = ""BPF_""; break; default: return -EINVAL; } <S2SV_StartVul> while ((p = strsep(&param->string, "":""))) { <S2SV_EndVul> if (strcmp(p, ""any"") == 0) { msk |= ~0ULL; } else if (find_btf_enum_const(info.btf, enum_t, enum_pfx, p, &val)) { msk |= 1ULL << val; } else { err = kstrtou64(p, 0, &msk); if (err) return err; } } if (msk && !capable(CAP_SYS_ADMIN)) return -EPERM; *delegate_msk |= msk; break; } default: break; } return 0; bad_value: return invalfc(fc, ""Bad value for '%s'"", param->key); }","- char *p;
- while ((p = strsep(&param->string, "":""))) {
+ char *p, *str;
+ str = param->string;
+ while ((p = strsep(&str, "":""))) {","static int bpf_parse_param(struct fs_context *fc, struct fs_parameter *param) { struct bpf_mount_opts *opts = fc->s_fs_info; struct fs_parse_result result; kuid_t uid; kgid_t gid; int opt, err; opt = fs_parse(fc, bpf_fs_parameters, param, &result); if (opt < 0) { if (opt == -ENOPARAM) { opt = vfs_parse_fs_param_source(fc, param); if (opt != -ENOPARAM) return opt; return 0; } if (opt < 0) return opt; } switch (opt) { case OPT_UID: uid = make_kuid(current_user_ns(), result.uint_32); if (!uid_valid(uid)) goto bad_value; if (!kuid_has_mapping(fc->user_ns, uid)) goto bad_value; opts->uid = uid; break; case OPT_GID: gid = make_kgid(current_user_ns(), result.uint_32); if (!gid_valid(gid)) goto bad_value; if (!kgid_has_mapping(fc->user_ns, gid)) goto bad_value; opts->gid = gid; break; case OPT_MODE: opts->mode = result.uint_32 & S_IALLUGO; break; case OPT_DELEGATE_CMDS: case OPT_DELEGATE_MAPS: case OPT_DELEGATE_PROGS: case OPT_DELEGATE_ATTACHS: { struct bpffs_btf_enums info; const struct btf_type *enum_t; const char *enum_pfx; u64 *delegate_msk, msk = 0; char *p, *str; int val; (void)find_bpffs_btf_enums(&info); switch (opt) { case OPT_DELEGATE_CMDS: delegate_msk = &opts->delegate_cmds; enum_t = info.cmd_t; enum_pfx = ""BPF_""; break; case OPT_DELEGATE_MAPS: delegate_msk = &opts->delegate_maps; enum_t = info.map_t; enum_pfx = ""BPF_MAP_TYPE_""; break; case OPT_DELEGATE_PROGS: delegate_msk = &opts->delegate_progs; enum_t = info.prog_t; enum_pfx = ""BPF_PROG_TYPE_""; break; case OPT_DELEGATE_ATTACHS: delegate_msk = &opts->delegate_attachs; enum_t = info.attach_t; enum_pfx = ""BPF_""; break; default: return -EINVAL; } str = param->string; while ((p = strsep(&str, "":""))) { if (strcmp(p, ""any"") == 0) { msk |= ~0ULL; } else if (find_btf_enum_const(info.btf, enum_t, enum_pfx, p, &val)) { msk |= 1ULL << val; } else { err = kstrtou64(p, 0, &msk); if (err) return err; } } if (msk && !capable(CAP_SYS_ADMIN)) return -EPERM; *delegate_msk |= msk; break; } default: break; } return 0; bad_value: return invalfc(fc, ""Bad value for '%s'"", param->key); }"
957----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50202/bad/dir.c----nilfs_find_entry,"nilfs_find_entry(struct inode *dir, const struct qstr *qstr, struct page **res_page) { const unsigned char *name = qstr->name; int namelen = qstr->len; unsigned int reclen = NILFS_DIR_REC_LEN(namelen); unsigned long start, n; unsigned long npages = dir_pages(dir); struct page *page = NULL; struct nilfs_inode_info *ei = NILFS_I(dir); struct nilfs_dir_entry *de; if (npages == 0) goto out; *res_page = NULL; start = ei->i_dir_start_lookup; if (start >= npages) start = 0; n = start; do { char *kaddr = nilfs_get_page(dir, n, &page); <S2SV_StartVul> if (!IS_ERR(kaddr)) { <S2SV_EndVul> <S2SV_StartVul> de = (struct nilfs_dir_entry *)kaddr; <S2SV_EndVul> <S2SV_StartVul> kaddr += nilfs_last_byte(dir, n) - reclen; <S2SV_EndVul> <S2SV_StartVul> while ((char *) de <= kaddr) { <S2SV_EndVul> <S2SV_StartVul> if (de->rec_len == 0) { <S2SV_EndVul> <S2SV_StartVul> nilfs_error(dir->i_sb, <S2SV_EndVul> <S2SV_StartVul> ""zero-length directory entry""); <S2SV_EndVul> <S2SV_StartVul> nilfs_put_page(page); <S2SV_EndVul> <S2SV_StartVul> goto out; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> if (nilfs_match(namelen, name, de)) <S2SV_EndVul> <S2SV_StartVul> goto found; <S2SV_EndVul> <S2SV_StartVul> de = nilfs_next_entry(de); <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> nilfs_put_page(page); <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> if (++n >= npages) n = 0; if (unlikely(n > (dir->i_blocks >> (PAGE_SHIFT - 9)))) { nilfs_error(dir->i_sb, ""dir %lu size %lld exceeds block count %llu"", dir->i_ino, dir->i_size, (unsigned long long)dir->i_blocks); goto out; } } while (n != start); out: <S2SV_StartVul> return NULL; <S2SV_EndVul> found: *res_page = page; ei->i_dir_start_lookup = n; return de; }","- if (!IS_ERR(kaddr)) {
- de = (struct nilfs_dir_entry *)kaddr;
- kaddr += nilfs_last_byte(dir, n) - reclen;
- while ((char *) de <= kaddr) {
- if (de->rec_len == 0) {
- nilfs_error(dir->i_sb,
- ""zero-length directory entry"");
- nilfs_put_page(page);
- goto out;
- }
- if (nilfs_match(namelen, name, de))
- goto found;
- de = nilfs_next_entry(de);
- }
- nilfs_put_page(page);
- }
- return NULL;
+ if (IS_ERR(kaddr))
+ return ERR_CAST(kaddr);
+ de = (struct nilfs_dir_entry *)kaddr;
+ kaddr += nilfs_last_byte(dir, n) - reclen;
+ while ((char *)de <= kaddr) {
+ if (de->rec_len == 0) {
+ nilfs_error(dir->i_sb,
+ ""zero-length directory entry"");
+ nilfs_put_page(page);
+ goto out;
+ if (nilfs_match(namelen, name, de))
+ goto found;
+ de = nilfs_next_entry(de);
+ nilfs_put_page(page);
+ return ERR_PTR(-ENOENT);","nilfs_find_entry(struct inode *dir, const struct qstr *qstr, struct page **res_page) { const unsigned char *name = qstr->name; int namelen = qstr->len; unsigned int reclen = NILFS_DIR_REC_LEN(namelen); unsigned long start, n; unsigned long npages = dir_pages(dir); struct page *page = NULL; struct nilfs_inode_info *ei = NILFS_I(dir); struct nilfs_dir_entry *de; if (npages == 0) goto out; *res_page = NULL; start = ei->i_dir_start_lookup; if (start >= npages) start = 0; n = start; do { char *kaddr = nilfs_get_page(dir, n, &page); if (IS_ERR(kaddr)) return ERR_CAST(kaddr); de = (struct nilfs_dir_entry *)kaddr; kaddr += nilfs_last_byte(dir, n) - reclen; while ((char *)de <= kaddr) { if (de->rec_len == 0) { nilfs_error(dir->i_sb, ""zero-length directory entry""); nilfs_put_page(page); goto out; } if (nilfs_match(namelen, name, de)) goto found; de = nilfs_next_entry(de); } nilfs_put_page(page); if (++n >= npages) n = 0; if (unlikely(n > (dir->i_blocks >> (PAGE_SHIFT - 9)))) { nilfs_error(dir->i_sb, ""dir %lu size %lld exceeds block count %llu"", dir->i_ino, dir->i_size, (unsigned long long)dir->i_blocks); goto out; } } while (n != start); out: return ERR_PTR(-ENOENT); found: *res_page = page; ei->i_dir_start_lookup = n; return de; }"
676----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49976/bad/trace_osnoise.c----stop_per_cpu_kthreads,static void stop_per_cpu_kthreads(void) { int cpu; <S2SV_StartVul> for_each_possible_cpu(cpu) <S2SV_EndVul> stop_kthread(cpu); },"- for_each_possible_cpu(cpu)
+ cpus_read_lock();
+ for_each_online_cpu(cpu)
+ cpus_read_unlock();",static void stop_per_cpu_kthreads(void) { int cpu; cpus_read_lock(); for_each_online_cpu(cpu) stop_kthread(cpu); cpus_read_unlock(); }
818----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50087/bad/tree-log.c----check_item_in_log,"static noinline int check_item_in_log(struct btrfs_trans_handle *trans, struct btrfs_root *log, struct btrfs_path *path, struct btrfs_path *log_path, struct inode *dir, struct btrfs_key *dir_key) { struct btrfs_root *root = BTRFS_I(dir)->root; int ret; struct extent_buffer *eb; int slot; struct btrfs_dir_item *di; <S2SV_StartVul> struct fscrypt_str name; <S2SV_EndVul> struct inode *inode = NULL; struct btrfs_key location; ASSERT(dir_key->type == BTRFS_DIR_INDEX_KEY); eb = path->nodes[0]; slot = path->slots[0]; di = btrfs_item_ptr(eb, slot, struct btrfs_dir_item); ret = read_alloc_one_name(eb, di + 1, btrfs_dir_name_len(eb, di), &name); if (ret) goto out; if (log) { struct btrfs_dir_item *log_di; log_di = btrfs_lookup_dir_index_item(trans, log, log_path, dir_key->objectid, dir_key->offset, &name, 0); if (IS_ERR(log_di)) { ret = PTR_ERR(log_di); goto out; } else if (log_di) { ret = 0; goto out; } } btrfs_dir_item_key_to_cpu(eb, di, &location); btrfs_release_path(path); btrfs_release_path(log_path); inode = read_one_inode(root, location.objectid); if (!inode) { ret = -EIO; goto out; } ret = link_to_fixup_dir(trans, root, path, location.objectid); if (ret) goto out; inc_nlink(inode); ret = unlink_inode_for_log_replay(trans, BTRFS_I(dir), BTRFS_I(inode), &name); out: btrfs_release_path(path); btrfs_release_path(log_path); kfree(name.name); iput(inode); return ret; }","- struct fscrypt_str name;
+ struct fscrypt_str name = { 0 };","static noinline int check_item_in_log(struct btrfs_trans_handle *trans, struct btrfs_root *log, struct btrfs_path *path, struct btrfs_path *log_path, struct inode *dir, struct btrfs_key *dir_key) { struct btrfs_root *root = BTRFS_I(dir)->root; int ret; struct extent_buffer *eb; int slot; struct btrfs_dir_item *di; struct fscrypt_str name = { 0 }; struct inode *inode = NULL; struct btrfs_key location; ASSERT(dir_key->type == BTRFS_DIR_INDEX_KEY); eb = path->nodes[0]; slot = path->slots[0]; di = btrfs_item_ptr(eb, slot, struct btrfs_dir_item); ret = read_alloc_one_name(eb, di + 1, btrfs_dir_name_len(eb, di), &name); if (ret) goto out; if (log) { struct btrfs_dir_item *log_di; log_di = btrfs_lookup_dir_index_item(trans, log, log_path, dir_key->objectid, dir_key->offset, &name, 0); if (IS_ERR(log_di)) { ret = PTR_ERR(log_di); goto out; } else if (log_di) { ret = 0; goto out; } } btrfs_dir_item_key_to_cpu(eb, di, &location); btrfs_release_path(path); btrfs_release_path(log_path); inode = read_one_inode(root, location.objectid); if (!inode) { ret = -EIO; goto out; } ret = link_to_fixup_dir(trans, root, path, location.objectid); if (ret) goto out; inc_nlink(inode); ret = unlink_inode_for_log_replay(trans, BTRFS_I(dir), BTRFS_I(inode), &name); out: btrfs_release_path(path); btrfs_release_path(log_path); kfree(name.name); iput(inode); return ret; }"
708----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49990/bad/xe_hdcp_gsc.c----intel_hdcp_gsc_check_status,"bool intel_hdcp_gsc_check_status(struct xe_device *xe) { struct xe_tile *tile = xe_device_get_root_tile(xe); struct xe_gt *gt = tile->media_gt; bool ret = true; <S2SV_StartVul> if (!xe_uc_fw_is_enabled(&gt->uc.gsc.fw)) <S2SV_EndVul> return false; xe_pm_runtime_get(xe); if (xe_force_wake_get(gt_to_fw(gt), XE_FW_GSC)) { drm_dbg_kms(&xe->drm, ""failed to get forcewake to check proxy status\n""); ret = false; goto out; } <S2SV_StartVul> if (!xe_gsc_proxy_init_done(&gt->uc.gsc)) <S2SV_EndVul> ret = false; xe_force_wake_put(gt_to_fw(gt), XE_FW_GSC); out: xe_pm_runtime_put(xe); return ret; }","- if (!xe_uc_fw_is_enabled(&gt->uc.gsc.fw))
- if (!xe_gsc_proxy_init_done(&gt->uc.gsc))
+ struct xe_gsc *gsc = &gt->uc.gsc;
+ if (!gsc && !xe_uc_fw_is_enabled(&gsc->fw)) {
+ drm_dbg_kms(&xe->drm,
+ ""GSC Components not ready for HDCP2.x\n"");
+ }
+ }
+ if (!xe_gsc_proxy_init_done(gsc))","bool intel_hdcp_gsc_check_status(struct xe_device *xe) { struct xe_tile *tile = xe_device_get_root_tile(xe); struct xe_gt *gt = tile->media_gt; struct xe_gsc *gsc = &gt->uc.gsc; bool ret = true; if (!gsc && !xe_uc_fw_is_enabled(&gsc->fw)) { drm_dbg_kms(&xe->drm, ""GSC Components not ready for HDCP2.x\n""); return false; } xe_pm_runtime_get(xe); if (xe_force_wake_get(gt_to_fw(gt), XE_FW_GSC)) { drm_dbg_kms(&xe->drm, ""failed to get forcewake to check proxy status\n""); ret = false; goto out; } if (!xe_gsc_proxy_init_done(gsc)) ret = false; xe_force_wake_put(gt_to_fw(gt), XE_FW_GSC); out: xe_pm_runtime_put(xe); return ret; }"
533----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49855/bad/nbd.c----nbd_xmit_timeout,"static enum blk_eh_timer_return nbd_xmit_timeout(struct request *req) { struct nbd_cmd *cmd = blk_mq_rq_to_pdu(req); struct nbd_device *nbd = cmd->nbd; struct nbd_config *config; if (!mutex_trylock(&cmd->lock)) return BLK_EH_RESET_TIMER; if (!test_bit(NBD_CMD_INFLIGHT, &cmd->flags)) { mutex_unlock(&cmd->lock); return BLK_EH_DONE; } config = nbd_get_config_unlocked(nbd); if (!config) { cmd->status = BLK_STS_TIMEOUT; __clear_bit(NBD_CMD_INFLIGHT, &cmd->flags); mutex_unlock(&cmd->lock); goto done; } if (config->num_connections > 1 || (config->num_connections == 1 && nbd->tag_set.timeout)) { dev_err_ratelimited(nbd_to_dev(nbd), ""Connection timed out, retrying (%d/%d alive)\n"", atomic_read(&config->live_connections), config->num_connections); if (config->socks) { if (cmd->index < config->num_connections) { struct nbd_sock *nsock = config->socks[cmd->index]; mutex_lock(&nsock->tx_lock); if (cmd->cookie == nsock->cookie) nbd_mark_nsock_dead(nbd, nsock, 1); mutex_unlock(&nsock->tx_lock); } <S2SV_StartVul> mutex_unlock(&cmd->lock); <S2SV_EndVul> nbd_requeue_cmd(cmd); nbd_config_put(nbd); return BLK_EH_DONE; } } if (!nbd->tag_set.timeout) { struct nbd_sock *nsock = config->socks[cmd->index]; cmd->retries++; dev_info(nbd_to_dev(nbd), ""Possible stuck request %p: control (%s@%llu,%uB). Runtime %u seconds\n"", req, nbdcmd_to_ascii(req_to_nbd_cmd_type(req)), (unsigned long long)blk_rq_pos(req) << 9, blk_rq_bytes(req), (req->timeout / HZ) * cmd->retries); mutex_lock(&nsock->tx_lock); if (cmd->cookie != nsock->cookie) { nbd_requeue_cmd(cmd); mutex_unlock(&nsock->tx_lock); mutex_unlock(&cmd->lock); nbd_config_put(nbd); return BLK_EH_DONE; } mutex_unlock(&nsock->tx_lock); mutex_unlock(&cmd->lock); nbd_config_put(nbd); return BLK_EH_RESET_TIMER; } dev_err_ratelimited(nbd_to_dev(nbd), ""Connection timed out\n""); set_bit(NBD_RT_TIMEDOUT, &config->runtime_flags); cmd->status = BLK_STS_IOERR; __clear_bit(NBD_CMD_INFLIGHT, &cmd->flags); mutex_unlock(&cmd->lock); sock_shutdown(nbd); nbd_config_put(nbd); done: blk_mq_complete_request(req); return BLK_EH_DONE; }","- mutex_unlock(&cmd->lock);
+ mutex_unlock(&cmd->lock);","static enum blk_eh_timer_return nbd_xmit_timeout(struct request *req) { struct nbd_cmd *cmd = blk_mq_rq_to_pdu(req); struct nbd_device *nbd = cmd->nbd; struct nbd_config *config; if (!mutex_trylock(&cmd->lock)) return BLK_EH_RESET_TIMER; if (!test_bit(NBD_CMD_INFLIGHT, &cmd->flags)) { mutex_unlock(&cmd->lock); return BLK_EH_DONE; } config = nbd_get_config_unlocked(nbd); if (!config) { cmd->status = BLK_STS_TIMEOUT; __clear_bit(NBD_CMD_INFLIGHT, &cmd->flags); mutex_unlock(&cmd->lock); goto done; } if (config->num_connections > 1 || (config->num_connections == 1 && nbd->tag_set.timeout)) { dev_err_ratelimited(nbd_to_dev(nbd), ""Connection timed out, retrying (%d/%d alive)\n"", atomic_read(&config->live_connections), config->num_connections); if (config->socks) { if (cmd->index < config->num_connections) { struct nbd_sock *nsock = config->socks[cmd->index]; mutex_lock(&nsock->tx_lock); if (cmd->cookie == nsock->cookie) nbd_mark_nsock_dead(nbd, nsock, 1); mutex_unlock(&nsock->tx_lock); } nbd_requeue_cmd(cmd); mutex_unlock(&cmd->lock); nbd_config_put(nbd); return BLK_EH_DONE; } } if (!nbd->tag_set.timeout) { struct nbd_sock *nsock = config->socks[cmd->index]; cmd->retries++; dev_info(nbd_to_dev(nbd), ""Possible stuck request %p: control (%s@%llu,%uB). Runtime %u seconds\n"", req, nbdcmd_to_ascii(req_to_nbd_cmd_type(req)), (unsigned long long)blk_rq_pos(req) << 9, blk_rq_bytes(req), (req->timeout / HZ) * cmd->retries); mutex_lock(&nsock->tx_lock); if (cmd->cookie != nsock->cookie) { nbd_requeue_cmd(cmd); mutex_unlock(&nsock->tx_lock); mutex_unlock(&cmd->lock); nbd_config_put(nbd); return BLK_EH_DONE; } mutex_unlock(&nsock->tx_lock); mutex_unlock(&cmd->lock); nbd_config_put(nbd); return BLK_EH_RESET_TIMER; } dev_err_ratelimited(nbd_to_dev(nbd), ""Connection timed out\n""); set_bit(NBD_RT_TIMEDOUT, &config->runtime_flags); cmd->status = BLK_STS_IOERR; __clear_bit(NBD_CMD_INFLIGHT, &cmd->flags); mutex_unlock(&cmd->lock); sock_shutdown(nbd); nbd_config_put(nbd); done: blk_mq_complete_request(req); return BLK_EH_DONE; }"
865----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50124/bad/iso.c----*iso_get_sock_listen,"static struct sock *iso_get_sock_listen(bdaddr_t *src, bdaddr_t *dst, iso_sock_match_t match, void *data); #define ISO_CONN_TIMEOUT (HZ * 40) #define ISO_DISCONN_TIMEOUT (HZ * 2) static void iso_sock_timeout(struct work_struct *work) { struct iso_conn *conn = container_of(work, struct iso_conn, timeout_work.work); struct sock *sk; iso_conn_lock(conn); <S2SV_StartVul> sk = conn->sk; <S2SV_EndVul> <S2SV_StartVul> if (sk) <S2SV_EndVul> <S2SV_StartVul> sock_hold(sk); <S2SV_EndVul> iso_conn_unlock(conn); if (!sk) return; BT_DBG(""sock %p state %d"", sk, sk->sk_state); lock_sock(sk); sk->sk_err = ETIMEDOUT; sk->sk_state_change(sk); release_sock(sk); sock_put(sk); }","- sk = conn->sk;
- if (sk)
- sock_hold(sk);
+ static struct sock *iso_sock_hold(struct iso_conn *conn)
+ {
+ if (!conn || !bt_sock_linked(&iso_sk_list, conn->sk))
+ return NULL;
+ sock_hold(conn->sk);
+ return conn->sk;
+ }","static struct sock *iso_get_sock_listen(bdaddr_t *src, bdaddr_t *dst, iso_sock_match_t match, void *data); #define ISO_CONN_TIMEOUT (HZ * 40) #define ISO_DISCONN_TIMEOUT (HZ * 2) static struct sock *iso_sock_hold(struct iso_conn *conn) { if (!conn || !bt_sock_linked(&iso_sk_list, conn->sk)) return NULL; sock_hold(conn->sk); return conn->sk; }"
1084----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53067/bad/ufshcd.c----ufshcd_device_init,"static int ufshcd_device_init(struct ufs_hba *hba, bool init_dev_params) { int ret; struct Scsi_Host *host = hba->host; hba->ufshcd_state = UFSHCD_STATE_RESET; ret = ufshcd_link_startup(hba); if (ret) return ret; if (hba->quirks & UFSHCD_QUIRK_SKIP_PH_CONFIGURATION) return ret; ufshcd_clear_dbg_ufs_stats(hba); ufshcd_set_link_active(hba); if (hba->mcq_enabled && !init_dev_params) { ufshcd_config_mcq(hba); ufshcd_mcq_enable(hba); } ret = ufshcd_verify_dev_init(hba); if (ret) return ret; ret = ufshcd_complete_dev_init(hba); if (ret) return ret; if (init_dev_params) { ret = ufshcd_device_params_init(hba); if (ret) return ret; if (is_mcq_supported(hba) && !hba->scsi_host_added) { ufshcd_mcq_enable(hba); ret = ufshcd_alloc_mcq(hba); if (!ret) { ufshcd_config_mcq(hba); } else { ufshcd_mcq_disable(hba); use_mcq_mode = false; dev_err(hba->dev, ""MCQ mode is disabled, err=%d\n"", ret); } ret = scsi_add_host(host, hba->dev); if (ret) { dev_err(hba->dev, ""scsi_add_host failed\n""); return ret; } hba->scsi_host_added = true; } else if (is_mcq_supported(hba)) { ufshcd_config_mcq(hba); ufshcd_mcq_enable(hba); } } ufshcd_tune_unipro_params(hba); ufshcd_set_ufs_dev_active(hba); ufshcd_force_reset_auto_bkops(hba); ufshcd_set_timestamp_attr(hba); <S2SV_StartVul> schedule_delayed_work(&hba->ufs_rtc_update_work, <S2SV_EndVul> <S2SV_StartVul> msecs_to_jiffies(UFS_RTC_UPDATE_INTERVAL_MS)); <S2SV_EndVul> if (hba->max_pwr_info.is_valid) { if (hba->dev_ref_clk_freq != REF_CLK_FREQ_INVAL) ufshcd_set_dev_ref_clk(hba); ret = ufshcd_config_pwr_mode(hba, &hba->max_pwr_info.info); if (ret) { dev_err(hba->dev, ""%s: Failed setting power mode, err = %d\n"", __func__, ret); return ret; } } return 0; }","- schedule_delayed_work(&hba->ufs_rtc_update_work,
- msecs_to_jiffies(UFS_RTC_UPDATE_INTERVAL_MS));","static int ufshcd_device_init(struct ufs_hba *hba, bool init_dev_params) { int ret; struct Scsi_Host *host = hba->host; hba->ufshcd_state = UFSHCD_STATE_RESET; ret = ufshcd_link_startup(hba); if (ret) return ret; if (hba->quirks & UFSHCD_QUIRK_SKIP_PH_CONFIGURATION) return ret; ufshcd_clear_dbg_ufs_stats(hba); ufshcd_set_link_active(hba); if (hba->mcq_enabled && !init_dev_params) { ufshcd_config_mcq(hba); ufshcd_mcq_enable(hba); } ret = ufshcd_verify_dev_init(hba); if (ret) return ret; ret = ufshcd_complete_dev_init(hba); if (ret) return ret; if (init_dev_params) { ret = ufshcd_device_params_init(hba); if (ret) return ret; if (is_mcq_supported(hba) && !hba->scsi_host_added) { ufshcd_mcq_enable(hba); ret = ufshcd_alloc_mcq(hba); if (!ret) { ufshcd_config_mcq(hba); } else { ufshcd_mcq_disable(hba); use_mcq_mode = false; dev_err(hba->dev, ""MCQ mode is disabled, err=%d\n"", ret); } ret = scsi_add_host(host, hba->dev); if (ret) { dev_err(hba->dev, ""scsi_add_host failed\n""); return ret; } hba->scsi_host_added = true; } else if (is_mcq_supported(hba)) { ufshcd_config_mcq(hba); ufshcd_mcq_enable(hba); } } ufshcd_tune_unipro_params(hba); ufshcd_set_ufs_dev_active(hba); ufshcd_force_reset_auto_bkops(hba); ufshcd_set_timestamp_attr(hba); if (hba->max_pwr_info.is_valid) { if (hba->dev_ref_clk_freq != REF_CLK_FREQ_INVAL) ufshcd_set_dev_ref_clk(hba); ret = ufshcd_config_pwr_mode(hba, &hba->max_pwr_info.info); if (ret) { dev_err(hba->dev, ""%s: Failed setting power mode, err = %d\n"", __func__, ret); return ret; } } return 0; }"
549----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49868/bad/relocation.c----btrfs_update_reloc_root,"int btrfs_update_reloc_root(struct btrfs_trans_handle *trans, struct btrfs_root *root) { struct btrfs_fs_info *fs_info = root->fs_info; struct btrfs_root *reloc_root; struct btrfs_root_item *root_item; int ret; if (!have_reloc_root(root)) return 0; reloc_root = root->reloc_root; root_item = &reloc_root->root_item; btrfs_grab_root(reloc_root); <S2SV_StartVul> if (fs_info->reloc_ctl->merge_reloc_tree && <S2SV_EndVul> btrfs_root_refs(root_item) == 0) { set_bit(BTRFS_ROOT_DEAD_RELOC_TREE, &root->state); smp_wmb(); __del_reloc_root(reloc_root); } if (reloc_root->commit_root != reloc_root->node) { __update_reloc_root(reloc_root); btrfs_set_root_node(root_item, reloc_root->node); free_extent_buffer(reloc_root->commit_root); reloc_root->commit_root = btrfs_root_node(reloc_root); } ret = btrfs_update_root(trans, fs_info->tree_root, &reloc_root->root_key, root_item); btrfs_put_root(reloc_root); return ret; }","- if (fs_info->reloc_ctl->merge_reloc_tree &&
+ if (fs_info->reloc_ctl && fs_info->reloc_ctl->merge_reloc_tree &&","int btrfs_update_reloc_root(struct btrfs_trans_handle *trans, struct btrfs_root *root) { struct btrfs_fs_info *fs_info = root->fs_info; struct btrfs_root *reloc_root; struct btrfs_root_item *root_item; int ret; if (!have_reloc_root(root)) return 0; reloc_root = root->reloc_root; root_item = &reloc_root->root_item; btrfs_grab_root(reloc_root); if (fs_info->reloc_ctl && fs_info->reloc_ctl->merge_reloc_tree && btrfs_root_refs(root_item) == 0) { set_bit(BTRFS_ROOT_DEAD_RELOC_TREE, &root->state); smp_wmb(); __del_reloc_root(reloc_root); } if (reloc_root->commit_root != reloc_root->node) { __update_reloc_root(reloc_root); btrfs_set_root_node(root_item, reloc_root->node); free_extent_buffer(reloc_root->commit_root); reloc_root->commit_root = btrfs_root_node(reloc_root); } ret = btrfs_update_root(trans, fs_info->tree_root, &reloc_root->root_key, root_item); btrfs_put_root(reloc_root); return ret; }"
504----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47743/bad/asymmetric_type.c----*find_asymmetric_key,"struct key *find_asymmetric_key(struct key *keyring, const struct asymmetric_key_id *id_0, const struct asymmetric_key_id *id_1, const struct asymmetric_key_id *id_2, bool partial) { struct key *key; key_ref_t ref; const char *lookup; char *req, *p; int len; <S2SV_StartVul> WARN_ON(!id_0 && !id_1 && !id_2); <S2SV_EndVul> if (id_0) { lookup = id_0->data; len = id_0->len; } else if (id_1) { lookup = id_1->data; len = id_1->len; <S2SV_StartVul> } else { <S2SV_EndVul> lookup = id_2->data; len = id_2->len; } p = req = kmalloc(2 + 1 + len * 2 + 1, GFP_KERNEL); if (!req) return ERR_PTR(-ENOMEM); if (!id_0 && !id_1) { *p++ = 'd'; *p++ = 'n'; } else if (partial) { *p++ = 'i'; *p++ = 'd'; } else { *p++ = 'e'; *p++ = 'x'; } *p++ = ':'; p = bin2hex(p, lookup, len); *p = 0; pr_debug(""Look up: \""%s\""\n"", req); ref = keyring_search(make_key_ref(keyring, 1), &key_type_asymmetric, req, true); if (IS_ERR(ref)) pr_debug(""Request for key '%s' err %ld\n"", req, PTR_ERR(ref)); kfree(req); if (IS_ERR(ref)) { switch (PTR_ERR(ref)) { case -EACCES: case -ENOTDIR: case -EAGAIN: return ERR_PTR(-ENOKEY); default: return ERR_CAST(ref); } } key = key_ref_to_ptr(ref); if (id_0 && id_1) { const struct asymmetric_key_ids *kids = asymmetric_key_ids(key); if (!kids->id[1]) { pr_debug(""First ID matches, but second is missing\n""); goto reject; } if (!asymmetric_key_id_same(id_1, kids->id[1])) { pr_debug(""First ID matches, but second does not\n""); goto reject; } } pr_devel(""<==%s() = 0 [%x]\n"", __func__, key_serial(key)); return key; reject: key_put(key); return ERR_PTR(-EKEYREJECTED); }","- WARN_ON(!id_0 && !id_1 && !id_2);
- } else {
+ } else if (id_2) {
+ } else {
+ WARN_ON(1);
+ return ERR_PTR(-EINVAL);","struct key *find_asymmetric_key(struct key *keyring, const struct asymmetric_key_id *id_0, const struct asymmetric_key_id *id_1, const struct asymmetric_key_id *id_2, bool partial) { struct key *key; key_ref_t ref; const char *lookup; char *req, *p; int len; if (id_0) { lookup = id_0->data; len = id_0->len; } else if (id_1) { lookup = id_1->data; len = id_1->len; } else if (id_2) { lookup = id_2->data; len = id_2->len; } else { WARN_ON(1); return ERR_PTR(-EINVAL); } p = req = kmalloc(2 + 1 + len * 2 + 1, GFP_KERNEL); if (!req) return ERR_PTR(-ENOMEM); if (!id_0 && !id_1) { *p++ = 'd'; *p++ = 'n'; } else if (partial) { *p++ = 'i'; *p++ = 'd'; } else { *p++ = 'e'; *p++ = 'x'; } *p++ = ':'; p = bin2hex(p, lookup, len); *p = 0; pr_debug(""Look up: \""%s\""\n"", req); ref = keyring_search(make_key_ref(keyring, 1), &key_type_asymmetric, req, true); if (IS_ERR(ref)) pr_debug(""Request for key '%s' err %ld\n"", req, PTR_ERR(ref)); kfree(req); if (IS_ERR(ref)) { switch (PTR_ERR(ref)) { case -EACCES: case -ENOTDIR: case -EAGAIN: return ERR_PTR(-ENOKEY); default: return ERR_CAST(ref); } } key = key_ref_to_ptr(ref); if (id_0 && id_1) { const struct asymmetric_key_ids *kids = asymmetric_key_ids(key); if (!kids->id[1]) { pr_debug(""First ID matches, but second is missing\n""); goto reject; } if (!asymmetric_key_id_same(id_1, kids->id[1])) { pr_debug(""First ID matches, but second does not\n""); goto reject; } } pr_devel(""<==%s() = 0 [%x]\n"", __func__, key_serial(key)); return key; reject: key_put(key); return ERR_PTR(-EKEYREJECTED); }"
1148----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53124/bad/tcp_ipv6.c----tcp_v6_do_rcv,"int tcp_v6_do_rcv(struct sock *sk, struct sk_buff *skb) { struct ipv6_pinfo *np = tcp_inet6_sk(sk); struct sk_buff *opt_skb = NULL; enum skb_drop_reason reason; struct tcp_sock *tp; if (skb->protocol == htons(ETH_P_IP)) return tcp_v4_do_rcv(sk, skb); <S2SV_StartVul> if (np->rxopt.all) <S2SV_EndVul> opt_skb = skb_clone_and_charge_r(skb, sk); reason = SKB_DROP_REASON_NOT_SPECIFIED; if (sk->sk_state == TCP_ESTABLISHED) { struct dst_entry *dst; dst = rcu_dereference_protected(sk->sk_rx_dst, lockdep_sock_is_held(sk)); sock_rps_save_rxhash(sk, skb); sk_mark_napi_id(sk, skb); if (dst) { if (sk->sk_rx_dst_ifindex != skb->skb_iif || INDIRECT_CALL_1(dst->ops->check, ip6_dst_check, dst, sk->sk_rx_dst_cookie) == NULL) { RCU_INIT_POINTER(sk->sk_rx_dst, NULL); dst_release(dst); } } tcp_rcv_established(sk, skb); if (opt_skb) goto ipv6_pktoptions; return 0; } if (tcp_checksum_complete(skb)) goto csum_err; if (sk->sk_state == TCP_LISTEN) { struct sock *nsk = tcp_v6_cookie_check(sk, skb); if (!nsk) goto discard; if (nsk != sk) { if (tcp_child_process(sk, nsk, skb)) goto reset; <S2SV_StartVul> if (opt_skb) <S2SV_EndVul> <S2SV_StartVul> __kfree_skb(opt_skb); <S2SV_EndVul> return 0; } } else sock_rps_save_rxhash(sk, skb); if (tcp_rcv_state_process(sk, skb)) goto reset; if (opt_skb) goto ipv6_pktoptions; return 0; reset: tcp_v6_send_reset(sk, skb); discard: if (opt_skb) __kfree_skb(opt_skb); kfree_skb_reason(skb, reason); return 0; csum_err: reason = SKB_DROP_REASON_TCP_CSUM; trace_tcp_bad_csum(skb); TCP_INC_STATS(sock_net(sk), TCP_MIB_CSUMERRORS); TCP_INC_STATS(sock_net(sk), TCP_MIB_INERRS); goto discard; ipv6_pktoptions: tp = tcp_sk(sk); if (TCP_SKB_CB(opt_skb)->end_seq == tp->rcv_nxt && !((1 << sk->sk_state) & (TCPF_CLOSE | TCPF_LISTEN))) { if (np->rxopt.bits.rxinfo || np->rxopt.bits.rxoinfo) np->mcast_oif = tcp_v6_iif(opt_skb); if (np->rxopt.bits.rxhlim || np->rxopt.bits.rxohlim) np->mcast_hops = ipv6_hdr(opt_skb)->hop_limit; if (np->rxopt.bits.rxflow || np->rxopt.bits.rxtclass) np->rcv_flowinfo = ip6_flowinfo(ipv6_hdr(opt_skb)); if (np->repflow) np->flow_label = ip6_flowlabel(ipv6_hdr(opt_skb)); if (ipv6_opt_accepted(sk, opt_skb, &TCP_SKB_CB(opt_skb)->header.h6)) { tcp_v6_restore_cb(opt_skb); opt_skb = xchg(&np->pktoptions, opt_skb); } else { __kfree_skb(opt_skb); opt_skb = xchg(&np->pktoptions, NULL); } } consume_skb(opt_skb); return 0; }","- if (np->rxopt.all)
- if (opt_skb)
- __kfree_skb(opt_skb);
+ if (np->rxopt.all && sk->sk_state != TCP_LISTEN)","int tcp_v6_do_rcv(struct sock *sk, struct sk_buff *skb) { struct ipv6_pinfo *np = tcp_inet6_sk(sk); struct sk_buff *opt_skb = NULL; enum skb_drop_reason reason; struct tcp_sock *tp; if (skb->protocol == htons(ETH_P_IP)) return tcp_v4_do_rcv(sk, skb); if (np->rxopt.all && sk->sk_state != TCP_LISTEN) opt_skb = skb_clone_and_charge_r(skb, sk); reason = SKB_DROP_REASON_NOT_SPECIFIED; if (sk->sk_state == TCP_ESTABLISHED) { struct dst_entry *dst; dst = rcu_dereference_protected(sk->sk_rx_dst, lockdep_sock_is_held(sk)); sock_rps_save_rxhash(sk, skb); sk_mark_napi_id(sk, skb); if (dst) { if (sk->sk_rx_dst_ifindex != skb->skb_iif || INDIRECT_CALL_1(dst->ops->check, ip6_dst_check, dst, sk->sk_rx_dst_cookie) == NULL) { RCU_INIT_POINTER(sk->sk_rx_dst, NULL); dst_release(dst); } } tcp_rcv_established(sk, skb); if (opt_skb) goto ipv6_pktoptions; return 0; } if (tcp_checksum_complete(skb)) goto csum_err; if (sk->sk_state == TCP_LISTEN) { struct sock *nsk = tcp_v6_cookie_check(sk, skb); if (!nsk) goto discard; if (nsk != sk) { if (tcp_child_process(sk, nsk, skb)) goto reset; return 0; } } else sock_rps_save_rxhash(sk, skb); if (tcp_rcv_state_process(sk, skb)) goto reset; if (opt_skb) goto ipv6_pktoptions; return 0; reset: tcp_v6_send_reset(sk, skb); discard: if (opt_skb) __kfree_skb(opt_skb); kfree_skb_reason(skb, reason); return 0; csum_err: reason = SKB_DROP_REASON_TCP_CSUM; trace_tcp_bad_csum(skb); TCP_INC_STATS(sock_net(sk), TCP_MIB_CSUMERRORS); TCP_INC_STATS(sock_net(sk), TCP_MIB_INERRS); goto discard; ipv6_pktoptions: tp = tcp_sk(sk); if (TCP_SKB_CB(opt_skb)->end_seq == tp->rcv_nxt && !((1 << sk->sk_state) & (TCPF_CLOSE | TCPF_LISTEN))) { if (np->rxopt.bits.rxinfo || np->rxopt.bits.rxoinfo) np->mcast_oif = tcp_v6_iif(opt_skb); if (np->rxopt.bits.rxhlim || np->rxopt.bits.rxohlim) np->mcast_hops = ipv6_hdr(opt_skb)->hop_limit; if (np->rxopt.bits.rxflow || np->rxopt.bits.rxtclass) np->rcv_flowinfo = ip6_flowinfo(ipv6_hdr(opt_skb)); if (np->repflow) np->flow_label = ip6_flowlabel(ipv6_hdr(opt_skb)); if (ipv6_opt_accepted(sk, opt_skb, &TCP_SKB_CB(opt_skb)->header.h6)) { tcp_v6_restore_cb(opt_skb); opt_skb = xchg(&np->pktoptions, opt_skb); } else { __kfree_skb(opt_skb); opt_skb = xchg(&np->pktoptions, NULL); } } consume_skb(opt_skb); return 0; }"
737----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50013/bad/balloc.c----exfat_load_bitmap,"int exfat_load_bitmap(struct super_block *sb) { unsigned int i, type; struct exfat_chain clu; struct exfat_sb_info *sbi = EXFAT_SB(sb); exfat_chain_set(&clu, sbi->root_dir, 0, ALLOC_FAT_CHAIN); while (clu.dir != EXFAT_EOF_CLUSTER) { for (i = 0; i < sbi->dentries_per_clu; i++) { struct exfat_dentry *ep; struct buffer_head *bh; ep = exfat_get_dentry(sb, &clu, i, &bh); if (!ep) return -EIO; type = exfat_get_entry_type(ep); <S2SV_StartVul> if (type == TYPE_UNUSED) <S2SV_EndVul> <S2SV_StartVul> break; <S2SV_EndVul> <S2SV_StartVul> if (type != TYPE_BITMAP) <S2SV_EndVul> <S2SV_StartVul> continue; <S2SV_EndVul> <S2SV_StartVul> if (ep->dentry.bitmap.flags == 0x0) { <S2SV_EndVul> int err; err = exfat_allocate_bitmap(sb, ep); brelse(bh); return err; } brelse(bh); } if (exfat_get_next_cluster(sb, &clu.dir)) return -EIO; } return -EINVAL; }","- if (type == TYPE_UNUSED)
- break;
- if (type != TYPE_BITMAP)
- continue;
- if (ep->dentry.bitmap.flags == 0x0) {
+ if (type == TYPE_BITMAP &&
+ ep->dentry.bitmap.flags == 0x0) {
+ if (type == TYPE_UNUSED)
+ return -EINVAL;","int exfat_load_bitmap(struct super_block *sb) { unsigned int i, type; struct exfat_chain clu; struct exfat_sb_info *sbi = EXFAT_SB(sb); exfat_chain_set(&clu, sbi->root_dir, 0, ALLOC_FAT_CHAIN); while (clu.dir != EXFAT_EOF_CLUSTER) { for (i = 0; i < sbi->dentries_per_clu; i++) { struct exfat_dentry *ep; struct buffer_head *bh; ep = exfat_get_dentry(sb, &clu, i, &bh); if (!ep) return -EIO; type = exfat_get_entry_type(ep); if (type == TYPE_BITMAP && ep->dentry.bitmap.flags == 0x0) { int err; err = exfat_allocate_bitmap(sb, ep); brelse(bh); return err; } brelse(bh); if (type == TYPE_UNUSED) return -EINVAL; } if (exfat_get_next_cluster(sb, &clu.dir)) return -EIO; } return -EINVAL; }"
352----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46814/bad/hdcp_msg.c----hdmi_14_process_transaction,"static bool hdmi_14_process_transaction( struct dc_link *link, struct hdcp_protection_message *message_info) { uint8_t *buff = NULL; bool result; const uint8_t hdcp_i2c_addr_link_primary = 0x3a; const uint8_t hdcp_i2c_addr_link_secondary = 0x3b; struct i2c_command i2c_command; <S2SV_StartVul> uint8_t offset = hdcp_i2c_offsets[message_info->msg_id]; <S2SV_EndVul> struct i2c_payload i2c_payloads[] = { <S2SV_StartVul> { true, 0, 1, &offset }, <S2SV_EndVul> { 0 } }; switch (message_info->link) { case HDCP_LINK_SECONDARY: i2c_payloads[0].address = hdcp_i2c_addr_link_secondary; i2c_payloads[1].address = hdcp_i2c_addr_link_secondary; break; case HDCP_LINK_PRIMARY: default: i2c_payloads[0].address = hdcp_i2c_addr_link_primary; i2c_payloads[1].address = hdcp_i2c_addr_link_primary; break; } if (hdcp_cmd_is_read[message_info->msg_id]) { i2c_payloads[1].write = false; i2c_command.number_of_payloads = ARRAY_SIZE(i2c_payloads); i2c_payloads[1].length = message_info->length; i2c_payloads[1].data = message_info->data; } else { i2c_command.number_of_payloads = 1; buff = kzalloc(message_info->length + 1, GFP_KERNEL); if (!buff) return false; buff[0] = offset; memmove(&buff[1], message_info->data, message_info->length); i2c_payloads[0].length = message_info->length + 1; i2c_payloads[0].data = buff; } i2c_command.payloads = i2c_payloads; i2c_command.engine = I2C_COMMAND_ENGINE_HW; i2c_command.speed = link->ddc->ctx->dc->caps.i2c_speed_in_khz; result = dm_helpers_submit_i2c( link->ctx, link, &i2c_command); kfree(buff); return result; }","- uint8_t offset = hdcp_i2c_offsets[message_info->msg_id];
- { true, 0, 1, &offset },
+ uint8_t offset;
+ { true, 0, 1, 0 },
+ if (message_info->msg_id == HDCP_MESSAGE_ID_INVALID) {
+ DC_LOG_ERROR(""%s: Invalid message_info msg_id - %d\n"", __func__, message_info->msg_id);
+ return false;
+ }
+ offset = hdcp_i2c_offsets[message_info->msg_id];
+ i2c_payloads[0].data = &offset;","static bool hdmi_14_process_transaction( struct dc_link *link, struct hdcp_protection_message *message_info) { uint8_t *buff = NULL; bool result; const uint8_t hdcp_i2c_addr_link_primary = 0x3a; const uint8_t hdcp_i2c_addr_link_secondary = 0x3b; struct i2c_command i2c_command; uint8_t offset; struct i2c_payload i2c_payloads[] = { { true, 0, 1, 0 }, { 0 } }; if (message_info->msg_id == HDCP_MESSAGE_ID_INVALID) { DC_LOG_ERROR(""%s: Invalid message_info msg_id - %d\n"", __func__, message_info->msg_id); return false; } offset = hdcp_i2c_offsets[message_info->msg_id]; i2c_payloads[0].data = &offset; switch (message_info->link) { case HDCP_LINK_SECONDARY: i2c_payloads[0].address = hdcp_i2c_addr_link_secondary; i2c_payloads[1].address = hdcp_i2c_addr_link_secondary; break; case HDCP_LINK_PRIMARY: default: i2c_payloads[0].address = hdcp_i2c_addr_link_primary; i2c_payloads[1].address = hdcp_i2c_addr_link_primary; break; } if (hdcp_cmd_is_read[message_info->msg_id]) { i2c_payloads[1].write = false; i2c_command.number_of_payloads = ARRAY_SIZE(i2c_payloads); i2c_payloads[1].length = message_info->length; i2c_payloads[1].data = message_info->data; } else { i2c_command.number_of_payloads = 1; buff = kzalloc(message_info->length + 1, GFP_KERNEL); if (!buff) return false; buff[0] = offset; memmove(&buff[1], message_info->data, message_info->length); i2c_payloads[0].length = message_info->length + 1; i2c_payloads[0].data = buff; } i2c_command.payloads = i2c_payloads; i2c_command.engine = I2C_COMMAND_ENGINE_HW; i2c_command.speed = link->ddc->ctx->dc->caps.i2c_speed_in_khz; result = dm_helpers_submit_i2c( link->ctx, link, &i2c_command); kfree(buff); return result; }"
1023----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50262/bad/lpm_trie.c----trie_get_next_key,"static int trie_get_next_key(struct bpf_map *map, void *_key, void *_next_key) { struct lpm_trie_node *node, *next_node = NULL, *parent, *search_root; struct lpm_trie *trie = container_of(map, struct lpm_trie, map); struct bpf_lpm_trie_key *key = _key, *next_key = _next_key; struct lpm_trie_node **node_stack = NULL; int err = 0, stack_ptr = -1; unsigned int next_bit; size_t matchlen; search_root = rcu_dereference(trie->root); if (!search_root) return -ENOENT; if (!key || key->prefixlen > trie->max_prefixlen) goto find_leftmost; <S2SV_StartVul> node_stack = kmalloc_array(trie->max_prefixlen, <S2SV_EndVul> sizeof(struct lpm_trie_node *), GFP_ATOMIC | __GFP_NOWARN); if (!node_stack) return -ENOMEM; for (node = search_root; node;) { node_stack[++stack_ptr] = node; matchlen = longest_prefix_match(trie, node, key); if (node->prefixlen != matchlen || node->prefixlen == key->prefixlen) break; next_bit = extract_bit(key->data, node->prefixlen); node = rcu_dereference(node->child[next_bit]); } if (!node || node->prefixlen != key->prefixlen || (node->flags & LPM_TREE_NODE_FLAG_IM)) goto find_leftmost; node = node_stack[stack_ptr]; while (stack_ptr > 0) { parent = node_stack[stack_ptr - 1]; if (rcu_dereference(parent->child[0]) == node) { search_root = rcu_dereference(parent->child[1]); if (search_root) goto find_leftmost; } if (!(parent->flags & LPM_TREE_NODE_FLAG_IM)) { next_node = parent; goto do_copy; } node = parent; stack_ptr--; } err = -ENOENT; goto free_stack; find_leftmost: for (node = search_root; node;) { if (node->flags & LPM_TREE_NODE_FLAG_IM) { node = rcu_dereference(node->child[0]); } else { next_node = node; node = rcu_dereference(node->child[0]); if (!node) node = rcu_dereference(next_node->child[1]); } } do_copy: next_key->prefixlen = next_node->prefixlen; memcpy((void *)next_key + offsetof(struct bpf_lpm_trie_key, data), next_node->data, trie->data_size); free_stack: kfree(node_stack); return err; }","- node_stack = kmalloc_array(trie->max_prefixlen,
+ node_stack = kmalloc_array(trie->max_prefixlen + 1,","static int trie_get_next_key(struct bpf_map *map, void *_key, void *_next_key) { struct lpm_trie_node *node, *next_node = NULL, *parent, *search_root; struct lpm_trie *trie = container_of(map, struct lpm_trie, map); struct bpf_lpm_trie_key *key = _key, *next_key = _next_key; struct lpm_trie_node **node_stack = NULL; int err = 0, stack_ptr = -1; unsigned int next_bit; size_t matchlen; search_root = rcu_dereference(trie->root); if (!search_root) return -ENOENT; if (!key || key->prefixlen > trie->max_prefixlen) goto find_leftmost; node_stack = kmalloc_array(trie->max_prefixlen + 1, sizeof(struct lpm_trie_node *), GFP_ATOMIC | __GFP_NOWARN); if (!node_stack) return -ENOMEM; for (node = search_root; node;) { node_stack[++stack_ptr] = node; matchlen = longest_prefix_match(trie, node, key); if (node->prefixlen != matchlen || node->prefixlen == key->prefixlen) break; next_bit = extract_bit(key->data, node->prefixlen); node = rcu_dereference(node->child[next_bit]); } if (!node || node->prefixlen != key->prefixlen || (node->flags & LPM_TREE_NODE_FLAG_IM)) goto find_leftmost; node = node_stack[stack_ptr]; while (stack_ptr > 0) { parent = node_stack[stack_ptr - 1]; if (rcu_dereference(parent->child[0]) == node) { search_root = rcu_dereference(parent->child[1]); if (search_root) goto find_leftmost; } if (!(parent->flags & LPM_TREE_NODE_FLAG_IM)) { next_node = parent; goto do_copy; } node = parent; stack_ptr--; } err = -ENOENT; goto free_stack; find_leftmost: for (node = search_root; node;) { if (node->flags & LPM_TREE_NODE_FLAG_IM) { node = rcu_dereference(node->child[0]); } else { next_node = node; node = rcu_dereference(node->child[0]); if (!node) node = rcu_dereference(next_node->child[1]); } } do_copy: next_key->prefixlen = next_node->prefixlen; memcpy((void *)next_key + offsetof(struct bpf_lpm_trie_key, data), next_node->data, trie->data_size); free_stack: kfree(node_stack); return err; }"
1316----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56618/bad/gpcv2.c----imx_pgc_power_up,"static int imx_pgc_power_up(struct generic_pm_domain *genpd) { struct imx_pgc_domain *domain = to_imx_pgc_domain(genpd); u32 reg_val, pgc; int ret; ret = pm_runtime_get_sync(domain->dev); if (ret < 0) { pm_runtime_put_noidle(domain->dev); return ret; } if (!IS_ERR(domain->regulator)) { ret = regulator_enable(domain->regulator); if (ret) { dev_err(domain->dev, ""failed to enable regulator: %pe\n"", ERR_PTR(ret)); goto out_put_pm; } } reset_control_assert(domain->reset); ret = clk_bulk_prepare_enable(domain->num_clks, domain->clks); if (ret) { dev_err(domain->dev, ""failed to enable reset clocks\n""); goto out_regulator_disable; } udelay(5); if (domain->bits.pxx) { regmap_update_bits(domain->regmap, domain->regs->pup, domain->bits.pxx, domain->bits.pxx); ret = regmap_read_poll_timeout(domain->regmap, domain->regs->pup, reg_val, !(reg_val & domain->bits.pxx), 0, USEC_PER_MSEC); if (ret) { dev_err(domain->dev, ""failed to command PGC\n""); goto out_clk_disable; } for_each_set_bit(pgc, &domain->pgc, 32) { regmap_clear_bits(domain->regmap, GPC_PGC_CTRL(pgc), GPC_PGC_CTRL_PCR); } } udelay(5); reset_control_deassert(domain->reset); if (domain->bits.hskreq) { regmap_update_bits(domain->regmap, domain->regs->hsk, domain->bits.hskreq, domain->bits.hskreq); regmap_read_bypassed(domain->regmap, domain->regs->hsk, &reg_val); <S2SV_StartVul> udelay(5); <S2SV_EndVul> } if (!domain->keep_clocks) clk_bulk_disable_unprepare(domain->num_clks, domain->clks); return 0; out_clk_disable: clk_bulk_disable_unprepare(domain->num_clks, domain->clks); out_regulator_disable: if (!IS_ERR(domain->regulator)) regulator_disable(domain->regulator); out_put_pm: pm_runtime_put(domain->dev); return ret; }","- udelay(5);
+ udelay(10);","static int imx_pgc_power_up(struct generic_pm_domain *genpd) { struct imx_pgc_domain *domain = to_imx_pgc_domain(genpd); u32 reg_val, pgc; int ret; ret = pm_runtime_get_sync(domain->dev); if (ret < 0) { pm_runtime_put_noidle(domain->dev); return ret; } if (!IS_ERR(domain->regulator)) { ret = regulator_enable(domain->regulator); if (ret) { dev_err(domain->dev, ""failed to enable regulator: %pe\n"", ERR_PTR(ret)); goto out_put_pm; } } reset_control_assert(domain->reset); ret = clk_bulk_prepare_enable(domain->num_clks, domain->clks); if (ret) { dev_err(domain->dev, ""failed to enable reset clocks\n""); goto out_regulator_disable; } udelay(5); if (domain->bits.pxx) { regmap_update_bits(domain->regmap, domain->regs->pup, domain->bits.pxx, domain->bits.pxx); ret = regmap_read_poll_timeout(domain->regmap, domain->regs->pup, reg_val, !(reg_val & domain->bits.pxx), 0, USEC_PER_MSEC); if (ret) { dev_err(domain->dev, ""failed to command PGC\n""); goto out_clk_disable; } for_each_set_bit(pgc, &domain->pgc, 32) { regmap_clear_bits(domain->regmap, GPC_PGC_CTRL(pgc), GPC_PGC_CTRL_PCR); } } udelay(5); reset_control_deassert(domain->reset); if (domain->bits.hskreq) { regmap_update_bits(domain->regmap, domain->regs->hsk, domain->bits.hskreq, domain->bits.hskreq); regmap_read_bypassed(domain->regmap, domain->regs->hsk, &reg_val); udelay(10); } if (!domain->keep_clocks) clk_bulk_disable_unprepare(domain->num_clks, domain->clks); return 0; out_clk_disable: clk_bulk_disable_unprepare(domain->num_clks, domain->clks); out_regulator_disable: if (!IS_ERR(domain->regulator)) regulator_disable(domain->regulator); out_put_pm: pm_runtime_put(domain->dev); return ret; }"
964----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50206/bad/mtk_eth_soc.c----mtk_init_fq_dma,"static int mtk_init_fq_dma(struct mtk_eth *eth) { const struct mtk_soc_data *soc = eth->soc; dma_addr_t phy_ring_tail; int cnt = soc->tx.fq_dma_size; dma_addr_t dma_addr; int i, j, len; if (MTK_HAS_CAPS(eth->soc->caps, MTK_SRAM)) eth->scratch_ring = eth->sram_base; else eth->scratch_ring = dma_alloc_coherent(eth->dma_dev, cnt * soc->tx.desc_size, &eth->phy_scratch_ring, GFP_KERNEL); if (unlikely(!eth->scratch_ring)) return -ENOMEM; phy_ring_tail = eth->phy_scratch_ring + soc->tx.desc_size * (cnt - 1); for (j = 0; j < DIV_ROUND_UP(soc->tx.fq_dma_size, MTK_FQ_DMA_LENGTH); j++) { len = min_t(int, cnt - j * MTK_FQ_DMA_LENGTH, MTK_FQ_DMA_LENGTH); eth->scratch_head[j] = kcalloc(len, MTK_QDMA_PAGE_SIZE, GFP_KERNEL); if (unlikely(!eth->scratch_head[j])) return -ENOMEM; dma_addr = dma_map_single(eth->dma_dev, eth->scratch_head[j], len * MTK_QDMA_PAGE_SIZE, DMA_FROM_DEVICE); if (unlikely(dma_mapping_error(eth->dma_dev, dma_addr))) return -ENOMEM; <S2SV_StartVul> for (i = 0; i < cnt; i++) { <S2SV_EndVul> struct mtk_tx_dma_v2 *txd; txd = eth->scratch_ring + (j * MTK_FQ_DMA_LENGTH + i) * soc->tx.desc_size; txd->txd1 = dma_addr + i * MTK_QDMA_PAGE_SIZE; if (j * MTK_FQ_DMA_LENGTH + i < cnt) txd->txd2 = eth->phy_scratch_ring + (j * MTK_FQ_DMA_LENGTH + i + 1) * soc->tx.desc_size; txd->txd3 = TX_DMA_PLEN0(MTK_QDMA_PAGE_SIZE); if (MTK_HAS_CAPS(soc->caps, MTK_36BIT_DMA)) txd->txd3 |= TX_DMA_PREP_ADDR64(dma_addr + i * MTK_QDMA_PAGE_SIZE); txd->txd4 = 0; if (mtk_is_netsys_v2_or_greater(eth)) { txd->txd5 = 0; txd->txd6 = 0; txd->txd7 = 0; txd->txd8 = 0; } } } mtk_w32(eth, eth->phy_scratch_ring, soc->reg_map->qdma.fq_head); mtk_w32(eth, phy_ring_tail, soc->reg_map->qdma.fq_tail); mtk_w32(eth, (cnt << 16) | cnt, soc->reg_map->qdma.fq_count); mtk_w32(eth, MTK_QDMA_PAGE_SIZE << 16, soc->reg_map->qdma.fq_blen); return 0; }","- for (i = 0; i < cnt; i++) {
+ for (i = 0; i < len; i++) {","static int mtk_init_fq_dma(struct mtk_eth *eth) { const struct mtk_soc_data *soc = eth->soc; dma_addr_t phy_ring_tail; int cnt = soc->tx.fq_dma_size; dma_addr_t dma_addr; int i, j, len; if (MTK_HAS_CAPS(eth->soc->caps, MTK_SRAM)) eth->scratch_ring = eth->sram_base; else eth->scratch_ring = dma_alloc_coherent(eth->dma_dev, cnt * soc->tx.desc_size, &eth->phy_scratch_ring, GFP_KERNEL); if (unlikely(!eth->scratch_ring)) return -ENOMEM; phy_ring_tail = eth->phy_scratch_ring + soc->tx.desc_size * (cnt - 1); for (j = 0; j < DIV_ROUND_UP(soc->tx.fq_dma_size, MTK_FQ_DMA_LENGTH); j++) { len = min_t(int, cnt - j * MTK_FQ_DMA_LENGTH, MTK_FQ_DMA_LENGTH); eth->scratch_head[j] = kcalloc(len, MTK_QDMA_PAGE_SIZE, GFP_KERNEL); if (unlikely(!eth->scratch_head[j])) return -ENOMEM; dma_addr = dma_map_single(eth->dma_dev, eth->scratch_head[j], len * MTK_QDMA_PAGE_SIZE, DMA_FROM_DEVICE); if (unlikely(dma_mapping_error(eth->dma_dev, dma_addr))) return -ENOMEM; for (i = 0; i < len; i++) { struct mtk_tx_dma_v2 *txd; txd = eth->scratch_ring + (j * MTK_FQ_DMA_LENGTH + i) * soc->tx.desc_size; txd->txd1 = dma_addr + i * MTK_QDMA_PAGE_SIZE; if (j * MTK_FQ_DMA_LENGTH + i < cnt) txd->txd2 = eth->phy_scratch_ring + (j * MTK_FQ_DMA_LENGTH + i + 1) * soc->tx.desc_size; txd->txd3 = TX_DMA_PLEN0(MTK_QDMA_PAGE_SIZE); if (MTK_HAS_CAPS(soc->caps, MTK_36BIT_DMA)) txd->txd3 |= TX_DMA_PREP_ADDR64(dma_addr + i * MTK_QDMA_PAGE_SIZE); txd->txd4 = 0; if (mtk_is_netsys_v2_or_greater(eth)) { txd->txd5 = 0; txd->txd6 = 0; txd->txd7 = 0; txd->txd8 = 0; } } } mtk_w32(eth, eth->phy_scratch_ring, soc->reg_map->qdma.fq_head); mtk_w32(eth, phy_ring_tail, soc->reg_map->qdma.fq_tail); mtk_w32(eth, (cnt << 16) | cnt, soc->reg_map->qdma.fq_count); mtk_w32(eth, MTK_QDMA_PAGE_SIZE << 16, soc->reg_map->qdma.fq_blen); return 0; }"
1217----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53228/bad/vcpu_sbi.c----kvm_riscv_vcpu_sbi_init,void kvm_riscv_vcpu_sbi_init(struct kvm_vcpu *vcpu) { struct kvm_vcpu_sbi_context *scontext = &vcpu->arch.sbi_context; const struct kvm_riscv_sbi_extension_entry *entry; const struct kvm_vcpu_sbi_extension *ext; <S2SV_StartVul> int i; <S2SV_EndVul> for (i = 0; i < ARRAY_SIZE(sbi_ext); i++) { entry = &sbi_ext[i]; ext = entry->ext_ptr; if (ext->probe && !ext->probe(vcpu)) { <S2SV_StartVul> scontext->ext_status[entry->ext_idx] = <S2SV_EndVul> <S2SV_StartVul> KVM_RISCV_SBI_EXT_STATUS_UNAVAILABLE; <S2SV_EndVul> continue; } <S2SV_StartVul> scontext->ext_status[entry->ext_idx] = ext->default_disabled ? <S2SV_EndVul> KVM_RISCV_SBI_EXT_STATUS_DISABLED : KVM_RISCV_SBI_EXT_STATUS_ENABLED; } },"- int i;
- scontext->ext_status[entry->ext_idx] =
- KVM_RISCV_SBI_EXT_STATUS_UNAVAILABLE;
- scontext->ext_status[entry->ext_idx] = ext->default_disabled ?
+ int idx, i;
+ idx = entry->ext_idx;
+ if (idx < 0 || idx >= ARRAY_SIZE(scontext->ext_status))
+ continue;
+ scontext->ext_status[idx] = KVM_RISCV_SBI_EXT_STATUS_UNAVAILABLE;
+ continue;
+ scontext->ext_status[idx] = ext->default_disabled ?","void kvm_riscv_vcpu_sbi_init(struct kvm_vcpu *vcpu) { struct kvm_vcpu_sbi_context *scontext = &vcpu->arch.sbi_context; const struct kvm_riscv_sbi_extension_entry *entry; const struct kvm_vcpu_sbi_extension *ext; int idx, i; for (i = 0; i < ARRAY_SIZE(sbi_ext); i++) { entry = &sbi_ext[i]; ext = entry->ext_ptr; idx = entry->ext_idx; if (idx < 0 || idx >= ARRAY_SIZE(scontext->ext_status)) continue; if (ext->probe && !ext->probe(vcpu)) { scontext->ext_status[idx] = KVM_RISCV_SBI_EXT_STATUS_UNAVAILABLE; continue; } scontext->ext_status[idx] = ext->default_disabled ? KVM_RISCV_SBI_EXT_STATUS_DISABLED : KVM_RISCV_SBI_EXT_STATUS_ENABLED; } }"
936----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50185/bad/protocol.c----__mptcp_move_skbs_from_subflow,"static bool __mptcp_move_skbs_from_subflow(struct mptcp_sock *msk, struct sock *ssk, unsigned int *bytes) { struct mptcp_subflow_context *subflow = mptcp_subflow_ctx(ssk); struct sock *sk = (struct sock *)msk; unsigned int moved = 0; bool more_data_avail; struct tcp_sock *tp; bool done = false; int sk_rbuf; sk_rbuf = READ_ONCE(sk->sk_rcvbuf); if (!(sk->sk_userlocks & SOCK_RCVBUF_LOCK)) { int ssk_rbuf = READ_ONCE(ssk->sk_rcvbuf); if (unlikely(ssk_rbuf > sk_rbuf)) { WRITE_ONCE(sk->sk_rcvbuf, ssk_rbuf); sk_rbuf = ssk_rbuf; } } pr_debug(""msk=%p ssk=%p\n"", msk, ssk); tp = tcp_sk(ssk); do { u32 map_remaining, offset; u32 seq = tp->copied_seq; struct sk_buff *skb; bool fin; map_remaining = subflow->map_data_len - mptcp_subflow_get_map_offset(subflow); skb = skb_peek(&ssk->sk_receive_queue); if (!skb) { if (!moved) done = true; break; } if (__mptcp_check_fallback(msk)) { map_remaining = skb->len; subflow->map_data_len = skb->len; } offset = seq - TCP_SKB_CB(skb)->seq; fin = TCP_SKB_CB(skb)->tcp_flags & TCPHDR_FIN; if (fin) { done = true; seq++; } if (offset < skb->len) { size_t len = skb->len - offset; if (tp->urg_data) done = true; if (__mptcp_move_skb(msk, ssk, skb, offset, len)) moved += len; seq += len; <S2SV_StartVul> if (WARN_ON_ONCE(map_remaining < len)) <S2SV_EndVul> <S2SV_StartVul> break; <S2SV_EndVul> } else { <S2SV_StartVul> WARN_ON_ONCE(!fin); <S2SV_EndVul> sk_eat_skb(ssk, skb); done = true; } WRITE_ONCE(tp->copied_seq, seq); more_data_avail = mptcp_subflow_data_available(ssk); if (atomic_read(&sk->sk_rmem_alloc) > sk_rbuf) { done = true; break; } } while (more_data_avail); if (moved > 0) msk->last_data_recv = tcp_jiffies32; *bytes += moved; return done; }","- if (WARN_ON_ONCE(map_remaining < len))
- break;
- WARN_ON_ONCE(!fin);
+ if (unlikely(map_remaining < len)) {
+ DEBUG_NET_WARN_ON_ONCE(1);
+ mptcp_dss_corruption(msk, ssk);
+ }
+ } else {
+ if (unlikely(!fin)) {
+ DEBUG_NET_WARN_ON_ONCE(1);
+ mptcp_dss_corruption(msk, ssk);
+ }
+ }","static bool __mptcp_move_skbs_from_subflow(struct mptcp_sock *msk, struct sock *ssk, unsigned int *bytes) { struct mptcp_subflow_context *subflow = mptcp_subflow_ctx(ssk); struct sock *sk = (struct sock *)msk; unsigned int moved = 0; bool more_data_avail; struct tcp_sock *tp; bool done = false; int sk_rbuf; sk_rbuf = READ_ONCE(sk->sk_rcvbuf); if (!(sk->sk_userlocks & SOCK_RCVBUF_LOCK)) { int ssk_rbuf = READ_ONCE(ssk->sk_rcvbuf); if (unlikely(ssk_rbuf > sk_rbuf)) { WRITE_ONCE(sk->sk_rcvbuf, ssk_rbuf); sk_rbuf = ssk_rbuf; } } pr_debug(""msk=%p ssk=%p\n"", msk, ssk); tp = tcp_sk(ssk); do { u32 map_remaining, offset; u32 seq = tp->copied_seq; struct sk_buff *skb; bool fin; map_remaining = subflow->map_data_len - mptcp_subflow_get_map_offset(subflow); skb = skb_peek(&ssk->sk_receive_queue); if (!skb) { if (!moved) done = true; break; } if (__mptcp_check_fallback(msk)) { map_remaining = skb->len; subflow->map_data_len = skb->len; } offset = seq - TCP_SKB_CB(skb)->seq; fin = TCP_SKB_CB(skb)->tcp_flags & TCPHDR_FIN; if (fin) { done = true; seq++; } if (offset < skb->len) { size_t len = skb->len - offset; if (tp->urg_data) done = true; if (__mptcp_move_skb(msk, ssk, skb, offset, len)) moved += len; seq += len; if (unlikely(map_remaining < len)) { DEBUG_NET_WARN_ON_ONCE(1); mptcp_dss_corruption(msk, ssk); } } else { if (unlikely(!fin)) { DEBUG_NET_WARN_ON_ONCE(1); mptcp_dss_corruption(msk, ssk); } sk_eat_skb(ssk, skb); done = true; } WRITE_ONCE(tp->copied_seq, seq); more_data_avail = mptcp_subflow_data_available(ssk); if (atomic_read(&sk->sk_rmem_alloc) > sk_rbuf) { done = true; break; } } while (more_data_avail); if (moved > 0) msk->last_data_recv = tcp_jiffies32; *bytes += moved; return done; }"
678----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49978/bad/udp_offload.c----*__udp_gso_segment,"struct sk_buff *__udp_gso_segment(struct sk_buff *gso_skb, netdev_features_t features, bool is_ipv6) { struct sock *sk = gso_skb->sk; unsigned int sum_truesize = 0; struct sk_buff *segs, *seg; struct udphdr *uh; unsigned int mss; bool copy_dtor; __sum16 check; __be16 newlen; mss = skb_shinfo(gso_skb)->gso_size; if (gso_skb->len <= sizeof(*uh) + mss) return ERR_PTR(-EINVAL); if (unlikely(skb_checksum_start(gso_skb) != skb_transport_header(gso_skb) && !(skb_shinfo(gso_skb)->gso_type & SKB_GSO_FRAGLIST))) return ERR_PTR(-EINVAL); if (skb_gso_ok(gso_skb, features | NETIF_F_GSO_ROBUST)) { skb_shinfo(gso_skb)->gso_segs = DIV_ROUND_UP(gso_skb->len - sizeof(*uh), mss); return NULL; } if (skb_shinfo(gso_skb)->gso_type & SKB_GSO_FRAGLIST) return __udp_gso_segment_list(gso_skb, features, is_ipv6); skb_pull(gso_skb, sizeof(*uh)); copy_dtor = gso_skb->destructor == sock_wfree; if (copy_dtor) gso_skb->destructor = NULL; segs = skb_segment(gso_skb, features); if (IS_ERR_OR_NULL(segs)) { if (copy_dtor) gso_skb->destructor = sock_wfree; return segs; <S2SV_StartVul> } <S2SV_EndVul> if (skb_is_gso(segs)) mss *= skb_shinfo(segs)->gso_segs; seg = segs; uh = udp_hdr(seg); skb_shinfo(seg)->tskey = skb_shinfo(gso_skb)->tskey; skb_shinfo(seg)->tx_flags |= (skb_shinfo(gso_skb)->tx_flags & SKBTX_ANY_TSTAMP); newlen = htons(sizeof(*uh) + mss); check = csum16_add(csum16_sub(uh->check, uh->len), newlen); for (;;) { if (copy_dtor) { seg->destructor = sock_wfree; seg->sk = sk; sum_truesize += seg->truesize; } if (!seg->next) break; uh->len = newlen; uh->check = check; if (seg->ip_summed == CHECKSUM_PARTIAL) gso_reset_checksum(seg, ~check); else uh->check = gso_make_checksum(seg, ~check) ? : CSUM_MANGLED_0; seg = seg->next; uh = udp_hdr(seg); } newlen = htons(skb_tail_pointer(seg) - skb_transport_header(seg) + seg->data_len); check = csum16_add(csum16_sub(uh->check, uh->len), newlen); uh->len = newlen; uh->check = check; if (seg->ip_summed == CHECKSUM_PARTIAL) gso_reset_checksum(seg, ~check); else uh->check = gso_make_checksum(seg, ~check) ? : CSUM_MANGLED_0; if (copy_dtor) { int delta = sum_truesize - gso_skb->truesize; if (likely(delta >= 0)) refcount_add(delta, &sk->sk_wmem_alloc); else WARN_ON_ONCE(refcount_sub_and_test(-delta, &sk->sk_wmem_alloc)); } return segs; }","- }
+ &ipv6_hdr(gso_skb)->saddr,
+ else
+ uh->check = ~udp_v4_check(gso_skb->len,
+ ip_hdr(gso_skb)->saddr,
+ ip_hdr(gso_skb)->daddr, 0);
+ skb_pull(gso_skb, sizeof(*uh));
+ if (copy_dtor)
+ gso_skb->destructor = NULL;
+ segs = skb_segment(gso_skb, features);
+ if (IS_ERR_OR_NULL(segs)) {
+ if (copy_dtor)
+ gso_skb->destructor = sock_wfree;
+ return segs;
+ }","struct sk_buff *__udp_gso_segment(struct sk_buff *gso_skb, netdev_features_t features, bool is_ipv6) { struct sock *sk = gso_skb->sk; unsigned int sum_truesize = 0; struct sk_buff *segs, *seg; struct udphdr *uh; unsigned int mss; bool copy_dtor; __sum16 check; __be16 newlen; mss = skb_shinfo(gso_skb)->gso_size; if (gso_skb->len <= sizeof(*uh) + mss) return ERR_PTR(-EINVAL); if (unlikely(skb_checksum_start(gso_skb) != skb_transport_header(gso_skb) && !(skb_shinfo(gso_skb)->gso_type & SKB_GSO_FRAGLIST))) return ERR_PTR(-EINVAL); if (skb_gso_ok(gso_skb, features | NETIF_F_GSO_ROBUST)) { skb_shinfo(gso_skb)->gso_segs = DIV_ROUND_UP(gso_skb->len - sizeof(*uh), mss); return NULL; } if (skb_shinfo(gso_skb)->gso_type & SKB_GSO_FRAGLIST) { if (skb_pagelen(gso_skb) - sizeof(*uh) == skb_shinfo(gso_skb)->gso_size) return __udp_gso_segment_list(gso_skb, features, is_ipv6); gso_skb->csum_start = skb_transport_header(gso_skb) - gso_skb->head; gso_skb->csum_offset = offsetof(struct udphdr, check); gso_skb->ip_summed = CHECKSUM_PARTIAL; uh = udp_hdr(gso_skb); if (is_ipv6) uh->check = ~udp_v6_check(gso_skb->len, &ipv6_hdr(gso_skb)->saddr, &ipv6_hdr(gso_skb)->daddr, 0); else uh->check = ~udp_v4_check(gso_skb->len, ip_hdr(gso_skb)->saddr, ip_hdr(gso_skb)->daddr, 0); } skb_pull(gso_skb, sizeof(*uh)); copy_dtor = gso_skb->destructor == sock_wfree; if (copy_dtor) gso_skb->destructor = NULL; segs = skb_segment(gso_skb, features); if (IS_ERR_OR_NULL(segs)) { if (copy_dtor) gso_skb->destructor = sock_wfree; return segs; } if (skb_is_gso(segs)) mss *= skb_shinfo(segs)->gso_segs; seg = segs; uh = udp_hdr(seg); skb_shinfo(seg)->tskey = skb_shinfo(gso_skb)->tskey; skb_shinfo(seg)->tx_flags |= (skb_shinfo(gso_skb)->tx_flags & SKBTX_ANY_TSTAMP); newlen = htons(sizeof(*uh) + mss); check = csum16_add(csum16_sub(uh->check, uh->len), newlen); for (;;) { if (copy_dtor) { seg->destructor = sock_wfree; seg->sk = sk; sum_truesize += seg->truesize; } if (!seg->next) break; uh->len = newlen; uh->check = check; if (seg->ip_summed == CHECKSUM_PARTIAL) gso_reset_checksum(seg, ~check); else uh->check = gso_make_checksum(seg, ~check) ? : CSUM_MANGLED_0; seg = seg->next; uh = udp_hdr(seg); } newlen = htons(skb_tail_pointer(seg) - skb_transport_header(seg) + seg->data_len); check = csum16_add(csum16_sub(uh->check, uh->len), newlen); uh->len = newlen; uh->check = check; if (seg->ip_summed == CHECKSUM_PARTIAL) gso_reset_checksum(seg, ~check); else uh->check = gso_make_checksum(seg, ~check) ? : CSUM_MANGLED_0; if (copy_dtor) { int delta = sum_truesize - gso_skb->truesize; if (likely(delta >= 0)) refcount_add(delta, &sk->sk_wmem_alloc); else WARN_ON_ONCE(refcount_sub_and_test(-delta, &sk->sk_wmem_alloc)); } return segs; }"
210----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46693/bad/pmic_glink_altmode.c----pmic_glink_altmode_probe,"static int pmic_glink_altmode_probe(struct auxiliary_device *adev, const struct auxiliary_device_id *id) { struct pmic_glink_altmode_port *alt_port; struct pmic_glink_altmode *altmode; const struct of_device_id *match; struct fwnode_handle *fwnode; struct device *dev = &adev->dev; u32 port; int ret; altmode = devm_kzalloc(dev, sizeof(*altmode), GFP_KERNEL); if (!altmode) return -ENOMEM; altmode->dev = dev; match = of_match_device(pmic_glink_altmode_of_quirks, dev->parent); if (match) altmode->owner_id = (unsigned long)match->data; else altmode->owner_id = PMIC_GLINK_OWNER_USBC_PAN; INIT_WORK(&altmode->enable_work, pmic_glink_altmode_enable_worker); init_completion(&altmode->pan_ack); mutex_init(&altmode->lock); device_for_each_child_node(dev, fwnode) { ret = fwnode_property_read_u32(fwnode, ""reg"", &port); if (ret < 0) { dev_err(dev, ""missing reg property of %pOFn\n"", fwnode); fwnode_handle_put(fwnode); return ret; } if (port >= ARRAY_SIZE(altmode->ports)) { dev_warn(dev, ""invalid connector number, ignoring\n""); continue; } if (altmode->ports[port].altmode) { dev_err(dev, ""multiple connector definition for port %u\n"", port); fwnode_handle_put(fwnode); return -EINVAL; } alt_port = &altmode->ports[port]; alt_port->altmode = altmode; alt_port->index = port; INIT_WORK(&alt_port->work, pmic_glink_altmode_worker); alt_port->bridge = devm_drm_dp_hpd_bridge_alloc(dev, to_of_node(fwnode)); if (IS_ERR(alt_port->bridge)) { fwnode_handle_put(fwnode); return PTR_ERR(alt_port->bridge); } alt_port->dp_alt.svid = USB_TYPEC_DP_SID; alt_port->dp_alt.mode = USB_TYPEC_DP_MODE; alt_port->dp_alt.active = 1; alt_port->typec_mux = fwnode_typec_mux_get(fwnode); if (IS_ERR(alt_port->typec_mux)) { fwnode_handle_put(fwnode); return dev_err_probe(dev, PTR_ERR(alt_port->typec_mux), ""failed to acquire mode-switch for port: %d\n"", port); } ret = devm_add_action_or_reset(dev, pmic_glink_altmode_put_mux, alt_port->typec_mux); if (ret) { fwnode_handle_put(fwnode); return ret; } alt_port->typec_retimer = fwnode_typec_retimer_get(fwnode); if (IS_ERR(alt_port->typec_retimer)) { fwnode_handle_put(fwnode); return dev_err_probe(dev, PTR_ERR(alt_port->typec_retimer), ""failed to acquire retimer-switch for port: %d\n"", port); } ret = devm_add_action_or_reset(dev, pmic_glink_altmode_put_retimer, alt_port->typec_retimer); if (ret) { fwnode_handle_put(fwnode); return ret; } alt_port->typec_switch = fwnode_typec_switch_get(fwnode); if (IS_ERR(alt_port->typec_switch)) { fwnode_handle_put(fwnode); return dev_err_probe(dev, PTR_ERR(alt_port->typec_switch), ""failed to acquire orientation-switch for port: %d\n"", port); } ret = devm_add_action_or_reset(dev, pmic_glink_altmode_put_switch, alt_port->typec_switch); if (ret) { fwnode_handle_put(fwnode); return ret; } } for (port = 0; port < ARRAY_SIZE(altmode->ports); port++) { alt_port = &altmode->ports[port]; if (!alt_port->bridge) continue; ret = devm_drm_dp_hpd_bridge_add(dev, alt_port->bridge); if (ret) return ret; } <S2SV_StartVul> altmode->client = devm_pmic_glink_register_client(dev, <S2SV_EndVul> <S2SV_StartVul> altmode->owner_id, <S2SV_EndVul> <S2SV_StartVul> pmic_glink_altmode_callback, <S2SV_EndVul> <S2SV_StartVul> pmic_glink_altmode_pdr_notify, <S2SV_EndVul> <S2SV_StartVul> altmode); <S2SV_EndVul> <S2SV_StartVul> return PTR_ERR_OR_ZERO(altmode->client); <S2SV_EndVul> }","- altmode->client = devm_pmic_glink_register_client(dev,
- altmode->owner_id,
- pmic_glink_altmode_callback,
- pmic_glink_altmode_pdr_notify,
- altmode);
- return PTR_ERR_OR_ZERO(altmode->client);
+ altmode->client = devm_pmic_glink_client_alloc(dev,
+ altmode->owner_id,
+ pmic_glink_altmode_callback,
+ pmic_glink_altmode_pdr_notify,
+ altmode);
+ if (IS_ERR(altmode->client))
+ return PTR_ERR(altmode->client);
+ pmic_glink_client_register(altmode->client);
+ return 0;","static int pmic_glink_altmode_probe(struct auxiliary_device *adev, const struct auxiliary_device_id *id) { struct pmic_glink_altmode_port *alt_port; struct pmic_glink_altmode *altmode; const struct of_device_id *match; struct fwnode_handle *fwnode; struct device *dev = &adev->dev; u32 port; int ret; altmode = devm_kzalloc(dev, sizeof(*altmode), GFP_KERNEL); if (!altmode) return -ENOMEM; altmode->dev = dev; match = of_match_device(pmic_glink_altmode_of_quirks, dev->parent); if (match) altmode->owner_id = (unsigned long)match->data; else altmode->owner_id = PMIC_GLINK_OWNER_USBC_PAN; INIT_WORK(&altmode->enable_work, pmic_glink_altmode_enable_worker); init_completion(&altmode->pan_ack); mutex_init(&altmode->lock); device_for_each_child_node(dev, fwnode) { ret = fwnode_property_read_u32(fwnode, ""reg"", &port); if (ret < 0) { dev_err(dev, ""missing reg property of %pOFn\n"", fwnode); fwnode_handle_put(fwnode); return ret; } if (port >= ARRAY_SIZE(altmode->ports)) { dev_warn(dev, ""invalid connector number, ignoring\n""); continue; } if (altmode->ports[port].altmode) { dev_err(dev, ""multiple connector definition for port %u\n"", port); fwnode_handle_put(fwnode); return -EINVAL; } alt_port = &altmode->ports[port]; alt_port->altmode = altmode; alt_port->index = port; INIT_WORK(&alt_port->work, pmic_glink_altmode_worker); alt_port->bridge = devm_drm_dp_hpd_bridge_alloc(dev, to_of_node(fwnode)); if (IS_ERR(alt_port->bridge)) { fwnode_handle_put(fwnode); return PTR_ERR(alt_port->bridge); } alt_port->dp_alt.svid = USB_TYPEC_DP_SID; alt_port->dp_alt.mode = USB_TYPEC_DP_MODE; alt_port->dp_alt.active = 1; alt_port->typec_mux = fwnode_typec_mux_get(fwnode); if (IS_ERR(alt_port->typec_mux)) { fwnode_handle_put(fwnode); return dev_err_probe(dev, PTR_ERR(alt_port->typec_mux), ""failed to acquire mode-switch for port: %d\n"", port); } ret = devm_add_action_or_reset(dev, pmic_glink_altmode_put_mux, alt_port->typec_mux); if (ret) { fwnode_handle_put(fwnode); return ret; } alt_port->typec_retimer = fwnode_typec_retimer_get(fwnode); if (IS_ERR(alt_port->typec_retimer)) { fwnode_handle_put(fwnode); return dev_err_probe(dev, PTR_ERR(alt_port->typec_retimer), ""failed to acquire retimer-switch for port: %d\n"", port); } ret = devm_add_action_or_reset(dev, pmic_glink_altmode_put_retimer, alt_port->typec_retimer); if (ret) { fwnode_handle_put(fwnode); return ret; } alt_port->typec_switch = fwnode_typec_switch_get(fwnode); if (IS_ERR(alt_port->typec_switch)) { fwnode_handle_put(fwnode); return dev_err_probe(dev, PTR_ERR(alt_port->typec_switch), ""failed to acquire orientation-switch for port: %d\n"", port); } ret = devm_add_action_or_reset(dev, pmic_glink_altmode_put_switch, alt_port->typec_switch); if (ret) { fwnode_handle_put(fwnode); return ret; } } for (port = 0; port < ARRAY_SIZE(altmode->ports); port++) { alt_port = &altmode->ports[port]; if (!alt_port->bridge) continue; ret = devm_drm_dp_hpd_bridge_add(dev, alt_port->bridge); if (ret) return ret; } altmode->client = devm_pmic_glink_client_alloc(dev, altmode->owner_id, pmic_glink_altmode_callback, pmic_glink_altmode_pdr_notify, altmode); if (IS_ERR(altmode->client)) return PTR_ERR(altmode->client); pmic_glink_client_register(altmode->client); return 0; }"
831----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50097/bad/fec_main.c----fec_stop,"fec_stop(struct net_device *ndev) { struct fec_enet_private *fep = netdev_priv(ndev); u32 rmii_mode = readl(fep->hwp + FEC_R_CNTRL) & FEC_RCR_RMII; u32 val; if (fep->link) { writel(1, fep->hwp + FEC_X_CNTRL); udelay(10); if (!(readl(fep->hwp + FEC_IEVENT) & FEC_ENET_GRA)) netdev_err(ndev, ""Graceful transmit stop did not complete!\n""); } <S2SV_StartVul> fec_ptp_save_state(fep); <S2SV_EndVul> if (!(fep->wol_flag & FEC_WOL_FLAG_SLEEP_ON)) { if (fep->quirks & FEC_QUIRK_HAS_MULTI_QUEUES) { writel(0, fep->hwp + FEC_ECNTRL); } else { writel(FEC_ECR_RESET, fep->hwp + FEC_ECNTRL); udelay(10); } } else { val = readl(fep->hwp + FEC_ECNTRL); val |= (FEC_ECR_MAGICEN | FEC_ECR_SLEEP); writel(val, fep->hwp + FEC_ECNTRL); } writel(fep->phy_speed, fep->hwp + FEC_MII_SPEED); writel(FEC_DEFAULT_IMASK, fep->hwp + FEC_IMASK); if (fep->quirks & FEC_QUIRK_ENET_MAC && !(fep->wol_flag & FEC_WOL_FLAG_SLEEP_ON)) { writel(FEC_ECR_ETHEREN, fep->hwp + FEC_ECNTRL); writel(rmii_mode, fep->hwp + FEC_R_CNTRL); } if (fep->bufdesc_ex) { val = readl(fep->hwp + FEC_ECNTRL); val |= FEC_ECR_EN1588; writel(val, fep->hwp + FEC_ECNTRL); fec_ptp_start_cyclecounter(ndev); fec_ptp_restore_state(fep); } }","- fec_ptp_save_state(fep);
+ if (fep->bufdesc_ex)
+ fec_ptp_save_state(fep);","fec_stop(struct net_device *ndev) { struct fec_enet_private *fep = netdev_priv(ndev); u32 rmii_mode = readl(fep->hwp + FEC_R_CNTRL) & FEC_RCR_RMII; u32 val; if (fep->link) { writel(1, fep->hwp + FEC_X_CNTRL); udelay(10); if (!(readl(fep->hwp + FEC_IEVENT) & FEC_ENET_GRA)) netdev_err(ndev, ""Graceful transmit stop did not complete!\n""); } if (fep->bufdesc_ex) fec_ptp_save_state(fep); if (!(fep->wol_flag & FEC_WOL_FLAG_SLEEP_ON)) { if (fep->quirks & FEC_QUIRK_HAS_MULTI_QUEUES) { writel(0, fep->hwp + FEC_ECNTRL); } else { writel(FEC_ECR_RESET, fep->hwp + FEC_ECNTRL); udelay(10); } } else { val = readl(fep->hwp + FEC_ECNTRL); val |= (FEC_ECR_MAGICEN | FEC_ECR_SLEEP); writel(val, fep->hwp + FEC_ECNTRL); } writel(fep->phy_speed, fep->hwp + FEC_MII_SPEED); writel(FEC_DEFAULT_IMASK, fep->hwp + FEC_IMASK); if (fep->quirks & FEC_QUIRK_ENET_MAC && !(fep->wol_flag & FEC_WOL_FLAG_SLEEP_ON)) { writel(FEC_ECR_ETHEREN, fep->hwp + FEC_ECNTRL); writel(rmii_mode, fep->hwp + FEC_R_CNTRL); } if (fep->bufdesc_ex) { val = readl(fep->hwp + FEC_ECNTRL); val |= FEC_ECR_EN1588; writel(val, fep->hwp + FEC_ECNTRL); fec_ptp_start_cyclecounter(ndev); fec_ptp_restore_state(fep); } }"
1311----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56614/bad/xskmap.c----xsk_map_delete_elem,"static long xsk_map_delete_elem(struct bpf_map *map, void *key) { struct xsk_map *m = container_of(map, struct xsk_map, map); struct xdp_sock __rcu **map_entry; struct xdp_sock *old_xs; <S2SV_StartVul> int k = *(u32 *)key; <S2SV_EndVul> if (k >= map->max_entries) return -EINVAL; spin_lock_bh(&m->lock); map_entry = &m->xsk_map[k]; old_xs = unrcu_pointer(xchg(map_entry, NULL)); if (old_xs) xsk_map_sock_delete(old_xs, map_entry); spin_unlock_bh(&m->lock); return 0; }","- int k = *(u32 *)key;
+ u32 k = *(u32 *)key;","static long xsk_map_delete_elem(struct bpf_map *map, void *key) { struct xsk_map *m = container_of(map, struct xsk_map, map); struct xdp_sock __rcu **map_entry; struct xdp_sock *old_xs; u32 k = *(u32 *)key; if (k >= map->max_entries) return -EINVAL; spin_lock_bh(&m->lock); map_entry = &m->xsk_map[k]; old_xs = unrcu_pointer(xchg(map_entry, NULL)); if (old_xs) xsk_map_sock_delete(old_xs, map_entry); spin_unlock_bh(&m->lock); return 0; }"
1201----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53207/bad/mgmt.c----set_ssp_complete,"static void set_ssp_complete(struct hci_dev *hdev, void *data, int err) { struct cmd_lookup match = { NULL, hdev }; struct mgmt_pending_cmd *cmd = data; struct mgmt_mode *cp = cmd->param; u8 enable = cp->val; bool changed; <S2SV_StartVul> if (cmd != pending_find(MGMT_OP_SET_SSP, hdev)) <S2SV_EndVul> return; if (err) { u8 mgmt_err = mgmt_status(err); if (enable && hci_dev_test_and_clear_flag(hdev, HCI_SSP_ENABLED)) { new_settings(hdev, NULL); } mgmt_pending_foreach(MGMT_OP_SET_SSP, hdev, cmd_status_rsp, &mgmt_err); return; } if (enable) { changed = !hci_dev_test_and_set_flag(hdev, HCI_SSP_ENABLED); } else { changed = hci_dev_test_and_clear_flag(hdev, HCI_SSP_ENABLED); } mgmt_pending_foreach(MGMT_OP_SET_SSP, hdev, settings_rsp, &match); if (changed) new_settings(hdev, match.sk); if (match.sk) sock_put(match.sk); hci_update_eir_sync(hdev); }","- if (cmd != pending_find(MGMT_OP_SET_SSP, hdev))
+ if (err == -ECANCELED || cmd != pending_find(MGMT_OP_SET_SSP, hdev))
+ return;","static void set_ssp_complete(struct hci_dev *hdev, void *data, int err) { struct cmd_lookup match = { NULL, hdev }; struct mgmt_pending_cmd *cmd = data; struct mgmt_mode *cp = cmd->param; u8 enable = cp->val; bool changed; if (err == -ECANCELED || cmd != pending_find(MGMT_OP_SET_SSP, hdev)) return; if (err) { u8 mgmt_err = mgmt_status(err); if (enable && hci_dev_test_and_clear_flag(hdev, HCI_SSP_ENABLED)) { new_settings(hdev, NULL); } mgmt_pending_foreach(MGMT_OP_SET_SSP, hdev, cmd_status_rsp, &mgmt_err); return; } if (enable) { changed = !hci_dev_test_and_set_flag(hdev, HCI_SSP_ENABLED); } else { changed = hci_dev_test_and_clear_flag(hdev, HCI_SSP_ENABLED); } mgmt_pending_foreach(MGMT_OP_SET_SSP, hdev, settings_rsp, &match); if (changed) new_settings(hdev, match.sk); if (match.sk) sock_put(match.sk); hci_update_eir_sync(hdev); }"
522----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47809/bad/lock.c----validate_lock_args,"static int validate_lock_args(struct dlm_ls *ls, struct dlm_lkb *lkb, struct dlm_args *args) { int rv = -EBUSY; if (args->flags & DLM_LKF_CONVERT) { if (lkb->lkb_status != DLM_LKSTS_GRANTED) goto out; if (lkb->lkb_wait_type || atomic_read(&lkb->lkb_wait_count)) goto out; if (is_overlap(lkb)) goto out; rv = -EINVAL; if (test_bit(DLM_IFL_MSTCPY_BIT, &lkb->lkb_iflags)) goto out; if (args->flags & DLM_LKF_QUECVT && !__quecvt_compat_matrix[lkb->lkb_grmode+1][args->mode+1]) goto out; } lkb->lkb_exflags = args->flags; dlm_set_sbflags_val(lkb, 0); lkb->lkb_astfn = args->astfn; lkb->lkb_astparam = args->astparam; lkb->lkb_bastfn = args->bastfn; lkb->lkb_rqmode = args->mode; lkb->lkb_lksb = args->lksb; lkb->lkb_lvbptr = args->lksb->sb_lvbptr; lkb->lkb_ownpid = (int) current->pid; rv = 0; out: switch (rv) { case 0: break; case -EINVAL: WARN_ON(1); <S2SV_StartVul> log_error(ls, ""%s %d %x %x %x %d %d %s"", __func__, <S2SV_EndVul> rv, lkb->lkb_id, dlm_iflags_val(lkb), args->flags, <S2SV_StartVul> lkb->lkb_status, lkb->lkb_wait_type, <S2SV_EndVul> <S2SV_StartVul> lkb->lkb_resource->res_name); <S2SV_EndVul> break; default: <S2SV_StartVul> log_debug(ls, ""%s %d %x %x %x %d %d %s"", __func__, <S2SV_EndVul> rv, lkb->lkb_id, dlm_iflags_val(lkb), args->flags, <S2SV_StartVul> lkb->lkb_status, lkb->lkb_wait_type, <S2SV_EndVul> <S2SV_StartVul> lkb->lkb_resource->res_name); <S2SV_EndVul> break; } return rv; }","- log_error(ls, ""%s %d %x %x %x %d %d %s"", __func__,
- lkb->lkb_status, lkb->lkb_wait_type,
- lkb->lkb_resource->res_name);
- log_debug(ls, ""%s %d %x %x %x %d %d %s"", __func__,
- lkb->lkb_status, lkb->lkb_wait_type,
- lkb->lkb_resource->res_name);
+ log_error(ls, ""%s %d %x %x %x %d %d"", __func__,
+ lkb->lkb_status, lkb->lkb_wait_type);
+ log_debug(ls, ""%s %d %x %x %x %d %d"", __func__,
+ lkb->lkb_status, lkb->lkb_wait_type);","static int validate_lock_args(struct dlm_ls *ls, struct dlm_lkb *lkb, struct dlm_args *args) { int rv = -EBUSY; if (args->flags & DLM_LKF_CONVERT) { if (lkb->lkb_status != DLM_LKSTS_GRANTED) goto out; if (lkb->lkb_wait_type || atomic_read(&lkb->lkb_wait_count)) goto out; if (is_overlap(lkb)) goto out; rv = -EINVAL; if (test_bit(DLM_IFL_MSTCPY_BIT, &lkb->lkb_iflags)) goto out; if (args->flags & DLM_LKF_QUECVT && !__quecvt_compat_matrix[lkb->lkb_grmode+1][args->mode+1]) goto out; } lkb->lkb_exflags = args->flags; dlm_set_sbflags_val(lkb, 0); lkb->lkb_astfn = args->astfn; lkb->lkb_astparam = args->astparam; lkb->lkb_bastfn = args->bastfn; lkb->lkb_rqmode = args->mode; lkb->lkb_lksb = args->lksb; lkb->lkb_lvbptr = args->lksb->sb_lvbptr; lkb->lkb_ownpid = (int) current->pid; rv = 0; out: switch (rv) { case 0: break; case -EINVAL: WARN_ON(1); log_error(ls, ""%s %d %x %x %x %d %d"", __func__, rv, lkb->lkb_id, dlm_iflags_val(lkb), args->flags, lkb->lkb_status, lkb->lkb_wait_type); break; default: log_debug(ls, ""%s %d %x %x %x %d %d"", __func__, rv, lkb->lkb_id, dlm_iflags_val(lkb), args->flags, lkb->lkb_status, lkb->lkb_wait_type); break; } return rv; }"
1515----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2025-21672/bad/addr_prefs.c----afs_proc_addr_prefs_write,"int afs_proc_addr_prefs_write(struct file *file, char *buf, size_t size) { struct afs_addr_preference_list *preflist, *old; struct seq_file *m = file->private_data; struct afs_net *net = afs_seq2net_single(m); size_t psize; char *argv[5]; int ret, argc, max_prefs; inode_lock(file_inode(file)); old = rcu_dereference_protected(net->address_prefs, lockdep_is_held(&file_inode(file)->i_rwsem)); if (old) max_prefs = old->nr + 1; else max_prefs = 1; psize = struct_size(old, prefs, max_prefs); psize = roundup_pow_of_two(psize); max_prefs = min_t(size_t, (psize - sizeof(*old)) / sizeof(old->prefs[0]), 255); ret = -ENOMEM; preflist = kmalloc(struct_size(preflist, prefs, max_prefs), GFP_KERNEL); if (!preflist) goto done; if (old) memcpy(preflist, old, struct_size(preflist, prefs, old->nr)); else memset(preflist, 0, sizeof(*preflist)); preflist->max_prefs = max_prefs; do { argc = afs_split_string(&buf, argv, ARRAY_SIZE(argv)); <S2SV_StartVul> if (argc < 0) <S2SV_EndVul> <S2SV_StartVul> return argc; <S2SV_EndVul> if (argc < 2) goto inval; if (strcmp(argv[0], ""add"") == 0) ret = afs_add_address_pref(net, &preflist, argc - 1, argv + 1); else if (strcmp(argv[0], ""del"") == 0) ret = afs_del_address_pref(net, &preflist, argc - 1, argv + 1); else goto inval; if (ret < 0) goto done; } while (*buf); preflist->version++; rcu_assign_pointer(net->address_prefs, preflist); smp_store_release(&net->address_pref_version, preflist->version); kfree_rcu(old, rcu); preflist = NULL; ret = 0; done: kfree(preflist); inode_unlock(file_inode(file)); _leave("" = %d"", ret); return ret; inval: pr_warn(""Invalid Command\n""); ret = -EINVAL; goto done; }","- if (argc < 0)
- return argc;
+ if (argc < 0) {
+ ret = argc;
+ goto done;
+ }","int afs_proc_addr_prefs_write(struct file *file, char *buf, size_t size) { struct afs_addr_preference_list *preflist, *old; struct seq_file *m = file->private_data; struct afs_net *net = afs_seq2net_single(m); size_t psize; char *argv[5]; int ret, argc, max_prefs; inode_lock(file_inode(file)); old = rcu_dereference_protected(net->address_prefs, lockdep_is_held(&file_inode(file)->i_rwsem)); if (old) max_prefs = old->nr + 1; else max_prefs = 1; psize = struct_size(old, prefs, max_prefs); psize = roundup_pow_of_two(psize); max_prefs = min_t(size_t, (psize - sizeof(*old)) / sizeof(old->prefs[0]), 255); ret = -ENOMEM; preflist = kmalloc(struct_size(preflist, prefs, max_prefs), GFP_KERNEL); if (!preflist) goto done; if (old) memcpy(preflist, old, struct_size(preflist, prefs, old->nr)); else memset(preflist, 0, sizeof(*preflist)); preflist->max_prefs = max_prefs; do { argc = afs_split_string(&buf, argv, ARRAY_SIZE(argv)); if (argc < 0) { ret = argc; goto done; } if (argc < 2) goto inval; if (strcmp(argv[0], ""add"") == 0) ret = afs_add_address_pref(net, &preflist, argc - 1, argv + 1); else if (strcmp(argv[0], ""del"") == 0) ret = afs_del_address_pref(net, &preflist, argc - 1, argv + 1); else goto inval; if (ret < 0) goto done; } while (*buf); preflist->version++; rcu_assign_pointer(net->address_prefs, preflist); smp_store_release(&net->address_pref_version, preflist->version); kfree_rcu(old, rcu); preflist = NULL; ret = 0; done: kfree(preflist); inode_unlock(file_inode(file)); _leave("" = %d"", ret); return ret; inval: pr_warn(""Invalid Command\n""); ret = -EINVAL; goto done; }"
49----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44932/bad/idpf_txrx.c----idpf_vport_intr_rel,"void idpf_vport_intr_rel(struct idpf_vport *vport) { int i, j, v_idx; for (v_idx = 0; v_idx < vport->num_q_vectors; v_idx++) { struct idpf_q_vector *q_vector = &vport->q_vectors[v_idx]; kfree(q_vector->complq); q_vector->complq = NULL; kfree(q_vector->bufq); q_vector->bufq = NULL; kfree(q_vector->tx); q_vector->tx = NULL; kfree(q_vector->rx); q_vector->rx = NULL; free_cpumask_var(q_vector->affinity_mask); <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> for (i = 0; i < vport->num_rxq_grp; i++) { <S2SV_EndVul> <S2SV_StartVul> struct idpf_rxq_group *rx_qgrp = &vport->rxq_grps[i]; <S2SV_EndVul> <S2SV_StartVul> if (idpf_is_queue_model_split(vport->rxq_model)) <S2SV_EndVul> <S2SV_StartVul> for (j = 0; j < rx_qgrp->splitq.num_rxq_sets; j++) <S2SV_EndVul> <S2SV_StartVul> rx_qgrp->splitq.rxq_sets[j]->rxq.q_vector = NULL; <S2SV_EndVul> <S2SV_StartVul> else <S2SV_EndVul> <S2SV_StartVul> for (j = 0; j < rx_qgrp->singleq.num_rxq; j++) <S2SV_EndVul> <S2SV_StartVul> rx_qgrp->singleq.rxqs[j]->q_vector = NULL; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> if (idpf_is_queue_model_split(vport->txq_model)) <S2SV_EndVul> <S2SV_StartVul> for (i = 0; i < vport->num_txq_grp; i++) <S2SV_EndVul> <S2SV_StartVul> vport->txq_grps[i].complq->q_vector = NULL; <S2SV_EndVul> <S2SV_StartVul> else <S2SV_EndVul> <S2SV_StartVul> for (i = 0; i < vport->num_txq_grp; i++) <S2SV_EndVul> <S2SV_StartVul> for (j = 0; j < vport->txq_grps[i].num_txq; j++) <S2SV_EndVul> <S2SV_StartVul> vport->txq_grps[i].txqs[j]->q_vector = NULL; <S2SV_EndVul> kfree(vport->q_vectors); vport->q_vectors = NULL; <S2SV_StartVul> } <S2SV_EndVul>","- }
- for (i = 0; i < vport->num_rxq_grp; i++) {
- struct idpf_rxq_group *rx_qgrp = &vport->rxq_grps[i];
- if (idpf_is_queue_model_split(vport->rxq_model))
- for (j = 0; j < rx_qgrp->splitq.num_rxq_sets; j++)
- rx_qgrp->splitq.rxq_sets[j]->rxq.q_vector = NULL;
- else
- for (j = 0; j < rx_qgrp->singleq.num_rxq; j++)
- rx_qgrp->singleq.rxqs[j]->q_vector = NULL;
- }
- if (idpf_is_queue_model_split(vport->txq_model))
- for (i = 0; i < vport->num_txq_grp; i++)
- vport->txq_grps[i].complq->q_vector = NULL;
- else
- for (i = 0; i < vport->num_txq_grp; i++)
- for (j = 0; j < vport->txq_grps[i].num_txq; j++)
- vport->txq_grps[i].txqs[j]->q_vector = NULL;
- }",void idpf_vport_intr_rel(struct idpf_vport *vport) { for (u32 v_idx = 0; v_idx < vport->num_q_vectors; v_idx++) { struct idpf_q_vector *q_vector = &vport->q_vectors[v_idx]; kfree(q_vector->complq); q_vector->complq = NULL; kfree(q_vector->bufq); q_vector->bufq = NULL; kfree(q_vector->tx); q_vector->tx = NULL; kfree(q_vector->rx); q_vector->rx = NULL; free_cpumask_var(q_vector->affinity_mask); } kfree(vport->q_vectors); vport->q_vectors = NULL; }
1230----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53239/bad/chip.c----usb6fire_chip_probe,"static int usb6fire_chip_probe(struct usb_interface *intf, const struct usb_device_id *usb_id) { int ret; int i; struct sfire_chip *chip = NULL; struct usb_device *device = interface_to_usbdev(intf); int regidx = -1; struct snd_card *card = NULL; mutex_lock(&register_mutex); for (i = 0; i < SNDRV_CARDS; i++) { if (devices[i] == device) { if (chips[i]) chips[i]->intf_count++; usb_set_intfdata(intf, chips[i]); mutex_unlock(&register_mutex); return 0; } else if (!devices[i] && regidx < 0) regidx = i; } if (regidx < 0) { mutex_unlock(&register_mutex); dev_err(&intf->dev, ""too many cards registered.\n""); return -ENODEV; } devices[regidx] = device; mutex_unlock(&register_mutex); ret = usb6fire_fw_init(intf); if (ret < 0) return ret; else if (ret == FW_NOT_READY) return 0; if (usb_set_interface(device, 0, 0) != 0) { dev_err(&intf->dev, ""can't set first interface.\n""); return -EIO; } ret = snd_card_new(&intf->dev, index[regidx], id[regidx], THIS_MODULE, sizeof(struct sfire_chip), &card); if (ret < 0) { dev_err(&intf->dev, ""cannot create alsa card.\n""); return ret; } strcpy(card->driver, ""6FireUSB""); strcpy(card->shortname, ""TerraTec DMX6FireUSB""); sprintf(card->longname, ""%s at %d:%d"", card->shortname, device->bus->busnum, device->devnum); chip = card->private_data; chips[regidx] = chip; chip->dev = device; chip->regidx = regidx; chip->intf_count = 1; chip->card = card; ret = usb6fire_comm_init(chip); if (ret < 0) goto destroy_chip; ret = usb6fire_midi_init(chip); if (ret < 0) goto destroy_chip; ret = usb6fire_pcm_init(chip); if (ret < 0) goto destroy_chip; ret = usb6fire_control_init(chip); if (ret < 0) goto destroy_chip; ret = snd_card_register(card); if (ret < 0) { dev_err(&intf->dev, ""cannot register card.""); goto destroy_chip; } usb_set_intfdata(intf, chip); return 0; destroy_chip: <S2SV_StartVul> usb6fire_chip_destroy(chip); <S2SV_EndVul> return ret; }","- usb6fire_chip_destroy(chip);
+ card->private_free = usb6fire_card_free;
+ snd_card_free(card);","static int usb6fire_chip_probe(struct usb_interface *intf, const struct usb_device_id *usb_id) { int ret; int i; struct sfire_chip *chip = NULL; struct usb_device *device = interface_to_usbdev(intf); int regidx = -1; struct snd_card *card = NULL; mutex_lock(&register_mutex); for (i = 0; i < SNDRV_CARDS; i++) { if (devices[i] == device) { if (chips[i]) chips[i]->intf_count++; usb_set_intfdata(intf, chips[i]); mutex_unlock(&register_mutex); return 0; } else if (!devices[i] && regidx < 0) regidx = i; } if (regidx < 0) { mutex_unlock(&register_mutex); dev_err(&intf->dev, ""too many cards registered.\n""); return -ENODEV; } devices[regidx] = device; mutex_unlock(&register_mutex); ret = usb6fire_fw_init(intf); if (ret < 0) return ret; else if (ret == FW_NOT_READY) return 0; if (usb_set_interface(device, 0, 0) != 0) { dev_err(&intf->dev, ""can't set first interface.\n""); return -EIO; } ret = snd_card_new(&intf->dev, index[regidx], id[regidx], THIS_MODULE, sizeof(struct sfire_chip), &card); if (ret < 0) { dev_err(&intf->dev, ""cannot create alsa card.\n""); return ret; } strcpy(card->driver, ""6FireUSB""); strcpy(card->shortname, ""TerraTec DMX6FireUSB""); sprintf(card->longname, ""%s at %d:%d"", card->shortname, device->bus->busnum, device->devnum); chip = card->private_data; chips[regidx] = chip; chip->dev = device; chip->regidx = regidx; chip->intf_count = 1; chip->card = card; card->private_free = usb6fire_card_free; ret = usb6fire_comm_init(chip); if (ret < 0) goto destroy_chip; ret = usb6fire_midi_init(chip); if (ret < 0) goto destroy_chip; ret = usb6fire_pcm_init(chip); if (ret < 0) goto destroy_chip; ret = usb6fire_control_init(chip); if (ret < 0) goto destroy_chip; ret = snd_card_register(card); if (ret < 0) { dev_err(&intf->dev, ""cannot register card.""); goto destroy_chip; } usb_set_intfdata(intf, chip); return 0; destroy_chip: snd_card_free(card); return ret; }"
400----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46859/bad/panasonic-laptop.c----acpi_pcc_hotkey_resume,"static int acpi_pcc_hotkey_resume(struct device *dev) { struct pcc_acpi *pcc; if (!dev) return -EINVAL; pcc = acpi_driver_data(to_acpi_device(dev)); if (!pcc) return -EINVAL; <S2SV_StartVul> acpi_pcc_write_sset(pcc, SINF_MUTE, pcc->mute); <S2SV_EndVul> <S2SV_StartVul> acpi_pcc_write_sset(pcc, SINF_ECO_MODE, pcc->eco_mode); <S2SV_EndVul> acpi_pcc_write_sset(pcc, SINF_STICKY_KEY, pcc->sticky_key); acpi_pcc_write_sset(pcc, SINF_AC_CUR_BRIGHT, pcc->ac_brightness); acpi_pcc_write_sset(pcc, SINF_DC_CUR_BRIGHT, pcc->dc_brightness); <S2SV_StartVul> acpi_pcc_write_sset(pcc, SINF_CUR_BRIGHT, pcc->current_brightness); <S2SV_EndVul> return 0; }","- acpi_pcc_write_sset(pcc, SINF_MUTE, pcc->mute);
- acpi_pcc_write_sset(pcc, SINF_ECO_MODE, pcc->eco_mode);
- acpi_pcc_write_sset(pcc, SINF_CUR_BRIGHT, pcc->current_brightness);
+ if (pcc->num_sifr > SINF_MUTE)
+ acpi_pcc_write_sset(pcc, SINF_MUTE, pcc->mute);
+ if (pcc->num_sifr > SINF_ECO_MODE)
+ acpi_pcc_write_sset(pcc, SINF_ECO_MODE, pcc->eco_mode);
+ if (pcc->num_sifr > SINF_CUR_BRIGHT)
+ acpi_pcc_write_sset(pcc, SINF_CUR_BRIGHT, pcc->current_brightness);
+ }","static int acpi_pcc_hotkey_resume(struct device *dev) { struct pcc_acpi *pcc; if (!dev) return -EINVAL; pcc = acpi_driver_data(to_acpi_device(dev)); if (!pcc) return -EINVAL; if (pcc->num_sifr > SINF_MUTE) acpi_pcc_write_sset(pcc, SINF_MUTE, pcc->mute); if (pcc->num_sifr > SINF_ECO_MODE) acpi_pcc_write_sset(pcc, SINF_ECO_MODE, pcc->eco_mode); acpi_pcc_write_sset(pcc, SINF_STICKY_KEY, pcc->sticky_key); acpi_pcc_write_sset(pcc, SINF_AC_CUR_BRIGHT, pcc->ac_brightness); acpi_pcc_write_sset(pcc, SINF_DC_CUR_BRIGHT, pcc->dc_brightness); if (pcc->num_sifr > SINF_CUR_BRIGHT) acpi_pcc_write_sset(pcc, SINF_CUR_BRIGHT, pcc->current_brightness); return 0; }"
786----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50062/bad/rtrs-srv.c----rtrs_srv_info_req_done,"static void rtrs_srv_info_req_done(struct ib_cq *cq, struct ib_wc *wc) { struct rtrs_srv_con *con = to_srv_con(wc->qp->qp_context); struct rtrs_path *s = con->c.path; struct rtrs_srv_path *srv_path = to_srv_path(s); struct rtrs_msg_info_req *msg; struct rtrs_iu *iu; int err; WARN_ON(con->c.cid); iu = container_of(wc->wr_cqe, struct rtrs_iu, cqe); if (wc->status != IB_WC_SUCCESS) { rtrs_err(s, ""Sess info request receive failed: %s\n"", ib_wc_status_msg(wc->status)); goto close; } WARN_ON(wc->opcode != IB_WC_RECV); if (wc->byte_len < sizeof(*msg)) { rtrs_err(s, ""Sess info request is malformed: size %d\n"", wc->byte_len); goto close; } ib_dma_sync_single_for_cpu(srv_path->s.dev->ib_dev, iu->dma_addr, iu->size, DMA_FROM_DEVICE); msg = iu->buf; if (le16_to_cpu(msg->type) != RTRS_MSG_INFO_REQ) { rtrs_err(s, ""Sess info request is malformed: type %d\n"", le16_to_cpu(msg->type)); goto close; } err = process_info_req(con, msg); if (err) goto close; <S2SV_StartVul> out: <S2SV_EndVul> rtrs_iu_free(iu, srv_path->s.dev->ib_dev, 1); return; close: close_path(srv_path); <S2SV_StartVul> goto out; <S2SV_EndVul> }","- out:
- goto out;
+ rtrs_iu_free(iu, srv_path->s.dev->ib_dev, 1);
+ rtrs_iu_free(iu, srv_path->s.dev->ib_dev, 1);
+ }","static void rtrs_srv_info_req_done(struct ib_cq *cq, struct ib_wc *wc) { struct rtrs_srv_con *con = to_srv_con(wc->qp->qp_context); struct rtrs_path *s = con->c.path; struct rtrs_srv_path *srv_path = to_srv_path(s); struct rtrs_msg_info_req *msg; struct rtrs_iu *iu; int err; WARN_ON(con->c.cid); iu = container_of(wc->wr_cqe, struct rtrs_iu, cqe); if (wc->status != IB_WC_SUCCESS) { rtrs_err(s, ""Sess info request receive failed: %s\n"", ib_wc_status_msg(wc->status)); goto close; } WARN_ON(wc->opcode != IB_WC_RECV); if (wc->byte_len < sizeof(*msg)) { rtrs_err(s, ""Sess info request is malformed: size %d\n"", wc->byte_len); goto close; } ib_dma_sync_single_for_cpu(srv_path->s.dev->ib_dev, iu->dma_addr, iu->size, DMA_FROM_DEVICE); msg = iu->buf; if (le16_to_cpu(msg->type) != RTRS_MSG_INFO_REQ) { rtrs_err(s, ""Sess info request is malformed: type %d\n"", le16_to_cpu(msg->type)); goto close; } err = process_info_req(con, msg); if (err) goto close; rtrs_iu_free(iu, srv_path->s.dev->ib_dev, 1); return; close: rtrs_iu_free(iu, srv_path->s.dev->ib_dev, 1); close_path(srv_path); }"
666----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49966/bad/quota_local.c----ocfs2_local_read_info,"static int ocfs2_local_read_info(struct super_block *sb, int type) { struct ocfs2_local_disk_dqinfo *ldinfo; struct mem_dqinfo *info = sb_dqinfo(sb, type); struct ocfs2_mem_dqinfo *oinfo; struct inode *lqinode = sb_dqopt(sb)->files[type]; int status; struct buffer_head *bh = NULL; struct ocfs2_quota_recovery *rec; <S2SV_StartVul> int locked = 0; <S2SV_EndVul> info->dqi_max_spc_limit = 0x7fffffffffffffffLL; info->dqi_max_ino_limit = 0x7fffffffffffffffLL; oinfo = kmalloc(sizeof(struct ocfs2_mem_dqinfo), GFP_NOFS); if (!oinfo) { mlog(ML_ERROR, ""failed to allocate memory for ocfs2 quota"" "" info.""); goto out_err; } info->dqi_priv = oinfo; oinfo->dqi_type = type; INIT_LIST_HEAD(&oinfo->dqi_chunk); oinfo->dqi_rec = NULL; oinfo->dqi_lqi_bh = NULL; oinfo->dqi_libh = NULL; status = ocfs2_global_read_info(sb, type); if (status < 0) goto out_err; status = ocfs2_inode_lock(lqinode, &oinfo->dqi_lqi_bh, 1); if (status < 0) { mlog_errno(status); goto out_err; } locked = 1; status = ocfs2_read_quota_block(lqinode, 0, &bh); if (status) { mlog_errno(status); mlog(ML_ERROR, ""failed to read quota file info header "" ""(type=%d)\n"", type); goto out_err; } ldinfo = (struct ocfs2_local_disk_dqinfo *)(bh->b_data + OCFS2_LOCAL_INFO_OFF); oinfo->dqi_flags = le32_to_cpu(ldinfo->dqi_flags); oinfo->dqi_chunks = le32_to_cpu(ldinfo->dqi_chunks); oinfo->dqi_blocks = le32_to_cpu(ldinfo->dqi_blocks); oinfo->dqi_libh = bh; if (!(oinfo->dqi_flags & OLQF_CLEAN)) { rec = OCFS2_SB(sb)->quota_rec; if (!rec) { rec = ocfs2_alloc_quota_recovery(); if (!rec) { status = -ENOMEM; mlog_errno(status); goto out_err; } OCFS2_SB(sb)->quota_rec = rec; } status = ocfs2_recovery_load_quota(lqinode, ldinfo, type, &rec->r_list[type]); if (status < 0) { mlog_errno(status); goto out_err; } } status = ocfs2_load_local_quota_bitmaps(lqinode, ldinfo, &oinfo->dqi_chunk); if (status < 0) { mlog_errno(status); goto out_err; } oinfo->dqi_flags &= ~OLQF_CLEAN; status = ocfs2_modify_bh(lqinode, bh, olq_update_info, info); if (status < 0) { mlog_errno(status); goto out_err; } return 0; out_err: if (oinfo) { iput(oinfo->dqi_gqinode); ocfs2_simple_drop_lockres(OCFS2_SB(sb), &oinfo->dqi_gqlock); ocfs2_lock_res_free(&oinfo->dqi_gqlock); brelse(oinfo->dqi_lqi_bh); if (locked) ocfs2_inode_unlock(lqinode, 1); ocfs2_release_local_quota_bitmaps(&oinfo->dqi_chunk); kfree(oinfo); } brelse(bh); <S2SV_StartVul> return -1; <S2SV_EndVul> }","- int locked = 0;
- return -1;
+ int locked = 0, global_read = 0;
+ status = -ENOMEM;
+ global_read = 1;
+ if (global_read)
+ cancel_delayed_work_sync(&oinfo->dqi_sync_work);
+ return status;","static int ocfs2_local_read_info(struct super_block *sb, int type) { struct ocfs2_local_disk_dqinfo *ldinfo; struct mem_dqinfo *info = sb_dqinfo(sb, type); struct ocfs2_mem_dqinfo *oinfo; struct inode *lqinode = sb_dqopt(sb)->files[type]; int status; struct buffer_head *bh = NULL; struct ocfs2_quota_recovery *rec; int locked = 0, global_read = 0; info->dqi_max_spc_limit = 0x7fffffffffffffffLL; info->dqi_max_ino_limit = 0x7fffffffffffffffLL; oinfo = kmalloc(sizeof(struct ocfs2_mem_dqinfo), GFP_NOFS); if (!oinfo) { mlog(ML_ERROR, ""failed to allocate memory for ocfs2 quota"" "" info.""); status = -ENOMEM; goto out_err; } info->dqi_priv = oinfo; oinfo->dqi_type = type; INIT_LIST_HEAD(&oinfo->dqi_chunk); oinfo->dqi_rec = NULL; oinfo->dqi_lqi_bh = NULL; oinfo->dqi_libh = NULL; status = ocfs2_global_read_info(sb, type); if (status < 0) goto out_err; global_read = 1; status = ocfs2_inode_lock(lqinode, &oinfo->dqi_lqi_bh, 1); if (status < 0) { mlog_errno(status); goto out_err; } locked = 1; status = ocfs2_read_quota_block(lqinode, 0, &bh); if (status) { mlog_errno(status); mlog(ML_ERROR, ""failed to read quota file info header "" ""(type=%d)\n"", type); goto out_err; } ldinfo = (struct ocfs2_local_disk_dqinfo *)(bh->b_data + OCFS2_LOCAL_INFO_OFF); oinfo->dqi_flags = le32_to_cpu(ldinfo->dqi_flags); oinfo->dqi_chunks = le32_to_cpu(ldinfo->dqi_chunks); oinfo->dqi_blocks = le32_to_cpu(ldinfo->dqi_blocks); oinfo->dqi_libh = bh; if (!(oinfo->dqi_flags & OLQF_CLEAN)) { rec = OCFS2_SB(sb)->quota_rec; if (!rec) { rec = ocfs2_alloc_quota_recovery(); if (!rec) { status = -ENOMEM; mlog_errno(status); goto out_err; } OCFS2_SB(sb)->quota_rec = rec; } status = ocfs2_recovery_load_quota(lqinode, ldinfo, type, &rec->r_list[type]); if (status < 0) { mlog_errno(status); goto out_err; } } status = ocfs2_load_local_quota_bitmaps(lqinode, ldinfo, &oinfo->dqi_chunk); if (status < 0) { mlog_errno(status); goto out_err; } oinfo->dqi_flags &= ~OLQF_CLEAN; status = ocfs2_modify_bh(lqinode, bh, olq_update_info, info); if (status < 0) { mlog_errno(status); goto out_err; } return 0; out_err: if (oinfo) { iput(oinfo->dqi_gqinode); ocfs2_simple_drop_lockres(OCFS2_SB(sb), &oinfo->dqi_gqlock); ocfs2_lock_res_free(&oinfo->dqi_gqlock); brelse(oinfo->dqi_lqi_bh); if (locked) ocfs2_inode_unlock(lqinode, 1); ocfs2_release_local_quota_bitmaps(&oinfo->dqi_chunk); if (global_read) cancel_delayed_work_sync(&oinfo->dqi_sync_work); kfree(oinfo); } brelse(bh); return status; }"
675----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49976/bad/trace_osnoise.c----stop_kthread,"static void stop_kthread(unsigned int cpu) { struct task_struct *kthread; <S2SV_StartVul> mutex_lock(&interface_lock); <S2SV_EndVul> <S2SV_StartVul> kthread = per_cpu(per_cpu_osnoise_var, cpu).kthread; <S2SV_EndVul> if (kthread) { <S2SV_StartVul> per_cpu(per_cpu_osnoise_var, cpu).kthread = NULL; <S2SV_EndVul> <S2SV_StartVul> mutex_unlock(&interface_lock); <S2SV_EndVul> if (cpumask_test_and_clear_cpu(cpu, &kthread_cpumask) && !WARN_ON(!test_bit(OSN_WORKLOAD, &osnoise_options))) { kthread_stop(kthread); } else if (!WARN_ON(test_bit(OSN_WORKLOAD, &osnoise_options))) { kill_pid(kthread->thread_pid, SIGKILL, 1); put_task_struct(kthread); } } else { <S2SV_StartVul> mutex_unlock(&interface_lock); <S2SV_EndVul> if (!test_bit(OSN_WORKLOAD, &osnoise_options)) { per_cpu(per_cpu_osnoise_var, cpu).sampling = false; barrier(); } } }","- mutex_lock(&interface_lock);
- kthread = per_cpu(per_cpu_osnoise_var, cpu).kthread;
- per_cpu(per_cpu_osnoise_var, cpu).kthread = NULL;
- mutex_unlock(&interface_lock);
- mutex_unlock(&interface_lock);
+ kthread = xchg_relaxed(&(per_cpu(per_cpu_osnoise_var, cpu).kthread), NULL);","static void stop_kthread(unsigned int cpu) { struct task_struct *kthread; kthread = xchg_relaxed(&(per_cpu(per_cpu_osnoise_var, cpu).kthread), NULL); if (kthread) { if (cpumask_test_and_clear_cpu(cpu, &kthread_cpumask) && !WARN_ON(!test_bit(OSN_WORKLOAD, &osnoise_options))) { kthread_stop(kthread); } else if (!WARN_ON(test_bit(OSN_WORKLOAD, &osnoise_options))) { kill_pid(kthread->thread_pid, SIGKILL, 1); put_task_struct(kthread); } } else { if (!test_bit(OSN_WORKLOAD, &osnoise_options)) { per_cpu(per_cpu_osnoise_var, cpu).sampling = false; barrier(); } } }"
1011----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50249/bad/cppc_acpi.c----acpi_cppc_processor_probe,"int acpi_cppc_processor_probe(struct acpi_processor *pr) { struct acpi_buffer output = {ACPI_ALLOCATE_BUFFER, NULL}; union acpi_object *out_obj, *cpc_obj; struct cpc_desc *cpc_ptr; struct cpc_reg *gas_t; struct device *cpu_dev; acpi_handle handle = pr->handle; unsigned int num_ent, i, cpc_rev; int pcc_subspace_id = -1; acpi_status status; int ret = -EFAULT; if (osc_sb_cppc_not_supported) return -ENODEV; status = acpi_evaluate_object_typed(handle, ""_CPC"", NULL, &output, ACPI_TYPE_PACKAGE); if (ACPI_FAILURE(status)) { ret = -ENODEV; goto out_buf_free; } out_obj = (union acpi_object *) output.pointer; cpc_ptr = kzalloc(sizeof(struct cpc_desc), GFP_KERNEL); if (!cpc_ptr) { ret = -ENOMEM; goto out_buf_free; } cpc_obj = &out_obj->package.elements[0]; if (cpc_obj->type == ACPI_TYPE_INTEGER) { num_ent = cpc_obj->integer.value; if (num_ent <= 1) { pr_debug(""Unexpected _CPC NumEntries value (%d) for CPU:%d\n"", num_ent, pr->id); goto out_free; } } else { pr_debug(""Unexpected entry type(%d) for NumEntries\n"", cpc_obj->type); goto out_free; } cpc_obj = &out_obj->package.elements[1]; if (cpc_obj->type == ACPI_TYPE_INTEGER) { cpc_rev = cpc_obj->integer.value; } else { pr_debug(""Unexpected entry type(%d) for Revision\n"", cpc_obj->type); goto out_free; } if (cpc_rev < CPPC_V2_REV) { pr_debug(""Unsupported _CPC Revision (%d) for CPU:%d\n"", cpc_rev, pr->id); goto out_free; } if ((cpc_rev == CPPC_V2_REV && num_ent != CPPC_V2_NUM_ENT) || (cpc_rev == CPPC_V3_REV && num_ent != CPPC_V3_NUM_ENT) || (cpc_rev > CPPC_V3_REV && num_ent <= CPPC_V3_NUM_ENT)) { pr_debug(""Unexpected number of _CPC return package entries (%d) for CPU:%d\n"", num_ent, pr->id); goto out_free; } if (cpc_rev > CPPC_V3_REV) { num_ent = CPPC_V3_NUM_ENT; cpc_rev = CPPC_V3_REV; } cpc_ptr->num_entries = num_ent; cpc_ptr->version = cpc_rev; for (i = 2; i < num_ent; i++) { cpc_obj = &out_obj->package.elements[i]; if (cpc_obj->type == ACPI_TYPE_INTEGER) { cpc_ptr->cpc_regs[i-2].type = ACPI_TYPE_INTEGER; cpc_ptr->cpc_regs[i-2].cpc_entry.int_value = cpc_obj->integer.value; } else if (cpc_obj->type == ACPI_TYPE_BUFFER) { gas_t = (struct cpc_reg *) cpc_obj->buffer.pointer; if (gas_t->space_id == ACPI_ADR_SPACE_PLATFORM_COMM) { if (pcc_subspace_id < 0) { pcc_subspace_id = gas_t->access_width; if (pcc_data_alloc(pcc_subspace_id)) goto out_free; } else if (pcc_subspace_id != gas_t->access_width) { pr_debug(""Mismatched PCC ids.\n""); goto out_free; } } else if (gas_t->space_id == ACPI_ADR_SPACE_SYSTEM_MEMORY) { if (gas_t->address) { void __iomem *addr; size_t access_width; access_width = GET_BIT_WIDTH(gas_t) / 8; addr = ioremap(gas_t->address, access_width); if (!addr) goto out_free; cpc_ptr->cpc_regs[i-2].sys_mem_vaddr = addr; } } else { if (gas_t->space_id != ACPI_ADR_SPACE_FIXED_HARDWARE || !cpc_ffh_supported()) { pr_debug(""Unsupported register type: %d\n"", gas_t->space_id); goto out_free; } } cpc_ptr->cpc_regs[i-2].type = ACPI_TYPE_BUFFER; memcpy(&cpc_ptr->cpc_regs[i-2].cpc_entry.reg, gas_t, sizeof(*gas_t)); } else { pr_debug(""Err in entry:%d in CPC table of CPU:%d\n"", i, pr->id); goto out_free; } } per_cpu(cpu_pcc_subspace_idx, pr->id) = pcc_subspace_id; for (i = num_ent - 2; i < MAX_CPC_REG_ENT; i++) { cpc_ptr->cpc_regs[i].type = ACPI_TYPE_INTEGER; cpc_ptr->cpc_regs[i].cpc_entry.int_value = 0; } cpc_ptr->cpu_id = pr->id; <S2SV_StartVul> spin_lock_init(&cpc_ptr->rmw_lock); <S2SV_EndVul> ret = acpi_get_psd(cpc_ptr, handle); if (ret) goto out_free; if (pcc_subspace_id >= 0 && !pcc_data[pcc_subspace_id]->pcc_channel_acquired) { ret = register_pcc_channel(pcc_subspace_id); if (ret) goto out_free; init_rwsem(&pcc_data[pcc_subspace_id]->pcc_lock); init_waitqueue_head(&pcc_data[pcc_subspace_id]->pcc_write_wait_q); } pr_debug(""Parsed CPC struct for CPU: %d\n"", pr->id); cpu_dev = get_cpu_device(pr->id); if (!cpu_dev) { ret = -EINVAL; goto out_free; } per_cpu(cpc_desc_ptr, pr->id) = cpc_ptr; ret = kobject_init_and_add(&cpc_ptr->kobj, &cppc_ktype, &cpu_dev->kobj, ""acpi_cppc""); if (ret) { per_cpu(cpc_desc_ptr, pr->id) = NULL; kobject_put(&cpc_ptr->kobj); goto out_free; } init_freq_invariance_cppc(); kfree(output.pointer); return 0; out_free: for (i = 2; i < cpc_ptr->num_entries; i++) { void __iomem *addr = cpc_ptr->cpc_regs[i-2].sys_mem_vaddr; if (addr) iounmap(addr); } kfree(cpc_ptr); out_buf_free: kfree(output.pointer); return ret; }","- spin_lock_init(&cpc_ptr->rmw_lock);
+ raw_spin_lock_init(&cpc_ptr->rmw_lock);","int acpi_cppc_processor_probe(struct acpi_processor *pr) { struct acpi_buffer output = {ACPI_ALLOCATE_BUFFER, NULL}; union acpi_object *out_obj, *cpc_obj; struct cpc_desc *cpc_ptr; struct cpc_reg *gas_t; struct device *cpu_dev; acpi_handle handle = pr->handle; unsigned int num_ent, i, cpc_rev; int pcc_subspace_id = -1; acpi_status status; int ret = -EFAULT; if (osc_sb_cppc_not_supported) return -ENODEV; status = acpi_evaluate_object_typed(handle, ""_CPC"", NULL, &output, ACPI_TYPE_PACKAGE); if (ACPI_FAILURE(status)) { ret = -ENODEV; goto out_buf_free; } out_obj = (union acpi_object *) output.pointer; cpc_ptr = kzalloc(sizeof(struct cpc_desc), GFP_KERNEL); if (!cpc_ptr) { ret = -ENOMEM; goto out_buf_free; } cpc_obj = &out_obj->package.elements[0]; if (cpc_obj->type == ACPI_TYPE_INTEGER) { num_ent = cpc_obj->integer.value; if (num_ent <= 1) { pr_debug(""Unexpected _CPC NumEntries value (%d) for CPU:%d\n"", num_ent, pr->id); goto out_free; } } else { pr_debug(""Unexpected entry type(%d) for NumEntries\n"", cpc_obj->type); goto out_free; } cpc_obj = &out_obj->package.elements[1]; if (cpc_obj->type == ACPI_TYPE_INTEGER) { cpc_rev = cpc_obj->integer.value; } else { pr_debug(""Unexpected entry type(%d) for Revision\n"", cpc_obj->type); goto out_free; } if (cpc_rev < CPPC_V2_REV) { pr_debug(""Unsupported _CPC Revision (%d) for CPU:%d\n"", cpc_rev, pr->id); goto out_free; } if ((cpc_rev == CPPC_V2_REV && num_ent != CPPC_V2_NUM_ENT) || (cpc_rev == CPPC_V3_REV && num_ent != CPPC_V3_NUM_ENT) || (cpc_rev > CPPC_V3_REV && num_ent <= CPPC_V3_NUM_ENT)) { pr_debug(""Unexpected number of _CPC return package entries (%d) for CPU:%d\n"", num_ent, pr->id); goto out_free; } if (cpc_rev > CPPC_V3_REV) { num_ent = CPPC_V3_NUM_ENT; cpc_rev = CPPC_V3_REV; } cpc_ptr->num_entries = num_ent; cpc_ptr->version = cpc_rev; for (i = 2; i < num_ent; i++) { cpc_obj = &out_obj->package.elements[i]; if (cpc_obj->type == ACPI_TYPE_INTEGER) { cpc_ptr->cpc_regs[i-2].type = ACPI_TYPE_INTEGER; cpc_ptr->cpc_regs[i-2].cpc_entry.int_value = cpc_obj->integer.value; } else if (cpc_obj->type == ACPI_TYPE_BUFFER) { gas_t = (struct cpc_reg *) cpc_obj->buffer.pointer; if (gas_t->space_id == ACPI_ADR_SPACE_PLATFORM_COMM) { if (pcc_subspace_id < 0) { pcc_subspace_id = gas_t->access_width; if (pcc_data_alloc(pcc_subspace_id)) goto out_free; } else if (pcc_subspace_id != gas_t->access_width) { pr_debug(""Mismatched PCC ids.\n""); goto out_free; } } else if (gas_t->space_id == ACPI_ADR_SPACE_SYSTEM_MEMORY) { if (gas_t->address) { void __iomem *addr; size_t access_width; access_width = GET_BIT_WIDTH(gas_t) / 8; addr = ioremap(gas_t->address, access_width); if (!addr) goto out_free; cpc_ptr->cpc_regs[i-2].sys_mem_vaddr = addr; } } else { if (gas_t->space_id != ACPI_ADR_SPACE_FIXED_HARDWARE || !cpc_ffh_supported()) { pr_debug(""Unsupported register type: %d\n"", gas_t->space_id); goto out_free; } } cpc_ptr->cpc_regs[i-2].type = ACPI_TYPE_BUFFER; memcpy(&cpc_ptr->cpc_regs[i-2].cpc_entry.reg, gas_t, sizeof(*gas_t)); } else { pr_debug(""Err in entry:%d in CPC table of CPU:%d\n"", i, pr->id); goto out_free; } } per_cpu(cpu_pcc_subspace_idx, pr->id) = pcc_subspace_id; for (i = num_ent - 2; i < MAX_CPC_REG_ENT; i++) { cpc_ptr->cpc_regs[i].type = ACPI_TYPE_INTEGER; cpc_ptr->cpc_regs[i].cpc_entry.int_value = 0; } cpc_ptr->cpu_id = pr->id; raw_spin_lock_init(&cpc_ptr->rmw_lock); ret = acpi_get_psd(cpc_ptr, handle); if (ret) goto out_free; if (pcc_subspace_id >= 0 && !pcc_data[pcc_subspace_id]->pcc_channel_acquired) { ret = register_pcc_channel(pcc_subspace_id); if (ret) goto out_free; init_rwsem(&pcc_data[pcc_subspace_id]->pcc_lock); init_waitqueue_head(&pcc_data[pcc_subspace_id]->pcc_write_wait_q); } pr_debug(""Parsed CPC struct for CPU: %d\n"", pr->id); cpu_dev = get_cpu_device(pr->id); if (!cpu_dev) { ret = -EINVAL; goto out_free; } per_cpu(cpc_desc_ptr, pr->id) = cpc_ptr; ret = kobject_init_and_add(&cpc_ptr->kobj, &cppc_ktype, &cpu_dev->kobj, ""acpi_cppc""); if (ret) { per_cpu(cpc_desc_ptr, pr->id) = NULL; kobject_put(&cpc_ptr->kobj); goto out_free; } init_freq_invariance_cppc(); kfree(output.pointer); return 0; out_free: for (i = 2; i < cpc_ptr->num_entries; i++) { void __iomem *addr = cpc_ptr->cpc_regs[i-2].sys_mem_vaddr; if (addr) iounmap(addr); } kfree(cpc_ptr); out_buf_free: kfree(output.pointer); return ret; }"
1042----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50283/bad/server.c----__handle_ksmbd_work,"static void __handle_ksmbd_work(struct ksmbd_work *work, struct ksmbd_conn *conn) { u16 command = 0; int rc; bool is_chained = false; if (conn->ops->is_transform_hdr && conn->ops->is_transform_hdr(work->request_buf)) { rc = conn->ops->decrypt_req(work); if (rc < 0) return; work->encrypted = true; } if (conn->ops->allocate_rsp_buf(work)) return; rc = conn->ops->init_rsp_hdr(work); if (rc) { conn->ops->set_rsp_status(work, STATUS_INVALID_HANDLE); goto send; } do { if (conn->ops->check_user_session) { rc = conn->ops->check_user_session(work); if (rc < 0) { if (rc == -EINVAL) conn->ops->set_rsp_status(work, STATUS_INVALID_PARAMETER); else conn->ops->set_rsp_status(work, STATUS_USER_SESSION_DELETED); goto send; } else if (rc > 0) { rc = conn->ops->get_ksmbd_tcon(work); if (rc < 0) { if (rc == -EINVAL) conn->ops->set_rsp_status(work, STATUS_INVALID_PARAMETER); else conn->ops->set_rsp_status(work, STATUS_NETWORK_NAME_DELETED); goto send; } } } rc = __process_request(work, conn, &command); if (rc == SERVER_HANDLER_ABORT) break; if (conn->ops->set_rsp_credits) { spin_lock(&conn->credits_lock); rc = conn->ops->set_rsp_credits(work); spin_unlock(&conn->credits_lock); if (rc < 0) { conn->ops->set_rsp_status(work, STATUS_INVALID_PARAMETER); goto send; } } is_chained = is_chained_smb2_message(work); if (work->sess && (work->sess->sign || smb3_11_final_sess_setup_resp(work) || conn->ops->is_sign_req(work, command))) conn->ops->set_sign_rsp(work); } while (is_chained == true); send: <S2SV_StartVul> if (work->sess) <S2SV_EndVul> <S2SV_StartVul> ksmbd_user_session_put(work->sess); <S2SV_EndVul> if (work->tcon) ksmbd_tree_connect_put(work->tcon); smb3_preauth_hash_rsp(work); if (work->sess && work->sess->enc && work->encrypted && conn->ops->encrypt_resp) { rc = conn->ops->encrypt_resp(work); if (rc < 0) conn->ops->set_rsp_status(work, STATUS_DATA_ERROR); } ksmbd_conn_write(work); }","- if (work->sess)
- ksmbd_user_session_put(work->sess);
+ if (work->sess)
+ ksmbd_user_session_put(work->sess);","static void __handle_ksmbd_work(struct ksmbd_work *work, struct ksmbd_conn *conn) { u16 command = 0; int rc; bool is_chained = false; if (conn->ops->is_transform_hdr && conn->ops->is_transform_hdr(work->request_buf)) { rc = conn->ops->decrypt_req(work); if (rc < 0) return; work->encrypted = true; } if (conn->ops->allocate_rsp_buf(work)) return; rc = conn->ops->init_rsp_hdr(work); if (rc) { conn->ops->set_rsp_status(work, STATUS_INVALID_HANDLE); goto send; } do { if (conn->ops->check_user_session) { rc = conn->ops->check_user_session(work); if (rc < 0) { if (rc == -EINVAL) conn->ops->set_rsp_status(work, STATUS_INVALID_PARAMETER); else conn->ops->set_rsp_status(work, STATUS_USER_SESSION_DELETED); goto send; } else if (rc > 0) { rc = conn->ops->get_ksmbd_tcon(work); if (rc < 0) { if (rc == -EINVAL) conn->ops->set_rsp_status(work, STATUS_INVALID_PARAMETER); else conn->ops->set_rsp_status(work, STATUS_NETWORK_NAME_DELETED); goto send; } } } rc = __process_request(work, conn, &command); if (rc == SERVER_HANDLER_ABORT) break; if (conn->ops->set_rsp_credits) { spin_lock(&conn->credits_lock); rc = conn->ops->set_rsp_credits(work); spin_unlock(&conn->credits_lock); if (rc < 0) { conn->ops->set_rsp_status(work, STATUS_INVALID_PARAMETER); goto send; } } is_chained = is_chained_smb2_message(work); if (work->sess && (work->sess->sign || smb3_11_final_sess_setup_resp(work) || conn->ops->is_sign_req(work, command))) conn->ops->set_sign_rsp(work); } while (is_chained == true); send: if (work->tcon) ksmbd_tree_connect_put(work->tcon); smb3_preauth_hash_rsp(work); if (work->sess) ksmbd_user_session_put(work->sess); if (work->sess && work->sess->enc && work->encrypted && conn->ops->encrypt_resp) { rc = conn->ops->encrypt_resp(work); if (rc < 0) conn->ops->set_rsp_status(work, STATUS_DATA_ERROR); } ksmbd_conn_write(work); }"
1258----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56540/bad/ivpu_jsm_msg.c----ivpu_jsm_hws_setup_priority_bands,"int ivpu_jsm_hws_setup_priority_bands(struct ivpu_device *vdev) { struct vpu_jsm_msg req = { .type = VPU_JSM_MSG_SET_PRIORITY_BAND_SETUP }; struct vpu_jsm_msg resp; int ret; req.payload.hws_priority_band_setup.grace_period[0] = 0; req.payload.hws_priority_band_setup.process_grace_period[0] = 50000; req.payload.hws_priority_band_setup.process_quantum[0] = 160000; req.payload.hws_priority_band_setup.grace_period[1] = 50000; req.payload.hws_priority_band_setup.process_grace_period[1] = 50000; req.payload.hws_priority_band_setup.process_quantum[1] = 300000; req.payload.hws_priority_band_setup.grace_period[2] = 50000; req.payload.hws_priority_band_setup.process_grace_period[2] = 50000; req.payload.hws_priority_band_setup.process_quantum[2] = 200000; req.payload.hws_priority_band_setup.grace_period[3] = 0; req.payload.hws_priority_band_setup.process_grace_period[3] = 50000; req.payload.hws_priority_band_setup.process_quantum[3] = 200000; req.payload.hws_priority_band_setup.normal_band_percentage = 10; <S2SV_StartVul> ret = ivpu_ipc_send_receive_active(vdev, &req, VPU_JSM_MSG_SET_PRIORITY_BAND_SETUP_RSP, <S2SV_EndVul> <S2SV_StartVul> &resp, VPU_IPC_CHAN_ASYNC_CMD, vdev->timeout.jsm); <S2SV_EndVul> if (ret) ivpu_warn_ratelimited(vdev, ""Failed to set priority bands: %d\n"", ret); return ret; }","- ret = ivpu_ipc_send_receive_active(vdev, &req, VPU_JSM_MSG_SET_PRIORITY_BAND_SETUP_RSP,
- &resp, VPU_IPC_CHAN_ASYNC_CMD, vdev->timeout.jsm);
+ ret = ivpu_ipc_send_receive_internal(vdev, &req, VPU_JSM_MSG_SET_PRIORITY_BAND_SETUP_RSP,
+ &resp, VPU_IPC_CHAN_ASYNC_CMD, vdev->timeout.jsm);","int ivpu_jsm_hws_setup_priority_bands(struct ivpu_device *vdev) { struct vpu_jsm_msg req = { .type = VPU_JSM_MSG_SET_PRIORITY_BAND_SETUP }; struct vpu_jsm_msg resp; int ret; req.payload.hws_priority_band_setup.grace_period[0] = 0; req.payload.hws_priority_band_setup.process_grace_period[0] = 50000; req.payload.hws_priority_band_setup.process_quantum[0] = 160000; req.payload.hws_priority_band_setup.grace_period[1] = 50000; req.payload.hws_priority_band_setup.process_grace_period[1] = 50000; req.payload.hws_priority_band_setup.process_quantum[1] = 300000; req.payload.hws_priority_band_setup.grace_period[2] = 50000; req.payload.hws_priority_band_setup.process_grace_period[2] = 50000; req.payload.hws_priority_band_setup.process_quantum[2] = 200000; req.payload.hws_priority_band_setup.grace_period[3] = 0; req.payload.hws_priority_band_setup.process_grace_period[3] = 50000; req.payload.hws_priority_band_setup.process_quantum[3] = 200000; req.payload.hws_priority_band_setup.normal_band_percentage = 10; ret = ivpu_ipc_send_receive_internal(vdev, &req, VPU_JSM_MSG_SET_PRIORITY_BAND_SETUP_RSP, &resp, VPU_IPC_CHAN_ASYNC_CMD, vdev->timeout.jsm); if (ret) ivpu_warn_ratelimited(vdev, ""Failed to set priority bands: %d\n"", ret); return ret; }"
525----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-48875/bad/volumes.c----btrfs_map_block,"int btrfs_map_block(struct btrfs_fs_info *fs_info, enum btrfs_map_op op, u64 logical, u64 *length, struct btrfs_io_context **bioc_ret, struct btrfs_io_stripe *smap, int *mirror_num_ret) { struct btrfs_chunk_map *map; struct btrfs_io_geometry io_geom = { 0 }; u64 map_offset; int ret = 0; int num_copies; struct btrfs_io_context *bioc = NULL; struct btrfs_dev_replace *dev_replace = &fs_info->dev_replace; int dev_replace_is_ongoing = 0; u16 num_alloc_stripes; u64 max_len; ASSERT(bioc_ret); io_geom.mirror_num = (mirror_num_ret ? *mirror_num_ret : 0); io_geom.num_stripes = 1; io_geom.stripe_index = 0; io_geom.op = op; map = btrfs_get_chunk_map(fs_info, logical, *length); if (IS_ERR(map)) return PTR_ERR(map); num_copies = btrfs_chunk_map_num_copies(map); if (io_geom.mirror_num > num_copies) return -EINVAL; map_offset = logical - map->start; io_geom.raid56_full_stripe_start = (u64)-1; max_len = btrfs_max_io_len(map, map_offset, &io_geom); *length = min_t(u64, map->chunk_len - map_offset, max_len); <S2SV_StartVul> down_read(&dev_replace->rwsem); <S2SV_EndVul> dev_replace_is_ongoing = btrfs_dev_replace_is_ongoing(dev_replace); <S2SV_StartVul> if (!dev_replace_is_ongoing) <S2SV_EndVul> up_read(&dev_replace->rwsem); switch (map->type & BTRFS_BLOCK_GROUP_PROFILE_MASK) { case BTRFS_BLOCK_GROUP_RAID0: map_blocks_raid0(map, &io_geom); break; case BTRFS_BLOCK_GROUP_RAID1: case BTRFS_BLOCK_GROUP_RAID1C3: case BTRFS_BLOCK_GROUP_RAID1C4: map_blocks_raid1(fs_info, map, &io_geom, dev_replace_is_ongoing); break; case BTRFS_BLOCK_GROUP_DUP: map_blocks_dup(map, &io_geom); break; case BTRFS_BLOCK_GROUP_RAID10: map_blocks_raid10(fs_info, map, &io_geom, dev_replace_is_ongoing); break; case BTRFS_BLOCK_GROUP_RAID5: case BTRFS_BLOCK_GROUP_RAID6: if (op != BTRFS_MAP_READ || io_geom.mirror_num > 1) map_blocks_raid56_write(map, &io_geom, logical, length); else map_blocks_raid56_read(map, &io_geom); break; default: map_blocks_single(map, &io_geom); break; } if (io_geom.stripe_index >= map->num_stripes) { btrfs_crit(fs_info, ""stripe index math went horribly wrong, got stripe_index=%u, num_stripes=%u"", io_geom.stripe_index, map->num_stripes); ret = -EINVAL; goto out; } num_alloc_stripes = io_geom.num_stripes; if (dev_replace_is_ongoing && dev_replace->tgtdev != NULL && op != BTRFS_MAP_READ) num_alloc_stripes += 2; if (is_single_device_io(fs_info, smap, map, num_alloc_stripes, op, io_geom.mirror_num)) { ret = set_io_stripe(fs_info, logical, length, smap, map, &io_geom); if (mirror_num_ret) *mirror_num_ret = io_geom.mirror_num; *bioc_ret = NULL; goto out; } bioc = alloc_btrfs_io_context(fs_info, logical, num_alloc_stripes); if (!bioc) { ret = -ENOMEM; goto out; } bioc->map_type = map->type; if (map->type & BTRFS_BLOCK_GROUP_RAID56_MASK && (op != BTRFS_MAP_READ || io_geom.mirror_num > 1)) { bioc->full_stripe_logical = map->start + btrfs_stripe_nr_to_offset(io_geom.stripe_nr * nr_data_stripes(map)); for (int i = 0; i < io_geom.num_stripes; i++) { struct btrfs_io_stripe *dst = &bioc->stripes[i]; u32 stripe_index; stripe_index = (i + io_geom.stripe_nr) % io_geom.num_stripes; dst->dev = map->stripes[stripe_index].dev; dst->physical = map->stripes[stripe_index].physical + io_geom.stripe_offset + btrfs_stripe_nr_to_offset(io_geom.stripe_nr); } } else { for (int i = 0; i < io_geom.num_stripes; i++) { ret = set_io_stripe(fs_info, logical, length, &bioc->stripes[i], map, &io_geom); if (ret < 0) break; io_geom.stripe_index++; } } if (ret) { *bioc_ret = NULL; btrfs_put_bioc(bioc); goto out; } if (op != BTRFS_MAP_READ) io_geom.max_errors = btrfs_chunk_max_errors(map); if (dev_replace_is_ongoing && dev_replace->tgtdev != NULL && op != BTRFS_MAP_READ) { handle_ops_on_dev_replace(bioc, dev_replace, logical, &io_geom); } *bioc_ret = bioc; bioc->num_stripes = io_geom.num_stripes; bioc->max_errors = io_geom.max_errors; bioc->mirror_num = io_geom.mirror_num; out: <S2SV_StartVul> if (dev_replace_is_ongoing) { <S2SV_EndVul> lockdep_assert_held(&dev_replace->rwsem); up_read(&dev_replace->rwsem); } btrfs_free_chunk_map(map); return ret; }","- down_read(&dev_replace->rwsem);
- if (!dev_replace_is_ongoing)
- if (dev_replace_is_ongoing) {
+ if (dev_replace->replace_task != current)
+ down_read(&dev_replace->rwsem);
+ if (!dev_replace_is_ongoing && dev_replace->replace_task != current)
+ if (dev_replace_is_ongoing && dev_replace->replace_task != current) {","int btrfs_map_block(struct btrfs_fs_info *fs_info, enum btrfs_map_op op, u64 logical, u64 *length, struct btrfs_io_context **bioc_ret, struct btrfs_io_stripe *smap, int *mirror_num_ret) { struct btrfs_chunk_map *map; struct btrfs_io_geometry io_geom = { 0 }; u64 map_offset; int ret = 0; int num_copies; struct btrfs_io_context *bioc = NULL; struct btrfs_dev_replace *dev_replace = &fs_info->dev_replace; int dev_replace_is_ongoing = 0; u16 num_alloc_stripes; u64 max_len; ASSERT(bioc_ret); io_geom.mirror_num = (mirror_num_ret ? *mirror_num_ret : 0); io_geom.num_stripes = 1; io_geom.stripe_index = 0; io_geom.op = op; map = btrfs_get_chunk_map(fs_info, logical, *length); if (IS_ERR(map)) return PTR_ERR(map); num_copies = btrfs_chunk_map_num_copies(map); if (io_geom.mirror_num > num_copies) return -EINVAL; map_offset = logical - map->start; io_geom.raid56_full_stripe_start = (u64)-1; max_len = btrfs_max_io_len(map, map_offset, &io_geom); *length = min_t(u64, map->chunk_len - map_offset, max_len); if (dev_replace->replace_task != current) down_read(&dev_replace->rwsem); dev_replace_is_ongoing = btrfs_dev_replace_is_ongoing(dev_replace); if (!dev_replace_is_ongoing && dev_replace->replace_task != current) up_read(&dev_replace->rwsem); switch (map->type & BTRFS_BLOCK_GROUP_PROFILE_MASK) { case BTRFS_BLOCK_GROUP_RAID0: map_blocks_raid0(map, &io_geom); break; case BTRFS_BLOCK_GROUP_RAID1: case BTRFS_BLOCK_GROUP_RAID1C3: case BTRFS_BLOCK_GROUP_RAID1C4: map_blocks_raid1(fs_info, map, &io_geom, dev_replace_is_ongoing); break; case BTRFS_BLOCK_GROUP_DUP: map_blocks_dup(map, &io_geom); break; case BTRFS_BLOCK_GROUP_RAID10: map_blocks_raid10(fs_info, map, &io_geom, dev_replace_is_ongoing); break; case BTRFS_BLOCK_GROUP_RAID5: case BTRFS_BLOCK_GROUP_RAID6: if (op != BTRFS_MAP_READ || io_geom.mirror_num > 1) map_blocks_raid56_write(map, &io_geom, logical, length); else map_blocks_raid56_read(map, &io_geom); break; default: map_blocks_single(map, &io_geom); break; } if (io_geom.stripe_index >= map->num_stripes) { btrfs_crit(fs_info, ""stripe index math went horribly wrong, got stripe_index=%u, num_stripes=%u"", io_geom.stripe_index, map->num_stripes); ret = -EINVAL; goto out; } num_alloc_stripes = io_geom.num_stripes; if (dev_replace_is_ongoing && dev_replace->tgtdev != NULL && op != BTRFS_MAP_READ) num_alloc_stripes += 2; if (is_single_device_io(fs_info, smap, map, num_alloc_stripes, op, io_geom.mirror_num)) { ret = set_io_stripe(fs_info, logical, length, smap, map, &io_geom); if (mirror_num_ret) *mirror_num_ret = io_geom.mirror_num; *bioc_ret = NULL; goto out; } bioc = alloc_btrfs_io_context(fs_info, logical, num_alloc_stripes); if (!bioc) { ret = -ENOMEM; goto out; } bioc->map_type = map->type; if (map->type & BTRFS_BLOCK_GROUP_RAID56_MASK && (op != BTRFS_MAP_READ || io_geom.mirror_num > 1)) { bioc->full_stripe_logical = map->start + btrfs_stripe_nr_to_offset(io_geom.stripe_nr * nr_data_stripes(map)); for (int i = 0; i < io_geom.num_stripes; i++) { struct btrfs_io_stripe *dst = &bioc->stripes[i]; u32 stripe_index; stripe_index = (i + io_geom.stripe_nr) % io_geom.num_stripes; dst->dev = map->stripes[stripe_index].dev; dst->physical = map->stripes[stripe_index].physical + io_geom.stripe_offset + btrfs_stripe_nr_to_offset(io_geom.stripe_nr); } } else { for (int i = 0; i < io_geom.num_stripes; i++) { ret = set_io_stripe(fs_info, logical, length, &bioc->stripes[i], map, &io_geom); if (ret < 0) break; io_geom.stripe_index++; } } if (ret) { *bioc_ret = NULL; btrfs_put_bioc(bioc); goto out; } if (op != BTRFS_MAP_READ) io_geom.max_errors = btrfs_chunk_max_errors(map); if (dev_replace_is_ongoing && dev_replace->tgtdev != NULL && op != BTRFS_MAP_READ) { handle_ops_on_dev_replace(bioc, dev_replace, logical, &io_geom); } *bioc_ret = bioc; bioc->num_stripes = io_geom.num_stripes; bioc->max_errors = io_geom.max_errors; bioc->mirror_num = io_geom.mirror_num; out: if (dev_replace_is_ongoing && dev_replace->replace_task != current) { lockdep_assert_held(&dev_replace->rwsem); up_read(&dev_replace->rwsem); } btrfs_free_chunk_map(map); return ret; }"
397----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46857/bad/legacy.c----mlx5_eswitch_get_vepa,"int mlx5_eswitch_get_vepa(struct mlx5_eswitch *esw, u8 *setting) { if (!esw) return -EOPNOTSUPP; if (!mlx5_esw_allowed(esw)) return -EPERM; <S2SV_StartVul> if (esw->mode != MLX5_ESWITCH_LEGACY) <S2SV_EndVul> return -EOPNOTSUPP; *setting = esw->fdb_table.legacy.vepa_uplink_rule ? 1 : 0; return 0; }","- if (esw->mode != MLX5_ESWITCH_LEGACY)
+ if (esw->mode != MLX5_ESWITCH_LEGACY || !mlx5_esw_is_fdb_created(esw))","int mlx5_eswitch_get_vepa(struct mlx5_eswitch *esw, u8 *setting) { if (!esw) return -EOPNOTSUPP; if (!mlx5_esw_allowed(esw)) return -EPERM; if (esw->mode != MLX5_ESWITCH_LEGACY || !mlx5_esw_is_fdb_created(esw)) return -EOPNOTSUPP; *setting = esw->fdb_table.legacy.vepa_uplink_rule ? 1 : 0; return 0; }"
361----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46821/bad/navi10_ppt.c----navi10_print_clk_levels,"static int navi10_print_clk_levels(struct smu_context *smu, enum smu_clk_type clk_type, char *buf) { uint16_t *curve_settings; int i, levels, size = 0, ret = 0; uint32_t cur_value = 0, value = 0, count = 0; uint32_t freq_values[3] = {0}; uint32_t mark_index = 0; struct smu_table_context *table_context = &smu->smu_table; uint32_t gen_speed, lane_width; struct smu_dpm_context *smu_dpm = &smu->smu_dpm; struct smu_11_0_dpm_context *dpm_context = smu_dpm->dpm_context; PPTable_t *pptable = (PPTable_t *)table_context->driver_pptable; OverDriveTable_t *od_table = (OverDriveTable_t *)table_context->overdrive_table; struct smu_11_0_overdrive_table *od_settings = smu->od_settings; uint32_t min_value, max_value; smu_cmn_get_sysfs_buf(&buf, &size); switch (clk_type) { case SMU_GFXCLK: case SMU_SCLK: case SMU_SOCCLK: case SMU_MCLK: case SMU_UCLK: case SMU_FCLK: case SMU_VCLK: case SMU_DCLK: case SMU_DCEFCLK: ret = navi10_get_current_clk_freq_by_table(smu, clk_type, &cur_value); if (ret) return size; ret = smu_v11_0_get_dpm_level_count(smu, clk_type, &count); if (ret) return size; <S2SV_StartVul> if (!navi10_is_support_fine_grained_dpm(smu, clk_type)) { <S2SV_EndVul> for (i = 0; i < count; i++) { ret = smu_v11_0_get_dpm_freq_by_index(smu, clk_type, i, &value); if (ret) return size; size += sysfs_emit_at(buf, size, ""%d: %uMhz %s\n"", i, value, cur_value == value ? ""*"" : """"); } } else { ret = smu_v11_0_get_dpm_freq_by_index(smu, clk_type, 0, &freq_values[0]); if (ret) return size; ret = smu_v11_0_get_dpm_freq_by_index(smu, clk_type, count - 1, &freq_values[2]); if (ret) return size; freq_values[1] = cur_value; mark_index = cur_value == freq_values[0] ? 0 : cur_value == freq_values[2] ? 2 : 1; levels = 3; if (mark_index != 1) { levels = 2; freq_values[1] = freq_values[2]; } for (i = 0; i < levels; i++) { size += sysfs_emit_at(buf, size, ""%d: %uMhz %s\n"", i, freq_values[i], i == mark_index ? ""*"" : """"); } } break; case SMU_PCIE: gen_speed = smu_v11_0_get_current_pcie_link_speed_level(smu); lane_width = smu_v11_0_get_current_pcie_link_width_level(smu); for (i = 0; i < NUM_LINK_LEVELS; i++) size += sysfs_emit_at(buf, size, ""%d: %s %s %dMhz %s\n"", i, (dpm_context->dpm_tables.pcie_table.pcie_gen[i] == 0) ? ""2.5GT/s,"" : (dpm_context->dpm_tables.pcie_table.pcie_gen[i] == 1) ? ""5.0GT/s,"" : (dpm_context->dpm_tables.pcie_table.pcie_gen[i] == 2) ? ""8.0GT/s,"" : (dpm_context->dpm_tables.pcie_table.pcie_gen[i] == 3) ? ""16.0GT/s,"" : """", (dpm_context->dpm_tables.pcie_table.pcie_lane[i] == 1) ? ""x1"" : (dpm_context->dpm_tables.pcie_table.pcie_lane[i] == 2) ? ""x2"" : (dpm_context->dpm_tables.pcie_table.pcie_lane[i] == 3) ? ""x4"" : (dpm_context->dpm_tables.pcie_table.pcie_lane[i] == 4) ? ""x8"" : (dpm_context->dpm_tables.pcie_table.pcie_lane[i] == 5) ? ""x12"" : (dpm_context->dpm_tables.pcie_table.pcie_lane[i] == 6) ? ""x16"" : """", pptable->LclkFreq[i], (gen_speed == dpm_context->dpm_tables.pcie_table.pcie_gen[i]) && (lane_width == dpm_context->dpm_tables.pcie_table.pcie_lane[i]) ? ""*"" : """"); break; case SMU_OD_SCLK: if (!smu->od_enabled || !od_table || !od_settings) break; if (!navi10_od_feature_is_supported(od_settings, SMU_11_0_ODCAP_GFXCLK_LIMITS)) break; size += sysfs_emit_at(buf, size, ""OD_SCLK:\n""); size += sysfs_emit_at(buf, size, ""0: %uMhz\n1: %uMhz\n"", od_table->GfxclkFmin, od_table->GfxclkFmax); break; case SMU_OD_MCLK: if (!smu->od_enabled || !od_table || !od_settings) break; if (!navi10_od_feature_is_supported(od_settings, SMU_11_0_ODCAP_UCLK_MAX)) break; size += sysfs_emit_at(buf, size, ""OD_MCLK:\n""); size += sysfs_emit_at(buf, size, ""1: %uMHz\n"", od_table->UclkFmax); break; case SMU_OD_VDDC_CURVE: if (!smu->od_enabled || !od_table || !od_settings) break; if (!navi10_od_feature_is_supported(od_settings, SMU_11_0_ODCAP_GFXCLK_CURVE)) break; size += sysfs_emit_at(buf, size, ""OD_VDDC_CURVE:\n""); for (i = 0; i < 3; i++) { switch (i) { case 0: curve_settings = &od_table->GfxclkFreq1; break; case 1: curve_settings = &od_table->GfxclkFreq2; break; case 2: curve_settings = &od_table->GfxclkFreq3; break; default: break; } size += sysfs_emit_at(buf, size, ""%d: %uMHz %umV\n"", i, curve_settings[0], curve_settings[1] / NAVI10_VOLTAGE_SCALE); } break; case SMU_OD_RANGE: if (!smu->od_enabled || !od_table || !od_settings) break; size += sysfs_emit_at(buf, size, ""%s:\n"", ""OD_RANGE""); if (navi10_od_feature_is_supported(od_settings, SMU_11_0_ODCAP_GFXCLK_LIMITS)) { navi10_od_setting_get_range(od_settings, SMU_11_0_ODSETTING_GFXCLKFMIN, &min_value, NULL); navi10_od_setting_get_range(od_settings, SMU_11_0_ODSETTING_GFXCLKFMAX, NULL, &max_value); size += sysfs_emit_at(buf, size, ""SCLK: %7uMhz %10uMhz\n"", min_value, max_value); } if (navi10_od_feature_is_supported(od_settings, SMU_11_0_ODCAP_UCLK_MAX)) { navi10_od_setting_get_range(od_settings, SMU_11_0_ODSETTING_UCLKFMAX, &min_value, &max_value); size += sysfs_emit_at(buf, size, ""MCLK: %7uMhz %10uMhz\n"", min_value, max_value); } if (navi10_od_feature_is_supported(od_settings, SMU_11_0_ODCAP_GFXCLK_CURVE)) { navi10_od_setting_get_range(od_settings, SMU_11_0_ODSETTING_VDDGFXCURVEFREQ_P1, &min_value, &max_value); size += sysfs_emit_at(buf, size, ""VDDC_CURVE_SCLK[0]: %7uMhz %10uMhz\n"", min_value, max_value); navi10_od_setting_get_range(od_settings, SMU_11_0_ODSETTING_VDDGFXCURVEVOLTAGE_P1, &min_value, &max_value); size += sysfs_emit_at(buf, size, ""VDDC_CURVE_VOLT[0]: %7dmV %11dmV\n"", min_value, max_value); navi10_od_setting_get_range(od_settings, SMU_11_0_ODSETTING_VDDGFXCURVEFREQ_P2, &min_value, &max_value); size += sysfs_emit_at(buf, size, ""VDDC_CURVE_SCLK[1]: %7uMhz %10uMhz\n"", min_value, max_value); navi10_od_setting_get_range(od_settings, SMU_11_0_ODSETTING_VDDGFXCURVEVOLTAGE_P2, &min_value, &max_value); size += sysfs_emit_at(buf, size, ""VDDC_CURVE_VOLT[1]: %7dmV %11dmV\n"", min_value, max_value); navi10_od_setting_get_range(od_settings, SMU_11_0_ODSETTING_VDDGFXCURVEFREQ_P3, &min_value, &max_value); size += sysfs_emit_at(buf, size, ""VDDC_CURVE_SCLK[2]: %7uMhz %10uMhz\n"", min_value, max_value); navi10_od_setting_get_range(od_settings, SMU_11_0_ODSETTING_VDDGFXCURVEVOLTAGE_P3, &min_value, &max_value); size += sysfs_emit_at(buf, size, ""VDDC_CURVE_VOLT[2]: %7dmV %11dmV\n"", min_value, max_value); } break; default: break; } return size; }","- if (!navi10_is_support_fine_grained_dpm(smu, clk_type)) {
+ ret = navi10_is_support_fine_grained_dpm(smu, clk_type);
+ if (ret < 0)
+ return ret;
+ if (!ret) {","static int navi10_print_clk_levels(struct smu_context *smu, enum smu_clk_type clk_type, char *buf) { uint16_t *curve_settings; int i, levels, size = 0, ret = 0; uint32_t cur_value = 0, value = 0, count = 0; uint32_t freq_values[3] = {0}; uint32_t mark_index = 0; struct smu_table_context *table_context = &smu->smu_table; uint32_t gen_speed, lane_width; struct smu_dpm_context *smu_dpm = &smu->smu_dpm; struct smu_11_0_dpm_context *dpm_context = smu_dpm->dpm_context; PPTable_t *pptable = (PPTable_t *)table_context->driver_pptable; OverDriveTable_t *od_table = (OverDriveTable_t *)table_context->overdrive_table; struct smu_11_0_overdrive_table *od_settings = smu->od_settings; uint32_t min_value, max_value; smu_cmn_get_sysfs_buf(&buf, &size); switch (clk_type) { case SMU_GFXCLK: case SMU_SCLK: case SMU_SOCCLK: case SMU_MCLK: case SMU_UCLK: case SMU_FCLK: case SMU_VCLK: case SMU_DCLK: case SMU_DCEFCLK: ret = navi10_get_current_clk_freq_by_table(smu, clk_type, &cur_value); if (ret) return size; ret = smu_v11_0_get_dpm_level_count(smu, clk_type, &count); if (ret) return size; ret = navi10_is_support_fine_grained_dpm(smu, clk_type); if (ret < 0) return ret; if (!ret) { for (i = 0; i < count; i++) { ret = smu_v11_0_get_dpm_freq_by_index(smu, clk_type, i, &value); if (ret) return size; size += sysfs_emit_at(buf, size, ""%d: %uMhz %s\n"", i, value, cur_value == value ? ""*"" : """"); } } else { ret = smu_v11_0_get_dpm_freq_by_index(smu, clk_type, 0, &freq_values[0]); if (ret) return size; ret = smu_v11_0_get_dpm_freq_by_index(smu, clk_type, count - 1, &freq_values[2]); if (ret) return size; freq_values[1] = cur_value; mark_index = cur_value == freq_values[0] ? 0 : cur_value == freq_values[2] ? 2 : 1; levels = 3; if (mark_index != 1) { levels = 2; freq_values[1] = freq_values[2]; } for (i = 0; i < levels; i++) { size += sysfs_emit_at(buf, size, ""%d: %uMhz %s\n"", i, freq_values[i], i == mark_index ? ""*"" : """"); } } break; case SMU_PCIE: gen_speed = smu_v11_0_get_current_pcie_link_speed_level(smu); lane_width = smu_v11_0_get_current_pcie_link_width_level(smu); for (i = 0; i < NUM_LINK_LEVELS; i++) size += sysfs_emit_at(buf, size, ""%d: %s %s %dMhz %s\n"", i, (dpm_context->dpm_tables.pcie_table.pcie_gen[i] == 0) ? ""2.5GT/s,"" : (dpm_context->dpm_tables.pcie_table.pcie_gen[i] == 1) ? ""5.0GT/s,"" : (dpm_context->dpm_tables.pcie_table.pcie_gen[i] == 2) ? ""8.0GT/s,"" : (dpm_context->dpm_tables.pcie_table.pcie_gen[i] == 3) ? ""16.0GT/s,"" : """", (dpm_context->dpm_tables.pcie_table.pcie_lane[i] == 1) ? ""x1"" : (dpm_context->dpm_tables.pcie_table.pcie_lane[i] == 2) ? ""x2"" : (dpm_context->dpm_tables.pcie_table.pcie_lane[i] == 3) ? ""x4"" : (dpm_context->dpm_tables.pcie_table.pcie_lane[i] == 4) ? ""x8"" : (dpm_context->dpm_tables.pcie_table.pcie_lane[i] == 5) ? ""x12"" : (dpm_context->dpm_tables.pcie_table.pcie_lane[i] == 6) ? ""x16"" : """", pptable->LclkFreq[i], (gen_speed == dpm_context->dpm_tables.pcie_table.pcie_gen[i]) && (lane_width == dpm_context->dpm_tables.pcie_table.pcie_lane[i]) ? ""*"" : """"); break; case SMU_OD_SCLK: if (!smu->od_enabled || !od_table || !od_settings) break; if (!navi10_od_feature_is_supported(od_settings, SMU_11_0_ODCAP_GFXCLK_LIMITS)) break; size += sysfs_emit_at(buf, size, ""OD_SCLK:\n""); size += sysfs_emit_at(buf, size, ""0: %uMhz\n1: %uMhz\n"", od_table->GfxclkFmin, od_table->GfxclkFmax); break; case SMU_OD_MCLK: if (!smu->od_enabled || !od_table || !od_settings) break; if (!navi10_od_feature_is_supported(od_settings, SMU_11_0_ODCAP_UCLK_MAX)) break; size += sysfs_emit_at(buf, size, ""OD_MCLK:\n""); size += sysfs_emit_at(buf, size, ""1: %uMHz\n"", od_table->UclkFmax); break; case SMU_OD_VDDC_CURVE: if (!smu->od_enabled || !od_table || !od_settings) break; if (!navi10_od_feature_is_supported(od_settings, SMU_11_0_ODCAP_GFXCLK_CURVE)) break; size += sysfs_emit_at(buf, size, ""OD_VDDC_CURVE:\n""); for (i = 0; i < 3; i++) { switch (i) { case 0: curve_settings = &od_table->GfxclkFreq1; break; case 1: curve_settings = &od_table->GfxclkFreq2; break; case 2: curve_settings = &od_table->GfxclkFreq3; break; default: break; } size += sysfs_emit_at(buf, size, ""%d: %uMHz %umV\n"", i, curve_settings[0], curve_settings[1] / NAVI10_VOLTAGE_SCALE); } break; case SMU_OD_RANGE: if (!smu->od_enabled || !od_table || !od_settings) break; size += sysfs_emit_at(buf, size, ""%s:\n"", ""OD_RANGE""); if (navi10_od_feature_is_supported(od_settings, SMU_11_0_ODCAP_GFXCLK_LIMITS)) { navi10_od_setting_get_range(od_settings, SMU_11_0_ODSETTING_GFXCLKFMIN, &min_value, NULL); navi10_od_setting_get_range(od_settings, SMU_11_0_ODSETTING_GFXCLKFMAX, NULL, &max_value); size += sysfs_emit_at(buf, size, ""SCLK: %7uMhz %10uMhz\n"", min_value, max_value); } if (navi10_od_feature_is_supported(od_settings, SMU_11_0_ODCAP_UCLK_MAX)) { navi10_od_setting_get_range(od_settings, SMU_11_0_ODSETTING_UCLKFMAX, &min_value, &max_value); size += sysfs_emit_at(buf, size, ""MCLK: %7uMhz %10uMhz\n"", min_value, max_value); } if (navi10_od_feature_is_supported(od_settings, SMU_11_0_ODCAP_GFXCLK_CURVE)) { navi10_od_setting_get_range(od_settings, SMU_11_0_ODSETTING_VDDGFXCURVEFREQ_P1, &min_value, &max_value); size += sysfs_emit_at(buf, size, ""VDDC_CURVE_SCLK[0]: %7uMhz %10uMhz\n"", min_value, max_value); navi10_od_setting_get_range(od_settings, SMU_11_0_ODSETTING_VDDGFXCURVEVOLTAGE_P1, &min_value, &max_value); size += sysfs_emit_at(buf, size, ""VDDC_CURVE_VOLT[0]: %7dmV %11dmV\n"", min_value, max_value); navi10_od_setting_get_range(od_settings, SMU_11_0_ODSETTING_VDDGFXCURVEFREQ_P2, &min_value, &max_value); size += sysfs_emit_at(buf, size, ""VDDC_CURVE_SCLK[1]: %7uMhz %10uMhz\n"", min_value, max_value); navi10_od_setting_get_range(od_settings, SMU_11_0_ODSETTING_VDDGFXCURVEVOLTAGE_P2, &min_value, &max_value); size += sysfs_emit_at(buf, size, ""VDDC_CURVE_VOLT[1]: %7dmV %11dmV\n"", min_value, max_value); navi10_od_setting_get_range(od_settings, SMU_11_0_ODSETTING_VDDGFXCURVEFREQ_P3, &min_value, &max_value); size += sysfs_emit_at(buf, size, ""VDDC_CURVE_SCLK[2]: %7uMhz %10uMhz\n"", min_value, max_value); navi10_od_setting_get_range(od_settings, SMU_11_0_ODSETTING_VDDGFXCURVEVOLTAGE_P3, &min_value, &max_value); size += sysfs_emit_at(buf, size, ""VDDC_CURVE_VOLT[2]: %7dmV %11dmV\n"", min_value, max_value); } break; default: break; } return size; }"
215----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46698/bad/platform.c----of_platform_default_populate_init,"static int __init of_platform_default_populate_init(void) { struct device_node *node; device_links_supplier_sync_state_pause(); if (IS_ENABLED(CONFIG_PPC)) { struct device_node *boot_display = NULL; struct platform_device *dev; int display_number = 0; int ret; if (of_property_present(of_chosen, ""linux,bootx-noscreen"")) { dev = platform_device_alloc(""bootx-noscreen"", 0); if (WARN_ON(!dev)) return -ENOMEM; ret = platform_device_add(dev); if (WARN_ON(ret)) { platform_device_put(dev); return ret; } } for_each_node_by_type(node, ""display"") { if (!of_get_property(node, ""linux,opened"", NULL) || !of_get_property(node, ""linux,boot-display"", NULL)) continue; dev = of_platform_device_create(node, ""of-display"", NULL); of_node_put(node); if (WARN_ON(!dev)) return -ENOMEM; boot_display = node; display_number++; break; } for_each_node_by_type(node, ""display"") { char buf[14]; const char *of_display_format = ""of-display.%d""; if (!of_get_property(node, ""linux,opened"", NULL) || node == boot_display) continue; ret = snprintf(buf, sizeof(buf), of_display_format, display_number++); if (ret < sizeof(buf)) of_platform_device_create(node, buf, NULL); } } else { for_each_matching_node(node, reserved_mem_matches) of_platform_device_create(node, NULL, NULL); node = of_find_node_by_path(""/firmware""); if (node) { of_platform_populate(node, NULL, NULL, NULL); of_node_put(node); } node = of_get_compatible_child(of_chosen, ""simple-framebuffer""); if (node) { <S2SV_StartVul> sysfb_disable(); <S2SV_EndVul> of_platform_device_create(node, NULL, NULL); of_node_put(node); } of_platform_default_populate(NULL, NULL, NULL); } return 0; }","- sysfb_disable();
+ sysfb_disable(NULL);","static int __init of_platform_default_populate_init(void) { struct device_node *node; device_links_supplier_sync_state_pause(); if (IS_ENABLED(CONFIG_PPC)) { struct device_node *boot_display = NULL; struct platform_device *dev; int display_number = 0; int ret; if (of_property_present(of_chosen, ""linux,bootx-noscreen"")) { dev = platform_device_alloc(""bootx-noscreen"", 0); if (WARN_ON(!dev)) return -ENOMEM; ret = platform_device_add(dev); if (WARN_ON(ret)) { platform_device_put(dev); return ret; } } for_each_node_by_type(node, ""display"") { if (!of_get_property(node, ""linux,opened"", NULL) || !of_get_property(node, ""linux,boot-display"", NULL)) continue; dev = of_platform_device_create(node, ""of-display"", NULL); of_node_put(node); if (WARN_ON(!dev)) return -ENOMEM; boot_display = node; display_number++; break; } for_each_node_by_type(node, ""display"") { char buf[14]; const char *of_display_format = ""of-display.%d""; if (!of_get_property(node, ""linux,opened"", NULL) || node == boot_display) continue; ret = snprintf(buf, sizeof(buf), of_display_format, display_number++); if (ret < sizeof(buf)) of_platform_device_create(node, buf, NULL); } } else { for_each_matching_node(node, reserved_mem_matches) of_platform_device_create(node, NULL, NULL); node = of_find_node_by_path(""/firmware""); if (node) { of_platform_populate(node, NULL, NULL, NULL); of_node_put(node); } node = of_get_compatible_child(of_chosen, ""simple-framebuffer""); if (node) { sysfb_disable(NULL); of_platform_device_create(node, NULL, NULL); of_node_put(node); } of_platform_default_populate(NULL, NULL, NULL); } return 0; }"
366----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46829/bad/rtmutex.c----__rt_mutex_slowlock,"static int __sched __rt_mutex_slowlock(struct rt_mutex_base *lock, struct ww_acquire_ctx *ww_ctx, unsigned int state, enum rtmutex_chainwalk chwalk, struct rt_mutex_waiter *waiter) { struct rt_mutex *rtm = container_of(lock, struct rt_mutex, rtmutex); struct ww_mutex *ww = ww_container_of(rtm); int ret; lockdep_assert_held(&lock->wait_lock); if (try_to_take_rt_mutex(lock, current, NULL)) { if (build_ww_mutex() && ww_ctx) { __ww_mutex_check_waiters(rtm, ww_ctx); ww_mutex_lock_acquired(ww, ww_ctx); } return 0; } set_current_state(state); trace_contention_begin(lock, LCB_F_RT); ret = task_blocks_on_rt_mutex(lock, waiter, current, ww_ctx, chwalk); if (likely(!ret)) ret = rt_mutex_slowlock_block(lock, ww_ctx, state, NULL, waiter); if (likely(!ret)) { if (build_ww_mutex() && ww_ctx) { if (!ww_ctx->is_wait_die) __ww_mutex_check_waiters(rtm, ww_ctx); ww_mutex_lock_acquired(ww, ww_ctx); } } else { __set_current_state(TASK_RUNNING); remove_waiter(lock, waiter); <S2SV_StartVul> rt_mutex_handle_deadlock(ret, chwalk, waiter); <S2SV_EndVul> } fixup_rt_mutex_waiters(lock, true); trace_contention_end(lock, ret); return ret; }","- rt_mutex_handle_deadlock(ret, chwalk, waiter);
+ rt_mutex_handle_deadlock(ret, chwalk, lock, waiter);","static int __sched __rt_mutex_slowlock(struct rt_mutex_base *lock, struct ww_acquire_ctx *ww_ctx, unsigned int state, enum rtmutex_chainwalk chwalk, struct rt_mutex_waiter *waiter) { struct rt_mutex *rtm = container_of(lock, struct rt_mutex, rtmutex); struct ww_mutex *ww = ww_container_of(rtm); int ret; lockdep_assert_held(&lock->wait_lock); if (try_to_take_rt_mutex(lock, current, NULL)) { if (build_ww_mutex() && ww_ctx) { __ww_mutex_check_waiters(rtm, ww_ctx); ww_mutex_lock_acquired(ww, ww_ctx); } return 0; } set_current_state(state); trace_contention_begin(lock, LCB_F_RT); ret = task_blocks_on_rt_mutex(lock, waiter, current, ww_ctx, chwalk); if (likely(!ret)) ret = rt_mutex_slowlock_block(lock, ww_ctx, state, NULL, waiter); if (likely(!ret)) { if (build_ww_mutex() && ww_ctx) { if (!ww_ctx->is_wait_die) __ww_mutex_check_waiters(rtm, ww_ctx); ww_mutex_lock_acquired(ww, ww_ctx); } } else { __set_current_state(TASK_RUNNING); remove_waiter(lock, waiter); rt_mutex_handle_deadlock(ret, chwalk, lock, waiter); } fixup_rt_mutex_waiters(lock, true); trace_contention_end(lock, ret); return ret; }"
917----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50162/bad/devmap.c----dev_map_bpf_prog_run,"static int dev_map_bpf_prog_run(struct bpf_prog *xdp_prog, struct xdp_frame **frames, int n, struct net_device *dev) { struct xdp_txq_info txq = { .dev = dev }; struct xdp_buff xdp; int i, nframes = 0; for (i = 0; i < n; i++) { struct xdp_frame *xdpf = frames[i]; u32 act; int err; xdp_convert_frame_to_buff(xdpf, &xdp); xdp.txq = &txq; act = bpf_prog_run_xdp(xdp_prog, &xdp); switch (act) { case XDP_PASS: err = xdp_update_frame_from_buff(&xdp, xdpf); if (unlikely(err < 0)) xdp_return_frame_rx_napi(xdpf); else frames[nframes++] = xdpf; break; default: bpf_warn_invalid_xdp_action(NULL, xdp_prog, act); fallthrough; case XDP_ABORTED: <S2SV_StartVul> trace_xdp_exception(dev, xdp_prog, act); <S2SV_EndVul> fallthrough; case XDP_DROP: xdp_return_frame_rx_napi(xdpf); break; } } return nframes; }","- trace_xdp_exception(dev, xdp_prog, act);
+ xdp.rxq = &rxq;
+ trace_xdp_exception(tx_dev, xdp_prog, act);","static int dev_map_bpf_prog_run(struct bpf_prog *xdp_prog, struct xdp_frame **frames, int n, struct net_device *tx_dev, struct net_device *rx_dev) { struct xdp_txq_info txq = { .dev = tx_dev }; struct xdp_rxq_info rxq = { .dev = rx_dev }; struct xdp_buff xdp; int i, nframes = 0; for (i = 0; i < n; i++) { struct xdp_frame *xdpf = frames[i]; u32 act; int err; xdp_convert_frame_to_buff(xdpf, &xdp); xdp.txq = &txq; xdp.rxq = &rxq; act = bpf_prog_run_xdp(xdp_prog, &xdp); switch (act) { case XDP_PASS: err = xdp_update_frame_from_buff(&xdp, xdpf); if (unlikely(err < 0)) xdp_return_frame_rx_napi(xdpf); else frames[nframes++] = xdpf; break; default: bpf_warn_invalid_xdp_action(NULL, xdp_prog, act); fallthrough; case XDP_ABORTED: trace_xdp_exception(tx_dev, xdp_prog, act); fallthrough; case XDP_DROP: xdp_return_frame_rx_napi(xdpf); break; } } return nframes; }"
322----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46788/bad/trace_osnoise.c----stop_kthread,"static void stop_kthread(unsigned int cpu) { struct task_struct *kthread; kthread = per_cpu(per_cpu_osnoise_var, cpu).kthread; if (kthread) { <S2SV_StartVul> if (test_bit(OSN_WORKLOAD, &osnoise_options)) { <S2SV_EndVul> kthread_stop(kthread); <S2SV_StartVul> } else { <S2SV_EndVul> kill_pid(kthread->thread_pid, SIGKILL, 1); put_task_struct(kthread); } per_cpu(per_cpu_osnoise_var, cpu).kthread = NULL; } else { if (!test_bit(OSN_WORKLOAD, &osnoise_options)) { per_cpu(per_cpu_osnoise_var, cpu).sampling = false; barrier(); return; } } }","- if (test_bit(OSN_WORKLOAD, &osnoise_options)) {
- } else {
+ kthread = per_cpu(per_cpu_osnoise_var, cpu).kthread;
+ if (cpumask_test_and_clear_cpu(cpu, &kthread_cpumask) &&
+ !WARN_ON(!test_bit(OSN_WORKLOAD, &osnoise_options))) {
+ kthread_stop(kthread);
+ } else if (!WARN_ON(test_bit(OSN_WORKLOAD, &osnoise_options))) {","static void stop_kthread(unsigned int cpu) { struct task_struct *kthread; kthread = per_cpu(per_cpu_osnoise_var, cpu).kthread; if (kthread) { if (cpumask_test_and_clear_cpu(cpu, &kthread_cpumask) && !WARN_ON(!test_bit(OSN_WORKLOAD, &osnoise_options))) { kthread_stop(kthread); } else if (!WARN_ON(test_bit(OSN_WORKLOAD, &osnoise_options))) { kill_pid(kthread->thread_pid, SIGKILL, 1); put_task_struct(kthread); } per_cpu(per_cpu_osnoise_var, cpu).kthread = NULL; } else { if (!test_bit(OSN_WORKLOAD, &osnoise_options)) { per_cpu(per_cpu_osnoise_var, cpu).sampling = false; barrier(); return; } } }"
94----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44963/bad/ctree.c----btrfs_force_cow_block,"int btrfs_force_cow_block(struct btrfs_trans_handle *trans, struct btrfs_root *root, struct extent_buffer *buf, struct extent_buffer *parent, int parent_slot, struct extent_buffer **cow_ret, u64 search_start, u64 empty_size, enum btrfs_lock_nesting nest) { struct btrfs_fs_info *fs_info = root->fs_info; struct btrfs_disk_key disk_key; struct extent_buffer *cow; int level, ret; int last_ref = 0; int unlock_orig = 0; u64 parent_start = 0; u64 reloc_src_root = 0; if (*cow_ret == buf) unlock_orig = 1; btrfs_assert_tree_write_locked(buf); WARN_ON(test_bit(BTRFS_ROOT_SHAREABLE, &root->state) && trans->transid != fs_info->running_transaction->transid); WARN_ON(test_bit(BTRFS_ROOT_SHAREABLE, &root->state) && trans->transid != root->last_trans); level = btrfs_header_level(buf); if (level == 0) btrfs_item_key(buf, &disk_key, 0); else btrfs_node_key(buf, &disk_key, 0); if (btrfs_root_id(root) == BTRFS_TREE_RELOC_OBJECTID) { if (parent) parent_start = parent->start; reloc_src_root = btrfs_header_owner(buf); } cow = btrfs_alloc_tree_block(trans, root, parent_start, btrfs_root_id(root), &disk_key, level, search_start, empty_size, reloc_src_root, nest); if (IS_ERR(cow)) return PTR_ERR(cow); copy_extent_buffer_full(cow, buf); btrfs_set_header_bytenr(cow, cow->start); btrfs_set_header_generation(cow, trans->transid); btrfs_set_header_backref_rev(cow, BTRFS_MIXED_BACKREF_REV); btrfs_clear_header_flag(cow, BTRFS_HEADER_FLAG_WRITTEN | BTRFS_HEADER_FLAG_RELOC); if (btrfs_root_id(root) == BTRFS_TREE_RELOC_OBJECTID) btrfs_set_header_flag(cow, BTRFS_HEADER_FLAG_RELOC); else btrfs_set_header_owner(cow, btrfs_root_id(root)); write_extent_buffer_fsid(cow, fs_info->fs_devices->metadata_uuid); ret = update_ref_for_cow(trans, root, buf, cow, &last_ref); if (ret) { btrfs_tree_unlock(cow); free_extent_buffer(cow); btrfs_abort_transaction(trans, ret); return ret; } if (test_bit(BTRFS_ROOT_SHAREABLE, &root->state)) { ret = btrfs_reloc_cow_block(trans, root, buf, cow); if (ret) { btrfs_tree_unlock(cow); free_extent_buffer(cow); btrfs_abort_transaction(trans, ret); return ret; } } if (buf == root->node) { WARN_ON(parent && parent != buf); if (btrfs_root_id(root) == BTRFS_TREE_RELOC_OBJECTID || btrfs_header_backref_rev(buf) < BTRFS_MIXED_BACKREF_REV) parent_start = buf->start; ret = btrfs_tree_mod_log_insert_root(root->node, cow, true); if (ret < 0) { btrfs_tree_unlock(cow); free_extent_buffer(cow); btrfs_abort_transaction(trans, ret); return ret; } atomic_inc(&cow->refs); rcu_assign_pointer(root->node, cow); <S2SV_StartVul> btrfs_free_tree_block(trans, btrfs_root_id(root), buf, <S2SV_EndVul> <S2SV_StartVul> parent_start, last_ref); <S2SV_EndVul> free_extent_buffer(buf); add_root_to_dirty_list(root); } else { WARN_ON(trans->transid != btrfs_header_generation(parent)); ret = btrfs_tree_mod_log_insert_key(parent, parent_slot, BTRFS_MOD_LOG_KEY_REPLACE); if (ret) { btrfs_tree_unlock(cow); free_extent_buffer(cow); btrfs_abort_transaction(trans, ret); return ret; } btrfs_set_node_blockptr(parent, parent_slot, cow->start); btrfs_set_node_ptr_generation(parent, parent_slot, trans->transid); btrfs_mark_buffer_dirty(trans, parent); if (last_ref) { ret = btrfs_tree_mod_log_free_eb(buf); if (ret) { btrfs_tree_unlock(cow); free_extent_buffer(cow); btrfs_abort_transaction(trans, ret); return ret; } } <S2SV_StartVul> btrfs_free_tree_block(trans, btrfs_root_id(root), buf, <S2SV_EndVul> <S2SV_StartVul> parent_start, last_ref); <S2SV_EndVul> } if (unlock_orig) btrfs_tree_unlock(buf); free_extent_buffer_stale(buf); btrfs_mark_buffer_dirty(trans, cow); *cow_ret = cow; return 0; }","- btrfs_free_tree_block(trans, btrfs_root_id(root), buf,
- parent_start, last_ref);
- btrfs_free_tree_block(trans, btrfs_root_id(root), buf,
- parent_start, last_ref);
+ ret = btrfs_free_tree_block(trans, btrfs_root_id(root), buf,
+ parent_start, last_ref);
+ if (ret < 0) {
+ btrfs_tree_unlock(cow);
+ free_extent_buffer(cow);
+ btrfs_abort_transaction(trans, ret);
+ return ret;
+ }
+ }
+ }
+ ret = btrfs_free_tree_block(trans, btrfs_root_id(root), buf,
+ parent_start, last_ref);
+ if (ret < 0) {
+ btrfs_tree_unlock(cow);
+ free_extent_buffer(cow);
+ btrfs_abort_transaction(trans, ret);
+ return ret;
+ }
+ }","int btrfs_force_cow_block(struct btrfs_trans_handle *trans, struct btrfs_root *root, struct extent_buffer *buf, struct extent_buffer *parent, int parent_slot, struct extent_buffer **cow_ret, u64 search_start, u64 empty_size, enum btrfs_lock_nesting nest) { struct btrfs_fs_info *fs_info = root->fs_info; struct btrfs_disk_key disk_key; struct extent_buffer *cow; int level, ret; int last_ref = 0; int unlock_orig = 0; u64 parent_start = 0; u64 reloc_src_root = 0; if (*cow_ret == buf) unlock_orig = 1; btrfs_assert_tree_write_locked(buf); WARN_ON(test_bit(BTRFS_ROOT_SHAREABLE, &root->state) && trans->transid != fs_info->running_transaction->transid); WARN_ON(test_bit(BTRFS_ROOT_SHAREABLE, &root->state) && trans->transid != root->last_trans); level = btrfs_header_level(buf); if (level == 0) btrfs_item_key(buf, &disk_key, 0); else btrfs_node_key(buf, &disk_key, 0); if (btrfs_root_id(root) == BTRFS_TREE_RELOC_OBJECTID) { if (parent) parent_start = parent->start; reloc_src_root = btrfs_header_owner(buf); } cow = btrfs_alloc_tree_block(trans, root, parent_start, btrfs_root_id(root), &disk_key, level, search_start, empty_size, reloc_src_root, nest); if (IS_ERR(cow)) return PTR_ERR(cow); copy_extent_buffer_full(cow, buf); btrfs_set_header_bytenr(cow, cow->start); btrfs_set_header_generation(cow, trans->transid); btrfs_set_header_backref_rev(cow, BTRFS_MIXED_BACKREF_REV); btrfs_clear_header_flag(cow, BTRFS_HEADER_FLAG_WRITTEN | BTRFS_HEADER_FLAG_RELOC); if (btrfs_root_id(root) == BTRFS_TREE_RELOC_OBJECTID) btrfs_set_header_flag(cow, BTRFS_HEADER_FLAG_RELOC); else btrfs_set_header_owner(cow, btrfs_root_id(root)); write_extent_buffer_fsid(cow, fs_info->fs_devices->metadata_uuid); ret = update_ref_for_cow(trans, root, buf, cow, &last_ref); if (ret) { btrfs_tree_unlock(cow); free_extent_buffer(cow); btrfs_abort_transaction(trans, ret); return ret; } if (test_bit(BTRFS_ROOT_SHAREABLE, &root->state)) { ret = btrfs_reloc_cow_block(trans, root, buf, cow); if (ret) { btrfs_tree_unlock(cow); free_extent_buffer(cow); btrfs_abort_transaction(trans, ret); return ret; } } if (buf == root->node) { WARN_ON(parent && parent != buf); if (btrfs_root_id(root) == BTRFS_TREE_RELOC_OBJECTID || btrfs_header_backref_rev(buf) < BTRFS_MIXED_BACKREF_REV) parent_start = buf->start; ret = btrfs_tree_mod_log_insert_root(root->node, cow, true); if (ret < 0) { btrfs_tree_unlock(cow); free_extent_buffer(cow); btrfs_abort_transaction(trans, ret); return ret; } atomic_inc(&cow->refs); rcu_assign_pointer(root->node, cow); ret = btrfs_free_tree_block(trans, btrfs_root_id(root), buf, parent_start, last_ref); free_extent_buffer(buf); add_root_to_dirty_list(root); if (ret < 0) { btrfs_tree_unlock(cow); free_extent_buffer(cow); btrfs_abort_transaction(trans, ret); return ret; } } else { WARN_ON(trans->transid != btrfs_header_generation(parent)); ret = btrfs_tree_mod_log_insert_key(parent, parent_slot, BTRFS_MOD_LOG_KEY_REPLACE); if (ret) { btrfs_tree_unlock(cow); free_extent_buffer(cow); btrfs_abort_transaction(trans, ret); return ret; } btrfs_set_node_blockptr(parent, parent_slot, cow->start); btrfs_set_node_ptr_generation(parent, parent_slot, trans->transid); btrfs_mark_buffer_dirty(trans, parent); if (last_ref) { ret = btrfs_tree_mod_log_free_eb(buf); if (ret) { btrfs_tree_unlock(cow); free_extent_buffer(cow); btrfs_abort_transaction(trans, ret); return ret; } } ret = btrfs_free_tree_block(trans, btrfs_root_id(root), buf, parent_start, last_ref); if (ret < 0) { btrfs_tree_unlock(cow); free_extent_buffer(cow); btrfs_abort_transaction(trans, ret); return ret; } } if (unlock_orig) btrfs_tree_unlock(buf); free_extent_buffer_stale(buf); btrfs_mark_buffer_dirty(trans, cow); *cow_ret = cow; return 0; }"
834----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50099/bad/decode-insn.c----arm_probe_decode_insn,"arm_probe_decode_insn(probe_opcode_t insn, struct arch_probe_insn *api) { if (aarch64_insn_is_steppable(insn)) return INSN_GOOD; if (aarch64_insn_is_bcond(insn)) { api->handler = simulate_b_cond; } else if (aarch64_insn_is_cbz(insn) || aarch64_insn_is_cbnz(insn)) { api->handler = simulate_cbz_cbnz; } else if (aarch64_insn_is_tbz(insn) || aarch64_insn_is_tbnz(insn)) { api->handler = simulate_tbz_tbnz; } else if (aarch64_insn_is_adr_adrp(insn)) { api->handler = simulate_adr_adrp; } else if (aarch64_insn_is_b(insn) || aarch64_insn_is_bl(insn)) { api->handler = simulate_b_bl; } else if (aarch64_insn_is_br(insn) || aarch64_insn_is_blr(insn) || aarch64_insn_is_ret(insn)) { api->handler = simulate_br_blr_ret; <S2SV_StartVul> } else if (aarch64_insn_is_ldr_lit(insn)) { <S2SV_EndVul> <S2SV_StartVul> api->handler = simulate_ldr_literal; <S2SV_EndVul> <S2SV_StartVul> } else if (aarch64_insn_is_ldrsw_lit(insn)) { <S2SV_EndVul> <S2SV_StartVul> api->handler = simulate_ldrsw_literal; <S2SV_EndVul> } else { return INSN_REJECTED; } return INSN_GOOD_NO_SLOT; }","- } else if (aarch64_insn_is_ldr_lit(insn)) {
- api->handler = simulate_ldr_literal;
- } else if (aarch64_insn_is_ldrsw_lit(insn)) {
- api->handler = simulate_ldrsw_literal;
+ } else {","arm_probe_decode_insn(probe_opcode_t insn, struct arch_probe_insn *api) { if (aarch64_insn_is_steppable(insn)) return INSN_GOOD; if (aarch64_insn_is_bcond(insn)) { api->handler = simulate_b_cond; } else if (aarch64_insn_is_cbz(insn) || aarch64_insn_is_cbnz(insn)) { api->handler = simulate_cbz_cbnz; } else if (aarch64_insn_is_tbz(insn) || aarch64_insn_is_tbnz(insn)) { api->handler = simulate_tbz_tbnz; } else if (aarch64_insn_is_adr_adrp(insn)) { api->handler = simulate_adr_adrp; } else if (aarch64_insn_is_b(insn) || aarch64_insn_is_bl(insn)) { api->handler = simulate_b_bl; } else if (aarch64_insn_is_br(insn) || aarch64_insn_is_blr(insn) || aarch64_insn_is_ret(insn)) { api->handler = simulate_br_blr_ret; } else { return INSN_REJECTED; } return INSN_GOOD_NO_SLOT; }"
1434----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56759/bad/ctree.c----btrfs_cow_block,"int btrfs_cow_block(struct btrfs_trans_handle *trans, struct btrfs_root *root, struct extent_buffer *buf, struct extent_buffer *parent, int parent_slot, struct extent_buffer **cow_ret, enum btrfs_lock_nesting nest) { struct btrfs_fs_info *fs_info = root->fs_info; u64 search_start; <S2SV_StartVul> int ret; <S2SV_EndVul> if (unlikely(test_bit(BTRFS_ROOT_DELETING, &root->state))) { btrfs_abort_transaction(trans, -EUCLEAN); btrfs_crit(fs_info, ""attempt to COW block %llu on root %llu that is being deleted"", buf->start, btrfs_root_id(root)); return -EUCLEAN; } if (unlikely(trans->transaction != fs_info->running_transaction || trans->transid != fs_info->generation)) { btrfs_abort_transaction(trans, -EUCLEAN); btrfs_crit(fs_info, ""unexpected transaction when attempting to COW block %llu on root %llu, transaction %llu running transaction %llu fs generation %llu"", buf->start, btrfs_root_id(root), trans->transid, fs_info->running_transaction->transid, fs_info->generation); return -EUCLEAN; } if (!should_cow_block(trans, root, buf)) { *cow_ret = buf; return 0; } search_start = round_down(buf->start, SZ_1G); btrfs_qgroup_trace_subtree_after_cow(trans, root, buf); <S2SV_StartVul> ret = btrfs_force_cow_block(trans, root, buf, parent, parent_slot, <S2SV_EndVul> <S2SV_StartVul> cow_ret, search_start, 0, nest); <S2SV_EndVul> <S2SV_StartVul> trace_btrfs_cow_block(root, buf, *cow_ret); <S2SV_EndVul> <S2SV_StartVul> return ret; <S2SV_EndVul> }","- int ret;
- ret = btrfs_force_cow_block(trans, root, buf, parent, parent_slot,
- cow_ret, search_start, 0, nest);
- trace_btrfs_cow_block(root, buf, *cow_ret);
- return ret;
+ return btrfs_force_cow_block(trans, root, buf, parent, parent_slot,
+ cow_ret, search_start, 0, nest);","int btrfs_cow_block(struct btrfs_trans_handle *trans, struct btrfs_root *root, struct extent_buffer *buf, struct extent_buffer *parent, int parent_slot, struct extent_buffer **cow_ret, enum btrfs_lock_nesting nest) { struct btrfs_fs_info *fs_info = root->fs_info; u64 search_start; if (unlikely(test_bit(BTRFS_ROOT_DELETING, &root->state))) { btrfs_abort_transaction(trans, -EUCLEAN); btrfs_crit(fs_info, ""attempt to COW block %llu on root %llu that is being deleted"", buf->start, btrfs_root_id(root)); return -EUCLEAN; } if (unlikely(trans->transaction != fs_info->running_transaction || trans->transid != fs_info->generation)) { btrfs_abort_transaction(trans, -EUCLEAN); btrfs_crit(fs_info, ""unexpected transaction when attempting to COW block %llu on root %llu, transaction %llu running transaction %llu fs generation %llu"", buf->start, btrfs_root_id(root), trans->transid, fs_info->running_transaction->transid, fs_info->generation); return -EUCLEAN; } if (!should_cow_block(trans, root, buf)) { *cow_ret = buf; return 0; } search_start = round_down(buf->start, SZ_1G); btrfs_qgroup_trace_subtree_after_cow(trans, root, buf); return btrfs_force_cow_block(trans, root, buf, parent, parent_slot, cow_ret, search_start, 0, nest); }"
1334----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56653/bad/btmtk.c----btmtk_process_coredump,"int btmtk_process_coredump(struct hci_dev *hdev, struct sk_buff *skb) { struct btmtk_data *data = hci_get_priv(hdev); int err; if (!IS_ENABLED(CONFIG_DEV_COREDUMP)) { kfree_skb(skb); return 0; } switch (data->cd_info.state) { case HCI_DEVCOREDUMP_IDLE: err = hci_devcd_init(hdev, MTK_COREDUMP_SIZE); if (err < 0) { kfree_skb(skb); break; } data->cd_info.cnt = 0; schedule_delayed_work(&hdev->dump.dump_timeout, msecs_to_jiffies(5000)); fallthrough; case HCI_DEVCOREDUMP_ACTIVE: default: err = hci_devcd_append(hdev, skb); if (err < 0) break; data->cd_info.cnt++; <S2SV_StartVul> if (data->cd_info.cnt > MTK_COREDUMP_NUM && <S2SV_EndVul> <S2SV_StartVul> skb->len > MTK_COREDUMP_END_LEN) <S2SV_EndVul> <S2SV_StartVul> if (!memcmp((char *)&skb->data[skb->len - MTK_COREDUMP_END_LEN], <S2SV_EndVul> <S2SV_StartVul> MTK_COREDUMP_END, MTK_COREDUMP_END_LEN - 1)) { <S2SV_EndVul> <S2SV_StartVul> bt_dev_info(hdev, ""Mediatek coredump end""); <S2SV_EndVul> <S2SV_StartVul> hci_devcd_complete(hdev); <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> break; <S2SV_StartVul> } <S2SV_EndVul> return err; }","- if (data->cd_info.cnt > MTK_COREDUMP_NUM &&
- skb->len > MTK_COREDUMP_END_LEN)
- if (!memcmp((char *)&skb->data[skb->len - MTK_COREDUMP_END_LEN],
- MTK_COREDUMP_END, MTK_COREDUMP_END_LEN - 1)) {
- bt_dev_info(hdev, ""Mediatek coredump end"");
- hci_devcd_complete(hdev);
- }
- }
+ bool complete = false;
+ if (data->cd_info.cnt >= MTK_COREDUMP_NUM &&
+ skb->len > MTK_COREDUMP_END_LEN)
+ if (!memcmp((char *)&skb->data[skb->len - MTK_COREDUMP_END_LEN],
+ MTK_COREDUMP_END, MTK_COREDUMP_END_LEN - 1))
+ complete = true;
+ if (complete) {
+ bt_dev_info(hdev, ""Mediatek coredump end"");
+ hci_devcd_complete(hdev);
+ }
+ }","int btmtk_process_coredump(struct hci_dev *hdev, struct sk_buff *skb) { struct btmtk_data *data = hci_get_priv(hdev); int err; bool complete = false; if (!IS_ENABLED(CONFIG_DEV_COREDUMP)) { kfree_skb(skb); return 0; } switch (data->cd_info.state) { case HCI_DEVCOREDUMP_IDLE: err = hci_devcd_init(hdev, MTK_COREDUMP_SIZE); if (err < 0) { kfree_skb(skb); break; } data->cd_info.cnt = 0; schedule_delayed_work(&hdev->dump.dump_timeout, msecs_to_jiffies(5000)); fallthrough; case HCI_DEVCOREDUMP_ACTIVE: default: if (data->cd_info.cnt >= MTK_COREDUMP_NUM && skb->len > MTK_COREDUMP_END_LEN) if (!memcmp((char *)&skb->data[skb->len - MTK_COREDUMP_END_LEN], MTK_COREDUMP_END, MTK_COREDUMP_END_LEN - 1)) complete = true; err = hci_devcd_append(hdev, skb); if (err < 0) break; data->cd_info.cnt++; if (complete) { bt_dev_info(hdev, ""Mediatek coredump end""); hci_devcd_complete(hdev); } break; } return err; }"
1315----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56617/bad/cacheinfo.c----last_level_cache_is_valid,"bool last_level_cache_is_valid(unsigned int cpu) { struct cacheinfo *llc; <S2SV_StartVul> if (!cache_leaves(cpu)) <S2SV_EndVul> return false; llc = per_cpu_cacheinfo_idx(cpu, cache_leaves(cpu) - 1); return (llc->attributes & CACHE_ID) || !!llc->fw_token; }","- if (!cache_leaves(cpu))
+ if (!cache_leaves(cpu) || !per_cpu_cacheinfo(cpu))","bool last_level_cache_is_valid(unsigned int cpu) { struct cacheinfo *llc; if (!cache_leaves(cpu) || !per_cpu_cacheinfo(cpu)) return false; llc = per_cpu_cacheinfo_idx(cpu, cache_leaves(cpu) - 1); return (llc->attributes & CACHE_ID) || !!llc->fw_token; }"
1310----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56612/bad/gup.c----sanity_check_pinned_pages,"static inline void sanity_check_pinned_pages(struct page **pages, unsigned long npages) { if (!IS_ENABLED(CONFIG_DEBUG_VM)) return; for (; npages; npages--, pages++) { struct page *page = *pages; <S2SV_StartVul> struct folio *folio = page_folio(page); <S2SV_EndVul> if (is_zero_page(page) || !folio_test_anon(folio)) continue; if (!folio_test_large(folio) || folio_test_hugetlb(folio)) VM_BUG_ON_PAGE(!PageAnonExclusive(&folio->page), page); else VM_BUG_ON_PAGE(!PageAnonExclusive(&folio->page) && !PageAnonExclusive(page), page); } }","- struct folio *folio = page_folio(page);
+ struct folio *folio;
+ if (!page)
+ continue;
+ folio = page_folio(page);","static inline void sanity_check_pinned_pages(struct page **pages, unsigned long npages) { if (!IS_ENABLED(CONFIG_DEBUG_VM)) return; for (; npages; npages--, pages++) { struct page *page = *pages; struct folio *folio; if (!page) continue; folio = page_folio(page); if (is_zero_page(page) || !folio_test_anon(folio)) continue; if (!folio_test_large(folio) || folio_test_hugetlb(folio)) VM_BUG_ON_PAGE(!PageAnonExclusive(&folio->page), page); else VM_BUG_ON_PAGE(!PageAnonExclusive(&folio->page) && !PageAnonExclusive(page), page); } }"
1388----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56722/bad/hns_roce_hw_v2.c----hns_roce_v2_destroy_qp_common,"static int hns_roce_v2_destroy_qp_common(struct hns_roce_dev *hr_dev, struct hns_roce_qp *hr_qp, struct ib_udata *udata) { struct ib_device *ibdev = &hr_dev->ib_dev; struct hns_roce_cq *send_cq, *recv_cq; unsigned long flags; int ret = 0; if (modify_qp_is_ok(hr_qp)) { ret = hns_roce_v2_modify_qp(&hr_qp->ibqp, NULL, 0, hr_qp->state, IB_QPS_RESET, udata); if (ret) <S2SV_StartVul> ibdev_err(ibdev, <S2SV_EndVul> <S2SV_StartVul> ""failed to modify QP to RST, ret = %d.\n"", <S2SV_EndVul> <S2SV_StartVul> ret); <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> send_cq = hr_qp->ibqp.send_cq ? to_hr_cq(hr_qp->ibqp.send_cq) : NULL; recv_cq = hr_qp->ibqp.recv_cq ? to_hr_cq(hr_qp->ibqp.recv_cq) : NULL; spin_lock_irqsave(&hr_dev->qp_list_lock, flags); hns_roce_lock_cqs(send_cq, recv_cq); if (!udata) { if (recv_cq) __hns_roce_v2_cq_clean(recv_cq, hr_qp->qpn, (hr_qp->ibqp.srq ? to_hr_srq(hr_qp->ibqp.srq) : NULL)); if (send_cq && send_cq != recv_cq) __hns_roce_v2_cq_clean(send_cq, hr_qp->qpn, NULL); } hns_roce_qp_remove(hr_dev, hr_qp); hns_roce_unlock_cqs(send_cq, recv_cq); spin_unlock_irqrestore(&hr_dev->qp_list_lock, flags); return ret; }","- ibdev_err(ibdev,
- ""failed to modify QP to RST, ret = %d.\n"",
- ret);
- }
+ ibdev_err_ratelimited(ibdev,
+ ""failed to modify QP to RST, ret = %d.\n"",
+ ret);","static int hns_roce_v2_destroy_qp_common(struct hns_roce_dev *hr_dev, struct hns_roce_qp *hr_qp, struct ib_udata *udata) { struct ib_device *ibdev = &hr_dev->ib_dev; struct hns_roce_cq *send_cq, *recv_cq; unsigned long flags; int ret = 0; if (modify_qp_is_ok(hr_qp)) { ret = hns_roce_v2_modify_qp(&hr_qp->ibqp, NULL, 0, hr_qp->state, IB_QPS_RESET, udata); if (ret) ibdev_err_ratelimited(ibdev, ""failed to modify QP to RST, ret = %d.\n"", ret); } send_cq = hr_qp->ibqp.send_cq ? to_hr_cq(hr_qp->ibqp.send_cq) : NULL; recv_cq = hr_qp->ibqp.recv_cq ? to_hr_cq(hr_qp->ibqp.recv_cq) : NULL; spin_lock_irqsave(&hr_dev->qp_list_lock, flags); hns_roce_lock_cqs(send_cq, recv_cq); if (!udata) { if (recv_cq) __hns_roce_v2_cq_clean(recv_cq, hr_qp->qpn, (hr_qp->ibqp.srq ? to_hr_srq(hr_qp->ibqp.srq) : NULL)); if (send_cq && send_cq != recv_cq) __hns_roce_v2_cq_clean(send_cq, hr_qp->qpn, NULL); } hns_roce_qp_remove(hr_dev, hr_qp); hns_roce_unlock_cqs(send_cq, recv_cq); spin_unlock_irqrestore(&hr_dev->qp_list_lock, flags); return ret; }"
1145----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53121/bad/fs_core.c----lookup_fte_locked,"lookup_fte_locked(struct mlx5_flow_group *g, const u32 *match_value, bool take_write) { struct fs_fte *fte_tmp; if (take_write) nested_down_write_ref_node(&g->node, FS_LOCK_PARENT); else nested_down_read_ref_node(&g->node, FS_LOCK_PARENT); fte_tmp = rhashtable_lookup_fast(&g->ftes_hash, match_value, rhash_fte); if (!fte_tmp || !tree_get_node(&fte_tmp->node)) { fte_tmp = NULL; <S2SV_StartVul> goto out; <S2SV_EndVul> } if (!fte_tmp->node.active) { tree_put_node(&fte_tmp->node, false); <S2SV_StartVul> fte_tmp = NULL; <S2SV_EndVul> <S2SV_StartVul> goto out; <S2SV_EndVul> } <S2SV_StartVul> nested_down_write_ref_node(&fte_tmp->node, FS_LOCK_CHILD); <S2SV_EndVul> out: if (take_write) up_write_ref_node(&g->node, false); else up_read_ref_node(&g->node); return fte_tmp; }","- goto out;
- fte_tmp = NULL;
- goto out;
- nested_down_write_ref_node(&fte_tmp->node, FS_LOCK_CHILD);
+ nested_down_write_ref_node(&fte_tmp->node, FS_LOCK_CHILD);
+ up_write_ref_node(&fte_tmp->node, false);
+ if (take_write)
+ up_write_ref_node(&g->node, false);
+ else
+ up_read_ref_node(&g->node);
+ return NULL;
+ if (take_write)
+ up_write_ref_node(&g->node, false);","lookup_fte_locked(struct mlx5_flow_group *g, const u32 *match_value, bool take_write) { struct fs_fte *fte_tmp; if (take_write) nested_down_write_ref_node(&g->node, FS_LOCK_PARENT); else nested_down_read_ref_node(&g->node, FS_LOCK_PARENT); fte_tmp = rhashtable_lookup_fast(&g->ftes_hash, match_value, rhash_fte); if (!fte_tmp || !tree_get_node(&fte_tmp->node)) { fte_tmp = NULL; goto out; } nested_down_write_ref_node(&fte_tmp->node, FS_LOCK_CHILD); if (!fte_tmp->node.active) { up_write_ref_node(&fte_tmp->node, false); if (take_write) up_write_ref_node(&g->node, false); else up_read_ref_node(&g->node); tree_put_node(&fte_tmp->node, false); return NULL; } out: if (take_write) up_write_ref_node(&g->node, false); else up_read_ref_node(&g->node); return fte_tmp; }"
1144----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53120/bad/tc_ct.c----mlx5_tc_ct_entry_add_rule,"mlx5_tc_ct_entry_add_rule(struct mlx5_tc_ct_priv *ct_priv, struct flow_rule *flow_rule, struct mlx5_ct_entry *entry, bool nat, u8 zone_restore_id) { struct mlx5_ct_zone_rule *zone_rule = &entry->zone_rules[nat]; struct mlx5e_priv *priv = netdev_priv(ct_priv->netdev); struct mlx5_flow_spec *spec = NULL; struct mlx5_flow_attr *attr; int err; zone_rule->nat = nat; spec = kvzalloc(sizeof(*spec), GFP_KERNEL); if (!spec) return -ENOMEM; attr = mlx5_alloc_flow_attr(ct_priv->ns_type); if (!attr) { err = -ENOMEM; goto err_attr; } err = mlx5_tc_ct_entry_create_mod_hdr(ct_priv, attr, flow_rule, &zone_rule->mh, zone_restore_id, nat, mlx5_tc_ct_entry_has_nat(entry)); if (err) { ct_dbg(""Failed to create ct entry mod hdr""); goto err_mod_hdr; } attr->action = MLX5_FLOW_CONTEXT_ACTION_MOD_HDR | MLX5_FLOW_CONTEXT_ACTION_FWD_DEST | MLX5_FLOW_CONTEXT_ACTION_COUNT; attr->dest_chain = 0; attr->dest_ft = mlx5e_tc_post_act_get_ft(ct_priv->post_act); attr->ft = nat ? ct_priv->ct_nat : ct_priv->ct; attr->outer_match_level = MLX5_MATCH_L4; attr->counter = entry->counter->counter; attr->flags |= MLX5_ESW_ATTR_FLAG_NO_IN_PORT; if (ct_priv->ns_type == MLX5_FLOW_NAMESPACE_FDB) attr->esw_attr->in_mdev = priv->mdev; mlx5_tc_ct_set_tuple_match(netdev_priv(ct_priv->netdev), spec, flow_rule); mlx5e_tc_match_to_reg_match(spec, ZONE_TO_REG, entry->tuple.zone, MLX5_CT_ZONE_MASK); zone_rule->rule = mlx5_tc_rule_insert(priv, spec, attr); if (IS_ERR(zone_rule->rule)) { err = PTR_ERR(zone_rule->rule); ct_dbg(""Failed to add ct entry rule, nat: %d"", nat); goto err_rule; } zone_rule->attr = attr; kvfree(spec); ct_dbg(""Offloaded ct entry rule in zone %d"", entry->tuple.zone); return 0; err_rule: <S2SV_StartVul> mlx5_tc_ct_entry_destroy_mod_hdr(ct_priv, zone_rule->attr, zone_rule->mh); <S2SV_EndVul> mlx5_put_label_mapping(ct_priv, attr->ct_attr.ct_labels_id); err_mod_hdr: kfree(attr); err_attr: kvfree(spec); return err; }","- mlx5_tc_ct_entry_destroy_mod_hdr(ct_priv, zone_rule->attr, zone_rule->mh);
+ mlx5_tc_ct_entry_destroy_mod_hdr(ct_priv, attr, zone_rule->mh);","mlx5_tc_ct_entry_add_rule(struct mlx5_tc_ct_priv *ct_priv, struct flow_rule *flow_rule, struct mlx5_ct_entry *entry, bool nat, u8 zone_restore_id) { struct mlx5_ct_zone_rule *zone_rule = &entry->zone_rules[nat]; struct mlx5e_priv *priv = netdev_priv(ct_priv->netdev); struct mlx5_flow_spec *spec = NULL; struct mlx5_flow_attr *attr; int err; zone_rule->nat = nat; spec = kvzalloc(sizeof(*spec), GFP_KERNEL); if (!spec) return -ENOMEM; attr = mlx5_alloc_flow_attr(ct_priv->ns_type); if (!attr) { err = -ENOMEM; goto err_attr; } err = mlx5_tc_ct_entry_create_mod_hdr(ct_priv, attr, flow_rule, &zone_rule->mh, zone_restore_id, nat, mlx5_tc_ct_entry_has_nat(entry)); if (err) { ct_dbg(""Failed to create ct entry mod hdr""); goto err_mod_hdr; } attr->action = MLX5_FLOW_CONTEXT_ACTION_MOD_HDR | MLX5_FLOW_CONTEXT_ACTION_FWD_DEST | MLX5_FLOW_CONTEXT_ACTION_COUNT; attr->dest_chain = 0; attr->dest_ft = mlx5e_tc_post_act_get_ft(ct_priv->post_act); attr->ft = nat ? ct_priv->ct_nat : ct_priv->ct; attr->outer_match_level = MLX5_MATCH_L4; attr->counter = entry->counter->counter; attr->flags |= MLX5_ESW_ATTR_FLAG_NO_IN_PORT; if (ct_priv->ns_type == MLX5_FLOW_NAMESPACE_FDB) attr->esw_attr->in_mdev = priv->mdev; mlx5_tc_ct_set_tuple_match(netdev_priv(ct_priv->netdev), spec, flow_rule); mlx5e_tc_match_to_reg_match(spec, ZONE_TO_REG, entry->tuple.zone, MLX5_CT_ZONE_MASK); zone_rule->rule = mlx5_tc_rule_insert(priv, spec, attr); if (IS_ERR(zone_rule->rule)) { err = PTR_ERR(zone_rule->rule); ct_dbg(""Failed to add ct entry rule, nat: %d"", nat); goto err_rule; } zone_rule->attr = attr; kvfree(spec); ct_dbg(""Offloaded ct entry rule in zone %d"", entry->tuple.zone); return 0; err_rule: mlx5_tc_ct_entry_destroy_mod_hdr(ct_priv, attr, zone_rule->mh); mlx5_put_label_mapping(ct_priv, attr->ct_attr.ct_labels_id); err_mod_hdr: kfree(attr); err_attr: kvfree(spec); return err; }"
79----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44952/bad/core.c----dev_uevent,"static int dev_uevent(struct kset *kset, struct kobject *kobj, struct kobj_uevent_env *env) { struct device *dev = kobj_to_dev(kobj); int retval = 0; if (MAJOR(dev->devt)) { const char *tmp; const char *name; umode_t mode = 0; kuid_t uid = GLOBAL_ROOT_UID; kgid_t gid = GLOBAL_ROOT_GID; add_uevent_var(env, ""MAJOR=%u"", MAJOR(dev->devt)); add_uevent_var(env, ""MINOR=%u"", MINOR(dev->devt)); name = device_get_devnode(dev, &mode, &uid, &gid, &tmp); if (name) { add_uevent_var(env, ""DEVNAME=%s"", name); if (mode) add_uevent_var(env, ""DEVMODE=%#o"", mode & 0777); if (!uid_eq(uid, GLOBAL_ROOT_UID)) add_uevent_var(env, ""DEVUID=%u"", from_kuid(&init_user_ns, uid)); if (!gid_eq(gid, GLOBAL_ROOT_GID)) add_uevent_var(env, ""DEVGID=%u"", from_kgid(&init_user_ns, gid)); kfree(tmp); } } if (dev->type && dev->type->name) add_uevent_var(env, ""DEVTYPE=%s"", dev->type->name); if (dev->driver) add_uevent_var(env, ""DRIVER=%s"", dev->driver->name); of_device_uevent(dev, env); if (dev->bus && dev->bus->uevent) { retval = dev->bus->uevent(dev, env); if (retval) pr_debug(""device: '%s': %s: bus uevent() returned %d\n"", dev_name(dev), __func__, retval); } if (dev->class && dev->class->dev_uevent) { retval = dev->class->dev_uevent(dev, env); if (retval) pr_debug(""device: '%s': %s: class uevent() "" ""returned %d\n"", dev_name(dev), __func__, retval); } if (dev->type && dev->type->uevent) { <S2SV_StartVul> retval = dev->type->uevent(dev, env); <S2SV_EndVul> <S2SV_StartVul> if (retval) <S2SV_EndVul> pr_debug(""device: '%s': %s: dev_type uevent() "" ""returned %d\n"", dev_name(dev), __func__, retval); } return retval; }","- retval = dev->type->uevent(dev, env);
- if (retval)
+ }
+ if (dev->type && dev->type->uevent) {
+ retval = dev->type->uevent(dev, env);
+ if (retval)","static int dev_uevent(struct kset *kset, struct kobject *kobj, struct kobj_uevent_env *env) { struct device *dev = kobj_to_dev(kobj); struct device_driver *driver; int retval = 0; if (MAJOR(dev->devt)) { const char *tmp; const char *name; umode_t mode = 0; kuid_t uid = GLOBAL_ROOT_UID; kgid_t gid = GLOBAL_ROOT_GID; add_uevent_var(env, ""MAJOR=%u"", MAJOR(dev->devt)); add_uevent_var(env, ""MINOR=%u"", MINOR(dev->devt)); name = device_get_devnode(dev, &mode, &uid, &gid, &tmp); if (name) { add_uevent_var(env, ""DEVNAME=%s"", name); if (mode) add_uevent_var(env, ""DEVMODE=%#o"", mode & 0777); if (!uid_eq(uid, GLOBAL_ROOT_UID)) add_uevent_var(env, ""DEVUID=%u"", from_kuid(&init_user_ns, uid)); if (!gid_eq(gid, GLOBAL_ROOT_GID)) add_uevent_var(env, ""DEVGID=%u"", from_kgid(&init_user_ns, gid)); kfree(tmp); } } if (dev->type && dev->type->name) add_uevent_var(env, ""DEVTYPE=%s"", dev->type->name); rcu_read_lock(); driver = READ_ONCE(dev->driver); if (driver) add_uevent_var(env, ""DRIVER=%s"", driver->name); rcu_read_unlock(); of_device_uevent(dev, env); if (dev->bus && dev->bus->uevent) { retval = dev->bus->uevent(dev, env); if (retval) pr_debug(""device: '%s': %s: bus uevent() returned %d\n"", dev_name(dev), __func__, retval); } if (dev->class && dev->class->dev_uevent) { retval = dev->class->dev_uevent(dev, env); if (retval) pr_debug(""device: '%s': %s: class uevent() "" ""returned %d\n"", dev_name(dev), __func__, retval); } if (dev->type && dev->type->uevent) { retval = dev->type->uevent(dev, env); if (retval) pr_debug(""device: '%s': %s: dev_type uevent() "" ""returned %d\n"", dev_name(dev), __func__, retval); } return retval; }"
134----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44993/bad/v3d_sched.c----v3d_csd_job_run,"v3d_csd_job_run(struct drm_sched_job *sched_job) { struct v3d_csd_job *job = to_csd_job(sched_job); struct v3d_dev *v3d = job->base.v3d; struct drm_device *dev = &v3d->drm; struct dma_fence *fence; <S2SV_StartVul> int i, csd_cfg0_reg, csd_cfg_reg_count; <S2SV_EndVul> v3d->csd_job = job; v3d_invalidate_caches(v3d); fence = v3d_fence_create(v3d, V3D_CSD); if (IS_ERR(fence)) return NULL; if (job->base.irq_fence) dma_fence_put(job->base.irq_fence); job->base.irq_fence = dma_fence_get(fence); trace_v3d_submit_csd(dev, to_v3d_fence(fence)->seqno); v3d_job_start_stats(&job->base, V3D_CSD); v3d_switch_perfmon(v3d, &job->base); csd_cfg0_reg = V3D_CSD_QUEUED_CFG0(v3d->ver); <S2SV_StartVul> csd_cfg_reg_count = v3d->ver < 71 ? 6 : 7; <S2SV_EndVul> <S2SV_StartVul> for (i = 1; i <= csd_cfg_reg_count; i++) <S2SV_EndVul> V3D_CORE_WRITE(0, csd_cfg0_reg + 4 * i, job->args.cfg[i]); V3D_CORE_WRITE(0, csd_cfg0_reg, job->args.cfg[0]); return fence; }","- int i, csd_cfg0_reg, csd_cfg_reg_count;
- csd_cfg_reg_count = v3d->ver < 71 ? 6 : 7;
- for (i = 1; i <= csd_cfg_reg_count; i++)
+ int i, csd_cfg0_reg;
+ for (i = 1; i <= 6; i++)
+ if (v3d->ver >= 71)
+ V3D_CORE_WRITE(0, V3D_V7_CSD_QUEUED_CFG7, 0);","v3d_csd_job_run(struct drm_sched_job *sched_job) { struct v3d_csd_job *job = to_csd_job(sched_job); struct v3d_dev *v3d = job->base.v3d; struct drm_device *dev = &v3d->drm; struct dma_fence *fence; int i, csd_cfg0_reg; v3d->csd_job = job; v3d_invalidate_caches(v3d); fence = v3d_fence_create(v3d, V3D_CSD); if (IS_ERR(fence)) return NULL; if (job->base.irq_fence) dma_fence_put(job->base.irq_fence); job->base.irq_fence = dma_fence_get(fence); trace_v3d_submit_csd(dev, to_v3d_fence(fence)->seqno); v3d_job_start_stats(&job->base, V3D_CSD); v3d_switch_perfmon(v3d, &job->base); csd_cfg0_reg = V3D_CSD_QUEUED_CFG0(v3d->ver); for (i = 1; i <= 6; i++) V3D_CORE_WRITE(0, csd_cfg0_reg + 4 * i, job->args.cfg[i]); if (v3d->ver >= 71) V3D_CORE_WRITE(0, V3D_V7_CSD_QUEUED_CFG7, 0); V3D_CORE_WRITE(0, csd_cfg0_reg, job->args.cfg[0]); return fence; }"
1503----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2025-21634/bad/cpuset.c----cpuset_write_resmask,"ssize_t cpuset_write_resmask(struct kernfs_open_file *of, char *buf, size_t nbytes, loff_t off) { struct cpuset *cs = css_cs(of_css(of)); struct cpuset *trialcs; int retval = -ENODEV; buf = strstrip(buf); <S2SV_StartVul> css_get(&cs->css); <S2SV_EndVul> <S2SV_StartVul> kernfs_break_active_protection(of->kn); <S2SV_EndVul> cpus_read_lock(); mutex_lock(&cpuset_mutex); if (!is_cpuset_online(cs)) goto out_unlock; trialcs = alloc_trial_cpuset(cs); if (!trialcs) { retval = -ENOMEM; goto out_unlock; } switch (of_cft(of)->private) { case FILE_CPULIST: retval = update_cpumask(cs, trialcs, buf); break; case FILE_EXCLUSIVE_CPULIST: retval = update_exclusive_cpumask(cs, trialcs, buf); break; case FILE_MEMLIST: retval = update_nodemask(cs, trialcs, buf); break; default: retval = -EINVAL; break; } free_cpuset(trialcs); if (force_sd_rebuild) rebuild_sched_domains_locked(); out_unlock: mutex_unlock(&cpuset_mutex); cpus_read_unlock(); <S2SV_StartVul> kernfs_unbreak_active_protection(of->kn); <S2SV_EndVul> <S2SV_StartVul> css_put(&cs->css); <S2SV_EndVul> flush_workqueue(cpuset_migrate_mm_wq); return retval ?: nbytes; }","- css_get(&cs->css);
- kernfs_break_active_protection(of->kn);
- kernfs_unbreak_active_protection(of->kn);
- css_put(&cs->css);","ssize_t cpuset_write_resmask(struct kernfs_open_file *of, char *buf, size_t nbytes, loff_t off) { struct cpuset *cs = css_cs(of_css(of)); struct cpuset *trialcs; int retval = -ENODEV; buf = strstrip(buf); cpus_read_lock(); mutex_lock(&cpuset_mutex); if (!is_cpuset_online(cs)) goto out_unlock; trialcs = alloc_trial_cpuset(cs); if (!trialcs) { retval = -ENOMEM; goto out_unlock; } switch (of_cft(of)->private) { case FILE_CPULIST: retval = update_cpumask(cs, trialcs, buf); break; case FILE_EXCLUSIVE_CPULIST: retval = update_exclusive_cpumask(cs, trialcs, buf); break; case FILE_MEMLIST: retval = update_nodemask(cs, trialcs, buf); break; default: retval = -EINVAL; break; } free_cpuset(trialcs); if (force_sd_rebuild) rebuild_sched_domains_locked(); out_unlock: mutex_unlock(&cpuset_mutex); cpus_read_unlock(); flush_workqueue(cpuset_migrate_mm_wq); return retval ?: nbytes; }"
1206----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53207/bad/mgmt.c----mgmt_set_connectable_complete,"static void mgmt_set_connectable_complete(struct hci_dev *hdev, void *data, int err) { struct mgmt_pending_cmd *cmd = data; bt_dev_dbg(hdev, ""err %d"", err); <S2SV_StartVul> if (cmd != pending_find(MGMT_OP_SET_CONNECTABLE, hdev)) <S2SV_EndVul> return; hci_dev_lock(hdev); if (err) { u8 mgmt_err = mgmt_status(err); mgmt_cmd_status(cmd->sk, cmd->index, cmd->opcode, mgmt_err); goto done; } send_settings_rsp(cmd->sk, MGMT_OP_SET_CONNECTABLE, hdev); new_settings(hdev, cmd->sk); done: mgmt_pending_remove(cmd); hci_dev_unlock(hdev); }","- if (cmd != pending_find(MGMT_OP_SET_CONNECTABLE, hdev))
+ if (err == -ECANCELED ||
+ cmd != pending_find(MGMT_OP_SET_CONNECTABLE, hdev))
+ return;","static void mgmt_set_connectable_complete(struct hci_dev *hdev, void *data, int err) { struct mgmt_pending_cmd *cmd = data; bt_dev_dbg(hdev, ""err %d"", err); if (err == -ECANCELED || cmd != pending_find(MGMT_OP_SET_CONNECTABLE, hdev)) return; hci_dev_lock(hdev); if (err) { u8 mgmt_err = mgmt_status(err); mgmt_cmd_status(cmd->sk, cmd->index, cmd->opcode, mgmt_err); goto done; } send_settings_rsp(cmd->sk, MGMT_OP_SET_CONNECTABLE, hdev); new_settings(hdev, cmd->sk); done: mgmt_pending_remove(cmd); hci_dev_unlock(hdev); }"
1039----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50279/bad/dm-cache-target.c----can_resize,"static bool can_resize(struct cache *cache, dm_cblock_t new_size) { if (from_cblock(new_size) > from_cblock(cache->cache_size)) { if (cache->sized) { DMERR(""%s: unable to extend cache due to missing cache table reload"", cache_device_name(cache)); return false; } } while (from_cblock(new_size) < from_cblock(cache->cache_size)) { <S2SV_StartVul> new_size = to_cblock(from_cblock(new_size) + 1); <S2SV_EndVul> if (is_dirty(cache, new_size)) { DMERR(""%s: unable to shrink cache; cache block %llu is dirty"", cache_device_name(cache), (unsigned long long) from_cblock(new_size)); return false; } } return true; }","- new_size = to_cblock(from_cblock(new_size) + 1);
+ new_size = to_cblock(from_cblock(new_size) + 1);","static bool can_resize(struct cache *cache, dm_cblock_t new_size) { if (from_cblock(new_size) > from_cblock(cache->cache_size)) { if (cache->sized) { DMERR(""%s: unable to extend cache due to missing cache table reload"", cache_device_name(cache)); return false; } } while (from_cblock(new_size) < from_cblock(cache->cache_size)) { if (is_dirty(cache, new_size)) { DMERR(""%s: unable to shrink cache; cache block %llu is dirty"", cache_device_name(cache), (unsigned long long) from_cblock(new_size)); return false; } new_size = to_cblock(from_cblock(new_size) + 1); } return true; }"
790----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50066/bad/mremap.c----move_normal_pmd,"static bool move_normal_pmd(struct vm_area_struct *vma, unsigned long old_addr, unsigned long new_addr, pmd_t *old_pmd, pmd_t *new_pmd) { spinlock_t *old_ptl, *new_ptl; struct mm_struct *mm = vma->vm_mm; pmd_t pmd; if (!arch_supports_page_table_move()) return false; if (WARN_ON_ONCE(!pmd_none(*new_pmd))) return false; old_ptl = pmd_lock(vma->vm_mm, old_pmd); new_ptl = pmd_lockptr(mm, new_pmd); if (new_ptl != old_ptl) spin_lock_nested(new_ptl, SINGLE_DEPTH_NESTING); pmd = *old_pmd; pmd_clear(old_pmd); VM_BUG_ON(!pmd_none(*new_pmd)); pmd_populate(mm, new_pmd, pmd_pgtable(pmd)); flush_tlb_range(vma, old_addr, old_addr + PMD_SIZE); if (new_ptl != old_ptl) spin_unlock(new_ptl); spin_unlock(old_ptl); <S2SV_StartVul> return true; <S2SV_EndVul> }","- return true;
+ bool res = false;
+ if (unlikely(!pmd_present(pmd) || pmd_leaf(pmd)))
+ goto out_unlock;
+ res = true;
+ out_unlock:
+ return res;","static bool move_normal_pmd(struct vm_area_struct *vma, unsigned long old_addr, unsigned long new_addr, pmd_t *old_pmd, pmd_t *new_pmd) { spinlock_t *old_ptl, *new_ptl; struct mm_struct *mm = vma->vm_mm; bool res = false; pmd_t pmd; if (!arch_supports_page_table_move()) return false; if (WARN_ON_ONCE(!pmd_none(*new_pmd))) return false; old_ptl = pmd_lock(vma->vm_mm, old_pmd); new_ptl = pmd_lockptr(mm, new_pmd); if (new_ptl != old_ptl) spin_lock_nested(new_ptl, SINGLE_DEPTH_NESTING); pmd = *old_pmd; if (unlikely(!pmd_present(pmd) || pmd_leaf(pmd))) goto out_unlock; pmd_clear(old_pmd); res = true; VM_BUG_ON(!pmd_none(*new_pmd)); pmd_populate(mm, new_pmd, pmd_pgtable(pmd)); flush_tlb_range(vma, old_addr, old_addr + PMD_SIZE); out_unlock: if (new_ptl != old_ptl) spin_unlock(new_ptl); spin_unlock(old_ptl); return res; }"
714----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49991/bad/kfd_process_queue_manager.c----pqm_clean_queue_resource,"static void pqm_clean_queue_resource(struct process_queue_manager *pqm, struct process_queue_node *pqn) { struct kfd_node *dev; struct kfd_process_device *pdd; dev = pqn->q->device; pdd = kfd_get_process_device_data(dev, pqm->process); if (!pdd) { pr_err(""Process device data doesn't exist\n""); return; } if (pqn->q->gws) { if (KFD_GC_VERSION(pqn->q->device) != IP_VERSION(9, 4, 3) && !dev->kfd->shared_resources.enable_mes) amdgpu_amdkfd_remove_gws_from_process( pqm->process->kgd_process_info, pqn->q->gws); pdd->qpd.num_gws = 0; } if (dev->kfd->shared_resources.enable_mes) { <S2SV_StartVul> amdgpu_amdkfd_free_gtt_mem(dev->adev, pqn->q->gang_ctx_bo); <S2SV_EndVul> if (pqn->q->wptr_bo) <S2SV_StartVul> amdgpu_amdkfd_free_gtt_mem(dev->adev, pqn->q->wptr_bo); <S2SV_EndVul> } }","- amdgpu_amdkfd_free_gtt_mem(dev->adev, pqn->q->gang_ctx_bo);
- amdgpu_amdkfd_free_gtt_mem(dev->adev, pqn->q->wptr_bo);
+ amdgpu_amdkfd_free_gtt_mem(dev->adev, &pqn->q->gang_ctx_bo);
+ amdgpu_amdkfd_free_gtt_mem(dev->adev, (void **)&pqn->q->wptr_bo);","static void pqm_clean_queue_resource(struct process_queue_manager *pqm, struct process_queue_node *pqn) { struct kfd_node *dev; struct kfd_process_device *pdd; dev = pqn->q->device; pdd = kfd_get_process_device_data(dev, pqm->process); if (!pdd) { pr_err(""Process device data doesn't exist\n""); return; } if (pqn->q->gws) { if (KFD_GC_VERSION(pqn->q->device) != IP_VERSION(9, 4, 3) && !dev->kfd->shared_resources.enable_mes) amdgpu_amdkfd_remove_gws_from_process( pqm->process->kgd_process_info, pqn->q->gws); pdd->qpd.num_gws = 0; } if (dev->kfd->shared_resources.enable_mes) { amdgpu_amdkfd_free_gtt_mem(dev->adev, &pqn->q->gang_ctx_bo); if (pqn->q->wptr_bo) amdgpu_amdkfd_free_gtt_mem(dev->adev, (void **)&pqn->q->wptr_bo); } }"
1100----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53079/bad/page_alloc.c----destroy_large_folio,"void destroy_large_folio(struct folio *folio) { if (folio_test_hugetlb(folio)) { free_huge_folio(folio); return; } <S2SV_StartVul> folio_undo_large_rmappable(folio); <S2SV_EndVul> mem_cgroup_uncharge(folio); free_the_page(&folio->page, folio_order(folio)); }","- folio_undo_large_rmappable(folio);
+ folio_unqueue_deferred_split(folio);","void destroy_large_folio(struct folio *folio) { if (folio_test_hugetlb(folio)) { free_huge_folio(folio); return; } folio_unqueue_deferred_split(folio); mem_cgroup_uncharge(folio); free_the_page(&folio->page, folio_order(folio)); }"
1415----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56744/bad/checkpoint.c----f2fs_stop_checkpoint,"void f2fs_stop_checkpoint(struct f2fs_sb_info *sbi, bool end_io, unsigned char reason) { f2fs_build_fault_attr(sbi, 0, 0); if (!end_io) f2fs_flush_merged_writes(sbi); <S2SV_StartVul> f2fs_handle_critical_error(sbi, reason, end_io); <S2SV_EndVul> }","- f2fs_handle_critical_error(sbi, reason, end_io);
+ f2fs_handle_critical_error(sbi, reason);","void f2fs_stop_checkpoint(struct f2fs_sb_info *sbi, bool end_io, unsigned char reason) { f2fs_build_fault_attr(sbi, 0, 0); if (!end_io) f2fs_flush_merged_writes(sbi); f2fs_handle_critical_error(sbi, reason); }"
375----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46837/bad/panthor_sched.c----panthor_group_create,"int panthor_group_create(struct panthor_file *pfile, const struct drm_panthor_group_create *group_args, const struct drm_panthor_queue_create *queue_args) { struct panthor_device *ptdev = pfile->ptdev; struct panthor_group_pool *gpool = pfile->groups; struct panthor_scheduler *sched = ptdev->scheduler; struct panthor_fw_csg_iface *csg_iface = panthor_fw_get_csg_iface(ptdev, 0); struct panthor_group *group = NULL; u32 gid, i, suspend_size; int ret; if (group_args->pad) return -EINVAL; <S2SV_StartVul> if (group_args->priority > PANTHOR_CSG_PRIORITY_HIGH) <S2SV_EndVul> return -EINVAL; if ((group_args->compute_core_mask & ~ptdev->gpu_info.shader_present) || (group_args->fragment_core_mask & ~ptdev->gpu_info.shader_present) || (group_args->tiler_core_mask & ~ptdev->gpu_info.tiler_present)) return -EINVAL; if (hweight64(group_args->compute_core_mask) < group_args->max_compute_cores || hweight64(group_args->fragment_core_mask) < group_args->max_fragment_cores || hweight64(group_args->tiler_core_mask) < group_args->max_tiler_cores) return -EINVAL; group = kzalloc(sizeof(*group), GFP_KERNEL); if (!group) return -ENOMEM; spin_lock_init(&group->fatal_lock); kref_init(&group->refcount); group->state = PANTHOR_CS_GROUP_CREATED; group->csg_id = -1; group->ptdev = ptdev; group->max_compute_cores = group_args->max_compute_cores; group->compute_core_mask = group_args->compute_core_mask; group->max_fragment_cores = group_args->max_fragment_cores; group->fragment_core_mask = group_args->fragment_core_mask; group->max_tiler_cores = group_args->max_tiler_cores; group->tiler_core_mask = group_args->tiler_core_mask; group->priority = group_args->priority; INIT_LIST_HEAD(&group->wait_node); INIT_LIST_HEAD(&group->run_node); INIT_WORK(&group->term_work, group_term_work); INIT_WORK(&group->sync_upd_work, group_sync_upd_work); INIT_WORK(&group->tiler_oom_work, group_tiler_oom_work); INIT_WORK(&group->release_work, group_release_work); group->vm = panthor_vm_pool_get_vm(pfile->vms, group_args->vm_id); if (!group->vm) { ret = -EINVAL; goto err_put_group; } suspend_size = csg_iface->control->suspend_size; group->suspend_buf = panthor_fw_alloc_suspend_buf_mem(ptdev, suspend_size); if (IS_ERR(group->suspend_buf)) { ret = PTR_ERR(group->suspend_buf); group->suspend_buf = NULL; goto err_put_group; } suspend_size = csg_iface->control->protm_suspend_size; group->protm_suspend_buf = panthor_fw_alloc_suspend_buf_mem(ptdev, suspend_size); if (IS_ERR(group->protm_suspend_buf)) { ret = PTR_ERR(group->protm_suspend_buf); group->protm_suspend_buf = NULL; goto err_put_group; } group->syncobjs = panthor_kernel_bo_create(ptdev, group->vm, group_args->queues.count * sizeof(struct panthor_syncobj_64b), DRM_PANTHOR_BO_NO_MMAP, DRM_PANTHOR_VM_BIND_OP_MAP_NOEXEC | DRM_PANTHOR_VM_BIND_OP_MAP_UNCACHED, PANTHOR_VM_KERNEL_AUTO_VA); if (IS_ERR(group->syncobjs)) { ret = PTR_ERR(group->syncobjs); goto err_put_group; } ret = panthor_kernel_bo_vmap(group->syncobjs); if (ret) goto err_put_group; memset(group->syncobjs->kmap, 0, group_args->queues.count * sizeof(struct panthor_syncobj_64b)); for (i = 0; i < group_args->queues.count; i++) { group->queues[i] = group_create_queue(group, &queue_args[i]); if (IS_ERR(group->queues[i])) { ret = PTR_ERR(group->queues[i]); group->queues[i] = NULL; goto err_put_group; } group->queue_count++; } group->idle_queues = GENMASK(group->queue_count - 1, 0); ret = xa_alloc(&gpool->xa, &gid, group, XA_LIMIT(1, MAX_GROUPS_PER_POOL), GFP_KERNEL); if (ret) goto err_put_group; mutex_lock(&sched->reset.lock); if (atomic_read(&sched->reset.in_progress)) { panthor_group_stop(group); } else { mutex_lock(&sched->lock); list_add_tail(&group->run_node, &sched->groups.idle[group->priority]); mutex_unlock(&sched->lock); } mutex_unlock(&sched->reset.lock); return gid; err_put_group: group_put(group); return ret; }","- if (group_args->priority > PANTHOR_CSG_PRIORITY_HIGH)
+ if (group_args->priority >= PANTHOR_CSG_PRIORITY_COUNT)","int panthor_group_create(struct panthor_file *pfile, const struct drm_panthor_group_create *group_args, const struct drm_panthor_queue_create *queue_args) { struct panthor_device *ptdev = pfile->ptdev; struct panthor_group_pool *gpool = pfile->groups; struct panthor_scheduler *sched = ptdev->scheduler; struct panthor_fw_csg_iface *csg_iface = panthor_fw_get_csg_iface(ptdev, 0); struct panthor_group *group = NULL; u32 gid, i, suspend_size; int ret; if (group_args->pad) return -EINVAL; if (group_args->priority >= PANTHOR_CSG_PRIORITY_COUNT) return -EINVAL; if ((group_args->compute_core_mask & ~ptdev->gpu_info.shader_present) || (group_args->fragment_core_mask & ~ptdev->gpu_info.shader_present) || (group_args->tiler_core_mask & ~ptdev->gpu_info.tiler_present)) return -EINVAL; if (hweight64(group_args->compute_core_mask) < group_args->max_compute_cores || hweight64(group_args->fragment_core_mask) < group_args->max_fragment_cores || hweight64(group_args->tiler_core_mask) < group_args->max_tiler_cores) return -EINVAL; group = kzalloc(sizeof(*group), GFP_KERNEL); if (!group) return -ENOMEM; spin_lock_init(&group->fatal_lock); kref_init(&group->refcount); group->state = PANTHOR_CS_GROUP_CREATED; group->csg_id = -1; group->ptdev = ptdev; group->max_compute_cores = group_args->max_compute_cores; group->compute_core_mask = group_args->compute_core_mask; group->max_fragment_cores = group_args->max_fragment_cores; group->fragment_core_mask = group_args->fragment_core_mask; group->max_tiler_cores = group_args->max_tiler_cores; group->tiler_core_mask = group_args->tiler_core_mask; group->priority = group_args->priority; INIT_LIST_HEAD(&group->wait_node); INIT_LIST_HEAD(&group->run_node); INIT_WORK(&group->term_work, group_term_work); INIT_WORK(&group->sync_upd_work, group_sync_upd_work); INIT_WORK(&group->tiler_oom_work, group_tiler_oom_work); INIT_WORK(&group->release_work, group_release_work); group->vm = panthor_vm_pool_get_vm(pfile->vms, group_args->vm_id); if (!group->vm) { ret = -EINVAL; goto err_put_group; } suspend_size = csg_iface->control->suspend_size; group->suspend_buf = panthor_fw_alloc_suspend_buf_mem(ptdev, suspend_size); if (IS_ERR(group->suspend_buf)) { ret = PTR_ERR(group->suspend_buf); group->suspend_buf = NULL; goto err_put_group; } suspend_size = csg_iface->control->protm_suspend_size; group->protm_suspend_buf = panthor_fw_alloc_suspend_buf_mem(ptdev, suspend_size); if (IS_ERR(group->protm_suspend_buf)) { ret = PTR_ERR(group->protm_suspend_buf); group->protm_suspend_buf = NULL; goto err_put_group; } group->syncobjs = panthor_kernel_bo_create(ptdev, group->vm, group_args->queues.count * sizeof(struct panthor_syncobj_64b), DRM_PANTHOR_BO_NO_MMAP, DRM_PANTHOR_VM_BIND_OP_MAP_NOEXEC | DRM_PANTHOR_VM_BIND_OP_MAP_UNCACHED, PANTHOR_VM_KERNEL_AUTO_VA); if (IS_ERR(group->syncobjs)) { ret = PTR_ERR(group->syncobjs); goto err_put_group; } ret = panthor_kernel_bo_vmap(group->syncobjs); if (ret) goto err_put_group; memset(group->syncobjs->kmap, 0, group_args->queues.count * sizeof(struct panthor_syncobj_64b)); for (i = 0; i < group_args->queues.count; i++) { group->queues[i] = group_create_queue(group, &queue_args[i]); if (IS_ERR(group->queues[i])) { ret = PTR_ERR(group->queues[i]); group->queues[i] = NULL; goto err_put_group; } group->queue_count++; } group->idle_queues = GENMASK(group->queue_count - 1, 0); ret = xa_alloc(&gpool->xa, &gid, group, XA_LIMIT(1, MAX_GROUPS_PER_POOL), GFP_KERNEL); if (ret) goto err_put_group; mutex_lock(&sched->reset.lock); if (atomic_read(&sched->reset.in_progress)) { panthor_group_stop(group); } else { mutex_lock(&sched->lock); list_add_tail(&group->run_node, &sched->groups.idle[group->priority]); mutex_unlock(&sched->lock); } mutex_unlock(&sched->reset.lock); return gid; err_put_group: group_put(group); return ret; }"
1116----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53090/bad/rxrpc.c----*afs_alloc_call,"static struct afs_call *afs_alloc_call(struct afs_net *net, const struct afs_call_type *type, gfp_t gfp) { struct afs_call *call; int o; call = kzalloc(sizeof(*call), gfp); if (!call) return NULL; call->type = type; call->net = net; call->debug_id = atomic_inc_return(&rxrpc_debug_id); refcount_set(&call->ref, 1); INIT_WORK(&call->async_work, afs_process_async_call); init_waitqueue_head(&call->waitq); spin_lock_init(&call->state_lock); call->iter = &call->def_iter; o = atomic_inc_return(&net->nr_outstanding_calls); trace_afs_call(call->debug_id, afs_call_trace_alloc, 1, o, __builtin_return_address(0)); return call; <S2SV_StartVul> } <S2SV_EndVul>","- }
+ INIT_WORK(&call->free_work, afs_deferred_free_worker);
+ }","static struct afs_call *afs_alloc_call(struct afs_net *net, const struct afs_call_type *type, gfp_t gfp) { struct afs_call *call; int o; call = kzalloc(sizeof(*call), gfp); if (!call) return NULL; call->type = type; call->net = net; call->debug_id = atomic_inc_return(&rxrpc_debug_id); refcount_set(&call->ref, 1); INIT_WORK(&call->async_work, afs_process_async_call); INIT_WORK(&call->free_work, afs_deferred_free_worker); init_waitqueue_head(&call->waitq); spin_lock_init(&call->state_lock); call->iter = &call->def_iter; o = atomic_inc_return(&net->nr_outstanding_calls); trace_afs_call(call->debug_id, afs_call_trace_alloc, 1, o, __builtin_return_address(0)); return call; }"
613----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49921/bad/link_dp_capability.c----dp_verify_link_cap_with_retries,"bool dp_verify_link_cap_with_retries( struct dc_link *link, struct dc_link_settings *known_limit_link_setting, int attempts) { int i = 0; bool success = false; int fail_count = 0; struct dc_link_settings last_verified_link_cap = fail_safe_link_settings; dp_trace_detect_lt_init(link); if (link->link_enc && link->link_enc->features.flags.bits.DP_IS_USB_C && link->dc->debug.usbc_combo_phy_reset_wa) apply_usbc_combo_phy_reset_wa(link, known_limit_link_setting); dp_trace_set_lt_start_timestamp(link, false); for (i = 0; i < attempts; i++) { enum dc_connection_type type = dc_connection_none; memset(&link->verified_link_cap, 0, sizeof(struct dc_link_settings)); <S2SV_StartVul> if (!link_detect_connection_type(link, &type) || type == dc_connection_none) { <S2SV_EndVul> link->verified_link_cap = fail_safe_link_settings; break; } else if (dp_verify_link_cap(link, known_limit_link_setting, &fail_count)) { last_verified_link_cap = link->verified_link_cap; if (fail_count == 0) { success = true; break; } } else { link->verified_link_cap = last_verified_link_cap; } fsleep(10 * 1000); } dp_trace_lt_fail_count_update(link, fail_count, true); dp_trace_set_lt_end_timestamp(link, true); return success; }","- if (!link_detect_connection_type(link, &type) || type == dc_connection_none) {
+ if (link->link_enc && (!link_detect_connection_type(link, &type) || type == dc_connection_none)) {","bool dp_verify_link_cap_with_retries( struct dc_link *link, struct dc_link_settings *known_limit_link_setting, int attempts) { int i = 0; bool success = false; int fail_count = 0; struct dc_link_settings last_verified_link_cap = fail_safe_link_settings; dp_trace_detect_lt_init(link); if (link->link_enc && link->link_enc->features.flags.bits.DP_IS_USB_C && link->dc->debug.usbc_combo_phy_reset_wa) apply_usbc_combo_phy_reset_wa(link, known_limit_link_setting); dp_trace_set_lt_start_timestamp(link, false); for (i = 0; i < attempts; i++) { enum dc_connection_type type = dc_connection_none; memset(&link->verified_link_cap, 0, sizeof(struct dc_link_settings)); if (link->link_enc && (!link_detect_connection_type(link, &type) || type == dc_connection_none)) { link->verified_link_cap = fail_safe_link_settings; break; } else if (dp_verify_link_cap(link, known_limit_link_setting, &fail_count)) { last_verified_link_cap = link->verified_link_cap; if (fail_count == 0) { success = true; break; } } else { link->verified_link_cap = last_verified_link_cap; } fsleep(10 * 1000); } dp_trace_lt_fail_count_update(link, fail_count, true); dp_trace_set_lt_end_timestamp(link, true); return success; }"
371----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46834/bad/ioctl.c----ethtool_set_channels,"static noinline_for_stack int ethtool_set_channels(struct net_device *dev, void __user *useraddr) { struct ethtool_channels channels, curr = { .cmd = ETHTOOL_GCHANNELS }; u16 from_channel, to_channel; u64 max_rxnfc_in_use; u32 max_rxfh_in_use; unsigned int i; int ret; if (!dev->ethtool_ops->set_channels || !dev->ethtool_ops->get_channels) return -EOPNOTSUPP; if (copy_from_user(&channels, useraddr, sizeof(channels))) return -EFAULT; dev->ethtool_ops->get_channels(dev, &curr); if (channels.rx_count == curr.rx_count && channels.tx_count == curr.tx_count && channels.combined_count == curr.combined_count && channels.other_count == curr.other_count) return 0; if (channels.rx_count > curr.max_rx || channels.tx_count > curr.max_tx || channels.combined_count > curr.max_combined || channels.other_count > curr.max_other) return -EINVAL; if (!channels.combined_count && (!channels.rx_count || !channels.tx_count)) return -EINVAL; if (ethtool_get_max_rxnfc_channel(dev, &max_rxnfc_in_use)) max_rxnfc_in_use = 0; <S2SV_StartVul> if (!netif_is_rxfh_configured(dev) || <S2SV_EndVul> <S2SV_StartVul> ethtool_get_max_rxfh_channel(dev, &max_rxfh_in_use)) <S2SV_EndVul> <S2SV_StartVul> max_rxfh_in_use = 0; <S2SV_EndVul> if (channels.combined_count + channels.rx_count <= max_t(u64, max_rxnfc_in_use, max_rxfh_in_use)) return -EINVAL; from_channel = channels.combined_count + min(channels.rx_count, channels.tx_count); to_channel = curr.combined_count + max(curr.rx_count, curr.tx_count); for (i = from_channel; i < to_channel; i++) if (xsk_get_pool_from_qid(dev, i)) return -EINVAL; ret = dev->ethtool_ops->set_channels(dev, &channels); if (!ret) ethtool_notify(dev, ETHTOOL_MSG_CHANNELS_NTF, NULL); return ret; }","- if (!netif_is_rxfh_configured(dev) ||
- ethtool_get_max_rxfh_channel(dev, &max_rxfh_in_use))
- max_rxfh_in_use = 0;
+ max_rxfh_in_use = ethtool_get_max_rxfh_channel(dev);","static noinline_for_stack int ethtool_set_channels(struct net_device *dev, void __user *useraddr) { struct ethtool_channels channels, curr = { .cmd = ETHTOOL_GCHANNELS }; u16 from_channel, to_channel; u64 max_rxnfc_in_use; u32 max_rxfh_in_use; unsigned int i; int ret; if (!dev->ethtool_ops->set_channels || !dev->ethtool_ops->get_channels) return -EOPNOTSUPP; if (copy_from_user(&channels, useraddr, sizeof(channels))) return -EFAULT; dev->ethtool_ops->get_channels(dev, &curr); if (channels.rx_count == curr.rx_count && channels.tx_count == curr.tx_count && channels.combined_count == curr.combined_count && channels.other_count == curr.other_count) return 0; if (channels.rx_count > curr.max_rx || channels.tx_count > curr.max_tx || channels.combined_count > curr.max_combined || channels.other_count > curr.max_other) return -EINVAL; if (!channels.combined_count && (!channels.rx_count || !channels.tx_count)) return -EINVAL; if (ethtool_get_max_rxnfc_channel(dev, &max_rxnfc_in_use)) max_rxnfc_in_use = 0; max_rxfh_in_use = ethtool_get_max_rxfh_channel(dev); if (channels.combined_count + channels.rx_count <= max_t(u64, max_rxnfc_in_use, max_rxfh_in_use)) return -EINVAL; from_channel = channels.combined_count + min(channels.rx_count, channels.tx_count); to_channel = curr.combined_count + max(curr.rx_count, curr.tx_count); for (i = from_channel; i < to_channel; i++) if (xsk_get_pool_from_qid(dev, i)) return -EINVAL; ret = dev->ethtool_ops->set_channels(dev, &channels); if (!ret) ethtool_notify(dev, ETHTOOL_MSG_CHANNELS_NTF, NULL); return ret; }"
1394----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56722/bad/hns_roce_hw_v2.c----hns_roce_v2_set_abs_fields,"static int hns_roce_v2_set_abs_fields(struct ib_qp *ibqp, const struct ib_qp_attr *attr, int attr_mask, enum ib_qp_state cur_state, enum ib_qp_state new_state, struct hns_roce_v2_qp_context *context, struct hns_roce_v2_qp_context *qpc_mask, struct ib_udata *udata) { struct hns_roce_dev *hr_dev = to_hr_dev(ibqp->device); int ret = 0; <S2SV_StartVul> if (!check_qp_state(cur_state, new_state)) { <S2SV_EndVul> <S2SV_StartVul> ibdev_err(&hr_dev->ib_dev, ""Illegal state for QP!\n""); <S2SV_EndVul> return -EINVAL; <S2SV_StartVul> } <S2SV_EndVul> if (cur_state == IB_QPS_RESET && new_state == IB_QPS_INIT) { memset(qpc_mask, 0, hr_dev->caps.qpc_sz); modify_qp_reset_to_init(ibqp, context, qpc_mask); } else if (cur_state == IB_QPS_INIT && new_state == IB_QPS_INIT) { modify_qp_init_to_init(ibqp, context, qpc_mask); } else if (cur_state == IB_QPS_INIT && new_state == IB_QPS_RTR) { ret = modify_qp_init_to_rtr(ibqp, attr, attr_mask, context, qpc_mask, udata); } else if (cur_state == IB_QPS_RTR && new_state == IB_QPS_RTS) { ret = modify_qp_rtr_to_rts(ibqp, attr_mask, context, qpc_mask); } return ret; }","- if (!check_qp_state(cur_state, new_state)) {
- ibdev_err(&hr_dev->ib_dev, ""Illegal state for QP!\n"");
- }
+ if (!check_qp_state(cur_state, new_state))","static int hns_roce_v2_set_abs_fields(struct ib_qp *ibqp, const struct ib_qp_attr *attr, int attr_mask, enum ib_qp_state cur_state, enum ib_qp_state new_state, struct hns_roce_v2_qp_context *context, struct hns_roce_v2_qp_context *qpc_mask, struct ib_udata *udata) { struct hns_roce_dev *hr_dev = to_hr_dev(ibqp->device); int ret = 0; if (!check_qp_state(cur_state, new_state)) return -EINVAL; if (cur_state == IB_QPS_RESET && new_state == IB_QPS_INIT) { memset(qpc_mask, 0, hr_dev->caps.qpc_sz); modify_qp_reset_to_init(ibqp, context, qpc_mask); } else if (cur_state == IB_QPS_INIT && new_state == IB_QPS_INIT) { modify_qp_init_to_init(ibqp, context, qpc_mask); } else if (cur_state == IB_QPS_INIT && new_state == IB_QPS_RTR) { ret = modify_qp_init_to_rtr(ibqp, attr, attr_mask, context, qpc_mask, udata); } else if (cur_state == IB_QPS_RTR && new_state == IB_QPS_RTS) { ret = modify_qp_rtr_to_rts(ibqp, attr_mask, context, qpc_mask); } return ret; }"
976----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50213/bad/drm_hdmi_state_helper_test.c----drm_test_check_output_bpc_format_vic_1,"static void drm_test_check_output_bpc_format_vic_1(struct kunit *test) { struct drm_atomic_helper_connector_hdmi_priv *priv; struct drm_modeset_acquire_ctx *ctx; struct drm_connector_state *conn_state; struct drm_display_info *info; struct drm_display_mode *mode; unsigned long long rate; struct drm_connector *conn; struct drm_device *drm; struct drm_crtc *crtc; int ret; priv = drm_atomic_helper_connector_hdmi_init(test, BIT(HDMI_COLORSPACE_RGB) | BIT(HDMI_COLORSPACE_YUV422) | BIT(HDMI_COLORSPACE_YUV444), 12); KUNIT_ASSERT_NOT_NULL(test, priv); drm = &priv->drm; conn = &priv->connector; ret = set_connector_edid(test, conn, test_edid_hdmi_1080p_rgb_yuv_dc_max_200mhz, ARRAY_SIZE(test_edid_hdmi_1080p_rgb_yuv_dc_max_200mhz)); KUNIT_ASSERT_EQ(test, ret, 0); info = &conn->display_info; KUNIT_ASSERT_TRUE(test, info->is_hdmi); KUNIT_ASSERT_GT(test, info->max_tmds_clock, 0); ctx = drm_kunit_helper_acquire_ctx_alloc(test); KUNIT_ASSERT_NOT_ERR_OR_NULL(test, ctx); <S2SV_StartVul> mode = drm_display_mode_from_cea_vic(drm, 1); <S2SV_EndVul> KUNIT_ASSERT_NOT_NULL(test, mode); rate = mode->clock * 1500; KUNIT_ASSERT_LT(test, rate, info->max_tmds_clock * 1000); drm = &priv->drm; crtc = priv->crtc; ret = light_up_connector(test, drm, crtc, conn, mode, ctx); KUNIT_EXPECT_EQ(test, ret, 0); conn_state = conn->state; KUNIT_ASSERT_NOT_NULL(test, conn_state); KUNIT_EXPECT_EQ(test, conn_state->hdmi.output_bpc, 8); KUNIT_EXPECT_EQ(test, conn_state->hdmi.output_format, HDMI_COLORSPACE_RGB); }","- mode = drm_display_mode_from_cea_vic(drm, 1);
+ mode = drm_kunit_display_mode_from_cea_vic(test, drm, 1);","static void drm_test_check_output_bpc_format_vic_1(struct kunit *test) { struct drm_atomic_helper_connector_hdmi_priv *priv; struct drm_modeset_acquire_ctx *ctx; struct drm_connector_state *conn_state; struct drm_display_info *info; struct drm_display_mode *mode; unsigned long long rate; struct drm_connector *conn; struct drm_device *drm; struct drm_crtc *crtc; int ret; priv = drm_atomic_helper_connector_hdmi_init(test, BIT(HDMI_COLORSPACE_RGB) | BIT(HDMI_COLORSPACE_YUV422) | BIT(HDMI_COLORSPACE_YUV444), 12); KUNIT_ASSERT_NOT_NULL(test, priv); drm = &priv->drm; conn = &priv->connector; ret = set_connector_edid(test, conn, test_edid_hdmi_1080p_rgb_yuv_dc_max_200mhz, ARRAY_SIZE(test_edid_hdmi_1080p_rgb_yuv_dc_max_200mhz)); KUNIT_ASSERT_EQ(test, ret, 0); info = &conn->display_info; KUNIT_ASSERT_TRUE(test, info->is_hdmi); KUNIT_ASSERT_GT(test, info->max_tmds_clock, 0); ctx = drm_kunit_helper_acquire_ctx_alloc(test); KUNIT_ASSERT_NOT_ERR_OR_NULL(test, ctx); mode = drm_kunit_display_mode_from_cea_vic(test, drm, 1); KUNIT_ASSERT_NOT_NULL(test, mode); rate = mode->clock * 1500; KUNIT_ASSERT_LT(test, rate, info->max_tmds_clock * 1000); drm = &priv->drm; crtc = priv->crtc; ret = light_up_connector(test, drm, crtc, conn, mode, ctx); KUNIT_EXPECT_EQ(test, ret, 0); conn_state = conn->state; KUNIT_ASSERT_NOT_NULL(test, conn_state); KUNIT_EXPECT_EQ(test, conn_state->hdmi.output_bpc, 8); KUNIT_EXPECT_EQ(test, conn_state->hdmi.output_format, HDMI_COLORSPACE_RGB); }"
147----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-45006/bad/xhci.c----xhci_setup_device,"static int xhci_setup_device(struct usb_hcd *hcd, struct usb_device *udev, enum xhci_setup_dev setup, unsigned int timeout_ms) { const char *act = setup == SETUP_CONTEXT_ONLY ? ""context"" : ""address""; unsigned long flags; struct xhci_virt_device *virt_dev; int ret = 0; struct xhci_hcd *xhci = hcd_to_xhci(hcd); struct xhci_slot_ctx *slot_ctx; struct xhci_input_control_ctx *ctrl_ctx; u64 temp_64; struct xhci_command *command = NULL; mutex_lock(&xhci->mutex); if (xhci->xhc_state) { ret = -ESHUTDOWN; goto out; } if (!udev->slot_id) { xhci_dbg_trace(xhci, trace_xhci_dbg_address, ""Bad Slot ID %d"", udev->slot_id); ret = -EINVAL; goto out; } virt_dev = xhci->devs[udev->slot_id]; if (WARN_ON(!virt_dev)) { xhci_warn(xhci, ""Virt dev invalid for slot_id 0x%x!\n"", udev->slot_id); ret = -EINVAL; goto out; } slot_ctx = xhci_get_slot_ctx(xhci, virt_dev->out_ctx); trace_xhci_setup_device_slot(slot_ctx); if (setup == SETUP_CONTEXT_ONLY) { if (GET_SLOT_STATE(le32_to_cpu(slot_ctx->dev_state)) == SLOT_STATE_DEFAULT) { xhci_dbg(xhci, ""Slot already in default state\n""); goto out; } } command = xhci_alloc_command(xhci, true, GFP_KERNEL); if (!command) { ret = -ENOMEM; goto out; } command->in_ctx = virt_dev->in_ctx; command->timeout_ms = timeout_ms; slot_ctx = xhci_get_slot_ctx(xhci, virt_dev->in_ctx); ctrl_ctx = xhci_get_input_control_ctx(virt_dev->in_ctx); if (!ctrl_ctx) { xhci_warn(xhci, ""%s: Could not get input context, bad type.\n"", __func__); ret = -EINVAL; goto out; } if (!slot_ctx->dev_info) xhci_setup_addressable_virt_dev(xhci, udev); else xhci_copy_ep0_dequeue_into_input_ctx(xhci, udev); ctrl_ctx->add_flags = cpu_to_le32(SLOT_FLAG | EP0_FLAG); ctrl_ctx->drop_flags = 0; trace_xhci_address_ctx(xhci, virt_dev->in_ctx, le32_to_cpu(slot_ctx->dev_info) >> 27); trace_xhci_address_ctrl_ctx(ctrl_ctx); spin_lock_irqsave(&xhci->lock, flags); trace_xhci_setup_device(virt_dev); ret = xhci_queue_address_device(xhci, command, virt_dev->in_ctx->dma, udev->slot_id, setup); if (ret) { spin_unlock_irqrestore(&xhci->lock, flags); xhci_dbg_trace(xhci, trace_xhci_dbg_address, ""FIXME: allocate a command ring segment""); goto out; } xhci_ring_cmd_db(xhci); spin_unlock_irqrestore(&xhci->lock, flags); wait_for_completion(command->completion); switch (command->status) { case COMP_COMMAND_ABORTED: case COMP_COMMAND_RING_STOPPED: xhci_warn(xhci, ""Timeout while waiting for setup device command\n""); ret = -ETIME; break; case COMP_CONTEXT_STATE_ERROR: case COMP_SLOT_NOT_ENABLED_ERROR: xhci_err(xhci, ""Setup ERROR: setup %s command for slot %d.\n"", act, udev->slot_id); ret = -EINVAL; break; case COMP_USB_TRANSACTION_ERROR: dev_warn(&udev->dev, ""Device not responding to setup %s.\n"", act); mutex_unlock(&xhci->mutex); ret = xhci_disable_slot(xhci, udev->slot_id); xhci_free_virt_device(xhci, udev->slot_id); <S2SV_StartVul> if (!ret) <S2SV_EndVul> <S2SV_StartVul> xhci_alloc_dev(hcd, udev); <S2SV_EndVul> kfree(command->completion); kfree(command); return -EPROTO; case COMP_INCOMPATIBLE_DEVICE_ERROR: dev_warn(&udev->dev, ""ERROR: Incompatible device for setup %s command\n"", act); ret = -ENODEV; break; case COMP_SUCCESS: xhci_dbg_trace(xhci, trace_xhci_dbg_address, ""Successful setup %s command"", act); break; default: xhci_err(xhci, ""ERROR: unexpected setup %s command completion code 0x%x.\n"", act, command->status); trace_xhci_address_ctx(xhci, virt_dev->out_ctx, 1); ret = -EINVAL; break; } if (ret) goto out; temp_64 = xhci_read_64(xhci, &xhci->op_regs->dcbaa_ptr); xhci_dbg_trace(xhci, trace_xhci_dbg_address, ""Op regs DCBAA ptr = %#016llx"", temp_64); xhci_dbg_trace(xhci, trace_xhci_dbg_address, ""Slot ID %d dcbaa entry @%p = %#016llx"", udev->slot_id, &xhci->dcbaa->dev_context_ptrs[udev->slot_id], (unsigned long long) le64_to_cpu(xhci->dcbaa->dev_context_ptrs[udev->slot_id])); xhci_dbg_trace(xhci, trace_xhci_dbg_address, ""Output Context DMA address = %#08llx"", (unsigned long long)virt_dev->out_ctx->dma); trace_xhci_address_ctx(xhci, virt_dev->in_ctx, le32_to_cpu(slot_ctx->dev_info) >> 27); trace_xhci_address_ctx(xhci, virt_dev->out_ctx, le32_to_cpu(slot_ctx->dev_info) >> 27); ctrl_ctx->add_flags = 0; ctrl_ctx->drop_flags = 0; slot_ctx = xhci_get_slot_ctx(xhci, virt_dev->out_ctx); udev->devaddr = (u8)(le32_to_cpu(slot_ctx->dev_state) & DEV_ADDR_MASK); xhci_dbg_trace(xhci, trace_xhci_dbg_address, ""Internal device address = %d"", le32_to_cpu(slot_ctx->dev_state) & DEV_ADDR_MASK); out: mutex_unlock(&xhci->mutex); if (command) { kfree(command->completion); kfree(command); } return ret; }","- if (!ret)
- xhci_alloc_dev(hcd, udev);
+ if (!ret) {
+ if (xhci_alloc_dev(hcd, udev) == 1)
+ xhci_setup_addressable_virt_dev(xhci, udev);
+ }","static int xhci_setup_device(struct usb_hcd *hcd, struct usb_device *udev, enum xhci_setup_dev setup, unsigned int timeout_ms) { const char *act = setup == SETUP_CONTEXT_ONLY ? ""context"" : ""address""; unsigned long flags; struct xhci_virt_device *virt_dev; int ret = 0; struct xhci_hcd *xhci = hcd_to_xhci(hcd); struct xhci_slot_ctx *slot_ctx; struct xhci_input_control_ctx *ctrl_ctx; u64 temp_64; struct xhci_command *command = NULL; mutex_lock(&xhci->mutex); if (xhci->xhc_state) { ret = -ESHUTDOWN; goto out; } if (!udev->slot_id) { xhci_dbg_trace(xhci, trace_xhci_dbg_address, ""Bad Slot ID %d"", udev->slot_id); ret = -EINVAL; goto out; } virt_dev = xhci->devs[udev->slot_id]; if (WARN_ON(!virt_dev)) { xhci_warn(xhci, ""Virt dev invalid for slot_id 0x%x!\n"", udev->slot_id); ret = -EINVAL; goto out; } slot_ctx = xhci_get_slot_ctx(xhci, virt_dev->out_ctx); trace_xhci_setup_device_slot(slot_ctx); if (setup == SETUP_CONTEXT_ONLY) { if (GET_SLOT_STATE(le32_to_cpu(slot_ctx->dev_state)) == SLOT_STATE_DEFAULT) { xhci_dbg(xhci, ""Slot already in default state\n""); goto out; } } command = xhci_alloc_command(xhci, true, GFP_KERNEL); if (!command) { ret = -ENOMEM; goto out; } command->in_ctx = virt_dev->in_ctx; command->timeout_ms = timeout_ms; slot_ctx = xhci_get_slot_ctx(xhci, virt_dev->in_ctx); ctrl_ctx = xhci_get_input_control_ctx(virt_dev->in_ctx); if (!ctrl_ctx) { xhci_warn(xhci, ""%s: Could not get input context, bad type.\n"", __func__); ret = -EINVAL; goto out; } if (!slot_ctx->dev_info) xhci_setup_addressable_virt_dev(xhci, udev); else xhci_copy_ep0_dequeue_into_input_ctx(xhci, udev); ctrl_ctx->add_flags = cpu_to_le32(SLOT_FLAG | EP0_FLAG); ctrl_ctx->drop_flags = 0; trace_xhci_address_ctx(xhci, virt_dev->in_ctx, le32_to_cpu(slot_ctx->dev_info) >> 27); trace_xhci_address_ctrl_ctx(ctrl_ctx); spin_lock_irqsave(&xhci->lock, flags); trace_xhci_setup_device(virt_dev); ret = xhci_queue_address_device(xhci, command, virt_dev->in_ctx->dma, udev->slot_id, setup); if (ret) { spin_unlock_irqrestore(&xhci->lock, flags); xhci_dbg_trace(xhci, trace_xhci_dbg_address, ""FIXME: allocate a command ring segment""); goto out; } xhci_ring_cmd_db(xhci); spin_unlock_irqrestore(&xhci->lock, flags); wait_for_completion(command->completion); switch (command->status) { case COMP_COMMAND_ABORTED: case COMP_COMMAND_RING_STOPPED: xhci_warn(xhci, ""Timeout while waiting for setup device command\n""); ret = -ETIME; break; case COMP_CONTEXT_STATE_ERROR: case COMP_SLOT_NOT_ENABLED_ERROR: xhci_err(xhci, ""Setup ERROR: setup %s command for slot %d.\n"", act, udev->slot_id); ret = -EINVAL; break; case COMP_USB_TRANSACTION_ERROR: dev_warn(&udev->dev, ""Device not responding to setup %s.\n"", act); mutex_unlock(&xhci->mutex); ret = xhci_disable_slot(xhci, udev->slot_id); xhci_free_virt_device(xhci, udev->slot_id); if (!ret) { if (xhci_alloc_dev(hcd, udev) == 1) xhci_setup_addressable_virt_dev(xhci, udev); } kfree(command->completion); kfree(command); return -EPROTO; case COMP_INCOMPATIBLE_DEVICE_ERROR: dev_warn(&udev->dev, ""ERROR: Incompatible device for setup %s command\n"", act); ret = -ENODEV; break; case COMP_SUCCESS: xhci_dbg_trace(xhci, trace_xhci_dbg_address, ""Successful setup %s command"", act); break; default: xhci_err(xhci, ""ERROR: unexpected setup %s command completion code 0x%x.\n"", act, command->status); trace_xhci_address_ctx(xhci, virt_dev->out_ctx, 1); ret = -EINVAL; break; } if (ret) goto out; temp_64 = xhci_read_64(xhci, &xhci->op_regs->dcbaa_ptr); xhci_dbg_trace(xhci, trace_xhci_dbg_address, ""Op regs DCBAA ptr = %#016llx"", temp_64); xhci_dbg_trace(xhci, trace_xhci_dbg_address, ""Slot ID %d dcbaa entry @%p = %#016llx"", udev->slot_id, &xhci->dcbaa->dev_context_ptrs[udev->slot_id], (unsigned long long) le64_to_cpu(xhci->dcbaa->dev_context_ptrs[udev->slot_id])); xhci_dbg_trace(xhci, trace_xhci_dbg_address, ""Output Context DMA address = %#08llx"", (unsigned long long)virt_dev->out_ctx->dma); trace_xhci_address_ctx(xhci, virt_dev->in_ctx, le32_to_cpu(slot_ctx->dev_info) >> 27); trace_xhci_address_ctx(xhci, virt_dev->out_ctx, le32_to_cpu(slot_ctx->dev_info) >> 27); ctrl_ctx->add_flags = 0; ctrl_ctx->drop_flags = 0; slot_ctx = xhci_get_slot_ctx(xhci, virt_dev->out_ctx); udev->devaddr = (u8)(le32_to_cpu(slot_ctx->dev_state) & DEV_ADDR_MASK); xhci_dbg_trace(xhci, trace_xhci_dbg_address, ""Internal device address = %d"", le32_to_cpu(slot_ctx->dev_state) & DEV_ADDR_MASK); out: mutex_unlock(&xhci->mutex); if (command) { kfree(command->completion); kfree(command); } return ret; }"
1331----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56650/bad/xt_LED.c----led_tg_check,"static int led_tg_check(const struct xt_tgchk_param *par) { struct xt_led_info *ledinfo = par->targinfo; struct xt_led_info_internal *ledinternal; int err; <S2SV_StartVul> if (ledinfo->id[0] == '\0') <S2SV_EndVul> return -EINVAL; mutex_lock(&xt_led_mutex); ledinternal = led_trigger_lookup(ledinfo->id); if (ledinternal) { ledinternal->refcnt++; goto out; } err = -ENOMEM; ledinternal = kzalloc(sizeof(struct xt_led_info_internal), GFP_KERNEL); if (!ledinternal) goto exit_mutex_only; ledinternal->trigger_id = kstrdup(ledinfo->id, GFP_KERNEL); if (!ledinternal->trigger_id) goto exit_internal_alloc; ledinternal->refcnt = 1; ledinternal->netfilter_led_trigger.name = ledinternal->trigger_id; err = led_trigger_register(&ledinternal->netfilter_led_trigger); if (err) { pr_info_ratelimited(""Trigger name is already in use.\n""); goto exit_alloc; } timer_setup(&ledinternal->timer, led_timeout_callback, 0); list_add_tail(&ledinternal->list, &xt_led_triggers); out: mutex_unlock(&xt_led_mutex); ledinfo->internal_data = ledinternal; return 0; exit_alloc: kfree(ledinternal->trigger_id); exit_internal_alloc: kfree(ledinternal); exit_mutex_only: mutex_unlock(&xt_led_mutex); return err; }","- if (ledinfo->id[0] == '\0')
+ if (ledinfo->id[0] == '\0' ||
+ !memchr(ledinfo->id, '\0', sizeof(ledinfo->id)))","static int led_tg_check(const struct xt_tgchk_param *par) { struct xt_led_info *ledinfo = par->targinfo; struct xt_led_info_internal *ledinternal; int err; if (ledinfo->id[0] == '\0' || !memchr(ledinfo->id, '\0', sizeof(ledinfo->id))) return -EINVAL; mutex_lock(&xt_led_mutex); ledinternal = led_trigger_lookup(ledinfo->id); if (ledinternal) { ledinternal->refcnt++; goto out; } err = -ENOMEM; ledinternal = kzalloc(sizeof(struct xt_led_info_internal), GFP_KERNEL); if (!ledinternal) goto exit_mutex_only; ledinternal->trigger_id = kstrdup(ledinfo->id, GFP_KERNEL); if (!ledinternal->trigger_id) goto exit_internal_alloc; ledinternal->refcnt = 1; ledinternal->netfilter_led_trigger.name = ledinternal->trigger_id; err = led_trigger_register(&ledinternal->netfilter_led_trigger); if (err) { pr_info_ratelimited(""Trigger name is already in use.\n""); goto exit_alloc; } timer_setup(&ledinternal->timer, led_timeout_callback, 0); list_add_tail(&ledinternal->list, &xt_led_triggers); out: mutex_unlock(&xt_led_mutex); ledinfo->internal_data = ledinternal; return 0; exit_alloc: kfree(ledinternal->trigger_id); exit_internal_alloc: kfree(ledinternal); exit_mutex_only: mutex_unlock(&xt_led_mutex); return err; }"
168----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-45023/bad/raid1.c----rdev_readable,"static bool rdev_readable(struct md_rdev *rdev, struct r1bio *r1_bio) { if (!rdev || test_bit(Faulty, &rdev->flags)) return false; <S2SV_StartVul> if (!test_bit(In_sync, &rdev->flags) && <S2SV_EndVul> <S2SV_StartVul> rdev->recovery_offset < r1_bio->sector + r1_bio->sectors) <S2SV_EndVul> return false; if (test_bit(WriteMostly, &rdev->flags)) return false; if (rdev_has_badblock(rdev, r1_bio->sector, r1_bio->sectors)) return false; return true; }","- if (!test_bit(In_sync, &rdev->flags) &&
- rdev->recovery_offset < r1_bio->sector + r1_bio->sectors)
+ if (rdev_in_recovery(rdev, r1_bio))","static bool rdev_readable(struct md_rdev *rdev, struct r1bio *r1_bio) { if (!rdev || test_bit(Faulty, &rdev->flags)) return false; if (rdev_in_recovery(rdev, r1_bio)) return false; if (test_bit(WriteMostly, &rdev->flags)) return false; if (rdev_has_badblock(rdev, r1_bio->sector, r1_bio->sectors)) return false; return true; }"
112----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44971/bad/bcm_sf2.c----bcm_sf2_mdio_register,"static int bcm_sf2_mdio_register(struct dsa_switch *ds) { struct bcm_sf2_priv *priv = bcm_sf2_to_priv(ds); struct device_node *dn, *child; struct phy_device *phydev; struct property *prop; static int index; int err, reg; dn = of_find_compatible_node(NULL, NULL, ""brcm,unimac-mdio""); priv->master_mii_bus = of_mdio_find_bus(dn); if (!priv->master_mii_bus) { err = -EPROBE_DEFER; goto err_of_node_put; } priv->master_mii_dn = dn; priv->slave_mii_bus = mdiobus_alloc(); if (!priv->slave_mii_bus) { err = -ENOMEM; goto err_put_master_mii_bus_dev; } priv->slave_mii_bus->priv = priv; priv->slave_mii_bus->name = ""sf2 slave mii""; priv->slave_mii_bus->read = bcm_sf2_sw_mdio_read; priv->slave_mii_bus->write = bcm_sf2_sw_mdio_write; snprintf(priv->slave_mii_bus->id, MII_BUS_ID_SIZE, ""sf2-%d"", index++); priv->slave_mii_bus->dev.of_node = dn; if (of_machine_is_compatible(""brcm,bcm7445d0"")) priv->indir_phy_mask |= (1 << BRCM_PSEUDO_PHY_ADDR) | (1 << 0); else priv->indir_phy_mask = 0; ds->phys_mii_mask = priv->indir_phy_mask; ds->slave_mii_bus = priv->slave_mii_bus; priv->slave_mii_bus->parent = ds->dev->parent; priv->slave_mii_bus->phy_mask = ~priv->indir_phy_mask; for_each_available_child_of_node(dn, child) { if (of_property_read_u32(child, ""reg"", &reg) || reg >= PHY_MAX_ADDR) continue; if (!(priv->indir_phy_mask & BIT(reg))) continue; prop = of_find_property(child, ""phandle"", NULL); if (prop) of_remove_property(child, prop); prop = of_find_property(child, ""linux,phandle"", NULL); if (prop) of_remove_property(child, prop); phydev = of_phy_find_device(child); <S2SV_StartVul> if (phydev) <S2SV_EndVul> phy_device_remove(phydev); } err = mdiobus_register(priv->slave_mii_bus); if (err && dn) goto err_free_slave_mii_bus; return 0; err_free_slave_mii_bus: mdiobus_free(priv->slave_mii_bus); err_put_master_mii_bus_dev: put_device(&priv->master_mii_bus->dev); err_of_node_put: of_node_put(dn); return err; }","- if (phydev)
+ if (phydev) {
+ phy_device_free(phydev);
+ }
+ }","static int bcm_sf2_mdio_register(struct dsa_switch *ds) { struct bcm_sf2_priv *priv = bcm_sf2_to_priv(ds); struct device_node *dn, *child; struct phy_device *phydev; struct property *prop; static int index; int err, reg; dn = of_find_compatible_node(NULL, NULL, ""brcm,unimac-mdio""); priv->master_mii_bus = of_mdio_find_bus(dn); if (!priv->master_mii_bus) { err = -EPROBE_DEFER; goto err_of_node_put; } priv->master_mii_dn = dn; priv->slave_mii_bus = mdiobus_alloc(); if (!priv->slave_mii_bus) { err = -ENOMEM; goto err_put_master_mii_bus_dev; } priv->slave_mii_bus->priv = priv; priv->slave_mii_bus->name = ""sf2 slave mii""; priv->slave_mii_bus->read = bcm_sf2_sw_mdio_read; priv->slave_mii_bus->write = bcm_sf2_sw_mdio_write; snprintf(priv->slave_mii_bus->id, MII_BUS_ID_SIZE, ""sf2-%d"", index++); priv->slave_mii_bus->dev.of_node = dn; if (of_machine_is_compatible(""brcm,bcm7445d0"")) priv->indir_phy_mask |= (1 << BRCM_PSEUDO_PHY_ADDR) | (1 << 0); else priv->indir_phy_mask = 0; ds->phys_mii_mask = priv->indir_phy_mask; ds->slave_mii_bus = priv->slave_mii_bus; priv->slave_mii_bus->parent = ds->dev->parent; priv->slave_mii_bus->phy_mask = ~priv->indir_phy_mask; for_each_available_child_of_node(dn, child) { if (of_property_read_u32(child, ""reg"", &reg) || reg >= PHY_MAX_ADDR) continue; if (!(priv->indir_phy_mask & BIT(reg))) continue; prop = of_find_property(child, ""phandle"", NULL); if (prop) of_remove_property(child, prop); prop = of_find_property(child, ""linux,phandle"", NULL); if (prop) of_remove_property(child, prop); phydev = of_phy_find_device(child); if (phydev) { phy_device_remove(phydev); phy_device_free(phydev); } } err = mdiobus_register(priv->slave_mii_bus); if (err && dn) goto err_free_slave_mii_bus; return 0; err_free_slave_mii_bus: mdiobus_free(priv->slave_mii_bus); err_put_master_mii_bus_dev: put_device(&priv->master_mii_bus->dev); err_of_node_put: of_node_put(dn); return err; }"
968----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50211/bad/directory.c----udf_fiiter_append_blk,"int udf_fiiter_append_blk(struct udf_fileident_iter *iter) { struct udf_inode_info *iinfo = UDF_I(iter->dir); int blksize = 1 << iter->dir->i_blkbits; struct buffer_head *bh; sector_t block; uint32_t old_elen = iter->elen; int err; if (WARN_ON_ONCE(iinfo->i_alloc_type == ICBTAG_FLAG_AD_IN_ICB)) return -EINVAL; udf_fiiter_update_elen(iter, ALIGN(iter->elen, blksize)); block = iinfo->i_lenExtents >> iter->dir->i_blkbits; bh = udf_bread(iter->dir, block, 1, &err); if (!bh) { udf_fiiter_update_elen(iter, old_elen); return err; } <S2SV_StartVul> if (inode_bmap(iter->dir, block, &iter->epos, &iter->eloc, &iter->elen, <S2SV_EndVul> <S2SV_StartVul> &iter->loffset) != (EXT_RECORDED_ALLOCATED >> 30)) { <S2SV_EndVul> udf_err(iter->dir->i_sb, ""block %llu not allocated in directory (ino %lu)\n"", (unsigned long long)block, iter->dir->i_ino); return -EFSCORRUPTED; } if (!(iter->pos & (blksize - 1))) { brelse(iter->bh[0]); iter->bh[0] = bh; } else { iter->bh[1] = bh; } return 0; }","- if (inode_bmap(iter->dir, block, &iter->epos, &iter->eloc, &iter->elen,
- &iter->loffset) != (EXT_RECORDED_ALLOCATED >> 30)) {
+ int8_t etype;
+ err = inode_bmap(iter->dir, block, &iter->epos, &iter->eloc, &iter->elen,
+ &iter->loffset, &etype);
+ if (err <= 0 || etype != (EXT_RECORDED_ALLOCATED >> 30)) {","int udf_fiiter_append_blk(struct udf_fileident_iter *iter) { struct udf_inode_info *iinfo = UDF_I(iter->dir); int blksize = 1 << iter->dir->i_blkbits; struct buffer_head *bh; sector_t block; uint32_t old_elen = iter->elen; int err; int8_t etype; if (WARN_ON_ONCE(iinfo->i_alloc_type == ICBTAG_FLAG_AD_IN_ICB)) return -EINVAL; udf_fiiter_update_elen(iter, ALIGN(iter->elen, blksize)); block = iinfo->i_lenExtents >> iter->dir->i_blkbits; bh = udf_bread(iter->dir, block, 1, &err); if (!bh) { udf_fiiter_update_elen(iter, old_elen); return err; } err = inode_bmap(iter->dir, block, &iter->epos, &iter->eloc, &iter->elen, &iter->loffset, &etype); if (err <= 0 || etype != (EXT_RECORDED_ALLOCATED >> 30)) { udf_err(iter->dir->i_sb, ""block %llu not allocated in directory (ino %lu)\n"", (unsigned long long)block, iter->dir->i_ino); return -EFSCORRUPTED; } if (!(iter->pos & (blksize - 1))) { brelse(iter->bh[0]); iter->bh[0] = bh; } else { iter->bh[1] = bh; } return 0; }"
471----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47712/bad/hif.c----*wilc_parse_join_bss_param,"void *wilc_parse_join_bss_param(struct cfg80211_bss *bss, struct cfg80211_crypto_settings *crypto) { const u8 *ies_data, *tim_elm, *ssid_elm, *rates_ie, *supp_rates_ie; const u8 *ht_ie, *wpa_ie, *wmm_ie, *rsn_ie; struct ieee80211_p2p_noa_attr noa_attr; const struct cfg80211_bss_ies *ies; struct wilc_join_bss_param *param; u8 rates_len = 0; int ies_len; int ret; param = kzalloc(sizeof(*param), GFP_KERNEL); if (!param) return NULL; rcu_read_lock(); ies = rcu_dereference(bss->ies); ies_data = kmemdup(ies->data, ies->len, GFP_ATOMIC); if (!ies_data) { rcu_read_unlock(); kfree(param); return NULL; } ies_len = ies->len; rcu_read_unlock(); param->beacon_period = cpu_to_le16(bss->beacon_interval); param->cap_info = cpu_to_le16(bss->capability); param->bss_type = WILC_FW_BSS_TYPE_INFRA; param->ch = ieee80211_frequency_to_channel(bss->channel->center_freq); ether_addr_copy(param->bssid, bss->bssid); ssid_elm = cfg80211_find_ie(WLAN_EID_SSID, ies_data, ies_len); if (ssid_elm) { if (ssid_elm[1] <= IEEE80211_MAX_SSID_LEN) memcpy(param->ssid, ssid_elm + 2, ssid_elm[1]); } tim_elm = cfg80211_find_ie(WLAN_EID_TIM, ies_data, ies_len); if (tim_elm && tim_elm[1] >= 2) param->dtim_period = tim_elm[3]; memset(param->p_suites, 0xFF, 3); memset(param->akm_suites, 0xFF, 3); rates_ie = cfg80211_find_ie(WLAN_EID_SUPP_RATES, ies_data, ies_len); if (rates_ie) { rates_len = rates_ie[1]; if (rates_len > WILC_MAX_RATES_SUPPORTED) rates_len = WILC_MAX_RATES_SUPPORTED; param->supp_rates[0] = rates_len; memcpy(&param->supp_rates[1], rates_ie + 2, rates_len); } if (rates_len < WILC_MAX_RATES_SUPPORTED) { supp_rates_ie = cfg80211_find_ie(WLAN_EID_EXT_SUPP_RATES, ies_data, ies_len); if (supp_rates_ie) { u8 ext_rates = supp_rates_ie[1]; if (ext_rates > (WILC_MAX_RATES_SUPPORTED - rates_len)) param->supp_rates[0] = WILC_MAX_RATES_SUPPORTED; else param->supp_rates[0] += ext_rates; memcpy(&param->supp_rates[rates_len + 1], supp_rates_ie + 2, (param->supp_rates[0] - rates_len)); } } ht_ie = cfg80211_find_ie(WLAN_EID_HT_CAPABILITY, ies_data, ies_len); if (ht_ie) param->ht_capable = true; ret = cfg80211_get_p2p_attr(ies_data, ies_len, IEEE80211_P2P_ATTR_ABSENCE_NOTICE, (u8 *)&noa_attr, sizeof(noa_attr)); if (ret > 0) { <S2SV_StartVul> param->tsf_lo = cpu_to_le32(ies->tsf); <S2SV_EndVul> param->noa_enabled = 1; param->idx = noa_attr.index; if (noa_attr.oppps_ctwindow & IEEE80211_P2P_OPPPS_ENABLE_BIT) { param->opp_enabled = 1; param->opp_en.ct_window = noa_attr.oppps_ctwindow; param->opp_en.cnt = noa_attr.desc[0].count; param->opp_en.duration = noa_attr.desc[0].duration; param->opp_en.interval = noa_attr.desc[0].interval; param->opp_en.start_time = noa_attr.desc[0].start_time; } else { param->opp_enabled = 0; param->opp_dis.cnt = noa_attr.desc[0].count; param->opp_dis.duration = noa_attr.desc[0].duration; param->opp_dis.interval = noa_attr.desc[0].interval; param->opp_dis.start_time = noa_attr.desc[0].start_time; } } wmm_ie = cfg80211_find_vendor_ie(WLAN_OUI_MICROSOFT, WLAN_OUI_TYPE_MICROSOFT_WMM, ies_data, ies_len); if (wmm_ie) { struct ieee80211_wmm_param_ie *ie; ie = (struct ieee80211_wmm_param_ie *)wmm_ie; if ((ie->oui_subtype == 0 || ie->oui_subtype == 1) && ie->version == 1) { param->wmm_cap = true; if (ie->qos_info & BIT(7)) param->uapsd_cap = true; } } wpa_ie = cfg80211_find_vendor_ie(WLAN_OUI_MICROSOFT, WLAN_OUI_TYPE_MICROSOFT_WPA, ies_data, ies_len); if (wpa_ie) { param->mode_802_11i = 1; param->rsn_found = true; } rsn_ie = cfg80211_find_ie(WLAN_EID_RSN, ies_data, ies_len); if (rsn_ie) { int rsn_ie_len = sizeof(struct element) + rsn_ie[1]; int offset = 8; param->mode_802_11i = 2; param->rsn_found = true; if (offset < rsn_ie_len) { offset += (rsn_ie[offset] * 4) + 2; if (offset < rsn_ie_len) { offset += (rsn_ie[offset] * 4) + 2; if (offset + 1 < rsn_ie_len) memcpy(param->rsn_cap, &rsn_ie[offset], 2); } } } if (param->rsn_found) { int i; param->rsn_grp_policy = crypto->cipher_group & 0xFF; for (i = 0; i < crypto->n_ciphers_pairwise && i < 3; i++) param->p_suites[i] = crypto->ciphers_pairwise[i] & 0xFF; for (i = 0; i < crypto->n_akm_suites && i < 3; i++) param->akm_suites[i] = crypto->akm_suites[i] & 0xFF; } kfree(ies_data); return (void *)param; }","- param->tsf_lo = cpu_to_le32(ies->tsf);
+ u64 ies_tsf;
+ ies_tsf = ies->tsf;
+ param->tsf_lo = cpu_to_le32(ies_tsf);","void *wilc_parse_join_bss_param(struct cfg80211_bss *bss, struct cfg80211_crypto_settings *crypto) { const u8 *ies_data, *tim_elm, *ssid_elm, *rates_ie, *supp_rates_ie; const u8 *ht_ie, *wpa_ie, *wmm_ie, *rsn_ie; struct ieee80211_p2p_noa_attr noa_attr; const struct cfg80211_bss_ies *ies; struct wilc_join_bss_param *param; u8 rates_len = 0; int ies_len; u64 ies_tsf; int ret; param = kzalloc(sizeof(*param), GFP_KERNEL); if (!param) return NULL; rcu_read_lock(); ies = rcu_dereference(bss->ies); ies_data = kmemdup(ies->data, ies->len, GFP_ATOMIC); if (!ies_data) { rcu_read_unlock(); kfree(param); return NULL; } ies_len = ies->len; ies_tsf = ies->tsf; rcu_read_unlock(); param->beacon_period = cpu_to_le16(bss->beacon_interval); param->cap_info = cpu_to_le16(bss->capability); param->bss_type = WILC_FW_BSS_TYPE_INFRA; param->ch = ieee80211_frequency_to_channel(bss->channel->center_freq); ether_addr_copy(param->bssid, bss->bssid); ssid_elm = cfg80211_find_ie(WLAN_EID_SSID, ies_data, ies_len); if (ssid_elm) { if (ssid_elm[1] <= IEEE80211_MAX_SSID_LEN) memcpy(param->ssid, ssid_elm + 2, ssid_elm[1]); } tim_elm = cfg80211_find_ie(WLAN_EID_TIM, ies_data, ies_len); if (tim_elm && tim_elm[1] >= 2) param->dtim_period = tim_elm[3]; memset(param->p_suites, 0xFF, 3); memset(param->akm_suites, 0xFF, 3); rates_ie = cfg80211_find_ie(WLAN_EID_SUPP_RATES, ies_data, ies_len); if (rates_ie) { rates_len = rates_ie[1]; if (rates_len > WILC_MAX_RATES_SUPPORTED) rates_len = WILC_MAX_RATES_SUPPORTED; param->supp_rates[0] = rates_len; memcpy(&param->supp_rates[1], rates_ie + 2, rates_len); } if (rates_len < WILC_MAX_RATES_SUPPORTED) { supp_rates_ie = cfg80211_find_ie(WLAN_EID_EXT_SUPP_RATES, ies_data, ies_len); if (supp_rates_ie) { u8 ext_rates = supp_rates_ie[1]; if (ext_rates > (WILC_MAX_RATES_SUPPORTED - rates_len)) param->supp_rates[0] = WILC_MAX_RATES_SUPPORTED; else param->supp_rates[0] += ext_rates; memcpy(&param->supp_rates[rates_len + 1], supp_rates_ie + 2, (param->supp_rates[0] - rates_len)); } } ht_ie = cfg80211_find_ie(WLAN_EID_HT_CAPABILITY, ies_data, ies_len); if (ht_ie) param->ht_capable = true; ret = cfg80211_get_p2p_attr(ies_data, ies_len, IEEE80211_P2P_ATTR_ABSENCE_NOTICE, (u8 *)&noa_attr, sizeof(noa_attr)); if (ret > 0) { param->tsf_lo = cpu_to_le32(ies_tsf); param->noa_enabled = 1; param->idx = noa_attr.index; if (noa_attr.oppps_ctwindow & IEEE80211_P2P_OPPPS_ENABLE_BIT) { param->opp_enabled = 1; param->opp_en.ct_window = noa_attr.oppps_ctwindow; param->opp_en.cnt = noa_attr.desc[0].count; param->opp_en.duration = noa_attr.desc[0].duration; param->opp_en.interval = noa_attr.desc[0].interval; param->opp_en.start_time = noa_attr.desc[0].start_time; } else { param->opp_enabled = 0; param->opp_dis.cnt = noa_attr.desc[0].count; param->opp_dis.duration = noa_attr.desc[0].duration; param->opp_dis.interval = noa_attr.desc[0].interval; param->opp_dis.start_time = noa_attr.desc[0].start_time; } } wmm_ie = cfg80211_find_vendor_ie(WLAN_OUI_MICROSOFT, WLAN_OUI_TYPE_MICROSOFT_WMM, ies_data, ies_len); if (wmm_ie) { struct ieee80211_wmm_param_ie *ie; ie = (struct ieee80211_wmm_param_ie *)wmm_ie; if ((ie->oui_subtype == 0 || ie->oui_subtype == 1) && ie->version == 1) { param->wmm_cap = true; if (ie->qos_info & BIT(7)) param->uapsd_cap = true; } } wpa_ie = cfg80211_find_vendor_ie(WLAN_OUI_MICROSOFT, WLAN_OUI_TYPE_MICROSOFT_WPA, ies_data, ies_len); if (wpa_ie) { param->mode_802_11i = 1; param->rsn_found = true; } rsn_ie = cfg80211_find_ie(WLAN_EID_RSN, ies_data, ies_len); if (rsn_ie) { int rsn_ie_len = sizeof(struct element) + rsn_ie[1]; int offset = 8; param->mode_802_11i = 2; param->rsn_found = true; if (offset < rsn_ie_len) { offset += (rsn_ie[offset] * 4) + 2; if (offset < rsn_ie_len) { offset += (rsn_ie[offset] * 4) + 2; if (offset + 1 < rsn_ie_len) memcpy(param->rsn_cap, &rsn_ie[offset], 2); } } } if (param->rsn_found) { int i; param->rsn_grp_policy = crypto->cipher_group & 0xFF; for (i = 0; i < crypto->n_ciphers_pairwise && i < 3; i++) param->p_suites[i] = crypto->ciphers_pairwise[i] & 0xFF; for (i = 0; i < crypto->n_akm_suites && i < 3; i++) param->akm_suites[i] = crypto->akm_suites[i] & 0xFF; } kfree(ies_data); return (void *)param; }"
862----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50121/bad/nfs4state.c----nfs4_state_shutdown_net,"nfs4_state_shutdown_net(struct net *net) { struct nfs4_delegation *dp = NULL; struct list_head *pos, *next, reaplist; struct nfsd_net *nn = net_generic(net, nfsd_net_id); unregister_shrinker(&nn->nfsd_client_shrinker); <S2SV_StartVul> cancel_work(&nn->nfsd_shrinker_work); <S2SV_EndVul> cancel_delayed_work_sync(&nn->laundromat_work); locks_end_grace(&nn->nfsd4_manager); INIT_LIST_HEAD(&reaplist); spin_lock(&state_lock); list_for_each_safe(pos, next, &nn->del_recall_lru) { dp = list_entry (pos, struct nfs4_delegation, dl_recall_lru); WARN_ON(!unhash_delegation_locked(dp)); list_add(&dp->dl_recall_lru, &reaplist); } spin_unlock(&state_lock); list_for_each_safe(pos, next, &reaplist) { dp = list_entry (pos, struct nfs4_delegation, dl_recall_lru); list_del_init(&dp->dl_recall_lru); destroy_unhashed_deleg(dp); } nfsd4_client_tracking_exit(net); nfs4_state_destroy_net(net); #ifdef CONFIG_NFSD_V4_2_INTER_SSC nfsd4_ssc_shutdown_umount(nn); #endif }","- cancel_work(&nn->nfsd_shrinker_work);
+ cancel_work_sync(&nn->nfsd_shrinker_work);","nfs4_state_shutdown_net(struct net *net) { struct nfs4_delegation *dp = NULL; struct list_head *pos, *next, reaplist; struct nfsd_net *nn = net_generic(net, nfsd_net_id); unregister_shrinker(&nn->nfsd_client_shrinker); cancel_work_sync(&nn->nfsd_shrinker_work); cancel_delayed_work_sync(&nn->laundromat_work); locks_end_grace(&nn->nfsd4_manager); INIT_LIST_HEAD(&reaplist); spin_lock(&state_lock); list_for_each_safe(pos, next, &nn->del_recall_lru) { dp = list_entry (pos, struct nfs4_delegation, dl_recall_lru); WARN_ON(!unhash_delegation_locked(dp)); list_add(&dp->dl_recall_lru, &reaplist); } spin_unlock(&state_lock); list_for_each_safe(pos, next, &reaplist) { dp = list_entry (pos, struct nfs4_delegation, dl_recall_lru); list_del_init(&dp->dl_recall_lru); destroy_unhashed_deleg(dp); } nfsd4_client_tracking_exit(net); nfs4_state_destroy_net(net); #ifdef CONFIG_NFSD_V4_2_INTER_SSC nfsd4_ssc_shutdown_umount(nn); #endif }"
901----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50148/bad/core.c----bnep_init,"static int __init bnep_init(void) { char flt[50] = """"; #ifdef CONFIG_BT_BNEP_PROTO_FILTER strcat(flt, ""protocol ""); #endif #ifdef CONFIG_BT_BNEP_MC_FILTER strcat(flt, ""multicast""); #endif BT_INFO(""BNEP (Ethernet Emulation) ver %s"", VERSION); if (flt[0]) BT_INFO(""BNEP filters: %s"", flt); <S2SV_StartVul> bnep_sock_init(); <S2SV_EndVul> <S2SV_StartVul> return 0; <S2SV_EndVul> }","- bnep_sock_init();
- return 0;
+ return bnep_sock_init();","static int __init bnep_init(void) { char flt[50] = """"; #ifdef CONFIG_BT_BNEP_PROTO_FILTER strcat(flt, ""protocol ""); #endif #ifdef CONFIG_BT_BNEP_MC_FILTER strcat(flt, ""multicast""); #endif BT_INFO(""BNEP (Ethernet Emulation) ver %s"", VERSION); if (flt[0]) BT_INFO(""BNEP filters: %s"", flt); return bnep_sock_init(); }"
1333----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56652/bad/xe_reg_sr.c----xe_reg_sr_add,"int xe_reg_sr_add(struct xe_reg_sr *sr, const struct xe_reg_sr_entry *e, struct xe_gt *gt) { unsigned long idx = e->reg.addr; struct xe_reg_sr_entry *pentry = xa_load(&sr->xa, idx); int ret; if (pentry) { if (!compatible_entries(pentry, e)) { ret = -EINVAL; goto fail; } pentry->clr_bits |= e->clr_bits; pentry->set_bits |= e->set_bits; pentry->read_mask |= e->read_mask; return 0; <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> pentry = alloc_entry(sr); <S2SV_EndVul> if (!pentry) { ret = -ENOMEM; goto fail; } *pentry = *e; ret = xa_err(xa_store(&sr->xa, idx, pentry, GFP_KERNEL)); if (ret) goto fail; return 0; fail: xe_gt_err(gt, ""discarding save-restore reg %04lx (clear: %08x, set: %08x, masked: %s, mcr: %s): ret=%d\n"", idx, e->clr_bits, e->set_bits, str_yes_no(e->reg.masked), str_yes_no(e->reg.mcr), ret); reg_sr_inc_error(sr); return ret; }","- }
- pentry = alloc_entry(sr);
+ pentry = kmalloc(sizeof(*pentry), GFP_KERNEL);","int xe_reg_sr_add(struct xe_reg_sr *sr, const struct xe_reg_sr_entry *e, struct xe_gt *gt) { unsigned long idx = e->reg.addr; struct xe_reg_sr_entry *pentry = xa_load(&sr->xa, idx); int ret; if (pentry) { if (!compatible_entries(pentry, e)) { ret = -EINVAL; goto fail; } pentry->clr_bits |= e->clr_bits; pentry->set_bits |= e->set_bits; pentry->read_mask |= e->read_mask; return 0; } pentry = kmalloc(sizeof(*pentry), GFP_KERNEL); if (!pentry) { ret = -ENOMEM; goto fail; } *pentry = *e; ret = xa_err(xa_store(&sr->xa, idx, pentry, GFP_KERNEL)); if (ret) goto fail; return 0; fail: xe_gt_err(gt, ""discarding save-restore reg %04lx (clear: %08x, set: %08x, masked: %s, mcr: %s): ret=%d\n"", idx, e->clr_bits, e->set_bits, str_yes_no(e->reg.masked), str_yes_no(e->reg.mcr), ret); reg_sr_inc_error(sr); return ret; }"
472----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47713/bad/iface.c----ieee80211_do_stop,"static void ieee80211_do_stop(struct ieee80211_sub_if_data *sdata, bool going_down) { struct ieee80211_local *local = sdata->local; unsigned long flags; struct sk_buff *skb, *tmp; u32 hw_reconf_flags = 0; int i, flushed; struct ps_data *ps; struct cfg80211_chan_def chandef; bool cancel_scan; struct cfg80211_nan_func *func; clear_bit(SDATA_STATE_RUNNING, &sdata->state); cancel_scan = rcu_access_pointer(local->scan_sdata) == sdata; if (cancel_scan) ieee80211_scan_cancel(local); if (sdata->dev) netif_tx_stop_all_queues(sdata->dev); ieee80211_roc_purge(local, sdata); switch (sdata->vif.type) { case NL80211_IFTYPE_STATION: ieee80211_mgd_stop(sdata); break; case NL80211_IFTYPE_ADHOC: ieee80211_ibss_stop(sdata); break; case NL80211_IFTYPE_AP: cancel_work_sync(&sdata->u.ap.request_smps_work); break; case NL80211_IFTYPE_MONITOR: if (sdata->u.mntr.flags & MONITOR_FLAG_COOK_FRAMES) break; list_del_rcu(&sdata->u.mntr.list); break; default: break; } flushed = sta_info_flush(sdata); WARN_ON_ONCE(sdata->vif.type != NL80211_IFTYPE_AP_VLAN && ((sdata->vif.type != NL80211_IFTYPE_WDS && flushed > 0) || (sdata->vif.type == NL80211_IFTYPE_WDS && flushed != 1))); if (sdata->flags & IEEE80211_SDATA_ALLMULTI) atomic_dec(&local->iff_allmultis); if (sdata->vif.type == NL80211_IFTYPE_AP) { local->fif_pspoll--; local->fif_probe_req--; } else if (sdata->vif.type == NL80211_IFTYPE_ADHOC) { local->fif_probe_req--; } if (sdata->dev) { netif_addr_lock_bh(sdata->dev); spin_lock_bh(&local->filter_lock); __hw_addr_unsync(&local->mc_list, &sdata->dev->mc, sdata->dev->addr_len); spin_unlock_bh(&local->filter_lock); netif_addr_unlock_bh(sdata->dev); } del_timer_sync(&local->dynamic_ps_timer); cancel_work_sync(&local->dynamic_ps_enable_work); cancel_work_sync(&sdata->recalc_smps); sdata_lock(sdata); mutex_lock(&local->mtx); sdata->vif.csa_active = false; if (sdata->vif.type == NL80211_IFTYPE_STATION) sdata->u.mgd.csa_waiting_bcn = false; if (sdata->csa_block_tx) { ieee80211_wake_vif_queues(local, sdata, IEEE80211_QUEUE_STOP_REASON_CSA); sdata->csa_block_tx = false; } mutex_unlock(&local->mtx); sdata_unlock(sdata); cancel_work_sync(&sdata->csa_finalize_work); cancel_delayed_work_sync(&sdata->dfs_cac_timer_work); if (sdata->wdev.cac_started) { chandef = sdata->vif.bss_conf.chandef; WARN_ON(local->suspended); mutex_lock(&local->mtx); ieee80211_vif_release_channel(sdata); mutex_unlock(&local->mtx); cfg80211_cac_event(sdata->dev, &chandef, NL80211_RADAR_CAC_ABORTED, GFP_KERNEL); } if (sdata->vif.type == NL80211_IFTYPE_AP) { struct ieee80211_sub_if_data *vlan, *tmpsdata; list_for_each_entry_safe(vlan, tmpsdata, &sdata->u.ap.vlans, u.vlan.list) dev_close(vlan->dev); WARN_ON(!list_empty(&sdata->u.ap.vlans)); } else if (sdata->vif.type == NL80211_IFTYPE_AP_VLAN) { ps = &sdata->bss->ps; spin_lock_irqsave(&ps->bc_buf.lock, flags); skb_queue_walk_safe(&ps->bc_buf, skb, tmp) { if (skb->dev == sdata->dev) { __skb_unlink(skb, &ps->bc_buf); local->total_ps_buffered--; ieee80211_free_txskb(&local->hw, skb); } } spin_unlock_irqrestore(&ps->bc_buf.lock, flags); } if (going_down) local->open_count--; switch (sdata->vif.type) { case NL80211_IFTYPE_AP_VLAN: mutex_lock(&local->mtx); list_del(&sdata->u.vlan.list); mutex_unlock(&local->mtx); RCU_INIT_POINTER(sdata->vif.chanctx_conf, NULL); ieee80211_free_keys(sdata, true); break; case NL80211_IFTYPE_MONITOR: if (sdata->u.mntr.flags & MONITOR_FLAG_COOK_FRAMES) { local->cooked_mntrs--; break; } local->monitors--; if (local->monitors == 0) { local->hw.conf.flags &= ~IEEE80211_CONF_MONITOR; hw_reconf_flags |= IEEE80211_CONF_CHANGE_MONITOR; } ieee80211_adjust_monitor_flags(sdata, -1); break; case NL80211_IFTYPE_NAN: spin_lock_bh(&sdata->u.nan.func_lock); idr_for_each_entry(&sdata->u.nan.function_inst_ids, func, i) { idr_remove(&sdata->u.nan.function_inst_ids, i); cfg80211_free_nan_func(func); } idr_destroy(&sdata->u.nan.function_inst_ids); spin_unlock_bh(&sdata->u.nan.func_lock); break; case NL80211_IFTYPE_P2P_DEVICE: RCU_INIT_POINTER(local->p2p_sdata, NULL); default: cancel_work_sync(&sdata->work); ieee80211_free_keys(sdata, true); skb_queue_purge(&sdata->skb_queue); } spin_lock_irqsave(&local->queue_stop_reason_lock, flags); for (i = 0; i < IEEE80211_MAX_QUEUES; i++) { skb_queue_walk_safe(&local->pending[i], skb, tmp) { struct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb); if (info->control.vif == &sdata->vif) { __skb_unlink(skb, &local->pending[i]); <S2SV_StartVul> ieee80211_free_txskb(&local->hw, skb); <S2SV_EndVul> } } } spin_unlock_irqrestore(&local->queue_stop_reason_lock, flags); if (sdata->vif.type == NL80211_IFTYPE_AP_VLAN) ieee80211_txq_remove_vlan(local, sdata); sdata->bss = NULL; if (local->open_count == 0) ieee80211_clear_tx_pending(local); sdata->vif.bss_conf.beacon_int = 0; if (local->suspended) { WARN_ON(local->wowlan); WARN_ON(rtnl_dereference(local->monitor_sdata)); return; } switch (sdata->vif.type) { case NL80211_IFTYPE_AP_VLAN: break; case NL80211_IFTYPE_MONITOR: if (local->monitors == 0) ieee80211_del_virtual_monitor(local); mutex_lock(&local->mtx); ieee80211_recalc_idle(local); mutex_unlock(&local->mtx); if (!(sdata->u.mntr.flags & MONITOR_FLAG_ACTIVE)) break; default: if (going_down) drv_remove_interface(local, sdata); } ieee80211_recalc_ps(local); if (cancel_scan) flush_delayed_work(&local->scan_work); if (local->open_count == 0) { ieee80211_stop_device(local); return; } ieee80211_configure_filter(local); ieee80211_hw_config(local, hw_reconf_flags); if (local->monitors == local->open_count) ieee80211_add_virtual_monitor(local); }","- ieee80211_free_txskb(&local->hw, skb);
+ struct sk_buff_head freeq;
+ }
+ __skb_queue_head_init(&freeq);
+ __skb_queue_tail(&freeq, skb);
+ }
+ }
+ }
+ skb_queue_walk_safe(&freeq, skb, tmp) {
+ __skb_unlink(skb, &freeq);
+ ieee80211_free_txskb(&local->hw, skb);
+ }","static void ieee80211_do_stop(struct ieee80211_sub_if_data *sdata, bool going_down) { struct ieee80211_local *local = sdata->local; unsigned long flags; struct sk_buff_head freeq; struct sk_buff *skb, *tmp; u32 hw_reconf_flags = 0; int i, flushed; struct ps_data *ps; struct cfg80211_chan_def chandef; bool cancel_scan; struct cfg80211_nan_func *func; clear_bit(SDATA_STATE_RUNNING, &sdata->state); cancel_scan = rcu_access_pointer(local->scan_sdata) == sdata; if (cancel_scan) ieee80211_scan_cancel(local); if (sdata->dev) netif_tx_stop_all_queues(sdata->dev); ieee80211_roc_purge(local, sdata); switch (sdata->vif.type) { case NL80211_IFTYPE_STATION: ieee80211_mgd_stop(sdata); break; case NL80211_IFTYPE_ADHOC: ieee80211_ibss_stop(sdata); break; case NL80211_IFTYPE_AP: cancel_work_sync(&sdata->u.ap.request_smps_work); break; case NL80211_IFTYPE_MONITOR: if (sdata->u.mntr.flags & MONITOR_FLAG_COOK_FRAMES) break; list_del_rcu(&sdata->u.mntr.list); break; default: break; } flushed = sta_info_flush(sdata); WARN_ON_ONCE(sdata->vif.type != NL80211_IFTYPE_AP_VLAN && ((sdata->vif.type != NL80211_IFTYPE_WDS && flushed > 0) || (sdata->vif.type == NL80211_IFTYPE_WDS && flushed != 1))); if (sdata->flags & IEEE80211_SDATA_ALLMULTI) atomic_dec(&local->iff_allmultis); if (sdata->vif.type == NL80211_IFTYPE_AP) { local->fif_pspoll--; local->fif_probe_req--; } else if (sdata->vif.type == NL80211_IFTYPE_ADHOC) { local->fif_probe_req--; } if (sdata->dev) { netif_addr_lock_bh(sdata->dev); spin_lock_bh(&local->filter_lock); __hw_addr_unsync(&local->mc_list, &sdata->dev->mc, sdata->dev->addr_len); spin_unlock_bh(&local->filter_lock); netif_addr_unlock_bh(sdata->dev); } del_timer_sync(&local->dynamic_ps_timer); cancel_work_sync(&local->dynamic_ps_enable_work); cancel_work_sync(&sdata->recalc_smps); sdata_lock(sdata); mutex_lock(&local->mtx); sdata->vif.csa_active = false; if (sdata->vif.type == NL80211_IFTYPE_STATION) sdata->u.mgd.csa_waiting_bcn = false; if (sdata->csa_block_tx) { ieee80211_wake_vif_queues(local, sdata, IEEE80211_QUEUE_STOP_REASON_CSA); sdata->csa_block_tx = false; } mutex_unlock(&local->mtx); sdata_unlock(sdata); cancel_work_sync(&sdata->csa_finalize_work); cancel_delayed_work_sync(&sdata->dfs_cac_timer_work); if (sdata->wdev.cac_started) { chandef = sdata->vif.bss_conf.chandef; WARN_ON(local->suspended); mutex_lock(&local->mtx); ieee80211_vif_release_channel(sdata); mutex_unlock(&local->mtx); cfg80211_cac_event(sdata->dev, &chandef, NL80211_RADAR_CAC_ABORTED, GFP_KERNEL); } if (sdata->vif.type == NL80211_IFTYPE_AP) { struct ieee80211_sub_if_data *vlan, *tmpsdata; list_for_each_entry_safe(vlan, tmpsdata, &sdata->u.ap.vlans, u.vlan.list) dev_close(vlan->dev); WARN_ON(!list_empty(&sdata->u.ap.vlans)); } else if (sdata->vif.type == NL80211_IFTYPE_AP_VLAN) { ps = &sdata->bss->ps; spin_lock_irqsave(&ps->bc_buf.lock, flags); skb_queue_walk_safe(&ps->bc_buf, skb, tmp) { if (skb->dev == sdata->dev) { __skb_unlink(skb, &ps->bc_buf); local->total_ps_buffered--; ieee80211_free_txskb(&local->hw, skb); } } spin_unlock_irqrestore(&ps->bc_buf.lock, flags); } if (going_down) local->open_count--; switch (sdata->vif.type) { case NL80211_IFTYPE_AP_VLAN: mutex_lock(&local->mtx); list_del(&sdata->u.vlan.list); mutex_unlock(&local->mtx); RCU_INIT_POINTER(sdata->vif.chanctx_conf, NULL); ieee80211_free_keys(sdata, true); break; case NL80211_IFTYPE_MONITOR: if (sdata->u.mntr.flags & MONITOR_FLAG_COOK_FRAMES) { local->cooked_mntrs--; break; } local->monitors--; if (local->monitors == 0) { local->hw.conf.flags &= ~IEEE80211_CONF_MONITOR; hw_reconf_flags |= IEEE80211_CONF_CHANGE_MONITOR; } ieee80211_adjust_monitor_flags(sdata, -1); break; case NL80211_IFTYPE_NAN: spin_lock_bh(&sdata->u.nan.func_lock); idr_for_each_entry(&sdata->u.nan.function_inst_ids, func, i) { idr_remove(&sdata->u.nan.function_inst_ids, i); cfg80211_free_nan_func(func); } idr_destroy(&sdata->u.nan.function_inst_ids); spin_unlock_bh(&sdata->u.nan.func_lock); break; case NL80211_IFTYPE_P2P_DEVICE: RCU_INIT_POINTER(local->p2p_sdata, NULL); default: cancel_work_sync(&sdata->work); ieee80211_free_keys(sdata, true); skb_queue_purge(&sdata->skb_queue); } __skb_queue_head_init(&freeq); spin_lock_irqsave(&local->queue_stop_reason_lock, flags); for (i = 0; i < IEEE80211_MAX_QUEUES; i++) { skb_queue_walk_safe(&local->pending[i], skb, tmp) { struct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb); if (info->control.vif == &sdata->vif) { __skb_unlink(skb, &local->pending[i]); __skb_queue_tail(&freeq, skb); } } } spin_unlock_irqrestore(&local->queue_stop_reason_lock, flags); skb_queue_walk_safe(&freeq, skb, tmp) { __skb_unlink(skb, &freeq); ieee80211_free_txskb(&local->hw, skb); } if (sdata->vif.type == NL80211_IFTYPE_AP_VLAN) ieee80211_txq_remove_vlan(local, sdata); sdata->bss = NULL; if (local->open_count == 0) ieee80211_clear_tx_pending(local); sdata->vif.bss_conf.beacon_int = 0; if (local->suspended) { WARN_ON(local->wowlan); WARN_ON(rtnl_dereference(local->monitor_sdata)); return; } switch (sdata->vif.type) { case NL80211_IFTYPE_AP_VLAN: break; case NL80211_IFTYPE_MONITOR: if (local->monitors == 0) ieee80211_del_virtual_monitor(local); mutex_lock(&local->mtx); ieee80211_recalc_idle(local); mutex_unlock(&local->mtx); if (!(sdata->u.mntr.flags & MONITOR_FLAG_ACTIVE)) break; default: if (going_down) drv_remove_interface(local, sdata); } ieee80211_recalc_ps(local); if (cancel_scan) flush_delayed_work(&local->scan_work); if (local->open_count == 0) { ieee80211_stop_device(local); return; } ieee80211_configure_filter(local); ieee80211_hw_config(local, hw_reconf_flags); if (local->monitors == local->open_count) ieee80211_add_virtual_monitor(local); }"
858----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50117/bad/amdgpu_acpi.c----*amdgpu_atif_call,"static union acpi_object *amdgpu_atif_call(struct amdgpu_atif *atif, int function, struct acpi_buffer *params) { acpi_status status; union acpi_object atif_arg_elements[2]; struct acpi_object_list atif_arg; struct acpi_buffer buffer = { ACPI_ALLOCATE_BUFFER, NULL }; atif_arg.count = 2; atif_arg.pointer = &atif_arg_elements[0]; atif_arg_elements[0].type = ACPI_TYPE_INTEGER; atif_arg_elements[0].integer.value = function; if (params) { atif_arg_elements[1].type = ACPI_TYPE_BUFFER; atif_arg_elements[1].buffer.length = params->length; atif_arg_elements[1].buffer.pointer = params->pointer; } else { atif_arg_elements[1].type = ACPI_TYPE_INTEGER; atif_arg_elements[1].integer.value = 0; } status = acpi_evaluate_object(atif->handle, NULL, &atif_arg, &buffer); if (ACPI_FAILURE(status) && status != AE_NOT_FOUND) { DRM_DEBUG_DRIVER(""failed to evaluate ATIF got %s\n"", acpi_format_exception(status)); <S2SV_StartVul> kfree(buffer.pointer); <S2SV_EndVul> return NULL; } <S2SV_StartVul> return buffer.pointer; <S2SV_EndVul> }","- kfree(buffer.pointer);
- return buffer.pointer;
+ union acpi_object *obj;
+ obj = (union acpi_object *)buffer.pointer;
+ kfree(obj);
+ return NULL;
+ }
+ if (obj->type != ACPI_TYPE_BUFFER) {
+ DRM_DEBUG_DRIVER(""bad object returned from ATIF: %d\n"",
+ obj->type);
+ kfree(obj);
+ return NULL;
+ }
+ return obj;
+ }","static union acpi_object *amdgpu_atif_call(struct amdgpu_atif *atif, int function, struct acpi_buffer *params) { acpi_status status; union acpi_object *obj; union acpi_object atif_arg_elements[2]; struct acpi_object_list atif_arg; struct acpi_buffer buffer = { ACPI_ALLOCATE_BUFFER, NULL }; atif_arg.count = 2; atif_arg.pointer = &atif_arg_elements[0]; atif_arg_elements[0].type = ACPI_TYPE_INTEGER; atif_arg_elements[0].integer.value = function; if (params) { atif_arg_elements[1].type = ACPI_TYPE_BUFFER; atif_arg_elements[1].buffer.length = params->length; atif_arg_elements[1].buffer.pointer = params->pointer; } else { atif_arg_elements[1].type = ACPI_TYPE_INTEGER; atif_arg_elements[1].integer.value = 0; } status = acpi_evaluate_object(atif->handle, NULL, &atif_arg, &buffer); obj = (union acpi_object *)buffer.pointer; if (ACPI_FAILURE(status) && status != AE_NOT_FOUND) { DRM_DEBUG_DRIVER(""failed to evaluate ATIF got %s\n"", acpi_format_exception(status)); kfree(obj); return NULL; } if (obj->type != ACPI_TYPE_BUFFER) { DRM_DEBUG_DRIVER(""bad object returned from ATIF: %d\n"", obj->type); kfree(obj); return NULL; } return obj; }"
1134----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53108/bad/amdgpu_dm.c----parse_amd_vsdb,"static int parse_amd_vsdb(struct amdgpu_dm_connector *aconnector, struct edid *edid, struct amdgpu_hdmi_vsdb_info *vsdb_info) { u8 *edid_ext = NULL; int i; int j = 0; if (edid == NULL || edid->extensions == 0) return -ENODEV; for (i = 0; i < edid->extensions; i++) { edid_ext = (void *)(edid + (i + 1)); if (edid_ext[0] == DISPLAYID_EXT) break; } <S2SV_StartVul> while (j < EDID_LENGTH) { <S2SV_EndVul> struct amd_vsdb_block *amd_vsdb = (struct amd_vsdb_block *)&edid_ext[j]; unsigned int ieeeId = (amd_vsdb->ieee_id[2] << 16) | (amd_vsdb->ieee_id[1] << 8) | (amd_vsdb->ieee_id[0]); if (ieeeId == HDMI_AMD_VENDOR_SPECIFIC_DATA_BLOCK_IEEE_REGISTRATION_ID && amd_vsdb->version == HDMI_AMD_VENDOR_SPECIFIC_DATA_BLOCK_VERSION_3) { vsdb_info->replay_mode = (amd_vsdb->feature_caps & AMD_VSDB_VERSION_3_FEATURECAP_REPLAYMODE) ? true : false; vsdb_info->amd_vsdb_version = HDMI_AMD_VENDOR_SPECIFIC_DATA_BLOCK_VERSION_3; DRM_DEBUG_KMS(""Panel supports Replay Mode: %d\n"", vsdb_info->replay_mode); return true; } j++; } return false; }","- while (j < EDID_LENGTH) {
+ while (j < EDID_LENGTH - sizeof(struct amd_vsdb_block)) {","static int parse_amd_vsdb(struct amdgpu_dm_connector *aconnector, struct edid *edid, struct amdgpu_hdmi_vsdb_info *vsdb_info) { u8 *edid_ext = NULL; int i; int j = 0; if (edid == NULL || edid->extensions == 0) return -ENODEV; for (i = 0; i < edid->extensions; i++) { edid_ext = (void *)(edid + (i + 1)); if (edid_ext[0] == DISPLAYID_EXT) break; } while (j < EDID_LENGTH - sizeof(struct amd_vsdb_block)) { struct amd_vsdb_block *amd_vsdb = (struct amd_vsdb_block *)&edid_ext[j]; unsigned int ieeeId = (amd_vsdb->ieee_id[2] << 16) | (amd_vsdb->ieee_id[1] << 8) | (amd_vsdb->ieee_id[0]); if (ieeeId == HDMI_AMD_VENDOR_SPECIFIC_DATA_BLOCK_IEEE_REGISTRATION_ID && amd_vsdb->version == HDMI_AMD_VENDOR_SPECIFIC_DATA_BLOCK_VERSION_3) { vsdb_info->replay_mode = (amd_vsdb->feature_caps & AMD_VSDB_VERSION_3_FEATURECAP_REPLAYMODE) ? true : false; vsdb_info->amd_vsdb_version = HDMI_AMD_VENDOR_SPECIFIC_DATA_BLOCK_VERSION_3; DRM_DEBUG_KMS(""Panel supports Replay Mode: %d\n"", vsdb_info->replay_mode); return true; } j++; } return false; }"
947----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50191/bad/super.c----ext4_handle_error,"static void ext4_handle_error(struct super_block *sb, bool force_ro, int error, __u32 ino, __u64 block, const char *func, unsigned int line) { journal_t *journal = EXT4_SB(sb)->s_journal; bool continue_fs = !force_ro && test_opt(sb, ERRORS_CONT); EXT4_SB(sb)->s_mount_state |= EXT4_ERROR_FS; if (test_opt(sb, WARN_ON_ERROR)) WARN_ON_ONCE(1); if (!continue_fs && !sb_rdonly(sb)) { set_bit(EXT4_FLAGS_SHUTDOWN, &EXT4_SB(sb)->s_ext4_flags); if (journal) jbd2_journal_abort(journal, -EIO); } if (!bdev_read_only(sb->s_bdev)) { save_error_info(sb, error, ino, block, func, line); if (continue_fs && journal) schedule_work(&EXT4_SB(sb)->s_sb_upd_work); else ext4_commit_super(sb); } if (test_opt(sb, ERRORS_PANIC) && !system_going_down()) { panic(""EXT4-fs (device %s): panic forced after error\n"", sb->s_id); } if (sb_rdonly(sb) || continue_fs) return; ext4_msg(sb, KERN_CRIT, ""Remounting filesystem read-only""); <S2SV_StartVul> smp_wmb(); <S2SV_EndVul> <S2SV_StartVul> sb->s_flags |= SB_RDONLY; <S2SV_EndVul> }","- smp_wmb();
- sb->s_flags |= SB_RDONLY;","static void ext4_handle_error(struct super_block *sb, bool force_ro, int error, __u32 ino, __u64 block, const char *func, unsigned int line) { journal_t *journal = EXT4_SB(sb)->s_journal; bool continue_fs = !force_ro && test_opt(sb, ERRORS_CONT); EXT4_SB(sb)->s_mount_state |= EXT4_ERROR_FS; if (test_opt(sb, WARN_ON_ERROR)) WARN_ON_ONCE(1); if (!continue_fs && !sb_rdonly(sb)) { set_bit(EXT4_FLAGS_SHUTDOWN, &EXT4_SB(sb)->s_ext4_flags); if (journal) jbd2_journal_abort(journal, -EIO); } if (!bdev_read_only(sb->s_bdev)) { save_error_info(sb, error, ino, block, func, line); if (continue_fs && journal) schedule_work(&EXT4_SB(sb)->s_sb_upd_work); else ext4_commit_super(sb); } if (test_opt(sb, ERRORS_PANIC) && !system_going_down()) { panic(""EXT4-fs (device %s): panic forced after error\n"", sb->s_id); } if (sb_rdonly(sb) || continue_fs) return; ext4_msg(sb, KERN_CRIT, ""Remounting filesystem read-only""); }"
1339----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56657/bad/control_led.c----snd_ctl_led_sysfs_add,"static void snd_ctl_led_sysfs_add(struct snd_card *card) { unsigned int group; struct snd_ctl_led_card *led_card; struct snd_ctl_led *led; char link_name[32]; for (group = 0; group < MAX_LED; group++) { led = &snd_ctl_leds[group]; led_card = kzalloc(sizeof(*led_card), GFP_KERNEL); if (!led_card) goto cerr2; led_card->number = card->number; led_card->led = led; device_initialize(&led_card->dev); led_card->dev.release = snd_ctl_led_card_release; if (dev_set_name(&led_card->dev, ""card%d"", card->number) < 0) goto cerr; led_card->dev.parent = &led->dev; led_card->dev.groups = snd_ctl_led_card_attr_groups; if (device_add(&led_card->dev)) goto cerr; led->cards[card->number] = led_card; snprintf(link_name, sizeof(link_name), ""led-%s"", led->name); <S2SV_StartVul> WARN(sysfs_create_link(&card->ctl_dev->kobj, &led_card->dev.kobj, link_name), <S2SV_EndVul> <S2SV_StartVul> ""can't create symlink to controlC%i device\n"", card->number); <S2SV_EndVul> <S2SV_StartVul> WARN(sysfs_create_link(&led_card->dev.kobj, &card->card_dev.kobj, ""card""), <S2SV_EndVul> <S2SV_StartVul> ""can't create symlink to card%i\n"", card->number); <S2SV_EndVul> continue; cerr: put_device(&led_card->dev); cerr2: dev_err(card->dev, ""snd_ctl_led: unable to add card%d"", card->number); } }","- WARN(sysfs_create_link(&card->ctl_dev->kobj, &led_card->dev.kobj, link_name),
- ""can't create symlink to controlC%i device\n"", card->number);
- WARN(sysfs_create_link(&led_card->dev.kobj, &card->card_dev.kobj, ""card""),
- ""can't create symlink to card%i\n"", card->number);
+ if (sysfs_create_link(&card->ctl_dev->kobj, &led_card->dev.kobj,
+ link_name))
+ dev_err(card->dev,
+ ""%s: can't create symlink to controlC%i device\n"",
+ __func__, card->number);
+ if (sysfs_create_link(&led_card->dev.kobj, &card->card_dev.kobj,
+ ""card""))
+ dev_err(card->dev,
+ ""%s: can't create symlink to card%i\n"",
+ __func__, card->number);","static void snd_ctl_led_sysfs_add(struct snd_card *card) { unsigned int group; struct snd_ctl_led_card *led_card; struct snd_ctl_led *led; char link_name[32]; for (group = 0; group < MAX_LED; group++) { led = &snd_ctl_leds[group]; led_card = kzalloc(sizeof(*led_card), GFP_KERNEL); if (!led_card) goto cerr2; led_card->number = card->number; led_card->led = led; device_initialize(&led_card->dev); led_card->dev.release = snd_ctl_led_card_release; if (dev_set_name(&led_card->dev, ""card%d"", card->number) < 0) goto cerr; led_card->dev.parent = &led->dev; led_card->dev.groups = snd_ctl_led_card_attr_groups; if (device_add(&led_card->dev)) goto cerr; led->cards[card->number] = led_card; snprintf(link_name, sizeof(link_name), ""led-%s"", led->name); if (sysfs_create_link(&card->ctl_dev->kobj, &led_card->dev.kobj, link_name)) dev_err(card->dev, ""%s: can't create symlink to controlC%i device\n"", __func__, card->number); if (sysfs_create_link(&led_card->dev.kobj, &card->card_dev.kobj, ""card"")) dev_err(card->dev, ""%s: can't create symlink to card%i\n"", __func__, card->number); continue; cerr: put_device(&led_card->dev); cerr2: dev_err(card->dev, ""snd_ctl_led: unable to add card%d"", card->number); } }"
420----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47661/bad/dcn21_hwseq.c----dmub_abm_set_pipe,"static bool dmub_abm_set_pipe(struct abm *abm, uint32_t otg_inst, uint32_t option, uint32_t panel_inst, uint32_t pwrseq_inst) { union dmub_rb_cmd cmd; struct dc_context *dc = abm->ctx; <S2SV_StartVul> uint32_t ramping_boundary = 0xFFFF; <S2SV_EndVul> memset(&cmd, 0, sizeof(cmd)); cmd.abm_set_pipe.header.type = DMUB_CMD__ABM; cmd.abm_set_pipe.header.sub_type = DMUB_CMD__ABM_SET_PIPE; cmd.abm_set_pipe.abm_set_pipe_data.otg_inst = otg_inst; cmd.abm_set_pipe.abm_set_pipe_data.pwrseq_inst = pwrseq_inst; cmd.abm_set_pipe.abm_set_pipe_data.set_pipe_option = option; cmd.abm_set_pipe.abm_set_pipe_data.panel_inst = panel_inst; cmd.abm_set_pipe.abm_set_pipe_data.ramping_boundary = ramping_boundary; cmd.abm_set_pipe.header.payload_bytes = sizeof(struct dmub_cmd_abm_set_pipe_data); dc_wake_and_execute_dmub_cmd(dc, &cmd, DM_DMUB_WAIT_TYPE_WAIT); return true; }","- uint32_t ramping_boundary = 0xFFFF;
+ uint8_t ramping_boundary = 0xFF;","static bool dmub_abm_set_pipe(struct abm *abm, uint32_t otg_inst, uint32_t option, uint32_t panel_inst, uint32_t pwrseq_inst) { union dmub_rb_cmd cmd; struct dc_context *dc = abm->ctx; uint8_t ramping_boundary = 0xFF; memset(&cmd, 0, sizeof(cmd)); cmd.abm_set_pipe.header.type = DMUB_CMD__ABM; cmd.abm_set_pipe.header.sub_type = DMUB_CMD__ABM_SET_PIPE; cmd.abm_set_pipe.abm_set_pipe_data.otg_inst = otg_inst; cmd.abm_set_pipe.abm_set_pipe_data.pwrseq_inst = pwrseq_inst; cmd.abm_set_pipe.abm_set_pipe_data.set_pipe_option = option; cmd.abm_set_pipe.abm_set_pipe_data.panel_inst = panel_inst; cmd.abm_set_pipe.abm_set_pipe_data.ramping_boundary = ramping_boundary; cmd.abm_set_pipe.header.payload_bytes = sizeof(struct dmub_cmd_abm_set_pipe_data); dc_wake_and_execute_dmub_cmd(dc, &cmd, DM_DMUB_WAIT_TYPE_WAIT); return true; }"
972----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50211/bad/partition.c----udf_try_read_meta,"static uint32_t udf_try_read_meta(struct inode *inode, uint32_t block, uint16_t partition, uint32_t offset) { struct super_block *sb = inode->i_sb; struct udf_part_map *map; struct kernel_lb_addr eloc; uint32_t elen; sector_t ext_offset; struct extent_position epos = {}; uint32_t phyblock; <S2SV_StartVul> if (inode_bmap(inode, block, &epos, &eloc, &elen, &ext_offset) != <S2SV_EndVul> <S2SV_StartVul> (EXT_RECORDED_ALLOCATED >> 30)) <S2SV_EndVul> phyblock = 0xFFFFFFFF; else { map = &UDF_SB(sb)->s_partmaps[partition]; phyblock = udf_get_pblock(sb, eloc.logicalBlockNum, map->s_type_specific.s_metadata.s_phys_partition_ref, ext_offset + offset); } brelse(epos.bh); return phyblock; }","- if (inode_bmap(inode, block, &epos, &eloc, &elen, &ext_offset) !=
- (EXT_RECORDED_ALLOCATED >> 30))
+ int8_t etype;
+ int err = 0;
+ err = inode_bmap(inode, block, &epos, &eloc, &elen, &ext_offset, &etype);
+ if (err <= 0 || etype != (EXT_RECORDED_ALLOCATED >> 30))","static uint32_t udf_try_read_meta(struct inode *inode, uint32_t block, uint16_t partition, uint32_t offset) { struct super_block *sb = inode->i_sb; struct udf_part_map *map; struct kernel_lb_addr eloc; uint32_t elen; sector_t ext_offset; struct extent_position epos = {}; uint32_t phyblock; int8_t etype; int err = 0; err = inode_bmap(inode, block, &epos, &eloc, &elen, &ext_offset, &etype); if (err <= 0 || etype != (EXT_RECORDED_ALLOCATED >> 30)) phyblock = 0xFFFFFFFF; else { map = &UDF_SB(sb)->s_partmaps[partition]; phyblock = udf_get_pblock(sb, eloc.logicalBlockNum, map->s_type_specific.s_metadata.s_phys_partition_ref, ext_offset + offset); } brelse(epos.bh); return phyblock; }"
203----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46687/bad/bio.c----btrfs_submit_chunk,"static bool btrfs_submit_chunk(struct btrfs_bio *bbio, int mirror_num) { struct btrfs_inode *inode = bbio->inode; struct btrfs_fs_info *fs_info = bbio->fs_info; <S2SV_StartVul> struct btrfs_bio *orig_bbio = bbio; <S2SV_EndVul> struct bio *bio = &bbio->bio; u64 logical = bio->bi_iter.bi_sector << SECTOR_SHIFT; u64 length = bio->bi_iter.bi_size; u64 map_length = length; bool use_append = btrfs_use_zone_append(bbio); struct btrfs_io_context *bioc = NULL; struct btrfs_io_stripe smap; blk_status_t ret; int error; btrfs_bio_counter_inc_blocked(fs_info); error = btrfs_map_block(fs_info, btrfs_op(bio), logical, &map_length, &bioc, &smap, &mirror_num, 1); if (error) { ret = errno_to_blk_status(error); goto fail; } map_length = min(map_length, length); if (use_append) map_length = min(map_length, fs_info->max_zone_append_size); if (map_length < length) { bbio = btrfs_split_bio(fs_info, bbio, map_length, use_append); bio = &bbio->bio; } if (bio_op(bio) == REQ_OP_READ && is_data_bbio(bbio)) { bbio->saved_iter = bio->bi_iter; ret = btrfs_lookup_bio_sums(bbio); if (ret) <S2SV_StartVul> goto fail_put_bio; <S2SV_EndVul> } if (btrfs_op(bio) == BTRFS_MAP_WRITE) { if (use_append) { bio->bi_opf &= ~REQ_OP_WRITE; bio->bi_opf |= REQ_OP_ZONE_APPEND; } if (inode && !(inode->flags & BTRFS_INODE_NODATASUM) && !test_bit(BTRFS_FS_STATE_NO_CSUMS, &fs_info->fs_state) && !btrfs_is_data_reloc_root(inode->root)) { if (should_async_write(bbio) && btrfs_wq_submit_bio(bbio, bioc, &smap, mirror_num)) goto done; ret = btrfs_bio_csum(bbio); if (ret) <S2SV_StartVul> goto fail_put_bio; <S2SV_EndVul> } else if (use_append || (btrfs_is_zoned(fs_info) && inode && inode->flags & BTRFS_INODE_NODATASUM)) { ret = btrfs_alloc_dummy_sum(bbio); if (ret) <S2SV_StartVul> goto fail_put_bio; <S2SV_EndVul> } } __btrfs_submit_bio(bio, bioc, &smap, mirror_num); done: return map_length == length; <S2SV_StartVul> fail_put_bio: <S2SV_EndVul> <S2SV_StartVul> if (map_length < length) <S2SV_EndVul> <S2SV_StartVul> btrfs_cleanup_bio(bbio); <S2SV_EndVul> fail: btrfs_bio_counter_dec(fs_info); <S2SV_StartVul> btrfs_bio_end_io(orig_bbio, ret); <S2SV_EndVul> return true; }","- struct btrfs_bio *orig_bbio = bbio;
- goto fail_put_bio;
- goto fail_put_bio;
- goto fail_put_bio;
- fail_put_bio:
- if (map_length < length)
- btrfs_cleanup_bio(bbio);
- btrfs_bio_end_io(orig_bbio, ret);
+ goto fail;
+ }
+ goto fail;
+ goto fail;
+ }
+ }
+ if (map_length < length) {
+ struct btrfs_bio *remaining = bbio->private;
+ ASSERT(bbio->bio.bi_pool == &btrfs_clone_bioset);
+ ASSERT(remaining);
+ remaining->bio.bi_status = ret;
+ btrfs_orig_bbio_end_io(remaining);
+ }
+ bbio->bio.bi_status = ret;
+ btrfs_orig_bbio_end_io(bbio);
+ }","static bool btrfs_submit_chunk(struct btrfs_bio *bbio, int mirror_num) { struct btrfs_inode *inode = bbio->inode; struct btrfs_fs_info *fs_info = bbio->fs_info; struct bio *bio = &bbio->bio; u64 logical = bio->bi_iter.bi_sector << SECTOR_SHIFT; u64 length = bio->bi_iter.bi_size; u64 map_length = length; bool use_append = btrfs_use_zone_append(bbio); struct btrfs_io_context *bioc = NULL; struct btrfs_io_stripe smap; blk_status_t ret; int error; btrfs_bio_counter_inc_blocked(fs_info); error = btrfs_map_block(fs_info, btrfs_op(bio), logical, &map_length, &bioc, &smap, &mirror_num, 1); if (error) { ret = errno_to_blk_status(error); goto fail; } map_length = min(map_length, length); if (use_append) map_length = min(map_length, fs_info->max_zone_append_size); if (map_length < length) { bbio = btrfs_split_bio(fs_info, bbio, map_length, use_append); bio = &bbio->bio; } if (bio_op(bio) == REQ_OP_READ && is_data_bbio(bbio)) { bbio->saved_iter = bio->bi_iter; ret = btrfs_lookup_bio_sums(bbio); if (ret) goto fail; } if (btrfs_op(bio) == BTRFS_MAP_WRITE) { if (use_append) { bio->bi_opf &= ~REQ_OP_WRITE; bio->bi_opf |= REQ_OP_ZONE_APPEND; } if (inode && !(inode->flags & BTRFS_INODE_NODATASUM) && !test_bit(BTRFS_FS_STATE_NO_CSUMS, &fs_info->fs_state) && !btrfs_is_data_reloc_root(inode->root)) { if (should_async_write(bbio) && btrfs_wq_submit_bio(bbio, bioc, &smap, mirror_num)) goto done; ret = btrfs_bio_csum(bbio); if (ret) goto fail; } else if (use_append || (btrfs_is_zoned(fs_info) && inode && inode->flags & BTRFS_INODE_NODATASUM)) { ret = btrfs_alloc_dummy_sum(bbio); if (ret) goto fail; } } __btrfs_submit_bio(bio, bioc, &smap, mirror_num); done: return map_length == length; fail: btrfs_bio_counter_dec(fs_info); if (map_length < length) { struct btrfs_bio *remaining = bbio->private; ASSERT(bbio->bio.bi_pool == &btrfs_clone_bioset); ASSERT(remaining); remaining->bio.bi_status = ret; btrfs_orig_bbio_end_io(remaining); } bbio->bio.bi_status = ret; btrfs_orig_bbio_end_io(bbio); return true; }"
279----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46759/bad/adc128d818.c----adc128_in_store,"static ssize_t adc128_in_store(struct device *dev, struct device_attribute *attr, const char *buf, size_t count) { struct adc128_data *data = dev_get_drvdata(dev); int index = to_sensor_dev_attr_2(attr)->index; int nr = to_sensor_dev_attr_2(attr)->nr; u8 reg, regval; long val; int err; err = kstrtol(buf, 10, &val); if (err < 0) return err; mutex_lock(&data->update_lock); <S2SV_StartVul> regval = clamp_val(DIV_ROUND_CLOSEST(val, 10), 0, 255); <S2SV_EndVul> data->in[index][nr] = regval << 4; reg = index == 1 ? ADC128_REG_IN_MIN(nr) : ADC128_REG_IN_MAX(nr); i2c_smbus_write_byte_data(data->client, reg, regval); mutex_unlock(&data->update_lock); return count; }","- regval = clamp_val(DIV_ROUND_CLOSEST(val, 10), 0, 255);
+ regval = DIV_ROUND_CLOSEST(clamp_val(val, 0, 2550), 10);","static ssize_t adc128_in_store(struct device *dev, struct device_attribute *attr, const char *buf, size_t count) { struct adc128_data *data = dev_get_drvdata(dev); int index = to_sensor_dev_attr_2(attr)->index; int nr = to_sensor_dev_attr_2(attr)->nr; u8 reg, regval; long val; int err; err = kstrtol(buf, 10, &val); if (err < 0) return err; mutex_lock(&data->update_lock); regval = DIV_ROUND_CLOSEST(clamp_val(val, 0, 2550), 10); data->in[index][nr] = regval << 4; reg = index == 1 ? ADC128_REG_IN_MIN(nr) : ADC128_REG_IN_MAX(nr); i2c_smbus_write_byte_data(data->client, reg, regval); mutex_unlock(&data->update_lock); return count; }"
1481----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-57906/bad/ti-ads8688.c----ads8688_trigger_handler,"static irqreturn_t ads8688_trigger_handler(int irq, void *p) { struct iio_poll_func *pf = p; struct iio_dev *indio_dev = pf->indio_dev; <S2SV_StartVul> u16 buffer[ADS8688_MAX_CHANNELS + sizeof(s64)/sizeof(u16)] __aligned(8); <S2SV_EndVul> int i, j = 0; iio_for_each_active_channel(indio_dev, i) { buffer[j] = ads8688_read(indio_dev, i); j++; } iio_push_to_buffers_with_timestamp(indio_dev, buffer, iio_get_time_ns(indio_dev)); iio_trigger_notify_done(indio_dev->trig); return IRQ_HANDLED; }","- u16 buffer[ADS8688_MAX_CHANNELS + sizeof(s64)/sizeof(u16)] __aligned(8);
+ u16 buffer[ADS8688_MAX_CHANNELS + sizeof(s64)/sizeof(u16)] __aligned(8) = { };","static irqreturn_t ads8688_trigger_handler(int irq, void *p) { struct iio_poll_func *pf = p; struct iio_dev *indio_dev = pf->indio_dev; u16 buffer[ADS8688_MAX_CHANNELS + sizeof(s64)/sizeof(u16)] __aligned(8) = { }; int i, j = 0; iio_for_each_active_channel(indio_dev, i) { buffer[j] = ads8688_read(indio_dev, i); j++; } iio_push_to_buffers_with_timestamp(indio_dev, buffer, iio_get_time_ns(indio_dev)); iio_trigger_notify_done(indio_dev->trig); return IRQ_HANDLED; }"
1244----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-54680/bad/connect.c----cifs_get_tcp_session,"cifs_get_tcp_session(struct smb3_fs_context *ctx, struct TCP_Server_Info *primary_server) { struct TCP_Server_Info *tcp_ses = NULL; int rc; cifs_dbg(FYI, ""UNC: %s\n"", ctx->UNC); tcp_ses = cifs_find_tcp_session(ctx); if (tcp_ses) return tcp_ses; tcp_ses = kzalloc(sizeof(struct TCP_Server_Info), GFP_KERNEL); if (!tcp_ses) { rc = -ENOMEM; goto out_err; } tcp_ses->hostname = kstrdup(ctx->server_hostname, GFP_KERNEL); if (!tcp_ses->hostname) { rc = -ENOMEM; goto out_err; } if (ctx->leaf_fullpath) { tcp_ses->leaf_fullpath = kstrdup(ctx->leaf_fullpath, GFP_KERNEL); if (!tcp_ses->leaf_fullpath) { rc = -ENOMEM; goto out_err; } } if (ctx->nosharesock) tcp_ses->nosharesock = true; tcp_ses->dfs_conn = ctx->dfs_conn; tcp_ses->ops = ctx->ops; tcp_ses->vals = ctx->vals; cifs_set_net_ns(tcp_ses, get_net(current->nsproxy->net_ns)); tcp_ses->conn_id = atomic_inc_return(&tcpSesNextId); tcp_ses->noblockcnt = ctx->rootfs; tcp_ses->noblocksnd = ctx->noblocksnd || ctx->rootfs; tcp_ses->noautotune = ctx->noautotune; tcp_ses->tcp_nodelay = ctx->sockopt_tcp_nodelay; tcp_ses->rdma = ctx->rdma; tcp_ses->in_flight = 0; tcp_ses->max_in_flight = 0; tcp_ses->credits = 1; if (primary_server) { spin_lock(&cifs_tcp_ses_lock); ++primary_server->srv_count; spin_unlock(&cifs_tcp_ses_lock); tcp_ses->primary_server = primary_server; } init_waitqueue_head(&tcp_ses->response_q); init_waitqueue_head(&tcp_ses->request_q); INIT_LIST_HEAD(&tcp_ses->pending_mid_q); mutex_init(&tcp_ses->_srv_mutex); memcpy(tcp_ses->workstation_RFC1001_name, ctx->source_rfc1001_name, RFC1001_NAME_LEN_WITH_NULL); memcpy(tcp_ses->server_RFC1001_name, ctx->target_rfc1001_name, RFC1001_NAME_LEN_WITH_NULL); tcp_ses->session_estab = false; tcp_ses->sequence_number = 0; tcp_ses->channel_sequence_num = 0; tcp_ses->reconnect_instance = 1; tcp_ses->lstrp = jiffies; tcp_ses->compression.requested = ctx->compress; spin_lock_init(&tcp_ses->req_lock); spin_lock_init(&tcp_ses->srv_lock); spin_lock_init(&tcp_ses->mid_lock); INIT_LIST_HEAD(&tcp_ses->tcp_ses_list); INIT_LIST_HEAD(&tcp_ses->smb_ses_list); INIT_DELAYED_WORK(&tcp_ses->echo, cifs_echo_request); INIT_DELAYED_WORK(&tcp_ses->reconnect, smb2_reconnect_server); mutex_init(&tcp_ses->reconnect_mutex); #ifdef CONFIG_CIFS_DFS_UPCALL mutex_init(&tcp_ses->refpath_lock); #endif memcpy(&tcp_ses->srcaddr, &ctx->srcaddr, sizeof(tcp_ses->srcaddr)); memcpy(&tcp_ses->dstaddr, &ctx->dstaddr, sizeof(tcp_ses->dstaddr)); if (ctx->use_client_guid) memcpy(tcp_ses->client_guid, ctx->client_guid, SMB2_CLIENT_GUID_SIZE); else generate_random_uuid(tcp_ses->client_guid); tcp_ses->tcpStatus = CifsNew; ++tcp_ses->srv_count; if (ctx->echo_interval >= SMB_ECHO_INTERVAL_MIN && ctx->echo_interval <= SMB_ECHO_INTERVAL_MAX) tcp_ses->echo_interval = ctx->echo_interval * HZ; else tcp_ses->echo_interval = SMB_ECHO_INTERVAL_DEFAULT * HZ; if (tcp_ses->rdma) { #ifndef CONFIG_CIFS_SMB_DIRECT cifs_dbg(VFS, ""CONFIG_CIFS_SMB_DIRECT is not enabled\n""); rc = -ENOENT; goto out_err_crypto_release; #endif tcp_ses->smbd_conn = smbd_get_connection( tcp_ses, (struct sockaddr *)&ctx->dstaddr); if (tcp_ses->smbd_conn) { cifs_dbg(VFS, ""RDMA transport established\n""); rc = 0; goto smbd_connected; } else { rc = -ENOENT; goto out_err_crypto_release; } } rc = ip_connect(tcp_ses); if (rc < 0) { cifs_dbg(VFS, ""Error connecting to socket. Aborting operation.\n""); goto out_err_crypto_release; } smbd_connected: __module_get(THIS_MODULE); tcp_ses->tsk = kthread_run(cifs_demultiplex_thread, tcp_ses, ""cifsd""); if (IS_ERR(tcp_ses->tsk)) { rc = PTR_ERR(tcp_ses->tsk); cifs_dbg(VFS, ""error %d create cifsd thread\n"", rc); module_put(THIS_MODULE); goto out_err_crypto_release; } tcp_ses->min_offload = ctx->min_offload; tcp_ses->retrans = ctx->retrans; spin_lock(&tcp_ses->srv_lock); tcp_ses->tcpStatus = CifsNeedNegotiate; spin_unlock(&tcp_ses->srv_lock); if ((ctx->max_credits < 20) || (ctx->max_credits > 60000)) tcp_ses->max_credits = SMB2_MAX_CREDITS_AVAILABLE; else tcp_ses->max_credits = ctx->max_credits; tcp_ses->nr_targets = 1; tcp_ses->ignore_signature = ctx->ignore_signature; spin_lock(&cifs_tcp_ses_lock); list_add(&tcp_ses->tcp_ses_list, &cifs_tcp_ses_list); spin_unlock(&cifs_tcp_ses_lock); queue_delayed_work(cifsiod_wq, &tcp_ses->echo, tcp_ses->echo_interval); return tcp_ses; out_err_crypto_release: cifs_crypto_secmech_release(tcp_ses); put_net(cifs_net_ns(tcp_ses)); out_err: if (tcp_ses) { if (SERVER_IS_CHAN(tcp_ses)) cifs_put_tcp_session(tcp_ses->primary_server, false); kfree(tcp_ses->hostname); kfree(tcp_ses->leaf_fullpath); <S2SV_StartVul> if (tcp_ses->ssocket) <S2SV_EndVul> sock_release(tcp_ses->ssocket); kfree(tcp_ses); } return ERR_PTR(rc); }","- if (tcp_ses->ssocket)
+ put_net(cifs_net_ns(tcp_ses));
+ if (tcp_ses->ssocket) {
+ put_net(cifs_net_ns(tcp_ses));
+ }
+ }","cifs_get_tcp_session(struct smb3_fs_context *ctx, struct TCP_Server_Info *primary_server) { struct TCP_Server_Info *tcp_ses = NULL; int rc; cifs_dbg(FYI, ""UNC: %s\n"", ctx->UNC); tcp_ses = cifs_find_tcp_session(ctx); if (tcp_ses) return tcp_ses; tcp_ses = kzalloc(sizeof(struct TCP_Server_Info), GFP_KERNEL); if (!tcp_ses) { rc = -ENOMEM; goto out_err; } tcp_ses->hostname = kstrdup(ctx->server_hostname, GFP_KERNEL); if (!tcp_ses->hostname) { rc = -ENOMEM; goto out_err; } if (ctx->leaf_fullpath) { tcp_ses->leaf_fullpath = kstrdup(ctx->leaf_fullpath, GFP_KERNEL); if (!tcp_ses->leaf_fullpath) { rc = -ENOMEM; goto out_err; } } if (ctx->nosharesock) tcp_ses->nosharesock = true; tcp_ses->dfs_conn = ctx->dfs_conn; tcp_ses->ops = ctx->ops; tcp_ses->vals = ctx->vals; cifs_set_net_ns(tcp_ses, get_net(current->nsproxy->net_ns)); tcp_ses->conn_id = atomic_inc_return(&tcpSesNextId); tcp_ses->noblockcnt = ctx->rootfs; tcp_ses->noblocksnd = ctx->noblocksnd || ctx->rootfs; tcp_ses->noautotune = ctx->noautotune; tcp_ses->tcp_nodelay = ctx->sockopt_tcp_nodelay; tcp_ses->rdma = ctx->rdma; tcp_ses->in_flight = 0; tcp_ses->max_in_flight = 0; tcp_ses->credits = 1; if (primary_server) { spin_lock(&cifs_tcp_ses_lock); ++primary_server->srv_count; spin_unlock(&cifs_tcp_ses_lock); tcp_ses->primary_server = primary_server; } init_waitqueue_head(&tcp_ses->response_q); init_waitqueue_head(&tcp_ses->request_q); INIT_LIST_HEAD(&tcp_ses->pending_mid_q); mutex_init(&tcp_ses->_srv_mutex); memcpy(tcp_ses->workstation_RFC1001_name, ctx->source_rfc1001_name, RFC1001_NAME_LEN_WITH_NULL); memcpy(tcp_ses->server_RFC1001_name, ctx->target_rfc1001_name, RFC1001_NAME_LEN_WITH_NULL); tcp_ses->session_estab = false; tcp_ses->sequence_number = 0; tcp_ses->channel_sequence_num = 0; tcp_ses->reconnect_instance = 1; tcp_ses->lstrp = jiffies; tcp_ses->compression.requested = ctx->compress; spin_lock_init(&tcp_ses->req_lock); spin_lock_init(&tcp_ses->srv_lock); spin_lock_init(&tcp_ses->mid_lock); INIT_LIST_HEAD(&tcp_ses->tcp_ses_list); INIT_LIST_HEAD(&tcp_ses->smb_ses_list); INIT_DELAYED_WORK(&tcp_ses->echo, cifs_echo_request); INIT_DELAYED_WORK(&tcp_ses->reconnect, smb2_reconnect_server); mutex_init(&tcp_ses->reconnect_mutex); #ifdef CONFIG_CIFS_DFS_UPCALL mutex_init(&tcp_ses->refpath_lock); #endif memcpy(&tcp_ses->srcaddr, &ctx->srcaddr, sizeof(tcp_ses->srcaddr)); memcpy(&tcp_ses->dstaddr, &ctx->dstaddr, sizeof(tcp_ses->dstaddr)); if (ctx->use_client_guid) memcpy(tcp_ses->client_guid, ctx->client_guid, SMB2_CLIENT_GUID_SIZE); else generate_random_uuid(tcp_ses->client_guid); tcp_ses->tcpStatus = CifsNew; ++tcp_ses->srv_count; if (ctx->echo_interval >= SMB_ECHO_INTERVAL_MIN && ctx->echo_interval <= SMB_ECHO_INTERVAL_MAX) tcp_ses->echo_interval = ctx->echo_interval * HZ; else tcp_ses->echo_interval = SMB_ECHO_INTERVAL_DEFAULT * HZ; if (tcp_ses->rdma) { #ifndef CONFIG_CIFS_SMB_DIRECT cifs_dbg(VFS, ""CONFIG_CIFS_SMB_DIRECT is not enabled\n""); rc = -ENOENT; goto out_err_crypto_release; #endif tcp_ses->smbd_conn = smbd_get_connection( tcp_ses, (struct sockaddr *)&ctx->dstaddr); if (tcp_ses->smbd_conn) { cifs_dbg(VFS, ""RDMA transport established\n""); rc = 0; goto smbd_connected; } else { rc = -ENOENT; goto out_err_crypto_release; } } rc = ip_connect(tcp_ses); if (rc < 0) { cifs_dbg(VFS, ""Error connecting to socket. Aborting operation.\n""); goto out_err_crypto_release; } smbd_connected: __module_get(THIS_MODULE); tcp_ses->tsk = kthread_run(cifs_demultiplex_thread, tcp_ses, ""cifsd""); if (IS_ERR(tcp_ses->tsk)) { rc = PTR_ERR(tcp_ses->tsk); cifs_dbg(VFS, ""error %d create cifsd thread\n"", rc); module_put(THIS_MODULE); goto out_err_crypto_release; } tcp_ses->min_offload = ctx->min_offload; tcp_ses->retrans = ctx->retrans; spin_lock(&tcp_ses->srv_lock); tcp_ses->tcpStatus = CifsNeedNegotiate; spin_unlock(&tcp_ses->srv_lock); if ((ctx->max_credits < 20) || (ctx->max_credits > 60000)) tcp_ses->max_credits = SMB2_MAX_CREDITS_AVAILABLE; else tcp_ses->max_credits = ctx->max_credits; tcp_ses->nr_targets = 1; tcp_ses->ignore_signature = ctx->ignore_signature; spin_lock(&cifs_tcp_ses_lock); list_add(&tcp_ses->tcp_ses_list, &cifs_tcp_ses_list); spin_unlock(&cifs_tcp_ses_lock); queue_delayed_work(cifsiod_wq, &tcp_ses->echo, tcp_ses->echo_interval); return tcp_ses; out_err_crypto_release: cifs_crypto_secmech_release(tcp_ses); put_net(cifs_net_ns(tcp_ses)); out_err: if (tcp_ses) { if (SERVER_IS_CHAN(tcp_ses)) cifs_put_tcp_session(tcp_ses->primary_server, false); kfree(tcp_ses->hostname); kfree(tcp_ses->leaf_fullpath); if (tcp_ses->ssocket) { sock_release(tcp_ses->ssocket); put_net(cifs_net_ns(tcp_ses)); } kfree(tcp_ses); } return ERR_PTR(rc); }"
927----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50174/bad/panthor_sched.c----panthor_job_create,"panthor_job_create(struct panthor_file *pfile, u16 group_handle, const struct drm_panthor_queue_submit *qsubmit) { struct panthor_group_pool *gpool = pfile->groups; struct panthor_job *job; int ret; if (qsubmit->pad) return ERR_PTR(-EINVAL); if ((qsubmit->stream_size == 0) != (qsubmit->stream_addr == 0)) return ERR_PTR(-EINVAL); if ((qsubmit->stream_addr & 63) || (qsubmit->stream_size & 7)) return ERR_PTR(-EINVAL); if (qsubmit->latest_flush & GENMASK(30, 24)) return ERR_PTR(-EINVAL); job = kzalloc(sizeof(*job), GFP_KERNEL); if (!job) return ERR_PTR(-ENOMEM); kref_init(&job->refcount); job->queue_idx = qsubmit->queue_index; job->call_info.size = qsubmit->stream_size; job->call_info.start = qsubmit->stream_addr; job->call_info.latest_flush = qsubmit->latest_flush; INIT_LIST_HEAD(&job->node); <S2SV_StartVul> job->group = group_get(xa_load(&gpool->xa, group_handle)); <S2SV_EndVul> if (!job->group) { ret = -EINVAL; goto err_put_job; } if (job->queue_idx >= job->group->queue_count || !job->group->queues[job->queue_idx]) { ret = -EINVAL; goto err_put_job; } if (job->call_info.size) { job->done_fence = kzalloc(sizeof(*job->done_fence), GFP_KERNEL); if (!job->done_fence) { ret = -ENOMEM; goto err_put_job; } } ret = drm_sched_job_init(&job->base, &job->group->queues[job->queue_idx]->entity, 1, job->group); if (ret) goto err_put_job; return &job->base; err_put_job: panthor_job_put(&job->base); return ERR_PTR(ret); }","- job->group = group_get(xa_load(&gpool->xa, group_handle));
+ job->group = group_from_handle(gpool, group_handle);","panthor_job_create(struct panthor_file *pfile, u16 group_handle, const struct drm_panthor_queue_submit *qsubmit) { struct panthor_group_pool *gpool = pfile->groups; struct panthor_job *job; int ret; if (qsubmit->pad) return ERR_PTR(-EINVAL); if ((qsubmit->stream_size == 0) != (qsubmit->stream_addr == 0)) return ERR_PTR(-EINVAL); if ((qsubmit->stream_addr & 63) || (qsubmit->stream_size & 7)) return ERR_PTR(-EINVAL); if (qsubmit->latest_flush & GENMASK(30, 24)) return ERR_PTR(-EINVAL); job = kzalloc(sizeof(*job), GFP_KERNEL); if (!job) return ERR_PTR(-ENOMEM); kref_init(&job->refcount); job->queue_idx = qsubmit->queue_index; job->call_info.size = qsubmit->stream_size; job->call_info.start = qsubmit->stream_addr; job->call_info.latest_flush = qsubmit->latest_flush; INIT_LIST_HEAD(&job->node); job->group = group_from_handle(gpool, group_handle); if (!job->group) { ret = -EINVAL; goto err_put_job; } if (job->queue_idx >= job->group->queue_count || !job->group->queues[job->queue_idx]) { ret = -EINVAL; goto err_put_job; } if (job->call_info.size) { job->done_fence = kzalloc(sizeof(*job->done_fence), GFP_KERNEL); if (!job->done_fence) { ret = -ENOMEM; goto err_put_job; } } ret = drm_sched_job_init(&job->base, &job->group->queues[job->queue_idx]->entity, 1, job->group); if (ret) goto err_put_job; return &job->base; err_put_job: panthor_job_put(&job->base); return ERR_PTR(ret); }"
417----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47658/bad/stm32-cryp.c----stm32_cryp_irq_thread,"static irqreturn_t stm32_cryp_irq_thread(int irq, void *arg) { struct stm32_cryp *cryp = arg; u32 ph; u32 it_mask = stm32_cryp_read(cryp, cryp->caps->imsc); if (cryp->irq_status & MISR_OUT) stm32_cryp_irq_read_data(cryp); if (cryp->irq_status & MISR_IN) { if (is_gcm(cryp) || is_ccm(cryp)) { ph = stm32_cryp_read(cryp, cryp->caps->cr) & CR_PH_MASK; if (unlikely(ph == CR_PH_HEADER)) stm32_cryp_irq_write_gcmccm_header(cryp); else stm32_cryp_irq_write_data(cryp); if (is_gcm(cryp)) cryp->gcm_ctr++; } else { stm32_cryp_irq_write_data(cryp); } } if (!cryp->payload_in && !cryp->header_in) it_mask &= ~IMSCR_IN; if (!cryp->payload_out) it_mask &= ~IMSCR_OUT; stm32_cryp_write(cryp, cryp->caps->imsc, it_mask); <S2SV_StartVul> if (!cryp->payload_in && !cryp->header_in && !cryp->payload_out) <S2SV_EndVul> stm32_cryp_finish_req(cryp, 0); return IRQ_HANDLED; }","- if (!cryp->payload_in && !cryp->header_in && !cryp->payload_out)
+ if (!cryp->payload_in && !cryp->header_in && !cryp->payload_out) {
+ local_bh_disable();
+ local_bh_enable();
+ }
+ }","static irqreturn_t stm32_cryp_irq_thread(int irq, void *arg) { struct stm32_cryp *cryp = arg; u32 ph; u32 it_mask = stm32_cryp_read(cryp, cryp->caps->imsc); if (cryp->irq_status & MISR_OUT) stm32_cryp_irq_read_data(cryp); if (cryp->irq_status & MISR_IN) { if (is_gcm(cryp) || is_ccm(cryp)) { ph = stm32_cryp_read(cryp, cryp->caps->cr) & CR_PH_MASK; if (unlikely(ph == CR_PH_HEADER)) stm32_cryp_irq_write_gcmccm_header(cryp); else stm32_cryp_irq_write_data(cryp); if (is_gcm(cryp)) cryp->gcm_ctr++; } else { stm32_cryp_irq_write_data(cryp); } } if (!cryp->payload_in && !cryp->header_in) it_mask &= ~IMSCR_IN; if (!cryp->payload_out) it_mask &= ~IMSCR_OUT; stm32_cryp_write(cryp, cryp->caps->imsc, it_mask); if (!cryp->payload_in && !cryp->header_in && !cryp->payload_out) { local_bh_disable(); stm32_cryp_finish_req(cryp, 0); local_bh_enable(); } return IRQ_HANDLED; }"
1485----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-57910/bad/vcnl4035.c----vcnl4035_trigger_consumer_handler,"static irqreturn_t vcnl4035_trigger_consumer_handler(int irq, void *p) { struct iio_poll_func *pf = p; struct iio_dev *indio_dev = pf->indio_dev; struct vcnl4035_data *data = iio_priv(indio_dev); <S2SV_StartVul> u8 buffer[ALIGN(sizeof(u16), sizeof(s64)) + sizeof(s64)] __aligned(8); <S2SV_EndVul> int ret; ret = regmap_read(data->regmap, VCNL4035_ALS_DATA, (int *)buffer); if (ret < 0) { dev_err(&data->client->dev, ""Trigger consumer can't read from sensor.\n""); goto fail_read; } iio_push_to_buffers_with_timestamp(indio_dev, buffer, iio_get_time_ns(indio_dev)); fail_read: iio_trigger_notify_done(indio_dev->trig); return IRQ_HANDLED; }","- u8 buffer[ALIGN(sizeof(u16), sizeof(s64)) + sizeof(s64)] __aligned(8);
+ u8 buffer[ALIGN(sizeof(u16), sizeof(s64)) + sizeof(s64)] __aligned(8) = { };","static irqreturn_t vcnl4035_trigger_consumer_handler(int irq, void *p) { struct iio_poll_func *pf = p; struct iio_dev *indio_dev = pf->indio_dev; struct vcnl4035_data *data = iio_priv(indio_dev); u8 buffer[ALIGN(sizeof(u16), sizeof(s64)) + sizeof(s64)] __aligned(8) = { }; int ret; ret = regmap_read(data->regmap, VCNL4035_ALS_DATA, (int *)buffer); if (ret < 0) { dev_err(&data->client->dev, ""Trigger consumer can't read from sensor.\n""); goto fail_read; } iio_push_to_buffers_with_timestamp(indio_dev, buffer, iio_get_time_ns(indio_dev)); fail_read: iio_trigger_notify_done(indio_dev->trig); return IRQ_HANDLED; }"
204----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46688/bad/zutil.c----z_erofs_gbuf_growsize,"int z_erofs_gbuf_growsize(unsigned int nrpages) { static DEFINE_MUTEX(gbuf_resize_mutex); struct page **tmp_pages = NULL; struct z_erofs_gbuf *gbuf; void *ptr, *old_ptr; int last, i, j; mutex_lock(&gbuf_resize_mutex); if (nrpages <= z_erofs_gbuf_nrpages) { mutex_unlock(&gbuf_resize_mutex); return 0; } for (i = 0; i < z_erofs_gbuf_count; ++i) { gbuf = &z_erofs_gbufpool[i]; tmp_pages = kcalloc(nrpages, sizeof(*tmp_pages), GFP_KERNEL); if (!tmp_pages) goto out; for (j = 0; j < gbuf->nrpages; ++j) tmp_pages[j] = gbuf->pages[j]; do { last = j; j = alloc_pages_bulk_array(GFP_KERNEL, nrpages, tmp_pages); if (last == j) goto out; } while (j != nrpages); ptr = vmap(tmp_pages, nrpages, VM_MAP, PAGE_KERNEL); if (!ptr) goto out; spin_lock(&gbuf->lock); kfree(gbuf->pages); gbuf->pages = tmp_pages; old_ptr = gbuf->ptr; gbuf->ptr = ptr; gbuf->nrpages = nrpages; spin_unlock(&gbuf->lock); if (old_ptr) vunmap(old_ptr); } z_erofs_gbuf_nrpages = nrpages; out: if (i < z_erofs_gbuf_count && tmp_pages) { for (j = 0; j < nrpages; ++j) <S2SV_StartVul> if (tmp_pages[j] && tmp_pages[j] != gbuf->pages[j]) <S2SV_EndVul> __free_page(tmp_pages[j]); kfree(tmp_pages); } mutex_unlock(&gbuf_resize_mutex); return i < z_erofs_gbuf_count ? -ENOMEM : 0; }","- if (tmp_pages[j] && tmp_pages[j] != gbuf->pages[j])
+ if (tmp_pages[j] && (j >= gbuf->nrpages ||
+ tmp_pages[j] != gbuf->pages[j]))","int z_erofs_gbuf_growsize(unsigned int nrpages) { static DEFINE_MUTEX(gbuf_resize_mutex); struct page **tmp_pages = NULL; struct z_erofs_gbuf *gbuf; void *ptr, *old_ptr; int last, i, j; mutex_lock(&gbuf_resize_mutex); if (nrpages <= z_erofs_gbuf_nrpages) { mutex_unlock(&gbuf_resize_mutex); return 0; } for (i = 0; i < z_erofs_gbuf_count; ++i) { gbuf = &z_erofs_gbufpool[i]; tmp_pages = kcalloc(nrpages, sizeof(*tmp_pages), GFP_KERNEL); if (!tmp_pages) goto out; for (j = 0; j < gbuf->nrpages; ++j) tmp_pages[j] = gbuf->pages[j]; do { last = j; j = alloc_pages_bulk_array(GFP_KERNEL, nrpages, tmp_pages); if (last == j) goto out; } while (j != nrpages); ptr = vmap(tmp_pages, nrpages, VM_MAP, PAGE_KERNEL); if (!ptr) goto out; spin_lock(&gbuf->lock); kfree(gbuf->pages); gbuf->pages = tmp_pages; old_ptr = gbuf->ptr; gbuf->ptr = ptr; gbuf->nrpages = nrpages; spin_unlock(&gbuf->lock); if (old_ptr) vunmap(old_ptr); } z_erofs_gbuf_nrpages = nrpages; out: if (i < z_erofs_gbuf_count && tmp_pages) { for (j = 0; j < nrpages; ++j) if (tmp_pages[j] && (j >= gbuf->nrpages || tmp_pages[j] != gbuf->pages[j])) __free_page(tmp_pages[j]); kfree(tmp_pages); } mutex_unlock(&gbuf_resize_mutex); return i < z_erofs_gbuf_count ? -ENOMEM : 0; }"
133----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44992/bad/file.c----cifs_free_subrequest,"static void cifs_free_subrequest(struct netfs_io_subrequest *subreq) { struct cifs_io_subrequest *rdata = container_of(subreq, struct cifs_io_subrequest, subreq); int rc = subreq->error; if (rdata->subreq.source == NETFS_DOWNLOAD_FROM_SERVER) { #ifdef CONFIG_CIFS_SMB_DIRECT if (rdata->mr) { smbd_deregister_mr(rdata->mr); rdata->mr = NULL; } #endif } <S2SV_StartVul> if (rdata->credits.value != 0) <S2SV_EndVul> trace_smb3_rw_credits(rdata->rreq->debug_id, rdata->subreq.debug_index, rdata->credits.value, rdata->server ? rdata->server->credits : 0, rdata->server ? rdata->server->in_flight : 0, -rdata->credits.value, cifs_trace_rw_credits_free_subreq); <S2SV_StartVul> add_credits_and_wake_if(rdata->server, &rdata->credits, 0); <S2SV_EndVul> if (rdata->have_xid) free_xid(rdata->xid); }","- if (rdata->credits.value != 0)
- add_credits_and_wake_if(rdata->server, &rdata->credits, 0);
+ }
+ if (rdata->credits.value != 0) {
+ if (rdata->server)
+ add_credits_and_wake_if(rdata->server, &rdata->credits, 0);
+ else
+ rdata->credits.value = 0;
+ }
+ }","static void cifs_free_subrequest(struct netfs_io_subrequest *subreq) { struct cifs_io_subrequest *rdata = container_of(subreq, struct cifs_io_subrequest, subreq); int rc = subreq->error; if (rdata->subreq.source == NETFS_DOWNLOAD_FROM_SERVER) { #ifdef CONFIG_CIFS_SMB_DIRECT if (rdata->mr) { smbd_deregister_mr(rdata->mr); rdata->mr = NULL; } #endif } if (rdata->credits.value != 0) { trace_smb3_rw_credits(rdata->rreq->debug_id, rdata->subreq.debug_index, rdata->credits.value, rdata->server ? rdata->server->credits : 0, rdata->server ? rdata->server->in_flight : 0, -rdata->credits.value, cifs_trace_rw_credits_free_subreq); if (rdata->server) add_credits_and_wake_if(rdata->server, &rdata->credits, 0); else rdata->credits.value = 0; } if (rdata->have_xid) free_xid(rdata->xid); }"
658----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49958/bad/xattr.c----ocfs2_reflink_xattr_inline,"static int ocfs2_reflink_xattr_inline(struct ocfs2_xattr_reflink *args) { int ret = 0, credits = 0; handle_t *handle; struct ocfs2_super *osb = OCFS2_SB(args->old_inode->i_sb); struct ocfs2_dinode *di = (struct ocfs2_dinode *)args->old_bh->b_data; int inline_size = le16_to_cpu(di->i_xattr_inline_size); int header_off = osb->sb->s_blocksize - inline_size; struct ocfs2_xattr_header *xh = (struct ocfs2_xattr_header *) (args->old_bh->b_data + header_off); struct ocfs2_xattr_header *new_xh = (struct ocfs2_xattr_header *) (args->new_bh->b_data + header_off); struct ocfs2_alloc_context *meta_ac = NULL; struct ocfs2_inode_info *new_oi; struct ocfs2_dinode *new_di; struct ocfs2_xattr_value_buf vb = { .vb_bh = args->new_bh, .vb_access = ocfs2_journal_access_di, }; ret = ocfs2_reflink_lock_xattr_allocators(osb, xh, args->ref_root_bh, &credits, &meta_ac); if (ret) { mlog_errno(ret); goto out; } handle = ocfs2_start_trans(osb, credits); if (IS_ERR(handle)) { ret = PTR_ERR(handle); mlog_errno(ret); goto out; } ret = ocfs2_journal_access_di(handle, INODE_CACHE(args->new_inode), args->new_bh, OCFS2_JOURNAL_ACCESS_WRITE); if (ret) { mlog_errno(ret); goto out_commit; } memcpy(args->new_bh->b_data + header_off, args->old_bh->b_data + header_off, inline_size); new_di = (struct ocfs2_dinode *)args->new_bh->b_data; new_di->i_xattr_inline_size = cpu_to_le16(inline_size); ret = ocfs2_reflink_xattr_header(handle, args, args->old_bh, xh, args->new_bh, new_xh, &vb, meta_ac, ocfs2_get_xattr_value_root, NULL); if (ret) { mlog_errno(ret); goto out_commit; } new_oi = OCFS2_I(args->new_inode); <S2SV_StartVul> if (!(new_oi->ip_dyn_features & OCFS2_INLINE_DATA_FL) && <S2SV_EndVul> <S2SV_StartVul> !(ocfs2_inode_is_fast_symlink(args->new_inode))) { <S2SV_EndVul> <S2SV_StartVul> struct ocfs2_extent_list *el = &new_di->id2.i_list; <S2SV_EndVul> <S2SV_StartVul> le16_add_cpu(&el->l_count, -(inline_size / <S2SV_EndVul> <S2SV_StartVul> sizeof(struct ocfs2_extent_rec))); <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> spin_lock(&new_oi->ip_lock); new_oi->ip_dyn_features |= OCFS2_HAS_XATTR_FL | OCFS2_INLINE_XATTR_FL; new_di->i_dyn_features = cpu_to_le16(new_oi->ip_dyn_features); spin_unlock(&new_oi->ip_lock); ocfs2_journal_dirty(handle, args->new_bh); out_commit: ocfs2_commit_trans(osb, handle); out: if (meta_ac) ocfs2_free_alloc_context(meta_ac); return ret; }","- if (!(new_oi->ip_dyn_features & OCFS2_INLINE_DATA_FL) &&
- !(ocfs2_inode_is_fast_symlink(args->new_inode))) {
- struct ocfs2_extent_list *el = &new_di->id2.i_list;
- le16_add_cpu(&el->l_count, -(inline_size /
- sizeof(struct ocfs2_extent_rec)));
- }","static int ocfs2_reflink_xattr_inline(struct ocfs2_xattr_reflink *args) { int ret = 0, credits = 0; handle_t *handle; struct ocfs2_super *osb = OCFS2_SB(args->old_inode->i_sb); struct ocfs2_dinode *di = (struct ocfs2_dinode *)args->old_bh->b_data; int inline_size = le16_to_cpu(di->i_xattr_inline_size); int header_off = osb->sb->s_blocksize - inline_size; struct ocfs2_xattr_header *xh = (struct ocfs2_xattr_header *) (args->old_bh->b_data + header_off); struct ocfs2_xattr_header *new_xh = (struct ocfs2_xattr_header *) (args->new_bh->b_data + header_off); struct ocfs2_alloc_context *meta_ac = NULL; struct ocfs2_inode_info *new_oi; struct ocfs2_dinode *new_di; struct ocfs2_xattr_value_buf vb = { .vb_bh = args->new_bh, .vb_access = ocfs2_journal_access_di, }; ret = ocfs2_reflink_lock_xattr_allocators(osb, xh, args->ref_root_bh, &credits, &meta_ac); if (ret) { mlog_errno(ret); goto out; } handle = ocfs2_start_trans(osb, credits); if (IS_ERR(handle)) { ret = PTR_ERR(handle); mlog_errno(ret); goto out; } ret = ocfs2_journal_access_di(handle, INODE_CACHE(args->new_inode), args->new_bh, OCFS2_JOURNAL_ACCESS_WRITE); if (ret) { mlog_errno(ret); goto out_commit; } memcpy(args->new_bh->b_data + header_off, args->old_bh->b_data + header_off, inline_size); new_di = (struct ocfs2_dinode *)args->new_bh->b_data; new_di->i_xattr_inline_size = cpu_to_le16(inline_size); ret = ocfs2_reflink_xattr_header(handle, args, args->old_bh, xh, args->new_bh, new_xh, &vb, meta_ac, ocfs2_get_xattr_value_root, NULL); if (ret) { mlog_errno(ret); goto out_commit; } new_oi = OCFS2_I(args->new_inode); spin_lock(&new_oi->ip_lock); new_oi->ip_dyn_features |= OCFS2_HAS_XATTR_FL | OCFS2_INLINE_XATTR_FL; new_di->i_dyn_features = cpu_to_le16(new_oi->ip_dyn_features); spin_unlock(&new_oi->ip_lock); ocfs2_journal_dirty(handle, args->new_bh); out_commit: ocfs2_commit_trans(osb, handle); out: if (meta_ac) ocfs2_free_alloc_context(meta_ac); return ret; }"
704----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49988/bad/oplock.c----*opinfo_get_list,"static struct oplock_info *opinfo_get_list(struct ksmbd_inode *ci) { struct oplock_info *opinfo; if (list_empty(&ci->m_op_list)) return NULL; rcu_read_lock(); opinfo = list_first_or_null_rcu(&ci->m_op_list, struct oplock_info, op_entry); if (opinfo) { if (opinfo->conn == NULL || !atomic_inc_not_zero(&opinfo->refcount)) opinfo = NULL; else { <S2SV_StartVul> atomic_inc(&opinfo->conn->r_count); <S2SV_EndVul> <S2SV_StartVul> if (ksmbd_conn_releasing(opinfo->conn)) { <S2SV_EndVul> <S2SV_StartVul> atomic_dec(&opinfo->conn->r_count); <S2SV_EndVul> atomic_dec(&opinfo->refcount); opinfo = NULL; <S2SV_StartVul> } <S2SV_EndVul> } } rcu_read_unlock(); return opinfo; <S2SV_StartVul> } <S2SV_EndVul>","- atomic_inc(&opinfo->conn->r_count);
- if (ksmbd_conn_releasing(opinfo->conn)) {
- atomic_dec(&opinfo->conn->r_count);
- }
- }","static struct oplock_info *opinfo_get_list(struct ksmbd_inode *ci) { struct oplock_info *opinfo; if (list_empty(&ci->m_op_list)) return NULL; rcu_read_lock(); opinfo = list_first_or_null_rcu(&ci->m_op_list, struct oplock_info, op_entry); if (opinfo) { if (opinfo->conn == NULL || !atomic_inc_not_zero(&opinfo->refcount)) opinfo = NULL; else { if (ksmbd_conn_releasing(opinfo->conn)) { atomic_dec(&opinfo->refcount); opinfo = NULL; } } } rcu_read_unlock(); return opinfo; }"
896----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50144/bad/xe_vm.c----xe_vm_invalidate_vma,"int xe_vm_invalidate_vma(struct xe_vma *vma) { struct xe_device *xe = xe_vma_vm(vma)->xe; struct xe_tile *tile; struct xe_gt_tlb_invalidation_fence fence[XE_MAX_TILES_PER_DEVICE * XE_MAX_GT_PER_TILE]; u8 id; u32 fence_id = 0; int ret = 0; xe_assert(xe, !xe_vma_is_null(vma)); trace_xe_vma_invalidate(vma); vm_dbg(&xe_vma_vm(vma)->xe->drm, ""INVALIDATE: addr=0x%016llx, range=0x%016llx"", xe_vma_start(vma), xe_vma_size(vma)); if (IS_ENABLED(CONFIG_PROVE_LOCKING)) { if (xe_vma_is_userptr(vma)) { WARN_ON_ONCE(!mmu_interval_check_retry (&to_userptr_vma(vma)->userptr.notifier, to_userptr_vma(vma)->userptr.notifier_seq)); WARN_ON_ONCE(!dma_resv_test_signaled(xe_vm_resv(xe_vma_vm(vma)), DMA_RESV_USAGE_BOOKKEEP)); } else { xe_bo_assert_held(xe_vma_bo(vma)); } } for_each_tile(tile, xe, id) { if (xe_pt_zap_ptes(tile, vma)) { xe_device_wmb(xe); xe_gt_tlb_invalidation_fence_init(tile->primary_gt, &fence[fence_id], true); ret = xe_gt_tlb_invalidation_vma(tile->primary_gt, &fence[fence_id], vma); <S2SV_StartVul> if (ret < 0) { <S2SV_EndVul> <S2SV_StartVul> xe_gt_tlb_invalidation_fence_fini(&fence[fence_id]); <S2SV_EndVul> goto wait; <S2SV_StartVul> } <S2SV_EndVul> ++fence_id; if (!tile->media_gt) continue; xe_gt_tlb_invalidation_fence_init(tile->media_gt, &fence[fence_id], true); ret = xe_gt_tlb_invalidation_vma(tile->media_gt, &fence[fence_id], vma); <S2SV_StartVul> if (ret < 0) { <S2SV_EndVul> <S2SV_StartVul> xe_gt_tlb_invalidation_fence_fini(&fence[fence_id]); <S2SV_EndVul> goto wait; <S2SV_StartVul> } <S2SV_EndVul> ++fence_id; <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> wait: for (id = 0; id < fence_id; ++id) xe_gt_tlb_invalidation_fence_wait(&fence[id]); vma->tile_invalidated = vma->tile_mask; return ret; }","- if (ret < 0) {
- xe_gt_tlb_invalidation_fence_fini(&fence[fence_id]);
- }
- if (ret < 0) {
- xe_gt_tlb_invalidation_fence_fini(&fence[fence_id]);
- }
- }
- }
+ if (ret)
+ if (ret)","int xe_vm_invalidate_vma(struct xe_vma *vma) { struct xe_device *xe = xe_vma_vm(vma)->xe; struct xe_tile *tile; struct xe_gt_tlb_invalidation_fence fence[XE_MAX_TILES_PER_DEVICE * XE_MAX_GT_PER_TILE]; u8 id; u32 fence_id = 0; int ret = 0; xe_assert(xe, !xe_vma_is_null(vma)); trace_xe_vma_invalidate(vma); vm_dbg(&xe_vma_vm(vma)->xe->drm, ""INVALIDATE: addr=0x%016llx, range=0x%016llx"", xe_vma_start(vma), xe_vma_size(vma)); if (IS_ENABLED(CONFIG_PROVE_LOCKING)) { if (xe_vma_is_userptr(vma)) { WARN_ON_ONCE(!mmu_interval_check_retry (&to_userptr_vma(vma)->userptr.notifier, to_userptr_vma(vma)->userptr.notifier_seq)); WARN_ON_ONCE(!dma_resv_test_signaled(xe_vm_resv(xe_vma_vm(vma)), DMA_RESV_USAGE_BOOKKEEP)); } else { xe_bo_assert_held(xe_vma_bo(vma)); } } for_each_tile(tile, xe, id) { if (xe_pt_zap_ptes(tile, vma)) { xe_device_wmb(xe); xe_gt_tlb_invalidation_fence_init(tile->primary_gt, &fence[fence_id], true); ret = xe_gt_tlb_invalidation_vma(tile->primary_gt, &fence[fence_id], vma); if (ret) goto wait; ++fence_id; if (!tile->media_gt) continue; xe_gt_tlb_invalidation_fence_init(tile->media_gt, &fence[fence_id], true); ret = xe_gt_tlb_invalidation_vma(tile->media_gt, &fence[fence_id], vma); if (ret) goto wait; ++fence_id; } } wait: for (id = 0; id < fence_id; ++id) xe_gt_tlb_invalidation_fence_wait(&fence[id]); vma->tile_invalidated = vma->tile_mask; return ret; }"
1229----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53239/bad/chip.c----usb6fire_chip_destroy,static void usb6fire_chip_destroy(struct sfire_chip *chip) { if (chip) { if (chip->pcm) usb6fire_pcm_destroy(chip); if (chip->midi) usb6fire_midi_destroy(chip); if (chip->comm) usb6fire_comm_destroy(chip); if (chip->control) usb6fire_control_destroy(chip); <S2SV_StartVul> if (chip->card) <S2SV_EndVul> <S2SV_StartVul> snd_card_free(chip->card); <S2SV_EndVul> } },"- if (chip->card)
- snd_card_free(chip->card);","#include ""chip.h"" #include ""firmware.h"" #include ""pcm.h"" #include ""control.h"" #include ""comm.h"" #include ""midi.h"" #include <linux/moduleparam.h> #include <linux/interrupt.h> #include <linux/module.h> #include <linux/init.h> #include <linux/gfp.h> #include <sound/initval.h> MODULE_AUTHOR(""Torsten Schenk <torsten.schenk@zoho.com>""); MODULE_DESCRIPTION(""TerraTec DMX 6Fire USB audio driver""); MODULE_LICENSE(""GPL v2""); MODULE_SUPPORTED_DEVICE(""{{TerraTec,DMX 6Fire USB}}"");"
919----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50164/bad/verifier.c----check_helper_mem_access,"static int check_helper_mem_access(struct bpf_verifier_env *env, int regno, int access_size, bool zero_size_allowed, struct bpf_call_arg_meta *meta) { struct bpf_reg_state *regs = cur_regs(env), *reg = &regs[regno]; u32 *max_access; switch (base_type(reg->type)) { case PTR_TO_PACKET: case PTR_TO_PACKET_META: return check_packet_access(env, regno, reg->off, access_size, zero_size_allowed); case PTR_TO_MAP_KEY: <S2SV_StartVul> if (meta && meta->raw_mode) { <S2SV_EndVul> verbose(env, ""R%d cannot write into %s\n"", regno, reg_type_str(env, reg->type)); return -EACCES; } return check_mem_region_access(env, regno, reg->off, access_size, reg->map_ptr->key_size, false); case PTR_TO_MAP_VALUE: <S2SV_StartVul> if (check_map_access_type(env, regno, reg->off, access_size, <S2SV_EndVul> <S2SV_StartVul> meta && meta->raw_mode ? BPF_WRITE : <S2SV_EndVul> <S2SV_StartVul> BPF_READ)) <S2SV_EndVul> return -EACCES; return check_map_access(env, regno, reg->off, access_size, zero_size_allowed, ACCESS_HELPER); case PTR_TO_MEM: if (type_is_rdonly_mem(reg->type)) { <S2SV_StartVul> if (meta && meta->raw_mode) { <S2SV_EndVul> verbose(env, ""R%d cannot write into %s\n"", regno, reg_type_str(env, reg->type)); return -EACCES; } } return check_mem_region_access(env, regno, reg->off, access_size, reg->mem_size, zero_size_allowed); case PTR_TO_BUF: if (type_is_rdonly_mem(reg->type)) { <S2SV_StartVul> if (meta && meta->raw_mode) { <S2SV_EndVul> verbose(env, ""R%d cannot write into %s\n"", regno, reg_type_str(env, reg->type)); return -EACCES; } max_access = &env->prog->aux->max_rdonly_access; } else { max_access = &env->prog->aux->max_rdwr_access; } return check_buffer_access(env, reg, regno, reg->off, access_size, zero_size_allowed, max_access); case PTR_TO_STACK: return check_stack_range_initialized( env, regno, reg->off, access_size, zero_size_allowed, ACCESS_HELPER, meta); case PTR_TO_BTF_ID: return check_ptr_to_btf_access(env, regs, regno, reg->off, access_size, BPF_READ, -1); case PTR_TO_CTX: if (!env->ops->convert_ctx_access) { <S2SV_StartVul> enum bpf_access_type atype = meta && meta->raw_mode ? BPF_WRITE : BPF_READ; <S2SV_EndVul> int offset = access_size - 1; if (access_size == 0) return zero_size_allowed ? 0 : -EACCES; return check_mem_access(env, env->insn_idx, regno, offset, BPF_B, <S2SV_StartVul> atype, -1, false, false); <S2SV_EndVul> } fallthrough; default: if (zero_size_allowed && access_size == 0 && register_is_null(reg)) return 0; verbose(env, ""R%d type=%s "", regno, reg_type_str(env, reg->type)); verbose(env, ""expected=%s\n"", reg_type_str(env, PTR_TO_STACK)); return -EACCES; } }","- if (meta && meta->raw_mode) {
- if (check_map_access_type(env, regno, reg->off, access_size,
- meta && meta->raw_mode ? BPF_WRITE :
- BPF_READ))
- if (meta && meta->raw_mode) {
- if (meta && meta->raw_mode) {
- enum bpf_access_type atype = meta && meta->raw_mode ? BPF_WRITE : BPF_READ;
- atype, -1, false, false);
+ if (access_type == BPF_WRITE) {
+ if (check_map_access_type(env, regno, reg->off, access_size, access_type))
+ if (access_type == BPF_WRITE) {
+ if (access_type == BPF_WRITE) {
+ access_type, -1, false, false);","static int check_helper_mem_access(struct bpf_verifier_env *env, int regno, int access_size, enum bpf_access_type access_type, bool zero_size_allowed, struct bpf_call_arg_meta *meta) { struct bpf_reg_state *regs = cur_regs(env), *reg = &regs[regno]; u32 *max_access; switch (base_type(reg->type)) { case PTR_TO_PACKET: case PTR_TO_PACKET_META: return check_packet_access(env, regno, reg->off, access_size, zero_size_allowed); case PTR_TO_MAP_KEY: if (access_type == BPF_WRITE) { verbose(env, ""R%d cannot write into %s\n"", regno, reg_type_str(env, reg->type)); return -EACCES; } return check_mem_region_access(env, regno, reg->off, access_size, reg->map_ptr->key_size, false); case PTR_TO_MAP_VALUE: if (check_map_access_type(env, regno, reg->off, access_size, access_type)) return -EACCES; return check_map_access(env, regno, reg->off, access_size, zero_size_allowed, ACCESS_HELPER); case PTR_TO_MEM: if (type_is_rdonly_mem(reg->type)) { if (access_type == BPF_WRITE) { verbose(env, ""R%d cannot write into %s\n"", regno, reg_type_str(env, reg->type)); return -EACCES; } } return check_mem_region_access(env, regno, reg->off, access_size, reg->mem_size, zero_size_allowed); case PTR_TO_BUF: if (type_is_rdonly_mem(reg->type)) { if (access_type == BPF_WRITE) { verbose(env, ""R%d cannot write into %s\n"", regno, reg_type_str(env, reg->type)); return -EACCES; } max_access = &env->prog->aux->max_rdonly_access; } else { max_access = &env->prog->aux->max_rdwr_access; } return check_buffer_access(env, reg, regno, reg->off, access_size, zero_size_allowed, max_access); case PTR_TO_STACK: return check_stack_range_initialized( env, regno, reg->off, access_size, zero_size_allowed, ACCESS_HELPER, meta); case PTR_TO_BTF_ID: return check_ptr_to_btf_access(env, regs, regno, reg->off, access_size, BPF_READ, -1); case PTR_TO_CTX: if (!env->ops->convert_ctx_access) { int offset = access_size - 1; if (access_size == 0) return zero_size_allowed ? 0 : -EACCES; return check_mem_access(env, env->insn_idx, regno, offset, BPF_B, access_type, -1, false, false); } fallthrough; default: if (zero_size_allowed && access_size == 0 && register_is_null(reg)) return 0; verbose(env, ""R%d type=%s "", regno, reg_type_str(env, reg->type)); verbose(env, ""expected=%s\n"", reg_type_str(env, PTR_TO_STACK)); return -EACCES; } }"
1192----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53187/bad/memmap.c----**io_pin_pages,"struct page **io_pin_pages(unsigned long uaddr, unsigned long len, int *npages) { unsigned long start, end, nr_pages; struct page **pages; int ret; <S2SV_StartVul> end = (uaddr + len + PAGE_SIZE - 1) >> PAGE_SHIFT; <S2SV_EndVul> start = uaddr >> PAGE_SHIFT; nr_pages = end - start; if (WARN_ON_ONCE(!nr_pages)) return ERR_PTR(-EINVAL); pages = kvmalloc_array(nr_pages, sizeof(struct page *), GFP_KERNEL); if (!pages) return ERR_PTR(-ENOMEM); ret = pin_user_pages_fast(uaddr, nr_pages, FOLL_WRITE | FOLL_LONGTERM, pages); if (ret == nr_pages) { *npages = nr_pages; return pages; } if (ret >= 0) { if (ret) unpin_user_pages(pages, ret); ret = -EFAULT; } kvfree(pages); return ERR_PTR(ret); }","- end = (uaddr + len + PAGE_SIZE - 1) >> PAGE_SHIFT;
+ if (check_add_overflow(uaddr, len, &end))
+ return ERR_PTR(-EOVERFLOW);
+ if (check_add_overflow(end, PAGE_SIZE - 1, &end))
+ return ERR_PTR(-EOVERFLOW);
+ end = end >> PAGE_SHIFT;","struct page **io_pin_pages(unsigned long uaddr, unsigned long len, int *npages) { unsigned long start, end, nr_pages; struct page **pages; int ret; if (check_add_overflow(uaddr, len, &end)) return ERR_PTR(-EOVERFLOW); if (check_add_overflow(end, PAGE_SIZE - 1, &end)) return ERR_PTR(-EOVERFLOW); end = end >> PAGE_SHIFT; start = uaddr >> PAGE_SHIFT; nr_pages = end - start; if (WARN_ON_ONCE(!nr_pages)) return ERR_PTR(-EINVAL); pages = kvmalloc_array(nr_pages, sizeof(struct page *), GFP_KERNEL); if (!pages) return ERR_PTR(-ENOMEM); ret = pin_user_pages_fast(uaddr, nr_pages, FOLL_WRITE | FOLL_LONGTERM, pages); if (ret == nr_pages) { *npages = nr_pages; return pages; } if (ret >= 0) { if (ret) unpin_user_pages(pages, ret); ret = -EFAULT; } kvfree(pages); return ERR_PTR(ret); }"
1205----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53207/bad/mgmt.c----mgmt_set_discoverable_complete,"static void mgmt_set_discoverable_complete(struct hci_dev *hdev, void *data, int err) { struct mgmt_pending_cmd *cmd = data; bt_dev_dbg(hdev, ""err %d"", err); <S2SV_StartVul> if (cmd != pending_find(MGMT_OP_SET_DISCOVERABLE, hdev)) <S2SV_EndVul> return; hci_dev_lock(hdev); if (err) { u8 mgmt_err = mgmt_status(err); mgmt_cmd_status(cmd->sk, cmd->index, cmd->opcode, mgmt_err); hci_dev_clear_flag(hdev, HCI_LIMITED_DISCOVERABLE); goto done; } if (hci_dev_test_flag(hdev, HCI_DISCOVERABLE) && hdev->discov_timeout > 0) { int to = msecs_to_jiffies(hdev->discov_timeout * 1000); queue_delayed_work(hdev->req_workqueue, &hdev->discov_off, to); } send_settings_rsp(cmd->sk, MGMT_OP_SET_DISCOVERABLE, hdev); new_settings(hdev, cmd->sk); done: mgmt_pending_remove(cmd); hci_dev_unlock(hdev); }","- if (cmd != pending_find(MGMT_OP_SET_DISCOVERABLE, hdev))
+ if (err == -ECANCELED ||
+ cmd != pending_find(MGMT_OP_SET_DISCOVERABLE, hdev))
+ return;","static void mgmt_set_discoverable_complete(struct hci_dev *hdev, void *data, int err) { struct mgmt_pending_cmd *cmd = data; bt_dev_dbg(hdev, ""err %d"", err); if (err == -ECANCELED || cmd != pending_find(MGMT_OP_SET_DISCOVERABLE, hdev)) return; hci_dev_lock(hdev); if (err) { u8 mgmt_err = mgmt_status(err); mgmt_cmd_status(cmd->sk, cmd->index, cmd->opcode, mgmt_err); hci_dev_clear_flag(hdev, HCI_LIMITED_DISCOVERABLE); goto done; } if (hci_dev_test_flag(hdev, HCI_DISCOVERABLE) && hdev->discov_timeout > 0) { int to = msecs_to_jiffies(hdev->discov_timeout * 1000); queue_delayed_work(hdev->req_workqueue, &hdev->discov_off, to); } send_settings_rsp(cmd->sk, MGMT_OP_SET_DISCOVERABLE, hdev); new_settings(hdev, cmd->sk); done: mgmt_pending_remove(cmd); hci_dev_unlock(hdev); }"
772----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50047/bad/smb2ops.c----smb3_init_transform_rq,"smb3_init_transform_rq(struct TCP_Server_Info *server, int num_rqst, struct smb_rqst *new_rq, struct smb_rqst *old_rq) { struct smb2_transform_hdr *tr_hdr = new_rq[0].rq_iov[0].iov_base; unsigned int orig_len = 0; int rc = -ENOMEM; for (int i = 1; i < num_rqst; i++) { struct smb_rqst *old = &old_rq[i - 1]; struct smb_rqst *new = &new_rq[i]; struct folio_queue *buffer; size_t size = iov_iter_count(&old->rq_iter); orig_len += smb_rqst_len(server, old); new->rq_iov = old->rq_iov; new->rq_nvec = old->rq_nvec; if (size > 0) { buffer = cifs_alloc_folioq_buffer(size); if (!buffer) goto err_free; new->rq_buffer = buffer; iov_iter_folio_queue(&new->rq_iter, ITER_SOURCE, buffer, 0, 0, size); if (!cifs_copy_iter_to_folioq(&old->rq_iter, size, buffer)) { rc = -EIO; goto err_free; } } } fill_transform_hdr(tr_hdr, orig_len, old_rq, server->cipher_type); <S2SV_StartVul> rc = crypt_message(server, num_rqst, new_rq, 1); <S2SV_EndVul> cifs_dbg(FYI, ""Encrypt message returned %d\n"", rc); if (rc) goto err_free; return rc; err_free: smb3_free_compound_rqst(num_rqst - 1, &new_rq[1]); return rc; }","- rc = crypt_message(server, num_rqst, new_rq, 1);
+ rc = crypt_message(server, num_rqst, new_rq, 1, server->secmech.enc);","smb3_init_transform_rq(struct TCP_Server_Info *server, int num_rqst, struct smb_rqst *new_rq, struct smb_rqst *old_rq) { struct smb2_transform_hdr *tr_hdr = new_rq[0].rq_iov[0].iov_base; unsigned int orig_len = 0; int rc = -ENOMEM; for (int i = 1; i < num_rqst; i++) { struct smb_rqst *old = &old_rq[i - 1]; struct smb_rqst *new = &new_rq[i]; struct folio_queue *buffer; size_t size = iov_iter_count(&old->rq_iter); orig_len += smb_rqst_len(server, old); new->rq_iov = old->rq_iov; new->rq_nvec = old->rq_nvec; if (size > 0) { buffer = cifs_alloc_folioq_buffer(size); if (!buffer) goto err_free; new->rq_buffer = buffer; iov_iter_folio_queue(&new->rq_iter, ITER_SOURCE, buffer, 0, 0, size); if (!cifs_copy_iter_to_folioq(&old->rq_iter, size, buffer)) { rc = -EIO; goto err_free; } } } fill_transform_hdr(tr_hdr, orig_len, old_rq, server->cipher_type); rc = crypt_message(server, num_rqst, new_rq, 1, server->secmech.enc); cifs_dbg(FYI, ""Encrypt message returned %d\n"", rc); if (rc) goto err_free; return rc; err_free: smb3_free_compound_rqst(num_rqst - 1, &new_rq[1]); return rc; }"
191----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46678/bad/bond_main.c----bond_ipsec_add_sa_all,"static void bond_ipsec_add_sa_all(struct bonding *bond) { struct net_device *bond_dev = bond->dev; struct net_device *real_dev; struct bond_ipsec *ipsec; struct slave *slave; <S2SV_StartVul> rcu_read_lock(); <S2SV_EndVul> <S2SV_StartVul> slave = rcu_dereference(bond->curr_active_slave); <S2SV_EndVul> <S2SV_StartVul> if (!slave) <S2SV_EndVul> <S2SV_StartVul> goto out; <S2SV_EndVul> <S2SV_StartVul> real_dev = slave->dev; <S2SV_EndVul> if (!real_dev->xfrmdev_ops || !real_dev->xfrmdev_ops->xdo_dev_state_add || netif_is_bond_master(real_dev)) { <S2SV_StartVul> spin_lock_bh(&bond->ipsec_lock); <S2SV_EndVul> if (!list_empty(&bond->ipsec_list)) slave_warn(bond_dev, real_dev, ""%s: no slave xdo_dev_state_add\n"", __func__); <S2SV_StartVul> spin_unlock_bh(&bond->ipsec_lock); <S2SV_EndVul> <S2SV_StartVul> goto out; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> spin_lock_bh(&bond->ipsec_lock); <S2SV_EndVul> list_for_each_entry(ipsec, &bond->ipsec_list, list) { ipsec->xs->xso.real_dev = real_dev; if (real_dev->xfrmdev_ops->xdo_dev_state_add(ipsec->xs, NULL)) { slave_warn(bond_dev, real_dev, ""%s: failed to add SA\n"", __func__); ipsec->xs->xso.real_dev = NULL; <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> spin_unlock_bh(&bond->ipsec_lock); <S2SV_EndVul> out: <S2SV_StartVul> rcu_read_unlock(); <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul>","- rcu_read_lock();
- slave = rcu_dereference(bond->curr_active_slave);
- if (!slave)
- goto out;
- real_dev = slave->dev;
- spin_lock_bh(&bond->ipsec_lock);
- spin_unlock_bh(&bond->ipsec_lock);
- goto out;
- }
- spin_lock_bh(&bond->ipsec_lock);
- }
- }
- spin_unlock_bh(&bond->ipsec_lock);
- rcu_read_unlock();
- }
+ slave = rtnl_dereference(bond->curr_active_slave);
+ real_dev = slave ? slave->dev : NULL;
+ if (!real_dev)
+ return;
+ mutex_lock(&bond->ipsec_lock);
+ goto out;
+ if (ipsec->xs->xso.real_dev == real_dev)
+ continue;
+ out:
+ mutex_unlock(&bond->ipsec_lock);","static void bond_ipsec_add_sa_all(struct bonding *bond) { struct net_device *bond_dev = bond->dev; struct net_device *real_dev; struct bond_ipsec *ipsec; struct slave *slave; slave = rtnl_dereference(bond->curr_active_slave); real_dev = slave ? slave->dev : NULL; if (!real_dev) return; mutex_lock(&bond->ipsec_lock); if (!real_dev->xfrmdev_ops || !real_dev->xfrmdev_ops->xdo_dev_state_add || netif_is_bond_master(real_dev)) { if (!list_empty(&bond->ipsec_list)) slave_warn(bond_dev, real_dev, ""%s: no slave xdo_dev_state_add\n"", __func__); goto out; } list_for_each_entry(ipsec, &bond->ipsec_list, list) { if (ipsec->xs->xso.real_dev == real_dev) continue; ipsec->xs->xso.real_dev = real_dev; if (real_dev->xfrmdev_ops->xdo_dev_state_add(ipsec->xs, NULL)) { slave_warn(bond_dev, real_dev, ""%s: failed to add SA\n"", __func__); ipsec->xs->xso.real_dev = NULL; } } out: mutex_unlock(&bond->ipsec_lock); }"
1237----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53689/bad/blk-mq-sysfs.c----blk_mq_sysfs_register_hctxs,"int blk_mq_sysfs_register_hctxs(struct request_queue *q) { struct blk_mq_hw_ctx *hctx; unsigned long i; int ret = 0; <S2SV_StartVul> mutex_lock(&q->sysfs_dir_lock); <S2SV_EndVul> if (!q->mq_sysfs_init_done) <S2SV_StartVul> goto unlock; <S2SV_EndVul> queue_for_each_hw_ctx(q, hctx, i) { ret = blk_mq_register_hctx(hctx); if (ret) break; } <S2SV_StartVul> unlock: <S2SV_EndVul> <S2SV_StartVul> mutex_unlock(&q->sysfs_dir_lock); <S2SV_EndVul> return ret; }","- mutex_lock(&q->sysfs_dir_lock);
- goto unlock;
- unlock:
- mutex_unlock(&q->sysfs_dir_lock);
+ lockdep_assert_held(&q->sysfs_dir_lock);
+ return ret;
+ return ret;","int blk_mq_sysfs_register_hctxs(struct request_queue *q) { struct blk_mq_hw_ctx *hctx; unsigned long i; int ret = 0; lockdep_assert_held(&q->sysfs_dir_lock); if (!q->mq_sysfs_init_done) return ret; queue_for_each_hw_ctx(q, hctx, i) { ret = blk_mq_register_hctx(hctx); if (ret) break; } return ret; }"
416----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47143/bad/debug.c----check_unmap,"static void check_unmap(struct dma_debug_entry *ref) { struct dma_debug_entry *entry; struct hash_bucket *bucket; unsigned long flags; bucket = get_hash_bucket(ref, &flags); entry = bucket_find_exact(bucket, ref); if (!entry) { put_hash_bucket(bucket, flags); if (dma_mapping_error(ref->dev, ref->dev_addr)) { err_printk(ref->dev, NULL, ""device driver tries to free an "" ""invalid DMA memory address\n""); } else { err_printk(ref->dev, NULL, ""device driver tries to free DMA "" ""memory it has not allocated [device "" ""address=0x%016llx] [size=%llu bytes]\n"", ref->dev_addr, ref->size); } return; } if (ref->size != entry->size) { err_printk(ref->dev, entry, ""device driver frees "" ""DMA memory with different size "" ""[device address=0x%016llx] [map size=%llu bytes] "" ""[unmap size=%llu bytes]\n"", ref->dev_addr, entry->size, ref->size); } if (ref->type != entry->type) { err_printk(ref->dev, entry, ""device driver frees "" ""DMA memory with wrong function "" ""[device address=0x%016llx] [size=%llu bytes] "" ""[mapped as %s] [unmapped as %s]\n"", ref->dev_addr, ref->size, type2name[entry->type], type2name[ref->type]); } else if ((entry->type == dma_debug_coherent) && (phys_addr(ref) != phys_addr(entry))) { err_printk(ref->dev, entry, ""device driver frees "" ""DMA memory with different CPU address "" ""[device address=0x%016llx] [size=%llu bytes] "" ""[cpu alloc address=0x%016llx] "" ""[cpu free address=0x%016llx]"", ref->dev_addr, ref->size, phys_addr(entry), phys_addr(ref)); } if (ref->sg_call_ents && ref->type == dma_debug_sg && ref->sg_call_ents != entry->sg_call_ents) { err_printk(ref->dev, entry, ""device driver frees "" ""DMA sg list with different entry count "" ""[map count=%d] [unmap count=%d]\n"", entry->sg_call_ents, ref->sg_call_ents); } if (ref->direction != entry->direction) { err_printk(ref->dev, entry, ""device driver frees "" ""DMA memory with different direction "" ""[device address=0x%016llx] [size=%llu bytes] "" ""[mapped with %s] [unmapped with %s]\n"", ref->dev_addr, ref->size, dir2name[entry->direction], dir2name[ref->direction]); } if (entry->map_err_type == MAP_ERR_NOT_CHECKED) { err_printk(ref->dev, entry, ""device driver failed to check map error"" ""[device address=0x%016llx] [size=%llu bytes] "" ""[mapped as %s]"", ref->dev_addr, ref->size, type2name[entry->type]); } hash_bucket_del(entry); <S2SV_StartVul> dma_entry_free(entry); <S2SV_EndVul> put_hash_bucket(bucket, flags); }","- dma_entry_free(entry);
+ dma_entry_free(entry);","static void check_unmap(struct dma_debug_entry *ref) { struct dma_debug_entry *entry; struct hash_bucket *bucket; unsigned long flags; bucket = get_hash_bucket(ref, &flags); entry = bucket_find_exact(bucket, ref); if (!entry) { put_hash_bucket(bucket, flags); if (dma_mapping_error(ref->dev, ref->dev_addr)) { err_printk(ref->dev, NULL, ""device driver tries to free an "" ""invalid DMA memory address\n""); } else { err_printk(ref->dev, NULL, ""device driver tries to free DMA "" ""memory it has not allocated [device "" ""address=0x%016llx] [size=%llu bytes]\n"", ref->dev_addr, ref->size); } return; } if (ref->size != entry->size) { err_printk(ref->dev, entry, ""device driver frees "" ""DMA memory with different size "" ""[device address=0x%016llx] [map size=%llu bytes] "" ""[unmap size=%llu bytes]\n"", ref->dev_addr, entry->size, ref->size); } if (ref->type != entry->type) { err_printk(ref->dev, entry, ""device driver frees "" ""DMA memory with wrong function "" ""[device address=0x%016llx] [size=%llu bytes] "" ""[mapped as %s] [unmapped as %s]\n"", ref->dev_addr, ref->size, type2name[entry->type], type2name[ref->type]); } else if ((entry->type == dma_debug_coherent) && (phys_addr(ref) != phys_addr(entry))) { err_printk(ref->dev, entry, ""device driver frees "" ""DMA memory with different CPU address "" ""[device address=0x%016llx] [size=%llu bytes] "" ""[cpu alloc address=0x%016llx] "" ""[cpu free address=0x%016llx]"", ref->dev_addr, ref->size, phys_addr(entry), phys_addr(ref)); } if (ref->sg_call_ents && ref->type == dma_debug_sg && ref->sg_call_ents != entry->sg_call_ents) { err_printk(ref->dev, entry, ""device driver frees "" ""DMA sg list with different entry count "" ""[map count=%d] [unmap count=%d]\n"", entry->sg_call_ents, ref->sg_call_ents); } if (ref->direction != entry->direction) { err_printk(ref->dev, entry, ""device driver frees "" ""DMA memory with different direction "" ""[device address=0x%016llx] [size=%llu bytes] "" ""[mapped with %s] [unmapped with %s]\n"", ref->dev_addr, ref->size, dir2name[entry->direction], dir2name[ref->direction]); } if (entry->map_err_type == MAP_ERR_NOT_CHECKED) { err_printk(ref->dev, entry, ""device driver failed to check map error"" ""[device address=0x%016llx] [size=%llu bytes] "" ""[mapped as %s]"", ref->dev_addr, ref->size, type2name[entry->type]); } hash_bucket_del(entry); put_hash_bucket(bucket, flags); dma_entry_free(entry); }"
510----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47748/bad/vdpa.c----vhost_vdpa_vring_ioctl,"static long vhost_vdpa_vring_ioctl(struct vhost_vdpa *v, unsigned int cmd, void __user *argp) { struct vdpa_device *vdpa = v->vdpa; const struct vdpa_config_ops *ops = vdpa->config; struct vdpa_vq_state vq_state; struct vdpa_callback cb; struct vhost_virtqueue *vq; struct vhost_vring_state s; u32 idx; long r; r = get_user(idx, (u32 __user *)argp); if (r < 0) return r; if (idx >= v->nvqs) return -ENOBUFS; idx = array_index_nospec(idx, v->nvqs); vq = &v->vqs[idx]; switch (cmd) { case VHOST_VDPA_SET_VRING_ENABLE: if (copy_from_user(&s, argp, sizeof(s))) return -EFAULT; ops->set_vq_ready(vdpa, idx, s.num); return 0; case VHOST_GET_VRING_BASE: r = ops->get_vq_state(v->vdpa, idx, &vq_state); if (r) return r; if (vhost_has_feature(vq, VIRTIO_F_RING_PACKED)) { vq->last_avail_idx = vq_state.packed.last_avail_idx | (vq_state.packed.last_avail_counter << 15); vq->last_used_idx = vq_state.packed.last_used_idx | (vq_state.packed.last_used_counter << 15); } else { vq->last_avail_idx = vq_state.split.avail_index; } break; } r = vhost_vring_ioctl(&v->vdev, cmd, argp); if (r) return r; switch (cmd) { case VHOST_SET_VRING_ADDR: if (ops->set_vq_address(vdpa, idx, (u64)(uintptr_t)vq->desc, (u64)(uintptr_t)vq->avail, (u64)(uintptr_t)vq->used)) r = -EINVAL; break; case VHOST_SET_VRING_BASE: if (vhost_has_feature(vq, VIRTIO_F_RING_PACKED)) { vq_state.packed.last_avail_idx = vq->last_avail_idx & 0x7fff; vq_state.packed.last_avail_counter = !!(vq->last_avail_idx & 0x8000); vq_state.packed.last_used_idx = vq->last_used_idx & 0x7fff; vq_state.packed.last_used_counter = !!(vq->last_used_idx & 0x8000); } else { vq_state.split.avail_index = vq->last_avail_idx; } r = ops->set_vq_state(vdpa, idx, &vq_state); break; case VHOST_SET_VRING_CALL: if (vq->call_ctx.ctx) { cb.callback = vhost_vdpa_virtqueue_cb; cb.private = vq; cb.trigger = vq->call_ctx.ctx; } else { cb.callback = NULL; cb.private = NULL; cb.trigger = NULL; } ops->set_vq_cb(vdpa, idx, &cb); <S2SV_StartVul> vhost_vdpa_setup_vq_irq(v, idx); <S2SV_EndVul> break; case VHOST_SET_VRING_NUM: ops->set_vq_num(vdpa, idx, vq->num); break; } return r; }","- vhost_vdpa_setup_vq_irq(v, idx);
+ }
+ break;
+ case VHOST_SET_VRING_CALL:
+ if (vq->call_ctx.ctx) {
+ if (ops->get_status(vdpa) &
+ VIRTIO_CONFIG_S_DRIVER_OK)
+ vhost_vdpa_unsetup_vq_irq(v, idx);
+ vq->call_ctx.producer.token = NULL;
+ }
+ break;
+ }
+ vq->call_ctx.producer.token = vq->call_ctx.ctx;
+ if (ops->get_status(vdpa) &
+ VIRTIO_CONFIG_S_DRIVER_OK)
+ vhost_vdpa_setup_vq_irq(v, idx);
+ }
+ break;","static long vhost_vdpa_vring_ioctl(struct vhost_vdpa *v, unsigned int cmd, void __user *argp) { struct vdpa_device *vdpa = v->vdpa; const struct vdpa_config_ops *ops = vdpa->config; struct vdpa_vq_state vq_state; struct vdpa_callback cb; struct vhost_virtqueue *vq; struct vhost_vring_state s; u32 idx; long r; r = get_user(idx, (u32 __user *)argp); if (r < 0) return r; if (idx >= v->nvqs) return -ENOBUFS; idx = array_index_nospec(idx, v->nvqs); vq = &v->vqs[idx]; switch (cmd) { case VHOST_VDPA_SET_VRING_ENABLE: if (copy_from_user(&s, argp, sizeof(s))) return -EFAULT; ops->set_vq_ready(vdpa, idx, s.num); return 0; case VHOST_GET_VRING_BASE: r = ops->get_vq_state(v->vdpa, idx, &vq_state); if (r) return r; if (vhost_has_feature(vq, VIRTIO_F_RING_PACKED)) { vq->last_avail_idx = vq_state.packed.last_avail_idx | (vq_state.packed.last_avail_counter << 15); vq->last_used_idx = vq_state.packed.last_used_idx | (vq_state.packed.last_used_counter << 15); } else { vq->last_avail_idx = vq_state.split.avail_index; } break; case VHOST_SET_VRING_CALL: if (vq->call_ctx.ctx) { if (ops->get_status(vdpa) & VIRTIO_CONFIG_S_DRIVER_OK) vhost_vdpa_unsetup_vq_irq(v, idx); vq->call_ctx.producer.token = NULL; } break; } r = vhost_vring_ioctl(&v->vdev, cmd, argp); if (r) return r; switch (cmd) { case VHOST_SET_VRING_ADDR: if (ops->set_vq_address(vdpa, idx, (u64)(uintptr_t)vq->desc, (u64)(uintptr_t)vq->avail, (u64)(uintptr_t)vq->used)) r = -EINVAL; break; case VHOST_SET_VRING_BASE: if (vhost_has_feature(vq, VIRTIO_F_RING_PACKED)) { vq_state.packed.last_avail_idx = vq->last_avail_idx & 0x7fff; vq_state.packed.last_avail_counter = !!(vq->last_avail_idx & 0x8000); vq_state.packed.last_used_idx = vq->last_used_idx & 0x7fff; vq_state.packed.last_used_counter = !!(vq->last_used_idx & 0x8000); } else { vq_state.split.avail_index = vq->last_avail_idx; } r = ops->set_vq_state(vdpa, idx, &vq_state); break; case VHOST_SET_VRING_CALL: if (vq->call_ctx.ctx) { cb.callback = vhost_vdpa_virtqueue_cb; cb.private = vq; cb.trigger = vq->call_ctx.ctx; vq->call_ctx.producer.token = vq->call_ctx.ctx; if (ops->get_status(vdpa) & VIRTIO_CONFIG_S_DRIVER_OK) vhost_vdpa_setup_vq_irq(v, idx); } else { cb.callback = NULL; cb.private = NULL; cb.trigger = NULL; } ops->set_vq_cb(vdpa, idx, &cb); break; case VHOST_SET_VRING_NUM: ops->set_vq_num(vdpa, idx, vq->num); break; } return r; }"
26----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-43900/bad/xc2028.c----load_firmware_cb,"static void load_firmware_cb(const struct firmware *fw, void *context) { struct dvb_frontend *fe = context; <S2SV_StartVul> struct xc2028_data *priv = fe->tuner_priv; <S2SV_EndVul> int rc; tuner_dbg(""request_firmware_nowait(): %s\n"", fw ? ""OK"" : ""error""); if (!fw) { tuner_err(""Could not load firmware %s.\n"", priv->fname); priv->state = XC2028_NODEV; return; } rc = load_all_firmwares(fe, fw); release_firmware(fw); if (rc < 0) return; priv->state = XC2028_ACTIVE; }","- struct xc2028_data *priv = fe->tuner_priv;
+ struct xc2028_data *priv;
+ if (!fe) {
+ pr_warn(""xc2028: No frontend in %s\n"", __func__);
+ return;
+ }
+ priv = fe->tuner_priv;","static void load_firmware_cb(const struct firmware *fw, void *context) { struct dvb_frontend *fe = context; struct xc2028_data *priv; int rc; if (!fe) { pr_warn(""xc2028: No frontend in %s\n"", __func__); return; } priv = fe->tuner_priv; tuner_dbg(""request_firmware_nowait(): %s\n"", fw ? ""OK"" : ""error""); if (!fw) { tuner_err(""Could not load firmware %s.\n"", priv->fname); priv->state = XC2028_NODEV; return; } rc = load_all_firmwares(fe, fw); release_firmware(fw); if (rc < 0) return; priv->state = XC2028_ACTIVE; }"
1531----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2025-21684/bad/gpio-xilinx.c----xgpio_dir_in,"static int xgpio_dir_in(struct gpio_chip *gc, unsigned int gpio) { unsigned long flags; struct xgpio_instance *chip = gpiochip_get_data(gc); int bit = xgpio_to_bit(chip, gpio); <S2SV_StartVul> spin_lock_irqsave(&chip->gpio_lock, flags); <S2SV_EndVul> __set_bit(bit, chip->dir); xgpio_write_ch(chip, XGPIO_TRI_OFFSET, bit, chip->dir); <S2SV_StartVul> spin_unlock_irqrestore(&chip->gpio_lock, flags); <S2SV_EndVul> return 0; }","- spin_lock_irqsave(&chip->gpio_lock, flags);
- spin_unlock_irqrestore(&chip->gpio_lock, flags);
+ raw_spin_lock_irqsave(&chip->gpio_lock, flags);
+ raw_spin_unlock_irqrestore(&chip->gpio_lock, flags);","static int xgpio_dir_in(struct gpio_chip *gc, unsigned int gpio) { unsigned long flags; struct xgpio_instance *chip = gpiochip_get_data(gc); int bit = xgpio_to_bit(chip, gpio); raw_spin_lock_irqsave(&chip->gpio_lock, flags); __set_bit(bit, chip->dir); xgpio_write_ch(chip, XGPIO_TRI_OFFSET, bit, chip->dir); raw_spin_unlock_irqrestore(&chip->gpio_lock, flags); return 0; }"
1393----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56722/bad/hns_roce_hw_v2.c----hns_roce_v2_modify_cq,"static int hns_roce_v2_modify_cq(struct ib_cq *cq, u16 cq_count, u16 cq_period) { struct hns_roce_dev *hr_dev = to_hr_dev(cq->device); struct hns_roce_v2_cq_context *cq_context; struct hns_roce_cq *hr_cq = to_hr_cq(cq); struct hns_roce_v2_cq_context *cqc_mask; struct hns_roce_cmd_mailbox *mailbox; int ret; mailbox = hns_roce_alloc_cmd_mailbox(hr_dev); ret = PTR_ERR_OR_ZERO(mailbox); if (ret) goto err_out; cq_context = mailbox->buf; cqc_mask = (struct hns_roce_v2_cq_context *)mailbox->buf + 1; memset(cqc_mask, 0xff, sizeof(*cqc_mask)); hr_reg_write(cq_context, CQC_CQ_MAX_CNT, cq_count); hr_reg_clear(cqc_mask, CQC_CQ_MAX_CNT); if (hr_dev->pci_dev->revision == PCI_REVISION_ID_HIP08) { if (cq_period * HNS_ROCE_CLOCK_ADJUST > USHRT_MAX) { dev_info(hr_dev->dev, ""cq_period(%u) reached the upper limit, adjusted to 65.\n"", cq_period); cq_period = HNS_ROCE_MAX_CQ_PERIOD_HIP08; } cq_period *= HNS_ROCE_CLOCK_ADJUST; } hr_reg_write(cq_context, CQC_CQ_PERIOD, cq_period); hr_reg_clear(cqc_mask, CQC_CQ_PERIOD); ret = hns_roce_cmd_mbox(hr_dev, mailbox->dma, 0, HNS_ROCE_CMD_MODIFY_CQC, hr_cq->cqn); hns_roce_free_cmd_mailbox(hr_dev, mailbox); if (ret) <S2SV_StartVul> ibdev_err(&hr_dev->ib_dev, <S2SV_EndVul> <S2SV_StartVul> ""failed to process cmd when modifying CQ, ret = %d.\n"", <S2SV_EndVul> <S2SV_StartVul> ret); <S2SV_EndVul> err_out: if (ret) atomic64_inc(&hr_dev->dfx_cnt[HNS_ROCE_DFX_CQ_MODIFY_ERR_CNT]); return ret; }","- ibdev_err(&hr_dev->ib_dev,
- ""failed to process cmd when modifying CQ, ret = %d.\n"",
- ret);
+ ibdev_err_ratelimited(&hr_dev->ib_dev,
+ ""failed to process cmd when modifying CQ, ret = %d.\n"",
+ ret);","static int hns_roce_v2_modify_cq(struct ib_cq *cq, u16 cq_count, u16 cq_period) { struct hns_roce_dev *hr_dev = to_hr_dev(cq->device); struct hns_roce_v2_cq_context *cq_context; struct hns_roce_cq *hr_cq = to_hr_cq(cq); struct hns_roce_v2_cq_context *cqc_mask; struct hns_roce_cmd_mailbox *mailbox; int ret; mailbox = hns_roce_alloc_cmd_mailbox(hr_dev); ret = PTR_ERR_OR_ZERO(mailbox); if (ret) goto err_out; cq_context = mailbox->buf; cqc_mask = (struct hns_roce_v2_cq_context *)mailbox->buf + 1; memset(cqc_mask, 0xff, sizeof(*cqc_mask)); hr_reg_write(cq_context, CQC_CQ_MAX_CNT, cq_count); hr_reg_clear(cqc_mask, CQC_CQ_MAX_CNT); if (hr_dev->pci_dev->revision == PCI_REVISION_ID_HIP08) { if (cq_period * HNS_ROCE_CLOCK_ADJUST > USHRT_MAX) { dev_info(hr_dev->dev, ""cq_period(%u) reached the upper limit, adjusted to 65.\n"", cq_period); cq_period = HNS_ROCE_MAX_CQ_PERIOD_HIP08; } cq_period *= HNS_ROCE_CLOCK_ADJUST; } hr_reg_write(cq_context, CQC_CQ_PERIOD, cq_period); hr_reg_clear(cqc_mask, CQC_CQ_PERIOD); ret = hns_roce_cmd_mbox(hr_dev, mailbox->dma, 0, HNS_ROCE_CMD_MODIFY_CQC, hr_cq->cqn); hns_roce_free_cmd_mailbox(hr_dev, mailbox); if (ret) ibdev_err_ratelimited(&hr_dev->ib_dev, ""failed to process cmd when modifying CQ, ret = %d.\n"", ret); err_out: if (ret) atomic64_inc(&hr_dev->dfx_cnt[HNS_ROCE_DFX_CQ_MODIFY_ERR_CNT]); return ret; }"
1266----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56549/bad/ondemand.c----cachefiles_ondemand_fd_write_iter,"static ssize_t cachefiles_ondemand_fd_write_iter(struct kiocb *kiocb, struct iov_iter *iter) { struct cachefiles_object *object = kiocb->ki_filp->private_data; struct cachefiles_cache *cache = object->volume->cache; <S2SV_StartVul> struct file *file = object->file; <S2SV_EndVul> size_t len = iter->count, aligned_len = len; loff_t pos = kiocb->ki_pos; const struct cred *saved_cred; int ret; <S2SV_StartVul> if (!file) <S2SV_EndVul> return -ENOBUFS; cachefiles_begin_secure(cache, &saved_cred); ret = __cachefiles_prepare_write(object, file, &pos, &aligned_len, len, true); cachefiles_end_secure(cache, saved_cred); if (ret < 0) <S2SV_StartVul> return ret; <S2SV_EndVul> trace_cachefiles_ondemand_fd_write(object, file_inode(file), pos, len); ret = __cachefiles_write(object, file, pos, iter, NULL, NULL); if (!ret) { ret = len; kiocb->ki_pos += ret; } <S2SV_StartVul> return ret; <S2SV_EndVul> }","- struct file *file = object->file;
- if (!file)
- return ret;
- return ret;
+ struct file *file;
+ spin_lock(&object->lock);
+ file = object->file;
+ if (!file) {
+ spin_unlock(&object->lock);
+ }
+ get_file(file);
+ spin_unlock(&object->lock);
+ goto out;
+ }
+ out:
+ fput(file);
+ return ret;
+ }","static ssize_t cachefiles_ondemand_fd_write_iter(struct kiocb *kiocb, struct iov_iter *iter) { struct cachefiles_object *object = kiocb->ki_filp->private_data; struct cachefiles_cache *cache = object->volume->cache; struct file *file; size_t len = iter->count, aligned_len = len; loff_t pos = kiocb->ki_pos; const struct cred *saved_cred; int ret; spin_lock(&object->lock); file = object->file; if (!file) { spin_unlock(&object->lock); return -ENOBUFS; } get_file(file); spin_unlock(&object->lock); cachefiles_begin_secure(cache, &saved_cred); ret = __cachefiles_prepare_write(object, file, &pos, &aligned_len, len, true); cachefiles_end_secure(cache, saved_cred); if (ret < 0) goto out; trace_cachefiles_ondemand_fd_write(object, file_inode(file), pos, len); ret = __cachefiles_write(object, file, pos, iter, NULL, NULL); if (!ret) { ret = len; kiocb->ki_pos += ret; } out: fput(file); return ret; }"
283----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46761/bad/pnv_php.c----pnv_php_disable_irq,"static void pnv_php_disable_irq(struct pnv_php_slot *php_slot, bool disable_device) { struct pci_dev *pdev = php_slot->pdev; <S2SV_StartVul> int irq = php_slot->irq; <S2SV_EndVul> u16 ctrl; if (php_slot->irq > 0) { pcie_capability_read_word(pdev, PCI_EXP_SLTCTL, &ctrl); ctrl &= ~(PCI_EXP_SLTCTL_HPIE | PCI_EXP_SLTCTL_PDCE | PCI_EXP_SLTCTL_DLLSCE); pcie_capability_write_word(pdev, PCI_EXP_SLTCTL, ctrl); free_irq(php_slot->irq, php_slot); php_slot->irq = 0; } if (php_slot->wq) { destroy_workqueue(php_slot->wq); php_slot->wq = NULL; } <S2SV_StartVul> if (disable_device || irq > 0) { <S2SV_EndVul> if (pdev->msix_enabled) pci_disable_msix(pdev); else if (pdev->msi_enabled) pci_disable_msi(pdev); pci_disable_device(pdev); } }","- int irq = php_slot->irq;
- if (disable_device || irq > 0) {
+ if (disable_device) {","static void pnv_php_disable_irq(struct pnv_php_slot *php_slot, bool disable_device) { struct pci_dev *pdev = php_slot->pdev; u16 ctrl; if (php_slot->irq > 0) { pcie_capability_read_word(pdev, PCI_EXP_SLTCTL, &ctrl); ctrl &= ~(PCI_EXP_SLTCTL_HPIE | PCI_EXP_SLTCTL_PDCE | PCI_EXP_SLTCTL_DLLSCE); pcie_capability_write_word(pdev, PCI_EXP_SLTCTL, ctrl); free_irq(php_slot->irq, php_slot); php_slot->irq = 0; } if (php_slot->wq) { destroy_workqueue(php_slot->wq); php_slot->wq = NULL; } if (disable_device) { if (pdev->msix_enabled) pci_disable_msix(pdev); else if (pdev->msi_enabled) pci_disable_msi(pdev); pci_disable_device(pdev); } }"
552----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49871/bad/adp5589-keys.c----adp5589_probe,"static int adp5589_probe(struct i2c_client *client) { const struct i2c_device_id *id = i2c_client_get_device_id(client); struct adp5589_kpad *kpad; const struct adp5589_kpad_platform_data *pdata = dev_get_platdata(&client->dev); unsigned int revid; int error, ret; if (!i2c_check_functionality(client->adapter, I2C_FUNC_SMBUS_BYTE_DATA)) { dev_err(&client->dev, ""SMBUS Byte Data not Supported\n""); return -EIO; } if (!pdata) { dev_err(&client->dev, ""no platform data?\n""); return -EINVAL; } kpad = devm_kzalloc(&client->dev, sizeof(*kpad), GFP_KERNEL); if (!kpad) return -ENOMEM; kpad->client = client; switch (id->driver_data) { case ADP5585_02: kpad->support_row5 = true; fallthrough; case ADP5585_01: kpad->is_adp5585 = true; kpad->var = &const_adp5585; break; case ADP5589: kpad->support_row5 = true; kpad->var = &const_adp5589; break; } error = devm_add_action_or_reset(&client->dev, adp5589_clear_config, <S2SV_StartVul> client); <S2SV_EndVul> if (error) return error; ret = adp5589_read(client, ADP5589_5_ID); if (ret < 0) return ret; revid = (u8) ret & ADP5589_5_DEVICE_ID_MASK; if (pdata->keymapsize) { error = adp5589_keypad_add(kpad, revid); if (error) return error; } error = adp5589_setup(kpad); if (error) return error; if (kpad->gpimapsize) adp5589_report_switch_state(kpad); error = adp5589_gpio_add(kpad); if (error) return error; <S2SV_StartVul> i2c_set_clientdata(client, kpad); <S2SV_EndVul> dev_info(&client->dev, ""Rev.%d keypad, irq %d\n"", revid, client->irq); return 0; }","- client);
- i2c_set_clientdata(client, kpad);
+ kpad);","static int adp5589_probe(struct i2c_client *client) { const struct i2c_device_id *id = i2c_client_get_device_id(client); struct adp5589_kpad *kpad; const struct adp5589_kpad_platform_data *pdata = dev_get_platdata(&client->dev); unsigned int revid; int error, ret; if (!i2c_check_functionality(client->adapter, I2C_FUNC_SMBUS_BYTE_DATA)) { dev_err(&client->dev, ""SMBUS Byte Data not Supported\n""); return -EIO; } if (!pdata) { dev_err(&client->dev, ""no platform data?\n""); return -EINVAL; } kpad = devm_kzalloc(&client->dev, sizeof(*kpad), GFP_KERNEL); if (!kpad) return -ENOMEM; kpad->client = client; switch (id->driver_data) { case ADP5585_02: kpad->support_row5 = true; fallthrough; case ADP5585_01: kpad->is_adp5585 = true; kpad->var = &const_adp5585; break; case ADP5589: kpad->support_row5 = true; kpad->var = &const_adp5589; break; } error = devm_add_action_or_reset(&client->dev, adp5589_clear_config, kpad); if (error) return error; ret = adp5589_read(client, ADP5589_5_ID); if (ret < 0) return ret; revid = (u8) ret & ADP5589_5_DEVICE_ID_MASK; if (pdata->keymapsize) { error = adp5589_keypad_add(kpad, revid); if (error) return error; } error = adp5589_setup(kpad); if (error) return error; if (kpad->gpimapsize) adp5589_report_switch_state(kpad); error = adp5589_gpio_add(kpad); if (error) return error; dev_info(&client->dev, ""Rev.%d keypad, irq %d\n"", revid, client->irq); return 0; }"
476----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47720/bad/dcn30_hwseq.c----dcn30_set_output_transfer_func,"bool dcn30_set_output_transfer_func(struct dc *dc, struct pipe_ctx *pipe_ctx, const struct dc_stream_state *stream) { int mpcc_id = pipe_ctx->plane_res.hubp->inst; struct mpc *mpc = pipe_ctx->stream_res.opp->ctx->dc->res_pool->mpc; struct pwl_params *params = NULL; bool ret = false; if (pipe_ctx->top_pipe == NULL) { ret = dcn30_set_mpc_shaper_3dlut(pipe_ctx, stream); if (ret == false && mpc->funcs->set_output_gamma && stream->out_transfer_func) { if (stream->out_transfer_func->type == TF_TYPE_HWPWL) params = &stream->out_transfer_func->pwl; else if (pipe_ctx->stream->out_transfer_func->type == TF_TYPE_DISTRIBUTED_POINTS && cm3_helper_translate_curve_to_hw_format( stream->out_transfer_func, &mpc->blender_params, false)) params = &mpc->blender_params; if (stream->out_transfer_func->type == TF_TYPE_PREDEFINED) BREAK_TO_DEBUGGER(); } } <S2SV_StartVul> mpc->funcs->set_output_gamma(mpc, mpcc_id, params); <S2SV_EndVul> return ret; }","- mpc->funcs->set_output_gamma(mpc, mpcc_id, params);
+ if (mpc->funcs->set_output_gamma)
+ mpc->funcs->set_output_gamma(mpc, mpcc_id, params);
+ else
+ DC_LOG_ERROR(""%s: set_output_gamma function pointer is NULL.\n"", __func__);","bool dcn30_set_output_transfer_func(struct dc *dc, struct pipe_ctx *pipe_ctx, const struct dc_stream_state *stream) { int mpcc_id = pipe_ctx->plane_res.hubp->inst; struct mpc *mpc = pipe_ctx->stream_res.opp->ctx->dc->res_pool->mpc; struct pwl_params *params = NULL; bool ret = false; if (pipe_ctx->top_pipe == NULL) { ret = dcn30_set_mpc_shaper_3dlut(pipe_ctx, stream); if (ret == false && mpc->funcs->set_output_gamma && stream->out_transfer_func) { if (stream->out_transfer_func->type == TF_TYPE_HWPWL) params = &stream->out_transfer_func->pwl; else if (pipe_ctx->stream->out_transfer_func->type == TF_TYPE_DISTRIBUTED_POINTS && cm3_helper_translate_curve_to_hw_format( stream->out_transfer_func, &mpc->blender_params, false)) params = &mpc->blender_params; if (stream->out_transfer_func->type == TF_TYPE_PREDEFINED) BREAK_TO_DEBUGGER(); } } if (mpc->funcs->set_output_gamma) mpc->funcs->set_output_gamma(mpc, mpcc_id, params); else DC_LOG_ERROR(""%s: set_output_gamma function pointer is NULL.\n"", __func__); return ret; }"
1030----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50271/bad/signal.c----__sigqueue_alloc,"__sigqueue_alloc(int sig, struct task_struct *t, gfp_t gfp_flags, int override_rlimit, const unsigned int sigqueue_flags) { struct sigqueue *q = NULL; struct ucounts *ucounts; long sigpending; rcu_read_lock(); ucounts = task_ucounts(t); <S2SV_StartVul> sigpending = inc_rlimit_get_ucounts(ucounts, UCOUNT_RLIMIT_SIGPENDING); <S2SV_EndVul> rcu_read_unlock(); if (!sigpending) return NULL; if (override_rlimit || likely(sigpending <= task_rlimit(t, RLIMIT_SIGPENDING))) { q = kmem_cache_alloc(sigqueue_cachep, gfp_flags); } else { print_dropped_signal(sig); } if (unlikely(q == NULL)) { dec_rlimit_put_ucounts(ucounts, UCOUNT_RLIMIT_SIGPENDING); } else { INIT_LIST_HEAD(&q->list); q->flags = sigqueue_flags; q->ucounts = ucounts; } return q; }","- sigpending = inc_rlimit_get_ucounts(ucounts, UCOUNT_RLIMIT_SIGPENDING);
+ sigpending = inc_rlimit_get_ucounts(ucounts, UCOUNT_RLIMIT_SIGPENDING,
+ override_rlimit);","__sigqueue_alloc(int sig, struct task_struct *t, gfp_t gfp_flags, int override_rlimit, const unsigned int sigqueue_flags) { struct sigqueue *q = NULL; struct ucounts *ucounts; long sigpending; rcu_read_lock(); ucounts = task_ucounts(t); sigpending = inc_rlimit_get_ucounts(ucounts, UCOUNT_RLIMIT_SIGPENDING, override_rlimit); rcu_read_unlock(); if (!sigpending) return NULL; if (override_rlimit || likely(sigpending <= task_rlimit(t, RLIMIT_SIGPENDING))) { q = kmem_cache_alloc(sigqueue_cachep, gfp_flags); } else { print_dropped_signal(sig); } if (unlikely(q == NULL)) { dec_rlimit_put_ucounts(ucounts, UCOUNT_RLIMIT_SIGPENDING); } else { INIT_LIST_HEAD(&q->list); q->flags = sigqueue_flags; q->ucounts = ucounts; } return q; }"
1537----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2025-21688/bad/v3d_irq.c----v3d_irq,"v3d_irq(int irq, void *arg) { struct v3d_dev *v3d = arg; u32 intsts; irqreturn_t status = IRQ_NONE; intsts = V3D_CORE_READ(0, V3D_CTL_INT_STS); V3D_CORE_WRITE(0, V3D_CTL_INT_CLR, intsts); if (intsts & V3D_INT_OUTOMEM) { schedule_work(&v3d->overflow_mem_work); status = IRQ_HANDLED; } if (intsts & V3D_INT_FLDONE) { struct v3d_fence *fence = to_v3d_fence(v3d->bin_job->base.irq_fence); trace_v3d_bcl_irq(&v3d->drm, fence->seqno); <S2SV_StartVul> dma_fence_signal(&fence->base); <S2SV_EndVul> v3d->bin_job = NULL; status = IRQ_HANDLED; } if (intsts & V3D_INT_FRDONE) { struct v3d_fence *fence = to_v3d_fence(v3d->render_job->base.irq_fence); trace_v3d_rcl_irq(&v3d->drm, fence->seqno); <S2SV_StartVul> dma_fence_signal(&fence->base); <S2SV_EndVul> v3d->render_job = NULL; status = IRQ_HANDLED; } if (intsts & V3D_INT_CSDDONE) { struct v3d_fence *fence = to_v3d_fence(v3d->csd_job->base.irq_fence); trace_v3d_csd_irq(&v3d->drm, fence->seqno); <S2SV_StartVul> dma_fence_signal(&fence->base); <S2SV_EndVul> v3d->csd_job = NULL; status = IRQ_HANDLED; } if (intsts & V3D_INT_GMPV) dev_err(v3d->dev, ""GMP violation\n""); if (v3d->single_irq_line && status == IRQ_NONE) return v3d_hub_irq(irq, arg); return status; }","- dma_fence_signal(&fence->base);
- dma_fence_signal(&fence->base);
- dma_fence_signal(&fence->base);
+ dma_fence_signal(&fence->base);
+ dma_fence_signal(&fence->base);
+ dma_fence_signal(&fence->base);","v3d_irq(int irq, void *arg) { struct v3d_dev *v3d = arg; u32 intsts; irqreturn_t status = IRQ_NONE; intsts = V3D_CORE_READ(0, V3D_CTL_INT_STS); V3D_CORE_WRITE(0, V3D_CTL_INT_CLR, intsts); if (intsts & V3D_INT_OUTOMEM) { schedule_work(&v3d->overflow_mem_work); status = IRQ_HANDLED; } if (intsts & V3D_INT_FLDONE) { struct v3d_fence *fence = to_v3d_fence(v3d->bin_job->base.irq_fence); trace_v3d_bcl_irq(&v3d->drm, fence->seqno); v3d->bin_job = NULL; dma_fence_signal(&fence->base); status = IRQ_HANDLED; } if (intsts & V3D_INT_FRDONE) { struct v3d_fence *fence = to_v3d_fence(v3d->render_job->base.irq_fence); trace_v3d_rcl_irq(&v3d->drm, fence->seqno); v3d->render_job = NULL; dma_fence_signal(&fence->base); status = IRQ_HANDLED; } if (intsts & V3D_INT_CSDDONE) { struct v3d_fence *fence = to_v3d_fence(v3d->csd_job->base.irq_fence); trace_v3d_csd_irq(&v3d->drm, fence->seqno); v3d->csd_job = NULL; dma_fence_signal(&fence->base); status = IRQ_HANDLED; } if (intsts & V3D_INT_GMPV) dev_err(v3d->dev, ""GMP violation\n""); if (v3d->single_irq_line && status == IRQ_NONE) return v3d_hub_irq(irq, arg); return status; }"
638----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49940/bad/l2tp_ppp.c----pppol2tp_session_setsockopt,"static int pppol2tp_session_setsockopt(struct sock *sk, struct l2tp_session *session, int optname, int val) { int err = 0; switch (optname) { case PPPOL2TP_SO_RECVSEQ: if (val != 0 && val != 1) { err = -EINVAL; break; } session->recv_seq = !!val; break; case PPPOL2TP_SO_SENDSEQ: if (val != 0 && val != 1) { err = -EINVAL; break; } session->send_seq = !!val; { struct pppox_sock *po = pppox_sk(sk); po->chan.hdrlen = val ? PPPOL2TP_L2TP_HDR_SIZE_SEQ : PPPOL2TP_L2TP_HDR_SIZE_NOSEQ; } <S2SV_StartVul> l2tp_session_set_header_len(session, session->tunnel->version); <S2SV_EndVul> break; case PPPOL2TP_SO_LNSMODE: if (val != 0 && val != 1) { err = -EINVAL; break; } session->lns_mode = !!val; break; case PPPOL2TP_SO_DEBUG: break; case PPPOL2TP_SO_REORDERTO: session->reorder_timeout = msecs_to_jiffies(val); break; default: err = -ENOPROTOOPT; break; } return err; }","- l2tp_session_set_header_len(session, session->tunnel->version);
+ l2tp_session_set_header_len(session, session->tunnel->version,
+ session->tunnel->encap);","static int pppol2tp_session_setsockopt(struct sock *sk, struct l2tp_session *session, int optname, int val) { int err = 0; switch (optname) { case PPPOL2TP_SO_RECVSEQ: if (val != 0 && val != 1) { err = -EINVAL; break; } session->recv_seq = !!val; break; case PPPOL2TP_SO_SENDSEQ: if (val != 0 && val != 1) { err = -EINVAL; break; } session->send_seq = !!val; { struct pppox_sock *po = pppox_sk(sk); po->chan.hdrlen = val ? PPPOL2TP_L2TP_HDR_SIZE_SEQ : PPPOL2TP_L2TP_HDR_SIZE_NOSEQ; } l2tp_session_set_header_len(session, session->tunnel->version, session->tunnel->encap); break; case PPPOL2TP_SO_LNSMODE: if (val != 0 && val != 1) { err = -EINVAL; break; } session->lns_mode = !!val; break; case PPPOL2TP_SO_DEBUG: break; case PPPOL2TP_SO_REORDERTO: session->reorder_timeout = msecs_to_jiffies(val); break; default: err = -ENOPROTOOPT; break; } return err; }"
934----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50183/bad/lpfc_vport.c----lpfc_vport_delete,"lpfc_vport_delete(struct fc_vport *fc_vport) { struct lpfc_nodelist *ndlp = NULL; struct lpfc_vport *vport = *(struct lpfc_vport **)fc_vport->dd_data; struct Scsi_Host *shost = lpfc_shost_from_vport(vport); struct lpfc_hba *phba = vport->phba; int rc; if (vport->port_type == LPFC_PHYSICAL_PORT) { lpfc_printf_vlog(vport, KERN_ERR, LOG_TRACE_EVENT, ""1812 vport_delete failed: Cannot delete "" ""physical host\n""); return VPORT_ERROR; } if ((vport->vport_flag & STATIC_VPORT) && !(phba->pport->load_flag & FC_UNLOADING)) { lpfc_printf_vlog(vport, KERN_ERR, LOG_TRACE_EVENT, ""1837 vport_delete failed: Cannot delete "" ""static vport.\n""); return VPORT_ERROR; } spin_lock_irq(&phba->hbalock); vport->load_flag |= FC_UNLOADING; spin_unlock_irq(&phba->hbalock); if (!(phba->pport->load_flag & FC_UNLOADING)) { int check_count = 0; while (check_count < ((phba->fc_ratov * 3) + 3) && vport->port_state > LPFC_VPORT_FAILED && vport->port_state < LPFC_VPORT_READY) { check_count++; msleep(1000); } if (vport->port_state > LPFC_VPORT_FAILED && vport->port_state < LPFC_VPORT_READY) return -EAGAIN; } if (!scsi_host_get(shost)) return VPORT_INVAL; lpfc_free_sysfs_attr(vport); lpfc_debugfs_terminate(vport); ndlp = lpfc_findnode_did(vport, Fabric_DID); if (!ndlp) goto skip_logo; if (ndlp && ndlp->nlp_state == NLP_STE_UNMAPPED_NODE && phba->link_state >= LPFC_LINK_UP && phba->fc_topology != LPFC_TOPOLOGY_LOOP) { if (vport->cfg_enable_da_id) { rc = lpfc_ns_cmd(vport, SLI_CTNS_DA_ID, 0, 0); <S2SV_StartVul> if (rc) { <S2SV_EndVul> <S2SV_StartVul> lpfc_printf_log(vport->phba, KERN_WARNING, <S2SV_EndVul> <S2SV_StartVul> LOG_VPORT, <S2SV_EndVul> <S2SV_StartVul> ""1829 CT command failed to "" <S2SV_EndVul> <S2SV_StartVul> ""delete objects on fabric, "" <S2SV_EndVul> <S2SV_StartVul> ""rc %d\n"", rc); <S2SV_EndVul> } } if (!(vport->vpi_state & LPFC_VPI_REGISTERED)) goto skip_logo; ndlp = lpfc_findnode_did(vport, Fabric_DID); if (!ndlp) goto skip_logo; rc = lpfc_send_npiv_logo(vport, ndlp); if (rc) goto skip_logo; } if (!(phba->pport->load_flag & FC_UNLOADING)) lpfc_discovery_wait(vport); skip_logo: fc_remove_host(shost); scsi_remove_host(shost); lpfc_cleanup(vport); lpfc_sli_host_down(vport); lpfc_stop_vport_timers(vport); if (!(phba->pport->load_flag & FC_UNLOADING)) { lpfc_unreg_all_rpis(vport); lpfc_unreg_default_rpis(vport); if (!(vport->vpi_state & LPFC_VPI_REGISTERED) || lpfc_mbx_unreg_vpi(vport)) scsi_host_put(shost); } else { scsi_host_put(shost); } lpfc_free_vpi(phba, vport->vpi); vport->work_port_events = 0; spin_lock_irq(&phba->port_list_lock); list_del_init(&vport->listentry); spin_unlock_irq(&phba->port_list_lock); lpfc_printf_vlog(vport, KERN_ERR, LOG_VPORT, ""1828 Vport Deleted.\n""); scsi_host_put(shost); return VPORT_OK; }","- if (rc) {
- lpfc_printf_log(vport->phba, KERN_WARNING,
- LOG_VPORT,
- ""1829 CT command failed to ""
- ""delete objects on fabric, ""
- ""rc %d\n"", rc);
+ DECLARE_WAIT_QUEUE_HEAD_ONSTACK(waitq);
+ ndlp = lpfc_findnode_did(vport, NameServer_DID);
+ if (!ndlp)
+ goto issue_logo;
+ spin_lock_irq(&ndlp->lock);
+ ndlp->da_id_waitq = &waitq;
+ ndlp->save_flags |= NLP_WAIT_FOR_DA_ID;
+ spin_unlock_irq(&ndlp->lock);
+ if (!rc) {
+ wait_event_timeout(waitq,
+ !(ndlp->save_flags & NLP_WAIT_FOR_DA_ID),
+ msecs_to_jiffies(phba->fc_ratov * 2000));
+ lpfc_printf_vlog(vport, KERN_INFO, LOG_VPORT | LOG_ELS,
+ ""1829 DA_ID issue status %d. ""
+ ""SFlag x%x NState x%x, NFlag x%x ""
+ ""Rpi x%x\n"",
+ rc, ndlp->save_flags, ndlp->nlp_state,
+ ndlp->nlp_flag, ndlp->nlp_rpi);
+ spin_lock_irq(&ndlp->lock);
+ ndlp->da_id_waitq = NULL;
+ ndlp->save_flags &= ~NLP_WAIT_FOR_DA_ID;
+ spin_unlock_irq(&ndlp->lock);
+ issue_logo:","lpfc_vport_delete(struct fc_vport *fc_vport) { struct lpfc_nodelist *ndlp = NULL; struct lpfc_vport *vport = *(struct lpfc_vport **)fc_vport->dd_data; struct Scsi_Host *shost = lpfc_shost_from_vport(vport); struct lpfc_hba *phba = vport->phba; int rc; DECLARE_WAIT_QUEUE_HEAD_ONSTACK(waitq); if (vport->port_type == LPFC_PHYSICAL_PORT) { lpfc_printf_vlog(vport, KERN_ERR, LOG_TRACE_EVENT, ""1812 vport_delete failed: Cannot delete "" ""physical host\n""); return VPORT_ERROR; } if ((vport->vport_flag & STATIC_VPORT) && !(phba->pport->load_flag & FC_UNLOADING)) { lpfc_printf_vlog(vport, KERN_ERR, LOG_TRACE_EVENT, ""1837 vport_delete failed: Cannot delete "" ""static vport.\n""); return VPORT_ERROR; } spin_lock_irq(&phba->hbalock); vport->load_flag |= FC_UNLOADING; spin_unlock_irq(&phba->hbalock); if (!(phba->pport->load_flag & FC_UNLOADING)) { int check_count = 0; while (check_count < ((phba->fc_ratov * 3) + 3) && vport->port_state > LPFC_VPORT_FAILED && vport->port_state < LPFC_VPORT_READY) { check_count++; msleep(1000); } if (vport->port_state > LPFC_VPORT_FAILED && vport->port_state < LPFC_VPORT_READY) return -EAGAIN; } if (!scsi_host_get(shost)) return VPORT_INVAL; lpfc_free_sysfs_attr(vport); lpfc_debugfs_terminate(vport); ndlp = lpfc_findnode_did(vport, Fabric_DID); if (!ndlp) goto skip_logo; if (ndlp && ndlp->nlp_state == NLP_STE_UNMAPPED_NODE && phba->link_state >= LPFC_LINK_UP && phba->fc_topology != LPFC_TOPOLOGY_LOOP) { if (vport->cfg_enable_da_id) { ndlp = lpfc_findnode_did(vport, NameServer_DID); if (!ndlp) goto issue_logo; spin_lock_irq(&ndlp->lock); ndlp->da_id_waitq = &waitq; ndlp->save_flags |= NLP_WAIT_FOR_DA_ID; spin_unlock_irq(&ndlp->lock); rc = lpfc_ns_cmd(vport, SLI_CTNS_DA_ID, 0, 0); if (!rc) { wait_event_timeout(waitq, !(ndlp->save_flags & NLP_WAIT_FOR_DA_ID), msecs_to_jiffies(phba->fc_ratov * 2000)); } lpfc_printf_vlog(vport, KERN_INFO, LOG_VPORT | LOG_ELS, ""1829 DA_ID issue status %d. "" ""SFlag x%x NState x%x, NFlag x%x "" ""Rpi x%x\n"", rc, ndlp->save_flags, ndlp->nlp_state, ndlp->nlp_flag, ndlp->nlp_rpi); spin_lock_irq(&ndlp->lock); ndlp->da_id_waitq = NULL; ndlp->save_flags &= ~NLP_WAIT_FOR_DA_ID; spin_unlock_irq(&ndlp->lock); } issue_logo: if (!(vport->vpi_state & LPFC_VPI_REGISTERED)) goto skip_logo; ndlp = lpfc_findnode_did(vport, Fabric_DID); if (!ndlp) goto skip_logo; rc = lpfc_send_npiv_logo(vport, ndlp); if (rc) goto skip_logo; } if (!(phba->pport->load_flag & FC_UNLOADING)) lpfc_discovery_wait(vport); skip_logo: fc_remove_host(shost); scsi_remove_host(shost); lpfc_cleanup(vport); lpfc_sli_host_down(vport); lpfc_stop_vport_timers(vport); if (!(phba->pport->load_flag & FC_UNLOADING)) { lpfc_unreg_all_rpis(vport); lpfc_unreg_default_rpis(vport); if (!(vport->vpi_state & LPFC_VPI_REGISTERED) || lpfc_mbx_unreg_vpi(vport)) scsi_host_put(shost); } else { scsi_host_put(shost); } lpfc_free_vpi(phba, vport->vpi); vport->work_port_events = 0; spin_lock_irq(&phba->port_list_lock); list_del_init(&vport->listentry); spin_unlock_irq(&phba->port_list_lock); lpfc_printf_vlog(vport, KERN_ERR, LOG_VPORT, ""1828 Vport Deleted.\n""); scsi_host_put(shost); return VPORT_OK; }"
1430----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56757/bad/btusb.c----btusb_mtk_reset,"static int btusb_mtk_reset(struct hci_dev *hdev, void *rst_data) { struct btusb_data *data = hci_get_drvdata(hdev); struct btmtk_data *btmtk_data = hci_get_priv(hdev); int err; if (test_and_set_bit(BTMTK_HW_RESET_ACTIVE, &btmtk_data->flags)) { bt_dev_err(hdev, ""last reset failed? Not resetting again""); return -EBUSY; } err = usb_autopm_get_interface(data->intf); if (err < 0) return err; if (test_bit(BTMTK_ISOPKT_RUNNING, &btmtk_data->flags)) <S2SV_StartVul> btusb_mtk_release_iso_intf(data); <S2SV_EndVul> btusb_stop_traffic(data); usb_kill_anchored_urbs(&data->tx_anchor); err = btmtk_usb_subsys_reset(hdev, btmtk_data->dev_id); usb_queue_reset_device(data->intf); clear_bit(BTMTK_HW_RESET_ACTIVE, &btmtk_data->flags); return err; }","- btusb_mtk_release_iso_intf(data);
+ btusb_mtk_release_iso_intf(hdev);","static int btusb_mtk_reset(struct hci_dev *hdev, void *rst_data) { struct btusb_data *data = hci_get_drvdata(hdev); struct btmtk_data *btmtk_data = hci_get_priv(hdev); int err; if (test_and_set_bit(BTMTK_HW_RESET_ACTIVE, &btmtk_data->flags)) { bt_dev_err(hdev, ""last reset failed? Not resetting again""); return -EBUSY; } err = usb_autopm_get_interface(data->intf); if (err < 0) return err; if (test_bit(BTMTK_ISOPKT_RUNNING, &btmtk_data->flags)) btusb_mtk_release_iso_intf(hdev); btusb_stop_traffic(data); usb_kill_anchored_urbs(&data->tx_anchor); err = btmtk_usb_subsys_reset(hdev, btmtk_data->dev_id); usb_queue_reset_device(data->intf); clear_bit(BTMTK_HW_RESET_ACTIVE, &btmtk_data->flags); return err; }"
444----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47682/bad/sd.c----sd_read_block_characteristics,"static void sd_read_block_characteristics(struct scsi_disk *sdkp, struct queue_limits *lim) { struct scsi_vpd *vpd; u16 rot; rcu_read_lock(); vpd = rcu_dereference(sdkp->device->vpd_pgb1); <S2SV_StartVul> if (!vpd || vpd->len < 8) { <S2SV_EndVul> rcu_read_unlock(); return; } rot = get_unaligned_be16(&vpd->data[4]); sdkp->zoned = (vpd->data[8] >> 4) & 3; rcu_read_unlock(); if (rot == 1) lim->features &= ~(BLK_FEAT_ROTATIONAL | BLK_FEAT_ADD_RANDOM); if (!sdkp->first_scan) return; if (sdkp->device->type == TYPE_ZBC) sd_printk(KERN_NOTICE, sdkp, ""Host-managed zoned block device\n""); else if (sdkp->zoned == 1) sd_printk(KERN_NOTICE, sdkp, ""Host-aware SMR disk used as regular disk\n""); else if (sdkp->zoned == 2) sd_printk(KERN_NOTICE, sdkp, ""Drive-managed SMR disk\n""); }","- if (!vpd || vpd->len < 8) {
+ if (!vpd || vpd->len <= 8) {","static void sd_read_block_characteristics(struct scsi_disk *sdkp, struct queue_limits *lim) { struct scsi_vpd *vpd; u16 rot; rcu_read_lock(); vpd = rcu_dereference(sdkp->device->vpd_pgb1); if (!vpd || vpd->len <= 8) { rcu_read_unlock(); return; } rot = get_unaligned_be16(&vpd->data[4]); sdkp->zoned = (vpd->data[8] >> 4) & 3; rcu_read_unlock(); if (rot == 1) lim->features &= ~(BLK_FEAT_ROTATIONAL | BLK_FEAT_ADD_RANDOM); if (!sdkp->first_scan) return; if (sdkp->device->type == TYPE_ZBC) sd_printk(KERN_NOTICE, sdkp, ""Host-managed zoned block device\n""); else if (sdkp->zoned == 1) sd_printk(KERN_NOTICE, sdkp, ""Host-aware SMR disk used as regular disk\n""); else if (sdkp->zoned == 2) sd_printk(KERN_NOTICE, sdkp, ""Drive-managed SMR disk\n""); }"
1070----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53054/bad/cgroup.c----cgroup_bpf_release_fn,"static void cgroup_bpf_release_fn(struct percpu_ref *ref) { struct cgroup *cgrp = container_of(ref, struct cgroup, bpf.refcnt); INIT_WORK(&cgrp->bpf.release_work, cgroup_bpf_release); <S2SV_StartVul> queue_work(system_wq, &cgrp->bpf.release_work); <S2SV_EndVul> }","- queue_work(system_wq, &cgrp->bpf.release_work);
+ queue_work(cgroup_bpf_destroy_wq, &cgrp->bpf.release_work);
+ }","static void cgroup_bpf_release_fn(struct percpu_ref *ref) { struct cgroup *cgrp = container_of(ref, struct cgroup, bpf.refcnt); INIT_WORK(&cgrp->bpf.release_work, cgroup_bpf_release); queue_work(cgroup_bpf_destroy_wq, &cgrp->bpf.release_work); }"
1326----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56642/bad/udp_media.c----cleanup_bearer,"static void cleanup_bearer(struct work_struct *work) { struct udp_bearer *ub = container_of(work, struct udp_bearer, work); struct udp_replicast *rcast, *tmp; list_for_each_entry_safe(rcast, tmp, &ub->rcast.list, list) { dst_cache_destroy(&rcast->dst_cache); list_del_rcu(&rcast->list); kfree_rcu(rcast, rcu); } <S2SV_StartVul> atomic_dec(&tipc_net(sock_net(ub->ubsock->sk))->wq_count); <S2SV_EndVul> dst_cache_destroy(&ub->rcast.dst_cache); udp_tunnel_sock_release(ub->ubsock); synchronize_net(); kfree(ub); }","- atomic_dec(&tipc_net(sock_net(ub->ubsock->sk))->wq_count);
+ atomic_dec(&tipc_net(sock_net(ub->ubsock->sk))->wq_count);","static void cleanup_bearer(struct work_struct *work) { struct udp_bearer *ub = container_of(work, struct udp_bearer, work); struct udp_replicast *rcast, *tmp; list_for_each_entry_safe(rcast, tmp, &ub->rcast.list, list) { dst_cache_destroy(&rcast->dst_cache); list_del_rcu(&rcast->list); kfree_rcu(rcast, rcu); } dst_cache_destroy(&ub->rcast.dst_cache); udp_tunnel_sock_release(ub->ubsock); synchronize_net(); atomic_dec(&tipc_net(sock_net(ub->ubsock->sk))->wq_count); kfree(ub); }"
2----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-43890/bad/tracing_map.c----*get_free_elt,"static struct tracing_map_elt *get_free_elt(struct tracing_map *map) { struct tracing_map_elt *elt = NULL; int idx; <S2SV_StartVul> idx = atomic_inc_return(&map->next_elt); <S2SV_EndVul> if (idx < map->max_elts) { elt = *(TRACING_MAP_ELT(map->elts, idx)); if (map->ops && map->ops->elt_init) map->ops->elt_init(elt); } return elt; }","- idx = atomic_inc_return(&map->next_elt);
+ idx = atomic_fetch_add_unless(&map->next_elt, 1, map->max_elts);","static struct tracing_map_elt *get_free_elt(struct tracing_map *map) { struct tracing_map_elt *elt = NULL; int idx; idx = atomic_fetch_add_unless(&map->next_elt, 1, map->max_elts); if (idx < map->max_elts) { elt = *(TRACING_MAP_ELT(map->elts, idx)); if (map->ops && map->ops->elt_init) map->ops->elt_init(elt); } return elt; }"
29----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-43902/bad/amdgpu_dm.c----dm_suspend,"static int dm_suspend(void *handle) { struct amdgpu_device *adev = handle; struct amdgpu_display_manager *dm = &adev->dm; int ret = 0; if (amdgpu_in_reset(adev)) { mutex_lock(&dm->dc_lock); #if defined(CONFIG_DRM_AMD_DC_DCN) dc_allow_idle_optimizations(adev->dm.dc, false); #endif dm->cached_dc_state = dc_copy_state(dm->dc->current_state); <S2SV_StartVul> dm_gpureset_toggle_interrupts(adev, dm->cached_dc_state, false); <S2SV_EndVul> amdgpu_dm_commit_zero_streams(dm->dc); amdgpu_dm_irq_suspend(adev); hpd_rx_irq_work_suspend(dm); return ret; } WARN_ON(adev->dm.cached_state); adev->dm.cached_state = drm_atomic_helper_suspend(adev_to_drm(adev)); s3_handle_mst(adev_to_drm(adev), true); amdgpu_dm_irq_suspend(adev); hpd_rx_irq_work_suspend(dm); dc_set_power_state(dm->dc, DC_ACPI_CM_POWER_STATE_D3); return 0; }","- dm_gpureset_toggle_interrupts(adev, dm->cached_dc_state, false);
+ if (dm->cached_dc_state)
+ dm_gpureset_toggle_interrupts(adev, dm->cached_dc_state, false);","static int dm_suspend(void *handle) { struct amdgpu_device *adev = handle; struct amdgpu_display_manager *dm = &adev->dm; int ret = 0; if (amdgpu_in_reset(adev)) { mutex_lock(&dm->dc_lock); #if defined(CONFIG_DRM_AMD_DC_DCN) dc_allow_idle_optimizations(adev->dm.dc, false); #endif dm->cached_dc_state = dc_copy_state(dm->dc->current_state); if (dm->cached_dc_state) dm_gpureset_toggle_interrupts(adev, dm->cached_dc_state, false); amdgpu_dm_commit_zero_streams(dm->dc); amdgpu_dm_irq_suspend(adev); hpd_rx_irq_work_suspend(dm); return ret; } WARN_ON(adev->dm.cached_state); adev->dm.cached_state = drm_atomic_helper_suspend(adev_to_drm(adev)); s3_handle_mst(adev_to_drm(adev), true); amdgpu_dm_irq_suspend(adev); hpd_rx_irq_work_suspend(dm); dc_set_power_state(dm->dc, DC_ACPI_CM_POWER_STATE_D3); return 0; }"
63----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44943/bad/gup.c----gup_huge_pgd,"static int gup_huge_pgd(pgd_t orig, pgd_t *pgdp, unsigned long addr, unsigned long end, unsigned int flags, struct page **pages, int *nr) { int refs; struct page *page; struct folio *folio; if (!pgd_access_permitted(orig, flags & FOLL_WRITE)) return 0; BUILD_BUG_ON(pgd_devmap(orig)); page = nth_page(pgd_page(orig), (addr & ~PGDIR_MASK) >> PAGE_SHIFT); refs = record_subpages(page, addr, end, pages + *nr); <S2SV_StartVul> folio = try_grab_folio(page, refs, flags); <S2SV_EndVul> <S2SV_StartVul> if (!folio) <S2SV_EndVul> return 0; if (unlikely(pgd_val(orig) != pgd_val(*pgdp))) { gup_put_folio(folio, refs, flags); return 0; } if (!pgd_write(orig) && gup_must_unshare(NULL, flags, &folio->page)) { gup_put_folio(folio, refs, flags); return 0; } if (!folio_fast_pin_allowed(folio, flags)) { gup_put_folio(folio, refs, flags); return 0; } *nr += refs; folio_set_referenced(folio); return 1; }","- folio = try_grab_folio(page, refs, flags);
- if (!folio)
+ folio = try_grab_folio_fast(page, refs, flags);
+ if (!folio)","static int gup_huge_pgd(pgd_t orig, pgd_t *pgdp, unsigned long addr, unsigned long end, unsigned int flags, struct page **pages, int *nr) { int refs; struct page *page; struct folio *folio; if (!pgd_access_permitted(orig, flags & FOLL_WRITE)) return 0; BUILD_BUG_ON(pgd_devmap(orig)); page = nth_page(pgd_page(orig), (addr & ~PGDIR_MASK) >> PAGE_SHIFT); refs = record_subpages(page, addr, end, pages + *nr); folio = try_grab_folio_fast(page, refs, flags); if (!folio) return 0; if (unlikely(pgd_val(orig) != pgd_val(*pgdp))) { gup_put_folio(folio, refs, flags); return 0; } if (!pgd_write(orig) && gup_must_unshare(NULL, flags, &folio->page)) { gup_put_folio(folio, refs, flags); return 0; } if (!folio_fast_pin_allowed(folio, flags)) { gup_put_folio(folio, refs, flags); return 0; } *nr += refs; folio_set_referenced(folio); return 1; }"
205----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46689/bad/cmd-db.c----cmd_db_dev_probe,"static int cmd_db_dev_probe(struct platform_device *pdev) { struct reserved_mem *rmem; int ret = 0; rmem = of_reserved_mem_lookup(pdev->dev.of_node); if (!rmem) { dev_err(&pdev->dev, ""failed to acquire memory region\n""); return -EINVAL; } <S2SV_StartVul> cmd_db_header = memremap(rmem->base, rmem->size, MEMREMAP_WB); <S2SV_EndVul> if (!cmd_db_header) { ret = -ENOMEM; cmd_db_header = NULL; return ret; } if (!cmd_db_magic_matches(cmd_db_header)) { dev_err(&pdev->dev, ""Invalid Command DB Magic\n""); return -EINVAL; } debugfs_create_file(""cmd-db"", 0400, NULL, NULL, &cmd_db_debugfs_ops); return 0; }","- cmd_db_header = memremap(rmem->base, rmem->size, MEMREMAP_WB);
+ cmd_db_header = memremap(rmem->base, rmem->size, MEMREMAP_WC);","static int cmd_db_dev_probe(struct platform_device *pdev) { struct reserved_mem *rmem; int ret = 0; rmem = of_reserved_mem_lookup(pdev->dev.of_node); if (!rmem) { dev_err(&pdev->dev, ""failed to acquire memory region\n""); return -EINVAL; } cmd_db_header = memremap(rmem->base, rmem->size, MEMREMAP_WC); if (!cmd_db_header) { ret = -ENOMEM; cmd_db_header = NULL; return ret; } if (!cmd_db_magic_matches(cmd_db_header)) { dev_err(&pdev->dev, ""Invalid Command DB Magic\n""); return -EINVAL; } debugfs_create_file(""cmd-db"", 0400, NULL, NULL, &cmd_db_debugfs_ops); return 0; }"
426----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47667/bad/pci-keystone.c----ks_pcie_quirk,"static void ks_pcie_quirk(struct pci_dev *dev) { struct pci_bus *bus = dev->bus; struct pci_dev *bridge; static const struct pci_device_id rc_pci_devids[] = { { PCI_DEVICE(PCI_VENDOR_ID_TI, PCIE_RC_K2HK), .class = PCI_CLASS_BRIDGE_PCI_NORMAL, .class_mask = ~0, }, { PCI_DEVICE(PCI_VENDOR_ID_TI, PCIE_RC_K2E), .class = PCI_CLASS_BRIDGE_PCI_NORMAL, .class_mask = ~0, }, { PCI_DEVICE(PCI_VENDOR_ID_TI, PCIE_RC_K2L), .class = PCI_CLASS_BRIDGE_PCI_NORMAL, .class_mask = ~0, }, { PCI_DEVICE(PCI_VENDOR_ID_TI, PCIE_RC_K2G), .class = PCI_CLASS_BRIDGE_PCI_NORMAL, .class_mask = ~0, }, { 0, }, }; if (pci_is_root_bus(bus)) bridge = dev; while (!pci_is_root_bus(bus)) { bridge = bus->self; bus = bus->parent; } if (!bridge) return; if (pci_match_id(rc_pci_devids, bridge)) { if (pcie_get_readrq(dev) > 256) { <S2SV_StartVul> dev_info(&dev->dev, ""limiting MRRS to 256\n""); <S2SV_EndVul> pcie_set_readrq(dev, 256); } } }","- dev_info(&dev->dev, ""limiting MRRS to 256\n"");
+ struct keystone_pcie *ks_pcie;
+ struct device *bridge_dev;
+ u32 val;
+ { 0, },
+ };
+ static const struct pci_device_id am6_pci_devids[] = {
+ { PCI_DEVICE(PCI_VENDOR_ID_TI, PCI_DEVICE_ID_TI_AM654X),
+ .class = PCI_CLASS_BRIDGE_PCI << 8, .class_mask = ~0, },
+ { 0, },
+ };
+ dev_info(&dev->dev, ""limiting MRRS to 256 bytes\n"");
+ }
+ }
+ if (pci_match_id(am6_pci_devids, bridge)) {
+ bridge_dev = pci_get_host_bridge_device(dev);
+ if (!bridge_dev && !bridge_dev->parent)
+ return;
+ ks_pcie = dev_get_drvdata(bridge_dev->parent);
+ if (!ks_pcie)
+ return;
+ val = ks_pcie_app_readl(ks_pcie, PID);
+ val &= RTL;
+ val >>= RTL_SHIFT;
+ if (val != AM6_PCI_PG1_RTL_VER)
+ return;
+ if (pcie_get_readrq(dev) > 128) {
+ dev_info(&dev->dev, ""limiting MRRS to 128 bytes\n"");
+ pcie_set_readrq(dev, 128);
+ }
+ }
+ }","static void ks_pcie_quirk(struct pci_dev *dev) { struct pci_bus *bus = dev->bus; struct keystone_pcie *ks_pcie; struct device *bridge_dev; struct pci_dev *bridge; u32 val; static const struct pci_device_id rc_pci_devids[] = { { PCI_DEVICE(PCI_VENDOR_ID_TI, PCIE_RC_K2HK), .class = PCI_CLASS_BRIDGE_PCI_NORMAL, .class_mask = ~0, }, { PCI_DEVICE(PCI_VENDOR_ID_TI, PCIE_RC_K2E), .class = PCI_CLASS_BRIDGE_PCI_NORMAL, .class_mask = ~0, }, { PCI_DEVICE(PCI_VENDOR_ID_TI, PCIE_RC_K2L), .class = PCI_CLASS_BRIDGE_PCI_NORMAL, .class_mask = ~0, }, { PCI_DEVICE(PCI_VENDOR_ID_TI, PCIE_RC_K2G), .class = PCI_CLASS_BRIDGE_PCI_NORMAL, .class_mask = ~0, }, { 0, }, }; static const struct pci_device_id am6_pci_devids[] = { { PCI_DEVICE(PCI_VENDOR_ID_TI, PCI_DEVICE_ID_TI_AM654X), .class = PCI_CLASS_BRIDGE_PCI << 8, .class_mask = ~0, }, { 0, }, }; if (pci_is_root_bus(bus)) bridge = dev; while (!pci_is_root_bus(bus)) { bridge = bus->self; bus = bus->parent; } if (!bridge) return; if (pci_match_id(rc_pci_devids, bridge)) { if (pcie_get_readrq(dev) > 256) { dev_info(&dev->dev, ""limiting MRRS to 256 bytes\n""); pcie_set_readrq(dev, 256); } } if (pci_match_id(am6_pci_devids, bridge)) { bridge_dev = pci_get_host_bridge_device(dev); if (!bridge_dev && !bridge_dev->parent) return; ks_pcie = dev_get_drvdata(bridge_dev->parent); if (!ks_pcie) return; val = ks_pcie_app_readl(ks_pcie, PID); val &= RTL; val >>= RTL_SHIFT; if (val != AM6_PCI_PG1_RTL_VER) return; if (pcie_get_readrq(dev) > 128) { dev_info(&dev->dev, ""limiting MRRS to 128 bytes\n""); pcie_set_readrq(dev, 128); } } }"
1168----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53140/bad/af_netlink.c----netlink_release,"static int netlink_release(struct socket *sock) { struct sock *sk = sock->sk; struct netlink_sock *nlk; if (!sk) return 0; netlink_remove(sk); sock_orphan(sk); nlk = nlk_sk(sk); if (nlk->netlink_unbind) { int i; for (i = 0; i < nlk->ngroups; i++) if (test_bit(i, nlk->groups)) nlk->netlink_unbind(sock_net(sk), i + 1); } if (sk->sk_protocol == NETLINK_GENERIC && atomic_dec_return(&genl_sk_destructing_cnt) == 0) wake_up(&genl_sk_destructing_waitq); sock->sk = NULL; wake_up_interruptible_all(&nlk->wait); skb_queue_purge(&sk->sk_write_queue); if (nlk->portid && nlk->bound) { struct netlink_notify n = { .net = sock_net(sk), .protocol = sk->sk_protocol, .portid = nlk->portid, }; blocking_notifier_call_chain(&netlink_chain, NETLINK_URELEASE, &n); <S2SV_StartVul> } <S2SV_EndVul> module_put(nlk->module); if (netlink_is_kernel(sk)) { netlink_table_grab(); BUG_ON(nl_table[sk->sk_protocol].registered == 0); if (--nl_table[sk->sk_protocol].registered == 0) { struct listeners *old; old = nl_deref_protected(nl_table[sk->sk_protocol].listeners); RCU_INIT_POINTER(nl_table[sk->sk_protocol].listeners, NULL); kfree_rcu(old, rcu); nl_table[sk->sk_protocol].module = NULL; nl_table[sk->sk_protocol].bind = NULL; nl_table[sk->sk_protocol].unbind = NULL; nl_table[sk->sk_protocol].flags = 0; nl_table[sk->sk_protocol].registered = 0; } netlink_table_ungrab(); } sock_prot_inuse_add(sock_net(sk), &netlink_proto, -1); call_rcu(&nlk->rcu, deferred_put_nlk_sk); return 0; }","- }
+ }
+ if (nlk->cb_running) {
+ if (nlk->cb.done)
+ nlk->cb.done(&nlk->cb);
+ module_put(nlk->cb.module);
+ kfree_skb(nlk->cb.skb);
+ }","static int netlink_release(struct socket *sock) { struct sock *sk = sock->sk; struct netlink_sock *nlk; if (!sk) return 0; netlink_remove(sk); sock_orphan(sk); nlk = nlk_sk(sk); if (nlk->netlink_unbind) { int i; for (i = 0; i < nlk->ngroups; i++) if (test_bit(i, nlk->groups)) nlk->netlink_unbind(sock_net(sk), i + 1); } if (sk->sk_protocol == NETLINK_GENERIC && atomic_dec_return(&genl_sk_destructing_cnt) == 0) wake_up(&genl_sk_destructing_waitq); sock->sk = NULL; wake_up_interruptible_all(&nlk->wait); skb_queue_purge(&sk->sk_write_queue); if (nlk->portid && nlk->bound) { struct netlink_notify n = { .net = sock_net(sk), .protocol = sk->sk_protocol, .portid = nlk->portid, }; blocking_notifier_call_chain(&netlink_chain, NETLINK_URELEASE, &n); } if (nlk->cb_running) { if (nlk->cb.done) nlk->cb.done(&nlk->cb); module_put(nlk->cb.module); kfree_skb(nlk->cb.skb); } module_put(nlk->module); if (netlink_is_kernel(sk)) { netlink_table_grab(); BUG_ON(nl_table[sk->sk_protocol].registered == 0); if (--nl_table[sk->sk_protocol].registered == 0) { struct listeners *old; old = nl_deref_protected(nl_table[sk->sk_protocol].listeners); RCU_INIT_POINTER(nl_table[sk->sk_protocol].listeners, NULL); kfree_rcu(old, rcu); nl_table[sk->sk_protocol].module = NULL; nl_table[sk->sk_protocol].bind = NULL; nl_table[sk->sk_protocol].unbind = NULL; nl_table[sk->sk_protocol].flags = 0; nl_table[sk->sk_protocol].registered = 0; } netlink_table_ungrab(); } sock_prot_inuse_add(sock_net(sk), &netlink_proto, -1); call_rcu(&nlk->rcu, deferred_put_nlk_sk); return 0; }"
1265----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56549/bad/interface.c----cachefiles_clean_up_object,"static void cachefiles_clean_up_object(struct cachefiles_object *object, struct cachefiles_cache *cache) { if (test_bit(FSCACHE_COOKIE_RETIRED, &object->cookie->flags)) { if (!test_bit(CACHEFILES_OBJECT_USING_TMPFILE, &object->flags)) { cachefiles_see_object(object, cachefiles_obj_see_clean_delete); _debug(""- inval object OBJ%x"", object->debug_id); cachefiles_delete_object(object, FSCACHE_OBJECT_WAS_RETIRED); } else { cachefiles_see_object(object, cachefiles_obj_see_clean_drop_tmp); _debug(""- inval object OBJ%x tmpfile"", object->debug_id); } } else { cachefiles_see_object(object, cachefiles_obj_see_clean_commit); cachefiles_commit_object(object, cache); } cachefiles_unmark_inode_in_use(object, object->file); <S2SV_StartVul> if (object->file) { <S2SV_EndVul> <S2SV_StartVul> fput(object->file); <S2SV_EndVul> <S2SV_StartVul> object->file = NULL; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul>","- if (object->file) {
- fput(object->file);
- object->file = NULL;
- }
- }
+ struct file *file;
+ spin_lock(&object->lock);
+ file = object->file;
+ object->file = NULL;
+ spin_unlock(&object->lock);
+ if (file)
+ fput(file);","static void cachefiles_clean_up_object(struct cachefiles_object *object, struct cachefiles_cache *cache) { struct file *file; if (test_bit(FSCACHE_COOKIE_RETIRED, &object->cookie->flags)) { if (!test_bit(CACHEFILES_OBJECT_USING_TMPFILE, &object->flags)) { cachefiles_see_object(object, cachefiles_obj_see_clean_delete); _debug(""- inval object OBJ%x"", object->debug_id); cachefiles_delete_object(object, FSCACHE_OBJECT_WAS_RETIRED); } else { cachefiles_see_object(object, cachefiles_obj_see_clean_drop_tmp); _debug(""- inval object OBJ%x tmpfile"", object->debug_id); } } else { cachefiles_see_object(object, cachefiles_obj_see_clean_commit); cachefiles_commit_object(object, cache); } cachefiles_unmark_inode_in_use(object, object->file); spin_lock(&object->lock); file = object->file; object->file = NULL; spin_unlock(&object->lock); if (file) fput(file); }"
1522----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2025-21680/bad/pktgen.c----get_imix_entries,"static ssize_t get_imix_entries(const char __user *buffer, struct pktgen_dev *pkt_dev) { const int max_digits = 10; int i = 0; long len; char c; pkt_dev->n_imix_entries = 0; do { unsigned long weight; unsigned long size; len = num_arg(&buffer[i], max_digits, &size); if (len < 0) return len; i += len; if (get_user(c, &buffer[i])) return -EFAULT; if (c != ',') return -EINVAL; i++; if (size < 14 + 20 + 8) size = 14 + 20 + 8; len = num_arg(&buffer[i], max_digits, &weight); if (len < 0) return len; if (weight <= 0) return -EINVAL; pkt_dev->imix_entries[pkt_dev->n_imix_entries].size = size; pkt_dev->imix_entries[pkt_dev->n_imix_entries].weight = weight; i += len; if (get_user(c, &buffer[i])) return -EFAULT; i++; pkt_dev->n_imix_entries++; <S2SV_StartVul> if (pkt_dev->n_imix_entries > MAX_IMIX_ENTRIES) <S2SV_EndVul> <S2SV_StartVul> return -E2BIG; <S2SV_EndVul> } while (c == ' '); return i; }","- if (pkt_dev->n_imix_entries > MAX_IMIX_ENTRIES)
- return -E2BIG;
+ if (pkt_dev->n_imix_entries >= MAX_IMIX_ENTRIES)
+ return -E2BIG;","static ssize_t get_imix_entries(const char __user *buffer, struct pktgen_dev *pkt_dev) { const int max_digits = 10; int i = 0; long len; char c; pkt_dev->n_imix_entries = 0; do { unsigned long weight; unsigned long size; if (pkt_dev->n_imix_entries >= MAX_IMIX_ENTRIES) return -E2BIG; len = num_arg(&buffer[i], max_digits, &size); if (len < 0) return len; i += len; if (get_user(c, &buffer[i])) return -EFAULT; if (c != ',') return -EINVAL; i++; if (size < 14 + 20 + 8) size = 14 + 20 + 8; len = num_arg(&buffer[i], max_digits, &weight); if (len < 0) return len; if (weight <= 0) return -EINVAL; pkt_dev->imix_entries[pkt_dev->n_imix_entries].size = size; pkt_dev->imix_entries[pkt_dev->n_imix_entries].weight = weight; i += len; if (get_user(c, &buffer[i])) return -EFAULT; i++; pkt_dev->n_imix_entries++; } while (c == ' '); return i; }"
422----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47663/bad/ad9834.c----ad9834_write_frequency,"static int ad9834_write_frequency(struct ad9834_state *st, unsigned long addr, unsigned long fout) { unsigned long clk_freq; unsigned long regval; clk_freq = clk_get_rate(st->mclk); <S2SV_StartVul> if (fout > (clk_freq / 2)) <S2SV_EndVul> return -EINVAL; regval = ad9834_calc_freqreg(clk_freq, fout); st->freq_data[0] = cpu_to_be16(addr | (regval & RES_MASK(AD9834_FREQ_BITS / 2))); st->freq_data[1] = cpu_to_be16(addr | ((regval >> (AD9834_FREQ_BITS / 2)) & RES_MASK(AD9834_FREQ_BITS / 2))); return spi_sync(st->spi, &st->freq_msg); }","- if (fout > (clk_freq / 2))
+ if (!clk_freq || fout > (clk_freq / 2))","static int ad9834_write_frequency(struct ad9834_state *st, unsigned long addr, unsigned long fout) { unsigned long clk_freq; unsigned long regval; clk_freq = clk_get_rate(st->mclk); if (!clk_freq || fout > (clk_freq / 2)) return -EINVAL; regval = ad9834_calc_freqreg(clk_freq, fout); st->freq_data[0] = cpu_to_be16(addr | (regval & RES_MASK(AD9834_FREQ_BITS / 2))); st->freq_data[1] = cpu_to_be16(addr | ((regval >> (AD9834_FREQ_BITS / 2)) & RES_MASK(AD9834_FREQ_BITS / 2))); return spi_sync(st->spi, &st->freq_msg); }"
160----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-45015/bad/dpu_encoder.c----dpu_encoder_virt_atomic_mode_set,"static void dpu_encoder_virt_atomic_mode_set(struct drm_encoder *drm_enc, struct drm_crtc_state *crtc_state, struct drm_connector_state *conn_state) { struct dpu_encoder_virt *dpu_enc; struct msm_drm_private *priv; struct dpu_kms *dpu_kms; struct dpu_crtc_state *cstate; struct dpu_global_state *global_state; struct dpu_hw_blk *hw_pp[MAX_CHANNELS_PER_ENC]; struct dpu_hw_blk *hw_ctl[MAX_CHANNELS_PER_ENC]; struct dpu_hw_blk *hw_lm[MAX_CHANNELS_PER_ENC]; struct dpu_hw_blk *hw_dspp[MAX_CHANNELS_PER_ENC] = { NULL }; struct dpu_hw_blk *hw_dsc[MAX_CHANNELS_PER_ENC]; int num_lm, num_ctl, num_pp, num_dsc; unsigned int dsc_mask = 0; int i; if (!drm_enc) { DPU_ERROR(""invalid encoder\n""); return; } dpu_enc = to_dpu_encoder_virt(drm_enc); DPU_DEBUG_ENC(dpu_enc, ""\n""); priv = drm_enc->dev->dev_private; dpu_kms = to_dpu_kms(priv->kms); global_state = dpu_kms_get_existing_global_state(dpu_kms); if (IS_ERR_OR_NULL(global_state)) { DPU_ERROR(""Failed to get global state""); return; } trace_dpu_enc_mode_set(DRMID(drm_enc)); num_pp = dpu_rm_get_assigned_resources(&dpu_kms->rm, global_state, drm_enc->base.id, DPU_HW_BLK_PINGPONG, hw_pp, ARRAY_SIZE(hw_pp)); num_ctl = dpu_rm_get_assigned_resources(&dpu_kms->rm, global_state, drm_enc->base.id, DPU_HW_BLK_CTL, hw_ctl, ARRAY_SIZE(hw_ctl)); num_lm = dpu_rm_get_assigned_resources(&dpu_kms->rm, global_state, drm_enc->base.id, DPU_HW_BLK_LM, hw_lm, ARRAY_SIZE(hw_lm)); dpu_rm_get_assigned_resources(&dpu_kms->rm, global_state, drm_enc->base.id, DPU_HW_BLK_DSPP, hw_dspp, ARRAY_SIZE(hw_dspp)); for (i = 0; i < MAX_CHANNELS_PER_ENC; i++) dpu_enc->hw_pp[i] = i < num_pp ? to_dpu_hw_pingpong(hw_pp[i]) : NULL; num_dsc = dpu_rm_get_assigned_resources(&dpu_kms->rm, global_state, drm_enc->base.id, DPU_HW_BLK_DSC, hw_dsc, ARRAY_SIZE(hw_dsc)); for (i = 0; i < num_dsc; i++) { dpu_enc->hw_dsc[i] = to_dpu_hw_dsc(hw_dsc[i]); dsc_mask |= BIT(dpu_enc->hw_dsc[i]->idx - DSC_0); } dpu_enc->dsc_mask = dsc_mask; cstate = to_dpu_crtc_state(crtc_state); for (i = 0; i < num_lm; i++) { int ctl_idx = (i < num_ctl) ? i : (num_ctl-1); cstate->mixers[i].hw_lm = to_dpu_hw_mixer(hw_lm[i]); cstate->mixers[i].lm_ctl = to_dpu_hw_ctl(hw_ctl[ctl_idx]); cstate->mixers[i].hw_dspp = to_dpu_hw_dspp(hw_dspp[i]); } cstate->num_mixers = num_lm; <S2SV_StartVul> dpu_enc->connector = conn_state->connector; <S2SV_EndVul> for (i = 0; i < dpu_enc->num_phys_encs; i++) { struct dpu_encoder_phys *phys = dpu_enc->phys_encs[i]; if (!dpu_enc->hw_pp[i]) { DPU_ERROR_ENC(dpu_enc, ""no pp block assigned at idx: %d\n"", i); return; } if (!hw_ctl[i]) { DPU_ERROR_ENC(dpu_enc, ""no ctl block assigned at idx: %d\n"", i); return; } phys->hw_pp = dpu_enc->hw_pp[i]; phys->hw_ctl = to_dpu_hw_ctl(hw_ctl[i]); phys->cached_mode = crtc_state->adjusted_mode; if (phys->ops.atomic_mode_set) phys->ops.atomic_mode_set(phys, crtc_state, conn_state); } }",- dpu_enc->connector = conn_state->connector;,"static void dpu_encoder_virt_atomic_mode_set(struct drm_encoder *drm_enc, struct drm_crtc_state *crtc_state, struct drm_connector_state *conn_state) { struct dpu_encoder_virt *dpu_enc; struct msm_drm_private *priv; struct dpu_kms *dpu_kms; struct dpu_crtc_state *cstate; struct dpu_global_state *global_state; struct dpu_hw_blk *hw_pp[MAX_CHANNELS_PER_ENC]; struct dpu_hw_blk *hw_ctl[MAX_CHANNELS_PER_ENC]; struct dpu_hw_blk *hw_lm[MAX_CHANNELS_PER_ENC]; struct dpu_hw_blk *hw_dspp[MAX_CHANNELS_PER_ENC] = { NULL }; struct dpu_hw_blk *hw_dsc[MAX_CHANNELS_PER_ENC]; int num_lm, num_ctl, num_pp, num_dsc; unsigned int dsc_mask = 0; int i; if (!drm_enc) { DPU_ERROR(""invalid encoder\n""); return; } dpu_enc = to_dpu_encoder_virt(drm_enc); DPU_DEBUG_ENC(dpu_enc, ""\n""); priv = drm_enc->dev->dev_private; dpu_kms = to_dpu_kms(priv->kms); global_state = dpu_kms_get_existing_global_state(dpu_kms); if (IS_ERR_OR_NULL(global_state)) { DPU_ERROR(""Failed to get global state""); return; } trace_dpu_enc_mode_set(DRMID(drm_enc)); num_pp = dpu_rm_get_assigned_resources(&dpu_kms->rm, global_state, drm_enc->base.id, DPU_HW_BLK_PINGPONG, hw_pp, ARRAY_SIZE(hw_pp)); num_ctl = dpu_rm_get_assigned_resources(&dpu_kms->rm, global_state, drm_enc->base.id, DPU_HW_BLK_CTL, hw_ctl, ARRAY_SIZE(hw_ctl)); num_lm = dpu_rm_get_assigned_resources(&dpu_kms->rm, global_state, drm_enc->base.id, DPU_HW_BLK_LM, hw_lm, ARRAY_SIZE(hw_lm)); dpu_rm_get_assigned_resources(&dpu_kms->rm, global_state, drm_enc->base.id, DPU_HW_BLK_DSPP, hw_dspp, ARRAY_SIZE(hw_dspp)); for (i = 0; i < MAX_CHANNELS_PER_ENC; i++) dpu_enc->hw_pp[i] = i < num_pp ? to_dpu_hw_pingpong(hw_pp[i]) : NULL; num_dsc = dpu_rm_get_assigned_resources(&dpu_kms->rm, global_state, drm_enc->base.id, DPU_HW_BLK_DSC, hw_dsc, ARRAY_SIZE(hw_dsc)); for (i = 0; i < num_dsc; i++) { dpu_enc->hw_dsc[i] = to_dpu_hw_dsc(hw_dsc[i]); dsc_mask |= BIT(dpu_enc->hw_dsc[i]->idx - DSC_0); } dpu_enc->dsc_mask = dsc_mask; cstate = to_dpu_crtc_state(crtc_state); for (i = 0; i < num_lm; i++) { int ctl_idx = (i < num_ctl) ? i : (num_ctl-1); cstate->mixers[i].hw_lm = to_dpu_hw_mixer(hw_lm[i]); cstate->mixers[i].lm_ctl = to_dpu_hw_ctl(hw_ctl[ctl_idx]); cstate->mixers[i].hw_dspp = to_dpu_hw_dspp(hw_dspp[i]); } cstate->num_mixers = num_lm; for (i = 0; i < dpu_enc->num_phys_encs; i++) { struct dpu_encoder_phys *phys = dpu_enc->phys_encs[i]; if (!dpu_enc->hw_pp[i]) { DPU_ERROR_ENC(dpu_enc, ""no pp block assigned at idx: %d\n"", i); return; } if (!hw_ctl[i]) { DPU_ERROR_ENC(dpu_enc, ""no ctl block assigned at idx: %d\n"", i); return; } phys->hw_pp = dpu_enc->hw_pp[i]; phys->hw_ctl = to_dpu_hw_ctl(hw_ctl[i]); phys->cached_mode = crtc_state->adjusted_mode; if (phys->ops.atomic_mode_set) phys->ops.atomic_mode_set(phys, crtc_state, conn_state); } }"
929----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50175/bad/camss-video.c----video_stop_streaming,"static void video_stop_streaming(struct vb2_queue *q) { struct camss_video *video = vb2_get_drv_priv(q); struct video_device *vdev = &video->vdev; struct media_entity *entity; struct media_pad *pad; struct v4l2_subdev *subdev; int ret; entity = &vdev->entity; while (1) { pad = &entity->pads[0]; if (!(pad->flags & MEDIA_PAD_FL_SINK)) break; pad = media_pad_remote_pad_first(pad); if (!pad || !is_media_entity_v4l2_subdev(pad->entity)) break; entity = pad->entity; subdev = media_entity_to_v4l2_subdev(entity); ret = v4l2_subdev_call(subdev, video, s_stream, 0); <S2SV_StartVul> if (entity->use_count > 1) { <S2SV_EndVul> <S2SV_StartVul> dev_dbg(video->camss->dev, ""Video pipeline still used, don't stop streaming.\n""); <S2SV_EndVul> <S2SV_StartVul> return; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> if (ret) { dev_err(video->camss->dev, ""Video pipeline stop failed: %d\n"", ret); <S2SV_StartVul> return; <S2SV_EndVul> } } video_device_pipeline_stop(vdev); video->ops->flush_buffers(video, VB2_BUF_STATE_ERROR); }","- if (entity->use_count > 1) {
- dev_dbg(video->camss->dev, ""Video pipeline still used, don't stop streaming.\n"");
- return;
- }
- return;","static void video_stop_streaming(struct vb2_queue *q) { struct camss_video *video = vb2_get_drv_priv(q); struct video_device *vdev = &video->vdev; struct media_entity *entity; struct media_pad *pad; struct v4l2_subdev *subdev; int ret; entity = &vdev->entity; while (1) { pad = &entity->pads[0]; if (!(pad->flags & MEDIA_PAD_FL_SINK)) break; pad = media_pad_remote_pad_first(pad); if (!pad || !is_media_entity_v4l2_subdev(pad->entity)) break; entity = pad->entity; subdev = media_entity_to_v4l2_subdev(entity); ret = v4l2_subdev_call(subdev, video, s_stream, 0); if (ret) { dev_err(video->camss->dev, ""Video pipeline stop failed: %d\n"", ret); return; } } video_device_pipeline_stop(vdev); video->ops->flush_buffers(video, VB2_BUF_STATE_ERROR); }"
1308----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56608/bad/dcn21_resource.c----*dcn21_link_encoder_create,"static struct link_encoder *dcn21_link_encoder_create( struct dc_context *ctx, const struct encoder_init_data *enc_init_data) { struct dcn21_link_encoder *enc21 = kzalloc(sizeof(struct dcn21_link_encoder), GFP_KERNEL); int link_regs_id; <S2SV_StartVul> if (!enc21) <S2SV_EndVul> return NULL; link_regs_id = map_transmitter_id_to_phy_instance(enc_init_data->transmitter); dcn21_link_encoder_construct(enc21, enc_init_data, &link_enc_feature, &link_enc_regs[link_regs_id], &link_enc_aux_regs[enc_init_data->channel - 1], &link_enc_hpd_regs[enc_init_data->hpd_source], &le_shift, &le_mask); return &enc21->enc10.base; }","- if (!enc21)
+ if (!enc21 || enc_init_data->hpd_source >= ARRAY_SIZE(link_enc_hpd_regs))","static struct link_encoder *dcn21_link_encoder_create( struct dc_context *ctx, const struct encoder_init_data *enc_init_data) { struct dcn21_link_encoder *enc21 = kzalloc(sizeof(struct dcn21_link_encoder), GFP_KERNEL); int link_regs_id; if (!enc21 || enc_init_data->hpd_source >= ARRAY_SIZE(link_enc_hpd_regs)) return NULL; link_regs_id = map_transmitter_id_to_phy_instance(enc_init_data->transmitter); dcn21_link_encoder_construct(enc21, enc_init_data, &link_enc_feature, &link_enc_regs[link_regs_id], &link_enc_aux_regs[enc_init_data->channel - 1], &link_enc_hpd_regs[enc_init_data->hpd_source], &le_shift, &le_mask); return &enc21->enc10.base; }"
1364----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56697/bad/amdgpu_discovery.c----amdgpu_discovery_get_nps_info,"int amdgpu_discovery_get_nps_info(struct amdgpu_device *adev, uint32_t *nps_type, struct amdgpu_gmc_memrange **ranges, int *range_cnt) { struct amdgpu_gmc_memrange *mem_ranges; struct binary_header *bhdr; union nps_info *nps_info; u16 offset; int i; if (!nps_type || !range_cnt || !ranges) return -EINVAL; if (!adev->mman.discovery_bin) { dev_err(adev->dev, ""fetch mem range failed, ip discovery uninitialized\n""); return -EINVAL; } bhdr = (struct binary_header *)adev->mman.discovery_bin; offset = le16_to_cpu(bhdr->table_list[NPS_INFO].offset); if (!offset) return -ENOENT; if (amdgpu_discovery_verify_npsinfo(adev, bhdr)) return -ENOENT; nps_info = (union nps_info *)(adev->mman.discovery_bin + offset); switch (le16_to_cpu(nps_info->v1.header.version_major)) { case 1: *nps_type = nps_info->v1.nps_type; *range_cnt = nps_info->v1.count; <S2SV_StartVul> mem_ranges = kvzalloc( <S2SV_EndVul> *range_cnt * sizeof(struct amdgpu_gmc_memrange), <S2SV_StartVul> GFP_KERNEL); <S2SV_EndVul> for (i = 0; i < *range_cnt; i++) { mem_ranges[i].base_address = nps_info->v1.instance_info[i].base_address; mem_ranges[i].limit_address = nps_info->v1.instance_info[i].limit_address; mem_ranges[i].nid_mask = -1; mem_ranges[i].flags = 0; } *ranges = mem_ranges; break; default: dev_err(adev->dev, ""Unhandled NPS info table %d.%d\n"", le16_to_cpu(nps_info->v1.header.version_major), le16_to_cpu(nps_info->v1.header.version_minor)); return -EINVAL; } return 0; }","- mem_ranges = kvzalloc(
- GFP_KERNEL);
+ mem_ranges = kvcalloc(nps_info->v1.count,
+ sizeof(*mem_ranges),
+ GFP_KERNEL);
+ if (!mem_ranges)
+ return -ENOMEM;","int amdgpu_discovery_get_nps_info(struct amdgpu_device *adev, uint32_t *nps_type, struct amdgpu_gmc_memrange **ranges, int *range_cnt) { struct amdgpu_gmc_memrange *mem_ranges; struct binary_header *bhdr; union nps_info *nps_info; u16 offset; int i; if (!nps_type || !range_cnt || !ranges) return -EINVAL; if (!adev->mman.discovery_bin) { dev_err(adev->dev, ""fetch mem range failed, ip discovery uninitialized\n""); return -EINVAL; } bhdr = (struct binary_header *)adev->mman.discovery_bin; offset = le16_to_cpu(bhdr->table_list[NPS_INFO].offset); if (!offset) return -ENOENT; if (amdgpu_discovery_verify_npsinfo(adev, bhdr)) return -ENOENT; nps_info = (union nps_info *)(adev->mman.discovery_bin + offset); switch (le16_to_cpu(nps_info->v1.header.version_major)) { case 1: mem_ranges = kvcalloc(nps_info->v1.count, sizeof(*mem_ranges), GFP_KERNEL); if (!mem_ranges) return -ENOMEM; *nps_type = nps_info->v1.nps_type; *range_cnt = nps_info->v1.count; for (i = 0; i < *range_cnt; i++) { mem_ranges[i].base_address = nps_info->v1.instance_info[i].base_address; mem_ranges[i].limit_address = nps_info->v1.instance_info[i].limit_address; mem_ranges[i].nid_mask = -1; mem_ranges[i].flags = 0; } *ranges = mem_ranges; break; default: dev_err(adev->dev, ""Unhandled NPS info table %d.%d\n"", le16_to_cpu(nps_info->v1.header.version_major), le16_to_cpu(nps_info->v1.header.version_minor)); return -EINVAL; } return 0; }"
226----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46705/bad/xe_device.c----xe_device_probe,"int xe_device_probe(struct xe_device *xe) { struct xe_tile *tile; struct xe_gt *gt; int err; u8 id; xe_pat_init_early(xe); err = xe_sriov_init(xe); if (err) return err; xe->info.mem_region_mask = 1; err = xe_display_init_nommio(xe); if (err) return err; err = xe_set_dma_info(xe); if (err) return err; xe_mmio_probe_tiles(xe); xe_ttm_sys_mgr_init(xe); for_each_gt(gt, xe, id) { err = xe_gt_init_early(gt); if (err) return err; } for_each_tile(tile, xe, id) { err = xe_ggtt_init_early(tile->mem.ggtt); if (err) return err; if (IS_SRIOV_VF(xe)) { err = xe_memirq_init(&tile->sriov.vf.memirq); if (err) return err; } } for_each_gt(gt, xe, id) { err = xe_gt_init_hwconfig(gt); if (err) return err; } err = xe_devcoredump_init(xe); if (err) return err; err = devm_add_action_or_reset(xe->drm.dev, xe_driver_flr_fini, xe); if (err) return err; err = xe_display_init_noirq(xe); if (err) return err; err = xe_irq_install(xe); if (err) goto err; err = xe_device_set_has_flat_ccs(xe); if (err) goto err_irq_shutdown; err = xe_mmio_probe_vram(xe); <S2SV_StartVul> if (err) <S2SV_EndVul> goto err_irq_shutdown; for_each_tile(tile, xe, id) { err = xe_tile_init_noalloc(tile); if (err) goto err_irq_shutdown; } xe_ttm_stolen_mgr_init(xe); err = xe_display_init_noaccel(xe); if (err) goto err_irq_shutdown; for_each_gt(gt, xe, id) { err = xe_gt_init(gt); if (err) goto err_irq_shutdown; } xe_heci_gsc_init(xe); err = xe_display_init(xe); if (err) goto err_irq_shutdown; err = drm_dev_register(&xe->drm, 0); if (err) goto err_fini_display; xe_display_register(xe); xe_debugfs_register(xe); xe_hwmon_register(xe); return devm_add_action_or_reset(xe->drm.dev, xe_device_sanitize, xe); err_fini_display: xe_display_driver_remove(xe); err_irq_shutdown: xe_irq_shutdown(xe); err: xe_display_fini(xe); return err; }","- if (err)
+ if (err)
+ err = xe_mmio_probe_vram(xe);
+ if (err)","int xe_device_probe(struct xe_device *xe) { struct xe_tile *tile; struct xe_gt *gt; int err; u8 id; xe_pat_init_early(xe); err = xe_sriov_init(xe); if (err) return err; xe->info.mem_region_mask = 1; err = xe_display_init_nommio(xe); if (err) return err; err = xe_set_dma_info(xe); if (err) return err; err = xe_mmio_probe_tiles(xe); if (err) return err; xe_ttm_sys_mgr_init(xe); for_each_gt(gt, xe, id) { err = xe_gt_init_early(gt); if (err) return err; } for_each_tile(tile, xe, id) { err = xe_ggtt_init_early(tile->mem.ggtt); if (err) return err; if (IS_SRIOV_VF(xe)) { err = xe_memirq_init(&tile->sriov.vf.memirq); if (err) return err; } } for_each_gt(gt, xe, id) { err = xe_gt_init_hwconfig(gt); if (err) return err; } err = xe_devcoredump_init(xe); if (err) return err; err = devm_add_action_or_reset(xe->drm.dev, xe_driver_flr_fini, xe); if (err) return err; err = xe_display_init_noirq(xe); if (err) return err; err = xe_irq_install(xe); if (err) goto err; err = xe_device_set_has_flat_ccs(xe); if (err) goto err_irq_shutdown; err = xe_mmio_probe_vram(xe); if (err) goto err_irq_shutdown; for_each_tile(tile, xe, id) { err = xe_tile_init_noalloc(tile); if (err) goto err_irq_shutdown; } xe_ttm_stolen_mgr_init(xe); err = xe_display_init_noaccel(xe); if (err) goto err_irq_shutdown; for_each_gt(gt, xe, id) { err = xe_gt_init(gt); if (err) goto err_irq_shutdown; } xe_heci_gsc_init(xe); err = xe_display_init(xe); if (err) goto err_irq_shutdown; err = drm_dev_register(&xe->drm, 0); if (err) goto err_fini_display; xe_display_register(xe); xe_debugfs_register(xe); xe_hwmon_register(xe); return devm_add_action_or_reset(xe->drm.dev, xe_device_sanitize, xe); err_fini_display: xe_display_driver_remove(xe); err_irq_shutdown: xe_irq_shutdown(xe); err: xe_display_fini(xe); return err; }"
674----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49975/bad/uprobes.c----*__create_xol_area,"static struct xol_area *__create_xol_area(unsigned long vaddr) { struct mm_struct *mm = current->mm; unsigned long insns_size; struct xol_area *area; void *insns; area = kzalloc(sizeof(*area), GFP_KERNEL); if (unlikely(!area)) goto out; area->bitmap = kcalloc(BITS_TO_LONGS(UINSNS_PER_PAGE), sizeof(long), GFP_KERNEL); if (!area->bitmap) goto free_area; area->xol_mapping.name = ""[uprobes]""; area->xol_mapping.pages = area->pages; <S2SV_StartVul> area->pages[0] = alloc_page(GFP_HIGHUSER); <S2SV_EndVul> if (!area->pages[0]) goto free_bitmap; area->pages[1] = NULL; area->vaddr = vaddr; init_waitqueue_head(&area->wq); set_bit(0, area->bitmap); atomic_set(&area->slot_count, 1); insns = arch_uprobe_trampoline(&insns_size); arch_uprobe_copy_ixol(area->pages[0], 0, insns, insns_size); if (!xol_add_vma(mm, area)) return area; __free_page(area->pages[0]); free_bitmap: kfree(area->bitmap); free_area: kfree(area); out: return NULL; }","- area->pages[0] = alloc_page(GFP_HIGHUSER);
+ area->pages[0] = alloc_page(GFP_HIGHUSER | __GFP_ZERO);","static struct xol_area *__create_xol_area(unsigned long vaddr) { struct mm_struct *mm = current->mm; unsigned long insns_size; struct xol_area *area; void *insns; area = kzalloc(sizeof(*area), GFP_KERNEL); if (unlikely(!area)) goto out; area->bitmap = kcalloc(BITS_TO_LONGS(UINSNS_PER_PAGE), sizeof(long), GFP_KERNEL); if (!area->bitmap) goto free_area; area->xol_mapping.name = ""[uprobes]""; area->xol_mapping.pages = area->pages; area->pages[0] = alloc_page(GFP_HIGHUSER | __GFP_ZERO); if (!area->pages[0]) goto free_bitmap; area->pages[1] = NULL; area->vaddr = vaddr; init_waitqueue_head(&area->wq); set_bit(0, area->bitmap); atomic_set(&area->slot_count, 1); insns = arch_uprobe_trampoline(&insns_size); arch_uprobe_copy_ixol(area->pages[0], 0, insns, insns_size); if (!xol_add_vma(mm, area)) return area; __free_page(area->pages[0]); free_bitmap: kfree(area->bitmap); free_area: kfree(area); out: return NULL; }"
1346----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56667/bad/i915_gpu_error.c----capture_engine,"capture_engine(struct intel_engine_cs *engine, struct i915_vma_compress *compress, u32 dump_flags) { struct intel_engine_capture_vma *capture = NULL; struct intel_engine_coredump *ee; struct intel_context *ce = NULL; struct i915_request *rq = NULL; ee = intel_engine_coredump_alloc(engine, ALLOW_FAIL, dump_flags); if (!ee) return NULL; intel_engine_get_hung_entity(engine, &ce, &rq); <S2SV_StartVul> if (rq && !i915_request_started(rq)) <S2SV_EndVul> <S2SV_StartVul> drm_info(&engine->gt->i915->drm, ""Got hung context on %s with active request %lld:%lld [0x%04X] not yet started\n"", <S2SV_EndVul> <S2SV_StartVul> engine->name, rq->fence.context, rq->fence.seqno, ce->guc_id.id); <S2SV_EndVul> if (rq) { capture = intel_engine_coredump_add_request(ee, rq, ATOMIC_MAYFAIL); i915_request_put(rq); } else if (ce) { capture = engine_coredump_add_context(ee, ce, ATOMIC_MAYFAIL); } if (capture) { intel_engine_coredump_add_vma(ee, capture, compress); if (dump_flags & CORE_DUMP_FLAG_IS_GUC_CAPTURE) intel_guc_capture_get_matching_node(engine->gt, ee, ce); } else { kfree(ee); ee = NULL; } return ee; }","- if (rq && !i915_request_started(rq))
- drm_info(&engine->gt->i915->drm, ""Got hung context on %s with active request %lld:%lld [0x%04X] not yet started\n"",
- engine->name, rq->fence.context, rq->fence.seqno, ce->guc_id.id);
+ if (rq && !i915_request_started(rq)) {
+ if (ce)
+ drm_info(&engine->gt->i915->drm,
+ ""Got hung context on %s with active request %lld:%lld [0x%04X] not yet started\n"",
+ engine->name, rq->fence.context, rq->fence.seqno, ce->guc_id.id);
+ else
+ drm_info(&engine->gt->i915->drm,
+ ""Got hung context on %s with active request %lld:%lld not yet started\n"",
+ engine->name, rq->fence.context, rq->fence.seqno);
+ }","capture_engine(struct intel_engine_cs *engine, struct i915_vma_compress *compress, u32 dump_flags) { struct intel_engine_capture_vma *capture = NULL; struct intel_engine_coredump *ee; struct intel_context *ce = NULL; struct i915_request *rq = NULL; ee = intel_engine_coredump_alloc(engine, ALLOW_FAIL, dump_flags); if (!ee) return NULL; intel_engine_get_hung_entity(engine, &ce, &rq); if (rq && !i915_request_started(rq)) { if (ce) drm_info(&engine->gt->i915->drm, ""Got hung context on %s with active request %lld:%lld [0x%04X] not yet started\n"", engine->name, rq->fence.context, rq->fence.seqno, ce->guc_id.id); else drm_info(&engine->gt->i915->drm, ""Got hung context on %s with active request %lld:%lld not yet started\n"", engine->name, rq->fence.context, rq->fence.seqno); } if (rq) { capture = intel_engine_coredump_add_request(ee, rq, ATOMIC_MAYFAIL); i915_request_put(rq); } else if (ce) { capture = engine_coredump_add_context(ee, ce, ATOMIC_MAYFAIL); } if (capture) { intel_engine_coredump_add_vma(ee, capture, compress); if (dump_flags & CORE_DUMP_FLAG_IS_GUC_CAPTURE) intel_guc_capture_get_matching_node(engine->gt, ee, ce); } else { kfree(ee); ee = NULL; } return ee; }"
889----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50140/bad/task_work.c----task_work_add,"int task_work_add(struct task_struct *task, struct callback_head *work, enum task_work_notify_mode notify) { struct callback_head *head; if (notify == TWA_NMI_CURRENT) { if (WARN_ON_ONCE(task != current)) return -EINVAL; if (!IS_ENABLED(CONFIG_IRQ_WORK)) return -EINVAL; } else { <S2SV_StartVul> kasan_record_aux_stack(work); <S2SV_EndVul> } head = READ_ONCE(task->task_works); do { if (unlikely(head == &work_exited)) return -ESRCH; work->next = head; } while (!try_cmpxchg(&task->task_works, &head, work)); switch (notify) { case TWA_NONE: break; case TWA_RESUME: set_notify_resume(task); break; case TWA_SIGNAL: set_notify_signal(task); break; case TWA_SIGNAL_NO_IPI: __set_notify_signal(task); break; #ifdef CONFIG_IRQ_WORK case TWA_NMI_CURRENT: irq_work_queue(this_cpu_ptr(&irq_work_NMI_resume)); break; #endif default: WARN_ON_ONCE(1); break; } return 0; }","- kasan_record_aux_stack(work);
+ int flags = notify & TWA_FLAGS;
+ notify &= ~TWA_FLAGS;
+ if (flags & TWAF_NO_ALLOC)
+ kasan_record_aux_stack_noalloc(work);
+ else
+ kasan_record_aux_stack(work);","int task_work_add(struct task_struct *task, struct callback_head *work, enum task_work_notify_mode notify) { struct callback_head *head; int flags = notify & TWA_FLAGS; notify &= ~TWA_FLAGS; if (notify == TWA_NMI_CURRENT) { if (WARN_ON_ONCE(task != current)) return -EINVAL; if (!IS_ENABLED(CONFIG_IRQ_WORK)) return -EINVAL; } else { if (flags & TWAF_NO_ALLOC) kasan_record_aux_stack_noalloc(work); else kasan_record_aux_stack(work); } head = READ_ONCE(task->task_works); do { if (unlikely(head == &work_exited)) return -ESRCH; work->next = head; } while (!try_cmpxchg(&task->task_works, &head, work)); switch (notify) { case TWA_NONE: break; case TWA_RESUME: set_notify_resume(task); break; case TWA_SIGNAL: set_notify_signal(task); break; case TWA_SIGNAL_NO_IPI: __set_notify_signal(task); break; #ifdef CONFIG_IRQ_WORK case TWA_NMI_CURRENT: irq_work_queue(this_cpu_ptr(&irq_work_NMI_resume)); break; #endif default: WARN_ON_ONCE(1); break; } return 0; }"
1335----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56654/bad/hci_event.c----hci_le_create_big_complete_evt,"static void hci_le_create_big_complete_evt(struct hci_dev *hdev, void *data, struct sk_buff *skb) { struct hci_evt_le_create_big_complete *ev = data; struct hci_conn *conn; __u8 i = 0; BT_DBG(""%s status 0x%2.2x"", hdev->name, ev->status); if (!hci_le_ev_skb_pull(hdev, skb, HCI_EVT_LE_CREATE_BIG_COMPLETE, flex_array_size(ev, bis_handle, ev->num_bis))) return; hci_dev_lock(hdev); <S2SV_StartVul> rcu_read_lock(); <S2SV_EndVul> <S2SV_StartVul> list_for_each_entry_rcu(conn, &hdev->conn_hash.list, list) { <S2SV_EndVul> <S2SV_StartVul> if (bacmp(&conn->dst, BDADDR_ANY) || <S2SV_EndVul> <S2SV_StartVul> conn->type != ISO_LINK || <S2SV_EndVul> <S2SV_StartVul> conn->iso_qos.bcast.big != ev->handle) <S2SV_EndVul> <S2SV_StartVul> continue; <S2SV_EndVul> if (hci_conn_set_handle(conn, __le16_to_cpu(ev->bis_handle[i++]))) <S2SV_StartVul> continue; <S2SV_EndVul> <S2SV_StartVul> if (!ev->status) { <S2SV_EndVul> <S2SV_StartVul> conn->state = BT_CONNECTED; <S2SV_EndVul> <S2SV_StartVul> set_bit(HCI_CONN_BIG_CREATED, &conn->flags); <S2SV_EndVul> <S2SV_StartVul> rcu_read_unlock(); <S2SV_EndVul> <S2SV_StartVul> hci_debugfs_create_conn(conn); <S2SV_EndVul> <S2SV_StartVul> hci_conn_add_sysfs(conn); <S2SV_EndVul> <S2SV_StartVul> hci_iso_setup_path(conn); <S2SV_EndVul> <S2SV_StartVul> rcu_read_lock(); <S2SV_EndVul> <S2SV_StartVul> continue; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> hci_connect_cfm(conn, ev->status); <S2SV_EndVul> <S2SV_StartVul> rcu_read_unlock(); <S2SV_EndVul> <S2SV_StartVul> hci_conn_del(conn); <S2SV_EndVul> <S2SV_StartVul> rcu_read_lock(); <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> rcu_read_unlock(); <S2SV_EndVul> if (!ev->status && !i) hci_cmd_sync_queue(hdev, hci_iso_term_big_sync, UINT_PTR(ev->handle), NULL); hci_dev_unlock(hdev); }","- rcu_read_lock();
- list_for_each_entry_rcu(conn, &hdev->conn_hash.list, list) {
- if (bacmp(&conn->dst, BDADDR_ANY) ||
- conn->type != ISO_LINK ||
- conn->iso_qos.bcast.big != ev->handle)
- continue;
- continue;
- if (!ev->status) {
- conn->state = BT_CONNECTED;
- set_bit(HCI_CONN_BIG_CREATED, &conn->flags);
- rcu_read_unlock();
- hci_debugfs_create_conn(conn);
- hci_conn_add_sysfs(conn);
- hci_iso_setup_path(conn);
- rcu_read_lock();
- continue;
- }
- hci_connect_cfm(conn, ev->status);
- rcu_read_unlock();
- hci_conn_del(conn);
- rcu_read_lock();
- }
- rcu_read_unlock();
+ while ((conn = hci_conn_hash_lookup_big_state(hdev, ev->handle,
+ BT_BOUND))) {
+ if (ev->status) {
+ hci_connect_cfm(conn, ev->status);
+ hci_conn_del(conn);
+ }
+ conn->state = BT_CONNECTED;
+ set_bit(HCI_CONN_BIG_CREATED, &conn->flags);
+ hci_debugfs_create_conn(conn);
+ hci_conn_add_sysfs(conn);
+ hci_iso_setup_path(conn);
+ }","static void hci_le_create_big_complete_evt(struct hci_dev *hdev, void *data, struct sk_buff *skb) { struct hci_evt_le_create_big_complete *ev = data; struct hci_conn *conn; __u8 i = 0; BT_DBG(""%s status 0x%2.2x"", hdev->name, ev->status); if (!hci_le_ev_skb_pull(hdev, skb, HCI_EVT_LE_CREATE_BIG_COMPLETE, flex_array_size(ev, bis_handle, ev->num_bis))) return; hci_dev_lock(hdev); while ((conn = hci_conn_hash_lookup_big_state(hdev, ev->handle, BT_BOUND))) { if (ev->status) { hci_connect_cfm(conn, ev->status); hci_conn_del(conn); continue; } if (hci_conn_set_handle(conn, __le16_to_cpu(ev->bis_handle[i++]))) continue; conn->state = BT_CONNECTED; set_bit(HCI_CONN_BIG_CREATED, &conn->flags); hci_debugfs_create_conn(conn); hci_conn_add_sysfs(conn); hci_iso_setup_path(conn); } if (!ev->status && !i) hci_cmd_sync_queue(hdev, hci_iso_term_big_sync, UINT_PTR(ev->handle), NULL); hci_dev_unlock(hdev); }"
925----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50172/bad/main.c----bnxt_re_setup_chip_ctx,"static int bnxt_re_setup_chip_ctx(struct bnxt_re_dev *rdev, u8 wqe_mode) { struct bnxt_qplib_chip_ctx *chip_ctx; struct bnxt_en_dev *en_dev; int rc; en_dev = rdev->en_dev; chip_ctx = kzalloc(sizeof(*chip_ctx), GFP_KERNEL); if (!chip_ctx) return -ENOMEM; chip_ctx->chip_num = en_dev->chip_num; chip_ctx->hw_stats_size = en_dev->hw_ring_stats_size; rdev->chip_ctx = chip_ctx; rdev->qplib_res.cctx = rdev->chip_ctx; rdev->rcfw.res = &rdev->qplib_res; rdev->qplib_res.dattr = &rdev->dev_attr; rdev->qplib_res.is_vf = BNXT_EN_VF(en_dev); bnxt_re_set_drv_mode(rdev, wqe_mode); bnxt_re_set_db_offset(rdev); rc = bnxt_qplib_map_db_bar(&rdev->qplib_res); <S2SV_StartVul> if (rc) <S2SV_EndVul> return rc; if (bnxt_qplib_determine_atomics(en_dev->pdev)) ibdev_info(&rdev->ibdev, ""platform doesn't support global atomics.""); return 0; }","- if (rc)
+ if (rc) {
+ kfree(rdev->chip_ctx);
+ rdev->chip_ctx = NULL;
+ }","static int bnxt_re_setup_chip_ctx(struct bnxt_re_dev *rdev, u8 wqe_mode) { struct bnxt_qplib_chip_ctx *chip_ctx; struct bnxt_en_dev *en_dev; int rc; en_dev = rdev->en_dev; chip_ctx = kzalloc(sizeof(*chip_ctx), GFP_KERNEL); if (!chip_ctx) return -ENOMEM; chip_ctx->chip_num = en_dev->chip_num; chip_ctx->hw_stats_size = en_dev->hw_ring_stats_size; rdev->chip_ctx = chip_ctx; rdev->qplib_res.cctx = rdev->chip_ctx; rdev->rcfw.res = &rdev->qplib_res; rdev->qplib_res.dattr = &rdev->dev_attr; rdev->qplib_res.is_vf = BNXT_EN_VF(en_dev); bnxt_re_set_drv_mode(rdev, wqe_mode); bnxt_re_set_db_offset(rdev); rc = bnxt_qplib_map_db_bar(&rdev->qplib_res); if (rc) { kfree(rdev->chip_ctx); rdev->chip_ctx = NULL; return rc; } if (bnxt_qplib_determine_atomics(en_dev->pdev)) ibdev_info(&rdev->ibdev, ""platform doesn't support global atomics.""); return 0; }"
1050----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50288/bad/vivid-vid-cap.c----vid_cap_start_streaming,"static int vid_cap_start_streaming(struct vb2_queue *vq, unsigned count) { struct vivid_dev *dev = vb2_get_drv_priv(vq); unsigned i; int err; dev->vid_cap_seq_count = 0; dprintk(dev, 1, ""%s\n"", __func__); <S2SV_StartVul> for (i = 0; i < VIDEO_MAX_FRAME; i++) <S2SV_EndVul> dev->must_blank[i] = tpg_g_perc_fill(&dev->tpg) < 100; if (dev->start_streaming_error) { dev->start_streaming_error = false; err = -EINVAL; } else { err = vivid_start_generating_vid_cap(dev, &dev->vid_cap_streaming); } if (err) { struct vivid_buffer *buf, *tmp; list_for_each_entry_safe(buf, tmp, &dev->vid_cap_active, list) { list_del(&buf->list); vb2_buffer_done(&buf->vb.vb2_buf, VB2_BUF_STATE_QUEUED); } } return err; }","- for (i = 0; i < VIDEO_MAX_FRAME; i++)
+ for (i = 0; i < MAX_VID_CAP_BUFFERS; i++)","static int vid_cap_start_streaming(struct vb2_queue *vq, unsigned count) { struct vivid_dev *dev = vb2_get_drv_priv(vq); unsigned i; int err; dev->vid_cap_seq_count = 0; dprintk(dev, 1, ""%s\n"", __func__); for (i = 0; i < MAX_VID_CAP_BUFFERS; i++) dev->must_blank[i] = tpg_g_perc_fill(&dev->tpg) < 100; if (dev->start_streaming_error) { dev->start_streaming_error = false; err = -EINVAL; } else { err = vivid_start_generating_vid_cap(dev, &dev->vid_cap_streaming); } if (err) { struct vivid_buffer *buf, *tmp; list_for_each_entry_safe(buf, tmp, &dev->vid_cap_active, list) { list_del(&buf->list); vb2_buffer_done(&buf->vb.vb2_buf, VB2_BUF_STATE_QUEUED); } } return err; }"
182----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-45828/bad/dma.c----hci_dma_cleanup,"static void hci_dma_cleanup(struct i3c_hci *hci) { struct hci_rings_data *rings = hci->io_data; struct hci_rh_data *rh; unsigned int i; if (!rings) return; for (i = 0; i < rings->total; i++) { rh = &rings->headers[i]; rh_reg_write(RING_CONTROL, 0); rh_reg_write(CR_SETUP, 0); rh_reg_write(IBI_SETUP, 0); <S2SV_StartVul> rh_reg_write(INTR_SIGNAL_ENABLE, 0); <S2SV_EndVul> if (rh->xfer) dma_free_coherent(&hci->master.dev, rh->xfer_struct_sz * rh->xfer_entries, rh->xfer, rh->xfer_dma); if (rh->resp) dma_free_coherent(&hci->master.dev, rh->resp_struct_sz * rh->xfer_entries, rh->resp, rh->resp_dma); kfree(rh->src_xfers); if (rh->ibi_status) dma_free_coherent(&hci->master.dev, rh->ibi_status_sz * rh->ibi_status_entries, rh->ibi_status, rh->ibi_status_dma); if (rh->ibi_data_dma) dma_unmap_single(&hci->master.dev, rh->ibi_data_dma, rh->ibi_chunk_sz * rh->ibi_chunks_total, DMA_FROM_DEVICE); kfree(rh->ibi_data); } rhs_reg_write(CONTROL, 0); kfree(rings); hci->io_data = NULL; }","- rh_reg_write(INTR_SIGNAL_ENABLE, 0);
+ rh_reg_write(INTR_SIGNAL_ENABLE, 0);","static void hci_dma_cleanup(struct i3c_hci *hci) { struct hci_rings_data *rings = hci->io_data; struct hci_rh_data *rh; unsigned int i; if (!rings) return; for (i = 0; i < rings->total; i++) { rh = &rings->headers[i]; rh_reg_write(INTR_SIGNAL_ENABLE, 0); rh_reg_write(RING_CONTROL, 0); rh_reg_write(CR_SETUP, 0); rh_reg_write(IBI_SETUP, 0); if (rh->xfer) dma_free_coherent(&hci->master.dev, rh->xfer_struct_sz * rh->xfer_entries, rh->xfer, rh->xfer_dma); if (rh->resp) dma_free_coherent(&hci->master.dev, rh->resp_struct_sz * rh->xfer_entries, rh->resp, rh->resp_dma); kfree(rh->src_xfers); if (rh->ibi_status) dma_free_coherent(&hci->master.dev, rh->ibi_status_sz * rh->ibi_status_entries, rh->ibi_status, rh->ibi_status_dma); if (rh->ibi_data_dma) dma_unmap_single(&hci->master.dev, rh->ibi_data_dma, rh->ibi_chunk_sz * rh->ibi_chunks_total, DMA_FROM_DEVICE); kfree(rh->ibi_data); } rhs_reg_write(CONTROL, 0); kfree(rings); hci->io_data = NULL; }"
1057----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50298/bad/enetc_pf.c----enetc_sriov_configure,"static int enetc_sriov_configure(struct pci_dev *pdev, int num_vfs) { struct enetc_si *si = pci_get_drvdata(pdev); struct enetc_pf *pf = enetc_si_priv(si); int err; if (!num_vfs) { enetc_msg_psi_free(pf); <S2SV_StartVul> kfree(pf->vf_state); <S2SV_EndVul> <S2SV_StartVul> pf->num_vfs = 0; <S2SV_EndVul> pci_disable_sriov(pdev); } else { pf->num_vfs = num_vfs; <S2SV_StartVul> pf->vf_state = kcalloc(num_vfs, sizeof(struct enetc_vf_state), <S2SV_EndVul> <S2SV_StartVul> GFP_KERNEL); <S2SV_EndVul> <S2SV_StartVul> if (!pf->vf_state) { <S2SV_EndVul> <S2SV_StartVul> pf->num_vfs = 0; <S2SV_EndVul> <S2SV_StartVul> return -ENOMEM; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> err = enetc_msg_psi_init(pf); if (err) { dev_err(&pdev->dev, ""enetc_msg_psi_init (%d)\n"", err); goto err_msg_psi; } err = pci_enable_sriov(pdev, num_vfs); if (err) { dev_err(&pdev->dev, ""pci_enable_sriov err %d\n"", err); goto err_en_sriov; } } return num_vfs; err_en_sriov: enetc_msg_psi_free(pf); err_msg_psi: <S2SV_StartVul> kfree(pf->vf_state); <S2SV_EndVul> <S2SV_StartVul> pf->num_vfs = 0; <S2SV_EndVul> return err; }","- kfree(pf->vf_state);
- pf->num_vfs = 0;
- pf->vf_state = kcalloc(num_vfs, sizeof(struct enetc_vf_state),
- GFP_KERNEL);
- if (!pf->vf_state) {
- pf->num_vfs = 0;
- return -ENOMEM;
- }
- kfree(pf->vf_state);
- pf->num_vfs = 0;","static int enetc_sriov_configure(struct pci_dev *pdev, int num_vfs) { struct enetc_si *si = pci_get_drvdata(pdev); struct enetc_pf *pf = enetc_si_priv(si); int err; if (!num_vfs) { enetc_msg_psi_free(pf); pf->num_vfs = 0; pci_disable_sriov(pdev); } else { pf->num_vfs = num_vfs; err = enetc_msg_psi_init(pf); if (err) { dev_err(&pdev->dev, ""enetc_msg_psi_init (%d)\n"", err); goto err_msg_psi; } err = pci_enable_sriov(pdev, num_vfs); if (err) { dev_err(&pdev->dev, ""pci_enable_sriov err %d\n"", err); goto err_en_sriov; } } return num_vfs; err_en_sriov: enetc_msg_psi_free(pf); err_msg_psi: pf->num_vfs = 0; return err; }"
1336----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56655/bad/nf_tables_api.c----nf_tables_deactivate_set,"void nf_tables_deactivate_set(const struct nft_ctx *ctx, struct nft_set *set, struct nft_set_binding *binding, enum nft_trans_phase phase) <S2SV_StartVul> { <S2SV_EndVul> switch (phase) { case NFT_TRANS_PREPARE_ERROR: nft_set_trans_unbind(ctx, set); if (nft_set_is_anonymous(set)) nft_deactivate_next(ctx->net, set); else list_del_rcu(&binding->list); nft_use_dec(&set->use); break; case NFT_TRANS_PREPARE: if (nft_set_is_anonymous(set)) { if (set->flags & (NFT_SET_MAP | NFT_SET_OBJECT)) nft_map_deactivate(ctx, set); nft_deactivate_next(ctx->net, set); } nft_use_dec(&set->use); return; case NFT_TRANS_ABORT: case NFT_TRANS_RELEASE: if (nft_set_is_anonymous(set) && set->flags & (NFT_SET_MAP | NFT_SET_OBJECT)) nft_map_deactivate(ctx, set); nft_use_dec(&set->use); fallthrough; default: nf_tables_unbind_set(ctx, set, binding, phase == NFT_TRANS_COMMIT); } }","- {
+ lockdep_commit_lock_is_held(ctx->net);","void nf_tables_deactivate_set(const struct nft_ctx *ctx, struct nft_set *set, struct nft_set_binding *binding, enum nft_trans_phase phase) { lockdep_commit_lock_is_held(ctx->net); switch (phase) { case NFT_TRANS_PREPARE_ERROR: nft_set_trans_unbind(ctx, set); if (nft_set_is_anonymous(set)) nft_deactivate_next(ctx->net, set); else list_del_rcu(&binding->list); nft_use_dec(&set->use); break; case NFT_TRANS_PREPARE: if (nft_set_is_anonymous(set)) { if (set->flags & (NFT_SET_MAP | NFT_SET_OBJECT)) nft_map_deactivate(ctx, set); nft_deactivate_next(ctx->net, set); } nft_use_dec(&set->use); return; case NFT_TRANS_ABORT: case NFT_TRANS_RELEASE: if (nft_set_is_anonymous(set) && set->flags & (NFT_SET_MAP | NFT_SET_OBJECT)) nft_map_deactivate(ctx, set); nft_use_dec(&set->use); fallthrough; default: nf_tables_unbind_set(ctx, set, binding, phase == NFT_TRANS_COMMIT); } }"
143----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-45003/bad/inode.c----inode_lru_isolate,"static enum lru_status inode_lru_isolate(struct list_head *item, struct list_lru_one *lru, spinlock_t *lru_lock, void *arg) { struct list_head *freeable = arg; struct inode *inode = container_of(item, struct inode, i_lru); if (!spin_trylock(&inode->i_lock)) return LRU_SKIP; if (atomic_read(&inode->i_count) || (inode->i_state & ~I_REFERENCED) || !mapping_shrinkable(&inode->i_data)) { list_lru_isolate(lru, &inode->i_lru); spin_unlock(&inode->i_lock); this_cpu_dec(nr_unused); return LRU_REMOVED; } if (inode->i_state & I_REFERENCED) { inode->i_state &= ~I_REFERENCED; spin_unlock(&inode->i_lock); return LRU_ROTATE; } if (inode_has_buffers(inode) || !mapping_empty(&inode->i_data)) { <S2SV_StartVul> __iget(inode); <S2SV_EndVul> spin_unlock(&inode->i_lock); spin_unlock(lru_lock); if (remove_inode_buffers(inode)) { unsigned long reap; reap = invalidate_mapping_pages(&inode->i_data, 0, -1); if (current_is_kswapd()) __count_vm_events(KSWAPD_INODESTEAL, reap); else __count_vm_events(PGINODESTEAL, reap); mm_account_reclaimed_pages(reap); } <S2SV_StartVul> iput(inode); <S2SV_EndVul> spin_lock(lru_lock); return LRU_RETRY; } WARN_ON(inode->i_state & I_NEW); inode->i_state |= I_FREEING; list_lru_isolate_move(lru, &inode->i_lru, freeable); spin_unlock(&inode->i_lock); this_cpu_dec(nr_unused); return LRU_REMOVED; }","- __iget(inode);
- iput(inode);
+ inode_pin_lru_isolating(inode);
+ spin_unlock(&inode->i_lock);
+ }
+ inode_unpin_lru_isolating(inode);
+ }","static enum lru_status inode_lru_isolate(struct list_head *item, struct list_lru_one *lru, spinlock_t *lru_lock, void *arg) { struct list_head *freeable = arg; struct inode *inode = container_of(item, struct inode, i_lru); if (!spin_trylock(&inode->i_lock)) return LRU_SKIP; if (atomic_read(&inode->i_count) || (inode->i_state & ~I_REFERENCED) || !mapping_shrinkable(&inode->i_data)) { list_lru_isolate(lru, &inode->i_lru); spin_unlock(&inode->i_lock); this_cpu_dec(nr_unused); return LRU_REMOVED; } if (inode->i_state & I_REFERENCED) { inode->i_state &= ~I_REFERENCED; spin_unlock(&inode->i_lock); return LRU_ROTATE; } if (inode_has_buffers(inode) || !mapping_empty(&inode->i_data)) { inode_pin_lru_isolating(inode); spin_unlock(&inode->i_lock); spin_unlock(lru_lock); if (remove_inode_buffers(inode)) { unsigned long reap; reap = invalidate_mapping_pages(&inode->i_data, 0, -1); if (current_is_kswapd()) __count_vm_events(KSWAPD_INODESTEAL, reap); else __count_vm_events(PGINODESTEAL, reap); mm_account_reclaimed_pages(reap); } inode_unpin_lru_isolating(inode); spin_lock(lru_lock); return LRU_RETRY; } WARN_ON(inode->i_state & I_NEW); inode->i_state |= I_FREEING; list_lru_isolate_move(lru, &inode->i_lru, freeable); spin_unlock(&inode->i_lock); this_cpu_dec(nr_unused); return LRU_REMOVED; }"
453----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47693/bad/cache.c----ib_cache_setup_one,"int ib_cache_setup_one(struct ib_device *device) { u32 p; int err; err = gid_table_setup_one(device); if (err) return err; rdma_for_each_port (device, p) { err = ib_cache_update(device, p, true, true, true); <S2SV_StartVul> if (err) <S2SV_EndVul> return err; } return 0; }","- if (err)
+ if (err) {
+ gid_table_cleanup_one(device);
+ }
+ }","int ib_cache_setup_one(struct ib_device *device) { u32 p; int err; err = gid_table_setup_one(device); if (err) return err; rdma_for_each_port (device, p) { err = ib_cache_update(device, p, true, true, true); if (err) { gid_table_cleanup_one(device); return err; } } return 0; }"
1360----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56694/bad/skmsg.c----sk_psock_strp_data_ready,static void sk_psock_strp_data_ready(struct sock *sk) { struct sk_psock *psock; rcu_read_lock(); psock = sk_psock(sk); if (likely(psock)) { if (tls_sw_has_ctx_rx(sk)) { psock->parser.saved_data_ready(sk); } else { <S2SV_StartVul> write_lock_bh(&sk->sk_callback_lock); <S2SV_EndVul> strp_data_ready(&psock->parser.strp); <S2SV_StartVul> write_unlock_bh(&sk->sk_callback_lock); <S2SV_EndVul> } } rcu_read_unlock(); },"- write_lock_bh(&sk->sk_callback_lock);
- write_unlock_bh(&sk->sk_callback_lock);
+ read_lock_bh(&sk->sk_callback_lock);
+ read_unlock_bh(&sk->sk_callback_lock);",static void sk_psock_strp_data_ready(struct sock *sk) { struct sk_psock *psock; rcu_read_lock(); psock = sk_psock(sk); if (likely(psock)) { if (tls_sw_has_ctx_rx(sk)) { psock->parser.saved_data_ready(sk); } else { read_lock_bh(&sk->sk_callback_lock); strp_data_ready(&psock->parser.strp); read_unlock_bh(&sk->sk_callback_lock); } } rcu_read_unlock(); }
1152----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53130/bad/btnode.c----nilfs_btnode_submit_block,"int nilfs_btnode_submit_block(struct address_space *btnc, __u64 blocknr, sector_t pblocknr, int mode, int mode_flags, struct buffer_head **pbh, sector_t *submit_ptr) { struct buffer_head *bh; struct inode *inode = btnc->host; struct page *page; int err; bh = nilfs_grab_buffer(inode, btnc, blocknr, BIT(BH_NILFS_Node)); if (unlikely(!bh)) return -ENOMEM; err = -EEXIST; page = bh->b_page; if (buffer_uptodate(bh) || buffer_dirty(bh)) goto found; if (pblocknr == 0) { pblocknr = blocknr; if (inode->i_ino != NILFS_DAT_INO) { struct the_nilfs *nilfs = inode->i_sb->s_fs_info; err = nilfs_dat_translate(nilfs->ns_dat, blocknr, &pblocknr); if (unlikely(err)) { brelse(bh); goto out_locked; } } } if (mode_flags & REQ_RAHEAD) { if (pblocknr != *submit_ptr + 1 || !trylock_buffer(bh)) { err = -EBUSY; brelse(bh); goto out_locked; } } else { lock_buffer(bh); } if (buffer_uptodate(bh)) { unlock_buffer(bh); err = -EEXIST; goto found; } set_buffer_mapped(bh); <S2SV_StartVul> bh->b_bdev = inode->i_sb->s_bdev; <S2SV_EndVul> bh->b_blocknr = pblocknr; bh->b_end_io = end_buffer_read_sync; get_bh(bh); submit_bh(mode, mode_flags, bh); bh->b_blocknr = blocknr; *submit_ptr = pblocknr; err = 0; found: *pbh = bh; out_locked: unlock_page(page); put_page(page); return err; }",- bh->b_bdev = inode->i_sb->s_bdev;,"int nilfs_btnode_submit_block(struct address_space *btnc, __u64 blocknr, sector_t pblocknr, int mode, int mode_flags, struct buffer_head **pbh, sector_t *submit_ptr) { struct buffer_head *bh; struct inode *inode = btnc->host; struct page *page; int err; bh = nilfs_grab_buffer(inode, btnc, blocknr, BIT(BH_NILFS_Node)); if (unlikely(!bh)) return -ENOMEM; err = -EEXIST; page = bh->b_page; if (buffer_uptodate(bh) || buffer_dirty(bh)) goto found; if (pblocknr == 0) { pblocknr = blocknr; if (inode->i_ino != NILFS_DAT_INO) { struct the_nilfs *nilfs = inode->i_sb->s_fs_info; err = nilfs_dat_translate(nilfs->ns_dat, blocknr, &pblocknr); if (unlikely(err)) { brelse(bh); goto out_locked; } } } if (mode_flags & REQ_RAHEAD) { if (pblocknr != *submit_ptr + 1 || !trylock_buffer(bh)) { err = -EBUSY; brelse(bh); goto out_locked; } } else { lock_buffer(bh); } if (buffer_uptodate(bh)) { unlock_buffer(bh); err = -EEXIST; goto found; } set_buffer_mapped(bh); bh->b_blocknr = pblocknr; bh->b_end_io = end_buffer_read_sync; get_bh(bh); submit_bh(mode, mode_flags, bh); bh->b_blocknr = blocknr; *submit_ptr = pblocknr; err = 0; found: *pbh = bh; out_locked: unlock_page(page); put_page(page); return err; }"
803----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50076/bad/vt.c----con_font_get,"static int con_font_get(struct vc_data *vc, struct console_font_op *op) { struct console_font font; int rc = -EINVAL; int c; if (op->data) { <S2SV_StartVul> font.data = kmalloc(max_font_size, GFP_KERNEL); <S2SV_EndVul> if (!font.data) return -ENOMEM; } else font.data = NULL; console_lock(); if (vc->vc_mode != KD_TEXT) rc = -EINVAL; else if (vc->vc_sw->con_font_get) rc = vc->vc_sw->con_font_get(vc, &font); else rc = -ENOSYS; console_unlock(); if (rc) goto out; c = (font.width+7)/8 * 32 * font.charcount; if (op->data && font.charcount > op->charcount) rc = -ENOSPC; if (font.width > op->width || font.height > op->height) rc = -ENOSPC; if (rc) goto out; op->height = font.height; op->width = font.width; op->charcount = font.charcount; if (op->data && copy_to_user(op->data, font.data, c)) rc = -EFAULT; out: kfree(font.data); return rc; }","- font.data = kmalloc(max_font_size, GFP_KERNEL);
+ font.data = kzalloc(max_font_size, GFP_KERNEL);","static int con_font_get(struct vc_data *vc, struct console_font_op *op) { struct console_font font; int rc = -EINVAL; int c; if (op->data) { font.data = kzalloc(max_font_size, GFP_KERNEL); if (!font.data) return -ENOMEM; } else font.data = NULL; console_lock(); if (vc->vc_mode != KD_TEXT) rc = -EINVAL; else if (vc->vc_sw->con_font_get) rc = vc->vc_sw->con_font_get(vc, &font); else rc = -ENOSYS; console_unlock(); if (rc) goto out; c = (font.width+7)/8 * 32 * font.charcount; if (op->data && font.charcount > op->charcount) rc = -ENOSPC; if (font.width > op->width || font.height > op->height) rc = -ENOSPC; if (rc) goto out; op->height = font.height; op->width = font.width; op->charcount = font.charcount; if (op->data && copy_to_user(op->data, font.data, c)) rc = -EFAULT; out: kfree(font.data); return rc; }"
916----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50161/bad/btf.c----btf_find_field_one,"static int btf_find_field_one(const struct btf *btf, const struct btf_type *var, const struct btf_type *var_type, int var_idx, u32 off, u32 expected_size, u32 field_mask, u32 *seen_mask, struct btf_field_info *info, int info_cnt, u32 level) { int ret, align, sz, field_type; struct btf_field_info tmp; const struct btf_array *array; u32 i, nelems = 1; for (i = 0; i < MAX_RESOLVE_DEPTH && btf_type_is_array(var_type); i++) { array = btf_array(var_type); nelems *= array->nelems; var_type = btf_type_by_id(btf, array->type); } if (i == MAX_RESOLVE_DEPTH) return -E2BIG; if (nelems == 0) return 0; field_type = btf_get_field_type(btf, var_type, field_mask, seen_mask, &align, &sz); if (!field_type && __btf_type_is_struct(var_type)) { sz = var_type->size; if (expected_size && expected_size != sz * nelems) return 0; ret = btf_find_nested_struct(btf, var_type, off, nelems, field_mask, &info[0], info_cnt, level); return ret; } if (field_type == 0) return 0; if (field_type < 0) return field_type; if (expected_size && expected_size != sz * nelems) return 0; if (off % align) return 0; switch (field_type) { case BPF_SPIN_LOCK: case BPF_TIMER: case BPF_WORKQUEUE: case BPF_LIST_NODE: case BPF_RB_NODE: case BPF_REFCOUNT: ret = btf_find_struct(btf, var_type, off, sz, field_type, info_cnt ? &info[0] : &tmp); if (ret < 0) return ret; break; case BPF_KPTR_UNREF: case BPF_KPTR_REF: case BPF_KPTR_PERCPU: ret = btf_find_kptr(btf, var_type, off, sz, info_cnt ? &info[0] : &tmp); if (ret < 0) return ret; break; case BPF_LIST_HEAD: case BPF_RB_ROOT: ret = btf_find_graph_root(btf, var, var_type, var_idx, off, sz, info_cnt ? &info[0] : &tmp, field_type); if (ret < 0) return ret; break; default: return -EFAULT; } if (ret == BTF_FIELD_IGNORE) return 0; <S2SV_StartVul> if (nelems > info_cnt) <S2SV_EndVul> return -E2BIG; if (nelems > 1) { <S2SV_StartVul> ret = btf_repeat_fields(info, 1, nelems - 1, sz); <S2SV_EndVul> if (ret < 0) return ret; } return nelems; }","- if (nelems > info_cnt)
- ret = btf_repeat_fields(info, 1, nelems - 1, sz);
+ if (!info_cnt)
+ return -E2BIG;
+ ret = btf_repeat_fields(info, info_cnt, 1, nelems - 1, sz);","static int btf_find_field_one(const struct btf *btf, const struct btf_type *var, const struct btf_type *var_type, int var_idx, u32 off, u32 expected_size, u32 field_mask, u32 *seen_mask, struct btf_field_info *info, int info_cnt, u32 level) { int ret, align, sz, field_type; struct btf_field_info tmp; const struct btf_array *array; u32 i, nelems = 1; for (i = 0; i < MAX_RESOLVE_DEPTH && btf_type_is_array(var_type); i++) { array = btf_array(var_type); nelems *= array->nelems; var_type = btf_type_by_id(btf, array->type); } if (i == MAX_RESOLVE_DEPTH) return -E2BIG; if (nelems == 0) return 0; field_type = btf_get_field_type(btf, var_type, field_mask, seen_mask, &align, &sz); if (!field_type && __btf_type_is_struct(var_type)) { sz = var_type->size; if (expected_size && expected_size != sz * nelems) return 0; ret = btf_find_nested_struct(btf, var_type, off, nelems, field_mask, &info[0], info_cnt, level); return ret; } if (field_type == 0) return 0; if (field_type < 0) return field_type; if (expected_size && expected_size != sz * nelems) return 0; if (off % align) return 0; switch (field_type) { case BPF_SPIN_LOCK: case BPF_TIMER: case BPF_WORKQUEUE: case BPF_LIST_NODE: case BPF_RB_NODE: case BPF_REFCOUNT: ret = btf_find_struct(btf, var_type, off, sz, field_type, info_cnt ? &info[0] : &tmp); if (ret < 0) return ret; break; case BPF_KPTR_UNREF: case BPF_KPTR_REF: case BPF_KPTR_PERCPU: ret = btf_find_kptr(btf, var_type, off, sz, info_cnt ? &info[0] : &tmp); if (ret < 0) return ret; break; case BPF_LIST_HEAD: case BPF_RB_ROOT: ret = btf_find_graph_root(btf, var, var_type, var_idx, off, sz, info_cnt ? &info[0] : &tmp, field_type); if (ret < 0) return ret; break; default: return -EFAULT; } if (ret == BTF_FIELD_IGNORE) return 0; if (!info_cnt) return -E2BIG; if (nelems > 1) { ret = btf_repeat_fields(info, info_cnt, 1, nelems - 1, sz); if (ret < 0) return ret; } return nelems; }"
906----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50152/bad/smb2ops.c----smb2_set_ea,"smb2_set_ea(const unsigned int xid, struct cifs_tcon *tcon, const char *path, const char *ea_name, const void *ea_value, const __u16 ea_value_len, const struct nls_table *nls_codepage, struct cifs_sb_info *cifs_sb) { struct smb2_compound_vars *vars; struct cifs_ses *ses = tcon->ses; struct TCP_Server_Info *server; struct smb_rqst *rqst; struct kvec *rsp_iov; __le16 *utf16_path = NULL; int ea_name_len = strlen(ea_name); int flags = CIFS_CP_CREATE_CLOSE_OP; int len; int resp_buftype[3]; struct cifs_open_parms oparms; __u8 oplock = SMB2_OPLOCK_LEVEL_NONE; struct cifs_fid fid; unsigned int size[1]; void *data[1]; <S2SV_StartVul> struct smb2_file_full_ea_info *ea = NULL; <S2SV_EndVul> struct smb2_query_info_rsp *rsp; int rc, used_len = 0; int retries = 0, cur_sleep = 1; replay_again: flags = CIFS_CP_CREATE_CLOSE_OP; oplock = SMB2_OPLOCK_LEVEL_NONE; server = cifs_pick_channel(ses); if (smb3_encryption_required(tcon)) flags |= CIFS_TRANSFORM_REQ; if (ea_name_len > 255) return -EINVAL; utf16_path = cifs_convert_path_to_utf16(path, cifs_sb); if (!utf16_path) return -ENOMEM; resp_buftype[0] = resp_buftype[1] = resp_buftype[2] = CIFS_NO_BUFFER; vars = kzalloc(sizeof(*vars), GFP_KERNEL); if (!vars) { rc = -ENOMEM; goto out_free_path; } rqst = vars->rqst; rsp_iov = vars->rsp_iov; if (ses->server->ops->query_all_EAs) { if (!ea_value) { rc = ses->server->ops->query_all_EAs(xid, tcon, path, ea_name, NULL, 0, cifs_sb); if (rc == -ENODATA) goto sea_exit; } else { rc = smb2_query_info_compound(xid, tcon, path, FILE_READ_EA, FILE_FULL_EA_INFORMATION, SMB2_O_INFO_FILE, CIFSMaxBufSize - MAX_SMB2_CREATE_RESPONSE_SIZE - MAX_SMB2_CLOSE_RESPONSE_SIZE, &rsp_iov[1], &resp_buftype[1], cifs_sb); if (rc == 0) { rsp = (struct smb2_query_info_rsp *)rsp_iov[1].iov_base; used_len = le32_to_cpu(rsp->OutputBufferLength); } free_rsp_buf(resp_buftype[1], rsp_iov[1].iov_base); resp_buftype[1] = CIFS_NO_BUFFER; memset(&rsp_iov[1], 0, sizeof(rsp_iov[1])); rc = 0; if (CIFSMaxBufSize - MAX_SMB2_CREATE_RESPONSE_SIZE - MAX_SMB2_CLOSE_RESPONSE_SIZE - 256 < used_len + ea_name_len + ea_value_len + 1) { rc = -ENOSPC; goto sea_exit; } } } rqst[0].rq_iov = vars->open_iov; rqst[0].rq_nvec = SMB2_CREATE_IOV_SIZE; oparms = (struct cifs_open_parms) { .tcon = tcon, .path = path, .desired_access = FILE_WRITE_EA, .disposition = FILE_OPEN, .create_options = cifs_create_options(cifs_sb, 0), .fid = &fid, .replay = !!(retries), }; rc = SMB2_open_init(tcon, server, &rqst[0], &oplock, &oparms, utf16_path); if (rc) goto sea_exit; smb2_set_next_command(tcon, &rqst[0]); rqst[1].rq_iov = vars->si_iov; rqst[1].rq_nvec = 1; len = sizeof(*ea) + ea_name_len + ea_value_len + 1; ea = kzalloc(len, GFP_KERNEL); if (ea == NULL) { rc = -ENOMEM; goto sea_exit; } ea->ea_name_length = ea_name_len; ea->ea_value_length = cpu_to_le16(ea_value_len); memcpy(ea->ea_data, ea_name, ea_name_len + 1); memcpy(ea->ea_data + ea_name_len + 1, ea_value, ea_value_len); size[0] = len; data[0] = ea; rc = SMB2_set_info_init(tcon, server, &rqst[1], COMPOUND_FID, COMPOUND_FID, current->tgid, FILE_FULL_EA_INFORMATION, SMB2_O_INFO_FILE, 0, data, size); if (rc) goto sea_exit; smb2_set_next_command(tcon, &rqst[1]); smb2_set_related(&rqst[1]); rqst[2].rq_iov = &vars->close_iov; rqst[2].rq_nvec = 1; rc = SMB2_close_init(tcon, server, &rqst[2], COMPOUND_FID, COMPOUND_FID, false); if (rc) goto sea_exit; smb2_set_related(&rqst[2]); if (retries) { smb2_set_replay(server, &rqst[0]); smb2_set_replay(server, &rqst[1]); smb2_set_replay(server, &rqst[2]); } rc = compound_send_recv(xid, ses, server, flags, 3, rqst, resp_buftype, rsp_iov); sea_exit: kfree(ea); SMB2_open_free(&rqst[0]); SMB2_set_info_free(&rqst[1]); SMB2_close_free(&rqst[2]); free_rsp_buf(resp_buftype[0], rsp_iov[0].iov_base); free_rsp_buf(resp_buftype[1], rsp_iov[1].iov_base); free_rsp_buf(resp_buftype[2], rsp_iov[2].iov_base); kfree(vars); out_free_path: kfree(utf16_path); if (is_replayable_error(rc) && smb2_should_replay(tcon, &retries, &cur_sleep)) goto replay_again; return rc; }","- struct smb2_file_full_ea_info *ea = NULL;
+ struct smb2_file_full_ea_info *ea;
+ ea = NULL;","smb2_set_ea(const unsigned int xid, struct cifs_tcon *tcon, const char *path, const char *ea_name, const void *ea_value, const __u16 ea_value_len, const struct nls_table *nls_codepage, struct cifs_sb_info *cifs_sb) { struct smb2_compound_vars *vars; struct cifs_ses *ses = tcon->ses; struct TCP_Server_Info *server; struct smb_rqst *rqst; struct kvec *rsp_iov; __le16 *utf16_path = NULL; int ea_name_len = strlen(ea_name); int flags = CIFS_CP_CREATE_CLOSE_OP; int len; int resp_buftype[3]; struct cifs_open_parms oparms; __u8 oplock = SMB2_OPLOCK_LEVEL_NONE; struct cifs_fid fid; unsigned int size[1]; void *data[1]; struct smb2_file_full_ea_info *ea; struct smb2_query_info_rsp *rsp; int rc, used_len = 0; int retries = 0, cur_sleep = 1; replay_again: flags = CIFS_CP_CREATE_CLOSE_OP; oplock = SMB2_OPLOCK_LEVEL_NONE; server = cifs_pick_channel(ses); if (smb3_encryption_required(tcon)) flags |= CIFS_TRANSFORM_REQ; if (ea_name_len > 255) return -EINVAL; utf16_path = cifs_convert_path_to_utf16(path, cifs_sb); if (!utf16_path) return -ENOMEM; ea = NULL; resp_buftype[0] = resp_buftype[1] = resp_buftype[2] = CIFS_NO_BUFFER; vars = kzalloc(sizeof(*vars), GFP_KERNEL); if (!vars) { rc = -ENOMEM; goto out_free_path; } rqst = vars->rqst; rsp_iov = vars->rsp_iov; if (ses->server->ops->query_all_EAs) { if (!ea_value) { rc = ses->server->ops->query_all_EAs(xid, tcon, path, ea_name, NULL, 0, cifs_sb); if (rc == -ENODATA) goto sea_exit; } else { rc = smb2_query_info_compound(xid, tcon, path, FILE_READ_EA, FILE_FULL_EA_INFORMATION, SMB2_O_INFO_FILE, CIFSMaxBufSize - MAX_SMB2_CREATE_RESPONSE_SIZE - MAX_SMB2_CLOSE_RESPONSE_SIZE, &rsp_iov[1], &resp_buftype[1], cifs_sb); if (rc == 0) { rsp = (struct smb2_query_info_rsp *)rsp_iov[1].iov_base; used_len = le32_to_cpu(rsp->OutputBufferLength); } free_rsp_buf(resp_buftype[1], rsp_iov[1].iov_base); resp_buftype[1] = CIFS_NO_BUFFER; memset(&rsp_iov[1], 0, sizeof(rsp_iov[1])); rc = 0; if (CIFSMaxBufSize - MAX_SMB2_CREATE_RESPONSE_SIZE - MAX_SMB2_CLOSE_RESPONSE_SIZE - 256 < used_len + ea_name_len + ea_value_len + 1) { rc = -ENOSPC; goto sea_exit; } } } rqst[0].rq_iov = vars->open_iov; rqst[0].rq_nvec = SMB2_CREATE_IOV_SIZE; oparms = (struct cifs_open_parms) { .tcon = tcon, .path = path, .desired_access = FILE_WRITE_EA, .disposition = FILE_OPEN, .create_options = cifs_create_options(cifs_sb, 0), .fid = &fid, .replay = !!(retries), }; rc = SMB2_open_init(tcon, server, &rqst[0], &oplock, &oparms, utf16_path); if (rc) goto sea_exit; smb2_set_next_command(tcon, &rqst[0]); rqst[1].rq_iov = vars->si_iov; rqst[1].rq_nvec = 1; len = sizeof(*ea) + ea_name_len + ea_value_len + 1; ea = kzalloc(len, GFP_KERNEL); if (ea == NULL) { rc = -ENOMEM; goto sea_exit; } ea->ea_name_length = ea_name_len; ea->ea_value_length = cpu_to_le16(ea_value_len); memcpy(ea->ea_data, ea_name, ea_name_len + 1); memcpy(ea->ea_data + ea_name_len + 1, ea_value, ea_value_len); size[0] = len; data[0] = ea; rc = SMB2_set_info_init(tcon, server, &rqst[1], COMPOUND_FID, COMPOUND_FID, current->tgid, FILE_FULL_EA_INFORMATION, SMB2_O_INFO_FILE, 0, data, size); if (rc) goto sea_exit; smb2_set_next_command(tcon, &rqst[1]); smb2_set_related(&rqst[1]); rqst[2].rq_iov = &vars->close_iov; rqst[2].rq_nvec = 1; rc = SMB2_close_init(tcon, server, &rqst[2], COMPOUND_FID, COMPOUND_FID, false); if (rc) goto sea_exit; smb2_set_related(&rqst[2]); if (retries) { smb2_set_replay(server, &rqst[0]); smb2_set_replay(server, &rqst[1]); smb2_set_replay(server, &rqst[2]); } rc = compound_send_recv(xid, ses, server, flags, 3, rqst, resp_buftype, rsp_iov); sea_exit: kfree(ea); SMB2_open_free(&rqst[0]); SMB2_set_info_free(&rqst[1]); SMB2_close_free(&rqst[2]); free_rsp_buf(resp_buftype[0], rsp_iov[0].iov_base); free_rsp_buf(resp_buftype[1], rsp_iov[1].iov_base); free_rsp_buf(resp_buftype[2], rsp_iov[2].iov_base); kfree(vars); out_free_path: kfree(utf16_path); if (is_replayable_error(rc) && smb2_should_replay(tcon, &retries, &cur_sleep)) goto replay_again; return rc; }"
89----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44957/bad/privcmd.c----privcmd_irqfd_assign,"static int privcmd_irqfd_assign(struct privcmd_irqfd *irqfd) { struct privcmd_kernel_irqfd *kirqfd, *tmp; __poll_t events; struct fd f; void *dm_op; int ret; kirqfd = kzalloc(sizeof(*kirqfd) + irqfd->size, GFP_KERNEL); if (!kirqfd) return -ENOMEM; dm_op = kirqfd + 1; if (copy_from_user(dm_op, u64_to_user_ptr(irqfd->dm_op), irqfd->size)) { ret = -EFAULT; goto error_kfree; } kirqfd->xbufs.size = irqfd->size; set_xen_guest_handle(kirqfd->xbufs.h, dm_op); kirqfd->dom = irqfd->dom; INIT_WORK(&kirqfd->shutdown, irqfd_shutdown); f = fdget(irqfd->fd); if (!f.file) { ret = -EBADF; goto error_kfree; } kirqfd->eventfd = eventfd_ctx_fileget(f.file); if (IS_ERR(kirqfd->eventfd)) { ret = PTR_ERR(kirqfd->eventfd); goto error_fd_put; } init_waitqueue_func_entry(&kirqfd->wait, irqfd_wakeup); init_poll_funcptr(&kirqfd->pt, irqfd_poll_func); <S2SV_StartVul> mutex_lock(&irqfds_lock); <S2SV_EndVul> list_for_each_entry(tmp, &irqfds_list, list) { if (kirqfd->eventfd == tmp->eventfd) { ret = -EBUSY; <S2SV_StartVul> mutex_unlock(&irqfds_lock); <S2SV_EndVul> goto error_eventfd; } } list_add_tail(&kirqfd->list, &irqfds_list); <S2SV_StartVul> mutex_unlock(&irqfds_lock); <S2SV_EndVul> events = vfs_poll(f.file, &kirqfd->pt); if (events & EPOLLIN) irqfd_inject(kirqfd); fdput(f); return 0; error_eventfd: eventfd_ctx_put(kirqfd->eventfd); error_fd_put: fdput(f); error_kfree: kfree(kirqfd); return ret; }","- mutex_lock(&irqfds_lock);
- mutex_unlock(&irqfds_lock);
- mutex_unlock(&irqfds_lock);
+ unsigned long flags;
+ spin_lock_irqsave(&irqfds_lock, flags);
+ spin_unlock_irqrestore(&irqfds_lock, flags);
+ spin_unlock_irqrestore(&irqfds_lock, flags);","static int privcmd_irqfd_assign(struct privcmd_irqfd *irqfd) { struct privcmd_kernel_irqfd *kirqfd, *tmp; unsigned long flags; __poll_t events; struct fd f; void *dm_op; int ret; kirqfd = kzalloc(sizeof(*kirqfd) + irqfd->size, GFP_KERNEL); if (!kirqfd) return -ENOMEM; dm_op = kirqfd + 1; if (copy_from_user(dm_op, u64_to_user_ptr(irqfd->dm_op), irqfd->size)) { ret = -EFAULT; goto error_kfree; } kirqfd->xbufs.size = irqfd->size; set_xen_guest_handle(kirqfd->xbufs.h, dm_op); kirqfd->dom = irqfd->dom; INIT_WORK(&kirqfd->shutdown, irqfd_shutdown); f = fdget(irqfd->fd); if (!f.file) { ret = -EBADF; goto error_kfree; } kirqfd->eventfd = eventfd_ctx_fileget(f.file); if (IS_ERR(kirqfd->eventfd)) { ret = PTR_ERR(kirqfd->eventfd); goto error_fd_put; } init_waitqueue_func_entry(&kirqfd->wait, irqfd_wakeup); init_poll_funcptr(&kirqfd->pt, irqfd_poll_func); spin_lock_irqsave(&irqfds_lock, flags); list_for_each_entry(tmp, &irqfds_list, list) { if (kirqfd->eventfd == tmp->eventfd) { ret = -EBUSY; spin_unlock_irqrestore(&irqfds_lock, flags); goto error_eventfd; } } list_add_tail(&kirqfd->list, &irqfds_list); spin_unlock_irqrestore(&irqfds_lock, flags); events = vfs_poll(f.file, &kirqfd->pt); if (events & EPOLLIN) irqfd_inject(kirqfd); fdput(f); return 0; error_eventfd: eventfd_ctx_put(kirqfd->eventfd); error_fd_put: fdput(f); error_kfree: kfree(kirqfd); return ret; }"
1048----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50288/bad/vivid-core.c----vivid_create_queue,"static int vivid_create_queue(struct vivid_dev *dev, struct vb2_queue *q, u32 buf_type, unsigned int min_reqbufs_allocation, const struct vb2_ops *ops) { if (buf_type == V4L2_BUF_TYPE_VIDEO_CAPTURE && dev->multiplanar) buf_type = V4L2_BUF_TYPE_VIDEO_CAPTURE_MPLANE; else if (buf_type == V4L2_BUF_TYPE_VIDEO_OUTPUT && dev->multiplanar) buf_type = V4L2_BUF_TYPE_VIDEO_OUTPUT_MPLANE; else if (buf_type == V4L2_BUF_TYPE_VBI_CAPTURE && !dev->has_raw_vbi_cap) buf_type = V4L2_BUF_TYPE_SLICED_VBI_CAPTURE; else if (buf_type == V4L2_BUF_TYPE_VBI_OUTPUT && !dev->has_raw_vbi_out) buf_type = V4L2_BUF_TYPE_SLICED_VBI_OUTPUT; q->type = buf_type; q->io_modes = VB2_MMAP | VB2_DMABUF; q->io_modes |= V4L2_TYPE_IS_OUTPUT(buf_type) ? VB2_WRITE : VB2_READ; if (buf_type == V4L2_BUF_TYPE_VIDEO_CAPTURE) <S2SV_StartVul> q->max_num_buffers = 64; <S2SV_EndVul> if (buf_type == V4L2_BUF_TYPE_SDR_CAPTURE) q->max_num_buffers = 1024; if (buf_type == V4L2_BUF_TYPE_VBI_CAPTURE) q->max_num_buffers = 32768; if (allocators[dev->inst] != 1) q->io_modes |= VB2_USERPTR; q->drv_priv = dev; q->buf_struct_size = sizeof(struct vivid_buffer); q->ops = ops; q->mem_ops = allocators[dev->inst] == 1 ? &vb2_dma_contig_memops : &vb2_vmalloc_memops; q->timestamp_flags = V4L2_BUF_FLAG_TIMESTAMP_MONOTONIC; q->min_reqbufs_allocation = min_reqbufs_allocation; q->lock = &dev->mutex; q->dev = dev->v4l2_dev.dev; q->supports_requests = supports_requests[dev->inst]; q->requires_requests = supports_requests[dev->inst] >= 2; q->allow_cache_hints = (cache_hints[dev->inst] == 1); return vb2_queue_init(q); }","- q->max_num_buffers = 64;
+ q->max_num_buffers = MAX_VID_CAP_BUFFERS;","static int vivid_create_queue(struct vivid_dev *dev, struct vb2_queue *q, u32 buf_type, unsigned int min_reqbufs_allocation, const struct vb2_ops *ops) { if (buf_type == V4L2_BUF_TYPE_VIDEO_CAPTURE && dev->multiplanar) buf_type = V4L2_BUF_TYPE_VIDEO_CAPTURE_MPLANE; else if (buf_type == V4L2_BUF_TYPE_VIDEO_OUTPUT && dev->multiplanar) buf_type = V4L2_BUF_TYPE_VIDEO_OUTPUT_MPLANE; else if (buf_type == V4L2_BUF_TYPE_VBI_CAPTURE && !dev->has_raw_vbi_cap) buf_type = V4L2_BUF_TYPE_SLICED_VBI_CAPTURE; else if (buf_type == V4L2_BUF_TYPE_VBI_OUTPUT && !dev->has_raw_vbi_out) buf_type = V4L2_BUF_TYPE_SLICED_VBI_OUTPUT; q->type = buf_type; q->io_modes = VB2_MMAP | VB2_DMABUF; q->io_modes |= V4L2_TYPE_IS_OUTPUT(buf_type) ? VB2_WRITE : VB2_READ; if (buf_type == V4L2_BUF_TYPE_VIDEO_CAPTURE) q->max_num_buffers = MAX_VID_CAP_BUFFERS; if (buf_type == V4L2_BUF_TYPE_SDR_CAPTURE) q->max_num_buffers = 1024; if (buf_type == V4L2_BUF_TYPE_VBI_CAPTURE) q->max_num_buffers = 32768; if (allocators[dev->inst] != 1) q->io_modes |= VB2_USERPTR; q->drv_priv = dev; q->buf_struct_size = sizeof(struct vivid_buffer); q->ops = ops; q->mem_ops = allocators[dev->inst] == 1 ? &vb2_dma_contig_memops : &vb2_vmalloc_memops; q->timestamp_flags = V4L2_BUF_FLAG_TIMESTAMP_MONOTONIC; q->min_reqbufs_allocation = min_reqbufs_allocation; q->lock = &dev->mutex; q->dev = dev->v4l2_dev.dev; q->supports_requests = supports_requests[dev->inst]; q->requires_requests = supports_requests[dev->inst] >= 2; q->allow_cache_hints = (cache_hints[dev->inst] == 1); return vb2_queue_init(q); }"
1077----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53061/bad/jpeg-core.c----exynos4_jpeg_parse_decode_h_tbl,"static void exynos4_jpeg_parse_decode_h_tbl(struct s5p_jpeg_ctx *ctx) { struct s5p_jpeg *jpeg = ctx->jpeg; struct vb2_v4l2_buffer *vb = v4l2_m2m_next_src_buf(ctx->fh.m2m_ctx); struct s5p_jpeg_buffer jpeg_buffer; unsigned int word; int c, x, components; jpeg_buffer.size = 2; jpeg_buffer.data = (unsigned long)vb2_plane_vaddr(&vb->vb2_buf, 0) + ctx->out_q.sos + 2; jpeg_buffer.curr = 0; <S2SV_StartVul> word = 0; <S2SV_EndVul> if (get_word_be(&jpeg_buffer, &word)) return; <S2SV_StartVul> jpeg_buffer.size = (long)word - 2; <S2SV_EndVul> jpeg_buffer.data += 2; jpeg_buffer.curr = 0; components = get_byte(&jpeg_buffer); if (components == -1) return; while (components--) { c = get_byte(&jpeg_buffer); if (c == -1) return; x = get_byte(&jpeg_buffer); if (x == -1) return; exynos4_jpeg_select_dec_h_tbl(jpeg->regs, c, (((x >> 4) & 0x1) << 1) | (x & 0x1)); } }","- word = 0;
- jpeg_buffer.size = (long)word - 2;
+ if (word < 2)
+ jpeg_buffer.size = 0;
+ else
+ jpeg_buffer.size = (long)word - 2;","static void exynos4_jpeg_parse_decode_h_tbl(struct s5p_jpeg_ctx *ctx) { struct s5p_jpeg *jpeg = ctx->jpeg; struct vb2_v4l2_buffer *vb = v4l2_m2m_next_src_buf(ctx->fh.m2m_ctx); struct s5p_jpeg_buffer jpeg_buffer; unsigned int word; int c, x, components; jpeg_buffer.size = 2; jpeg_buffer.data = (unsigned long)vb2_plane_vaddr(&vb->vb2_buf, 0) + ctx->out_q.sos + 2; jpeg_buffer.curr = 0; if (get_word_be(&jpeg_buffer, &word)) return; if (word < 2) jpeg_buffer.size = 0; else jpeg_buffer.size = (long)word - 2; jpeg_buffer.data += 2; jpeg_buffer.curr = 0; components = get_byte(&jpeg_buffer); if (components == -1) return; while (components--) { c = get_byte(&jpeg_buffer); if (c == -1) return; x = get_byte(&jpeg_buffer); if (x == -1) return; exynos4_jpeg_select_dec_h_tbl(jpeg->regs, c, (((x >> 4) & 0x1) << 1) | (x & 0x1)); } }"
1545----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2025-21696/bad/mremap.c----move_ptes,"static int move_ptes(struct vm_area_struct *vma, pmd_t *old_pmd, unsigned long old_addr, unsigned long old_end, struct vm_area_struct *new_vma, pmd_t *new_pmd, unsigned long new_addr, bool need_rmap_locks) { struct mm_struct *mm = vma->vm_mm; pte_t *old_pte, *new_pte, pte; pmd_t dummy_pmdval; spinlock_t *old_ptl, *new_ptl; bool force_flush = false; unsigned long len = old_end - old_addr; int err = 0; if (need_rmap_locks) take_rmap_locks(vma); old_pte = pte_offset_map_lock(mm, old_pmd, old_addr, &old_ptl); if (!old_pte) { err = -EAGAIN; goto out; } new_pte = pte_offset_map_rw_nolock(mm, new_pmd, new_addr, &dummy_pmdval, &new_ptl); if (!new_pte) { pte_unmap_unlock(old_pte, old_ptl); err = -EAGAIN; goto out; } if (new_ptl != old_ptl) spin_lock_nested(new_ptl, SINGLE_DEPTH_NESTING); flush_tlb_batched_pending(vma->vm_mm); arch_enter_lazy_mmu_mode(); for (; old_addr < old_end; old_pte++, old_addr += PAGE_SIZE, new_pte++, new_addr += PAGE_SIZE) { if (pte_none(ptep_get(old_pte))) continue; pte = ptep_get_and_clear(mm, old_addr, old_pte); if (pte_present(pte)) force_flush = true; pte = move_pte(pte, old_addr, new_addr); pte = move_soft_dirty_pte(pte); <S2SV_StartVul> set_pte_at(mm, new_addr, new_pte, pte); <S2SV_EndVul> } arch_leave_lazy_mmu_mode(); if (force_flush) flush_tlb_range(vma, old_end - len, old_end); if (new_ptl != old_ptl) spin_unlock(new_ptl); pte_unmap(new_pte - 1); pte_unmap_unlock(old_pte - 1, old_ptl); out: if (need_rmap_locks) drop_rmap_locks(vma); return err; }","- set_pte_at(mm, new_addr, new_pte, pte);
+ bool need_clear_uffd_wp = vma_has_uffd_without_event_remap(vma);
+ if (need_clear_uffd_wp && pte_marker_uffd_wp(pte))
+ pte_clear(mm, new_addr, new_pte);
+ else {
+ if (need_clear_uffd_wp) {
+ if (pte_present(pte))
+ pte = pte_clear_uffd_wp(pte);
+ else if (is_swap_pte(pte))
+ pte = pte_swp_clear_uffd_wp(pte);
+ }
+ set_pte_at(mm, new_addr, new_pte, pte);
+ }
+ }","static int move_ptes(struct vm_area_struct *vma, pmd_t *old_pmd, unsigned long old_addr, unsigned long old_end, struct vm_area_struct *new_vma, pmd_t *new_pmd, unsigned long new_addr, bool need_rmap_locks) { bool need_clear_uffd_wp = vma_has_uffd_without_event_remap(vma); struct mm_struct *mm = vma->vm_mm; pte_t *old_pte, *new_pte, pte; pmd_t dummy_pmdval; spinlock_t *old_ptl, *new_ptl; bool force_flush = false; unsigned long len = old_end - old_addr; int err = 0; if (need_rmap_locks) take_rmap_locks(vma); old_pte = pte_offset_map_lock(mm, old_pmd, old_addr, &old_ptl); if (!old_pte) { err = -EAGAIN; goto out; } new_pte = pte_offset_map_rw_nolock(mm, new_pmd, new_addr, &dummy_pmdval, &new_ptl); if (!new_pte) { pte_unmap_unlock(old_pte, old_ptl); err = -EAGAIN; goto out; } if (new_ptl != old_ptl) spin_lock_nested(new_ptl, SINGLE_DEPTH_NESTING); flush_tlb_batched_pending(vma->vm_mm); arch_enter_lazy_mmu_mode(); for (; old_addr < old_end; old_pte++, old_addr += PAGE_SIZE, new_pte++, new_addr += PAGE_SIZE) { if (pte_none(ptep_get(old_pte))) continue; pte = ptep_get_and_clear(mm, old_addr, old_pte); if (pte_present(pte)) force_flush = true; pte = move_pte(pte, old_addr, new_addr); pte = move_soft_dirty_pte(pte); if (need_clear_uffd_wp && pte_marker_uffd_wp(pte)) pte_clear(mm, new_addr, new_pte); else { if (need_clear_uffd_wp) { if (pte_present(pte)) pte = pte_clear_uffd_wp(pte); else if (is_swap_pte(pte)) pte = pte_swp_clear_uffd_wp(pte); } set_pte_at(mm, new_addr, new_pte, pte); } } arch_leave_lazy_mmu_mode(); if (force_flush) flush_tlb_range(vma, old_end - len, old_end); if (new_ptl != old_ptl) spin_unlock(new_ptl); pte_unmap(new_pte - 1); pte_unmap_unlock(old_pte - 1, old_ptl); out: if (need_rmap_locks) drop_rmap_locks(vma); return err; }"
337----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46794/bad/tdx.c----mmio_read,"static bool mmio_read(int size, unsigned long addr, unsigned long *val) { struct tdx_module_args args = { .r10 = TDX_HYPERCALL_STANDARD, .r11 = hcall_func(EXIT_REASON_EPT_VIOLATION), .r12 = size, .r13 = EPT_READ, .r14 = addr, <S2SV_StartVul> .r15 = *val, <S2SV_EndVul> }; if (__tdx_hypercall(&args)) return false; *val = args.r11; return true; }","- .r15 = *val,","static bool mmio_read(int size, unsigned long addr, unsigned long *val) { struct tdx_module_args args = { .r10 = TDX_HYPERCALL_STANDARD, .r11 = hcall_func(EXIT_REASON_EPT_VIOLATION), .r12 = size, .r13 = EPT_READ, .r14 = addr, }; if (__tdx_hypercall(&args)) return false; *val = args.r11; return true; }"
198----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46682/bad/nfs4state.c----nfs4_show_open,"static int nfs4_show_open(struct seq_file *s, struct nfs4_stid *st) { struct nfs4_ol_stateid *ols; struct nfs4_file *nf; struct nfsd_file *file; struct nfs4_stateowner *oo; unsigned int access, deny; ols = openlockstateid(st); oo = ols->st_stateowner; nf = st->sc_file; seq_puts(s, ""- ""); nfs4_show_stateid(s, &st->sc_stateid); seq_puts(s, "": { type: open, ""); access = bmap_to_share_mode(ols->st_access_bmap); deny = bmap_to_share_mode(ols->st_deny_bmap); seq_printf(s, ""access: %s%s, "", access & NFS4_SHARE_ACCESS_READ ? ""r"" : ""-"", access & NFS4_SHARE_ACCESS_WRITE ? ""w"" : ""-""); seq_printf(s, ""deny: %s%s, "", deny & NFS4_SHARE_ACCESS_READ ? ""r"" : ""-"", deny & NFS4_SHARE_ACCESS_WRITE ? ""w"" : ""-""); <S2SV_StartVul> spin_lock(&nf->fi_lock); <S2SV_EndVul> <S2SV_StartVul> file = find_any_file_locked(nf); <S2SV_EndVul> <S2SV_StartVul> if (file) { <S2SV_EndVul> <S2SV_StartVul> nfs4_show_superblock(s, file); <S2SV_EndVul> <S2SV_StartVul> seq_puts(s, "", ""); <S2SV_EndVul> <S2SV_StartVul> nfs4_show_fname(s, file); <S2SV_EndVul> <S2SV_StartVul> seq_puts(s, "", ""); <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> spin_unlock(&nf->fi_lock); <S2SV_EndVul> nfs4_show_owner(s, oo); if (st->sc_status & SC_STATUS_ADMIN_REVOKED) seq_puts(s, "", admin-revoked""); seq_puts(s, "" }\n""); return 0; }","- spin_lock(&nf->fi_lock);
- file = find_any_file_locked(nf);
- if (file) {
- nfs4_show_superblock(s, file);
- seq_puts(s, "", "");
- nfs4_show_fname(s, file);
- seq_puts(s, "", "");
- }
- spin_unlock(&nf->fi_lock);
+ if (nf) {
+ spin_lock(&nf->fi_lock);
+ file = find_any_file_locked(nf);
+ if (file) {
+ nfs4_show_superblock(s, file);
+ seq_puts(s, "", "");
+ nfs4_show_fname(s, file);
+ seq_puts(s, "", "");
+ }
+ spin_unlock(&nf->fi_lock);
+ } else
+ seq_puts(s, ""closed, "");","static int nfs4_show_open(struct seq_file *s, struct nfs4_stid *st) { struct nfs4_ol_stateid *ols; struct nfs4_file *nf; struct nfsd_file *file; struct nfs4_stateowner *oo; unsigned int access, deny; ols = openlockstateid(st); oo = ols->st_stateowner; nf = st->sc_file; seq_puts(s, ""- ""); nfs4_show_stateid(s, &st->sc_stateid); seq_puts(s, "": { type: open, ""); access = bmap_to_share_mode(ols->st_access_bmap); deny = bmap_to_share_mode(ols->st_deny_bmap); seq_printf(s, ""access: %s%s, "", access & NFS4_SHARE_ACCESS_READ ? ""r"" : ""-"", access & NFS4_SHARE_ACCESS_WRITE ? ""w"" : ""-""); seq_printf(s, ""deny: %s%s, "", deny & NFS4_SHARE_ACCESS_READ ? ""r"" : ""-"", deny & NFS4_SHARE_ACCESS_WRITE ? ""w"" : ""-""); if (nf) { spin_lock(&nf->fi_lock); file = find_any_file_locked(nf); if (file) { nfs4_show_superblock(s, file); seq_puts(s, "", ""); nfs4_show_fname(s, file); seq_puts(s, "", ""); } spin_unlock(&nf->fi_lock); } else seq_puts(s, ""closed, ""); nfs4_show_owner(s, oo); if (st->sc_status & SC_STATUS_ADMIN_REVOKED) seq_puts(s, "", admin-revoked""); seq_puts(s, "" }\n""); return 0; }"
202----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46686/bad/smb2pdu.c----smb2_new_read_req,"smb2_new_read_req(void **buf, unsigned int *total_len, struct cifs_io_parms *io_parms, struct cifs_io_subrequest *rdata, unsigned int remaining_bytes, int request_type) { int rc = -EACCES; struct smb2_read_req *req = NULL; struct smb2_hdr *shdr; struct TCP_Server_Info *server = io_parms->server; rc = smb2_plain_req_init(SMB2_READ, io_parms->tcon, server, (void **) &req, total_len); if (rc) return rc; if (server == NULL) return -ECONNABORTED; shdr = &req->hdr; shdr->Id.SyncId.ProcessId = cpu_to_le32(io_parms->pid); req->PersistentFileId = io_parms->persistent_fid; req->VolatileFileId = io_parms->volatile_fid; req->ReadChannelInfoOffset = 0; req->ReadChannelInfoLength = 0; req->Channel = 0; req->MinimumCount = 0; req->Length = cpu_to_le32(io_parms->length); req->Offset = cpu_to_le64(io_parms->offset); trace_smb3_read_enter(rdata ? rdata->rreq->debug_id : 0, rdata ? rdata->subreq.debug_index : 0, rdata ? rdata->xid : 0, io_parms->persistent_fid, io_parms->tcon->tid, io_parms->tcon->ses->Suid, io_parms->offset, io_parms->length); #ifdef CONFIG_CIFS_SMB_DIRECT <S2SV_StartVul> if (smb3_use_rdma_offload(io_parms)) { <S2SV_EndVul> struct smbd_buffer_descriptor_v1 *v1; bool need_invalidate = server->dialect == SMB30_PROT_ID; rdata->mr = smbd_register_mr(server->smbd_conn, &rdata->subreq.io_iter, true, need_invalidate); if (!rdata->mr) return -EAGAIN; req->Channel = SMB2_CHANNEL_RDMA_V1_INVALIDATE; if (need_invalidate) req->Channel = SMB2_CHANNEL_RDMA_V1; req->ReadChannelInfoOffset = cpu_to_le16(offsetof(struct smb2_read_req, Buffer)); req->ReadChannelInfoLength = cpu_to_le16(sizeof(struct smbd_buffer_descriptor_v1)); v1 = (struct smbd_buffer_descriptor_v1 *) &req->Buffer[0]; v1->offset = cpu_to_le64(rdata->mr->mr->iova); v1->token = cpu_to_le32(rdata->mr->mr->rkey); v1->length = cpu_to_le32(rdata->mr->mr->length); *total_len += sizeof(*v1) - 1; } #endif if (request_type & CHAINED_REQUEST) { if (!(request_type & END_OF_CHAIN)) { *total_len = ALIGN(*total_len, 8); shdr->NextCommand = cpu_to_le32(*total_len); } else shdr->NextCommand = 0; if (request_type & RELATED_REQUEST) { shdr->Flags |= SMB2_FLAGS_RELATED_OPERATIONS; shdr->SessionId = cpu_to_le64(0xFFFFFFFFFFFFFFFF); shdr->Id.SyncId.TreeId = cpu_to_le32(0xFFFFFFFF); req->PersistentFileId = (u64)-1; req->VolatileFileId = (u64)-1; } } if (remaining_bytes > io_parms->length) req->RemainingBytes = cpu_to_le32(remaining_bytes); else req->RemainingBytes = 0; *buf = req; return rc; }","- if (smb3_use_rdma_offload(io_parms)) {
+ if (rdata && smb3_use_rdma_offload(io_parms)) {","smb2_new_read_req(void **buf, unsigned int *total_len, struct cifs_io_parms *io_parms, struct cifs_io_subrequest *rdata, unsigned int remaining_bytes, int request_type) { int rc = -EACCES; struct smb2_read_req *req = NULL; struct smb2_hdr *shdr; struct TCP_Server_Info *server = io_parms->server; rc = smb2_plain_req_init(SMB2_READ, io_parms->tcon, server, (void **) &req, total_len); if (rc) return rc; if (server == NULL) return -ECONNABORTED; shdr = &req->hdr; shdr->Id.SyncId.ProcessId = cpu_to_le32(io_parms->pid); req->PersistentFileId = io_parms->persistent_fid; req->VolatileFileId = io_parms->volatile_fid; req->ReadChannelInfoOffset = 0; req->ReadChannelInfoLength = 0; req->Channel = 0; req->MinimumCount = 0; req->Length = cpu_to_le32(io_parms->length); req->Offset = cpu_to_le64(io_parms->offset); trace_smb3_read_enter(rdata ? rdata->rreq->debug_id : 0, rdata ? rdata->subreq.debug_index : 0, rdata ? rdata->xid : 0, io_parms->persistent_fid, io_parms->tcon->tid, io_parms->tcon->ses->Suid, io_parms->offset, io_parms->length); #ifdef CONFIG_CIFS_SMB_DIRECT if (rdata && smb3_use_rdma_offload(io_parms)) { struct smbd_buffer_descriptor_v1 *v1; bool need_invalidate = server->dialect == SMB30_PROT_ID; rdata->mr = smbd_register_mr(server->smbd_conn, &rdata->subreq.io_iter, true, need_invalidate); if (!rdata->mr) return -EAGAIN; req->Channel = SMB2_CHANNEL_RDMA_V1_INVALIDATE; if (need_invalidate) req->Channel = SMB2_CHANNEL_RDMA_V1; req->ReadChannelInfoOffset = cpu_to_le16(offsetof(struct smb2_read_req, Buffer)); req->ReadChannelInfoLength = cpu_to_le16(sizeof(struct smbd_buffer_descriptor_v1)); v1 = (struct smbd_buffer_descriptor_v1 *) &req->Buffer[0]; v1->offset = cpu_to_le64(rdata->mr->mr->iova); v1->token = cpu_to_le32(rdata->mr->mr->rkey); v1->length = cpu_to_le32(rdata->mr->mr->length); *total_len += sizeof(*v1) - 1; } #endif if (request_type & CHAINED_REQUEST) { if (!(request_type & END_OF_CHAIN)) { *total_len = ALIGN(*total_len, 8); shdr->NextCommand = cpu_to_le32(*total_len); } else shdr->NextCommand = 0; if (request_type & RELATED_REQUEST) { shdr->Flags |= SMB2_FLAGS_RELATED_OPERATIONS; shdr->SessionId = cpu_to_le64(0xFFFFFFFFFFFFFFFF); shdr->Id.SyncId.TreeId = cpu_to_le32(0xFFFFFFFF); req->PersistentFileId = (u64)-1; req->VolatileFileId = (u64)-1; } } if (remaining_bytes > io_parms->length) req->RemainingBytes = cpu_to_le32(remaining_bytes); else req->RemainingBytes = 0; *buf = req; return rc; }"
622----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49927/bad/io_apic.c----alloc_isa_irq_from_domain,"static int alloc_isa_irq_from_domain(struct irq_domain *domain, int irq, int ioapic, int pin, struct irq_alloc_info *info) { struct mp_chip_data *data; struct irq_data *irq_data = irq_get_irq_data(irq); int node = ioapic_alloc_attr_node(info); if (irq_data && irq_data->parent_data) { if (!mp_check_pin_attr(irq, info)) return -EBUSY; <S2SV_StartVul> if (__add_pin_to_irq_node(irq_data->chip_data, node, ioapic, <S2SV_EndVul> <S2SV_StartVul> info->ioapic.pin)) <S2SV_EndVul> <S2SV_StartVul> return -ENOMEM; <S2SV_EndVul> } else { info->flags |= X86_IRQ_ALLOC_LEGACY; irq = __irq_domain_alloc_irqs(domain, irq, 1, node, info, true, NULL); if (irq >= 0) { irq_data = irq_domain_get_irq_data(domain, irq); data = irq_data->chip_data; data->isa_irq = true; } } return irq; }","- if (__add_pin_to_irq_node(irq_data->chip_data, node, ioapic,
- info->ioapic.pin))
- return -ENOMEM;
+ if (!add_pin_to_irq_node(irq_data->chip_data, node, ioapic, info->ioapic.pin))","static int alloc_isa_irq_from_domain(struct irq_domain *domain, int irq, int ioapic, int pin, struct irq_alloc_info *info) { struct mp_chip_data *data; struct irq_data *irq_data = irq_get_irq_data(irq); int node = ioapic_alloc_attr_node(info); if (irq_data && irq_data->parent_data) { if (!mp_check_pin_attr(irq, info)) return -EBUSY; if (!add_pin_to_irq_node(irq_data->chip_data, node, ioapic, info->ioapic.pin)) return -ENOMEM; } else { info->flags |= X86_IRQ_ALLOC_LEGACY; irq = __irq_domain_alloc_irqs(domain, irq, 1, node, info, true, NULL); if (irq >= 0) { irq_data = irq_domain_get_irq_data(domain, irq); data = irq_data->chip_data; data->isa_irq = true; } } return irq; }"
130----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44989/bad/bond_main.c----bond_ipsec_del_sa_all,"static void bond_ipsec_del_sa_all(struct bonding *bond) { struct net_device *bond_dev = bond->dev; struct bond_ipsec *ipsec; struct slave *slave; rcu_read_lock(); slave = rcu_dereference(bond->curr_active_slave); if (!slave) { rcu_read_unlock(); return; } spin_lock_bh(&bond->ipsec_lock); list_for_each_entry(ipsec, &bond->ipsec_list, list) { if (!ipsec->xs->xso.real_dev) continue; if (!slave->dev->xfrmdev_ops || !slave->dev->xfrmdev_ops->xdo_dev_state_delete || netif_is_bond_master(slave->dev)) { slave_warn(bond_dev, slave->dev, ""%s: no slave xdo_dev_state_delete\n"", __func__); } else { slave->dev->xfrmdev_ops->xdo_dev_state_delete(ipsec->xs); } <S2SV_StartVul> ipsec->xs->xso.real_dev = NULL; <S2SV_EndVul> } spin_unlock_bh(&bond->ipsec_lock); rcu_read_unlock(); }",- ipsec->xs->xso.real_dev = NULL;,"static void bond_ipsec_del_sa_all(struct bonding *bond) { struct net_device *bond_dev = bond->dev; struct bond_ipsec *ipsec; struct slave *slave; rcu_read_lock(); slave = rcu_dereference(bond->curr_active_slave); if (!slave) { rcu_read_unlock(); return; } spin_lock_bh(&bond->ipsec_lock); list_for_each_entry(ipsec, &bond->ipsec_list, list) { if (!ipsec->xs->xso.real_dev) continue; if (!slave->dev->xfrmdev_ops || !slave->dev->xfrmdev_ops->xdo_dev_state_delete || netif_is_bond_master(slave->dev)) { slave_warn(bond_dev, slave->dev, ""%s: no slave xdo_dev_state_delete\n"", __func__); } else { slave->dev->xfrmdev_ops->xdo_dev_state_delete(ipsec->xs); } } spin_unlock_bh(&bond->ipsec_lock); rcu_read_unlock(); }"
911----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50155/bad/dev.c----nsim_dev_trap_report_work,"static void nsim_dev_trap_report_work(struct work_struct *work) { struct nsim_trap_data *nsim_trap_data; struct nsim_dev_port *nsim_dev_port; struct nsim_dev *nsim_dev; nsim_trap_data = container_of(work, struct nsim_trap_data, trap_report_dw.work); nsim_dev = nsim_trap_data->nsim_dev; if (!devl_trylock(priv_to_devlink(nsim_dev))) { <S2SV_StartVul> schedule_delayed_work(&nsim_dev->trap_data->trap_report_dw, 1); <S2SV_EndVul> return; } list_for_each_entry(nsim_dev_port, &nsim_dev->port_list, list) { if (!netif_running(nsim_dev_port->ns->netdev)) continue; nsim_dev_trap_report(nsim_dev_port); } devl_unlock(priv_to_devlink(nsim_dev)); <S2SV_StartVul> schedule_delayed_work(&nsim_dev->trap_data->trap_report_dw, <S2SV_EndVul> <S2SV_StartVul> msecs_to_jiffies(NSIM_TRAP_REPORT_INTERVAL_MS)); <S2SV_EndVul> }","- schedule_delayed_work(&nsim_dev->trap_data->trap_report_dw, 1);
- schedule_delayed_work(&nsim_dev->trap_data->trap_report_dw,
- msecs_to_jiffies(NSIM_TRAP_REPORT_INTERVAL_MS));
+ queue_delayed_work(system_unbound_wq,
+ &nsim_dev->trap_data->trap_report_dw, 1);
+ cond_resched();
+ queue_delayed_work(system_unbound_wq,
+ &nsim_dev->trap_data->trap_report_dw,
+ msecs_to_jiffies(NSIM_TRAP_REPORT_INTERVAL_MS));","static void nsim_dev_trap_report_work(struct work_struct *work) { struct nsim_trap_data *nsim_trap_data; struct nsim_dev_port *nsim_dev_port; struct nsim_dev *nsim_dev; nsim_trap_data = container_of(work, struct nsim_trap_data, trap_report_dw.work); nsim_dev = nsim_trap_data->nsim_dev; if (!devl_trylock(priv_to_devlink(nsim_dev))) { queue_delayed_work(system_unbound_wq, &nsim_dev->trap_data->trap_report_dw, 1); return; } list_for_each_entry(nsim_dev_port, &nsim_dev->port_list, list) { if (!netif_running(nsim_dev_port->ns->netdev)) continue; nsim_dev_trap_report(nsim_dev_port); cond_resched(); } devl_unlock(priv_to_devlink(nsim_dev)); queue_delayed_work(system_unbound_wq, &nsim_dev->trap_data->trap_report_dw, msecs_to_jiffies(NSIM_TRAP_REPORT_INTERVAL_MS)); }"
87----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44957/bad/privcmd.c----privcmd_irqfd_deassign,"static int privcmd_irqfd_deassign(struct privcmd_irqfd *irqfd) { struct privcmd_kernel_irqfd *kirqfd; struct eventfd_ctx *eventfd; eventfd = eventfd_ctx_fdget(irqfd->fd); if (IS_ERR(eventfd)) return PTR_ERR(eventfd); <S2SV_StartVul> mutex_lock(&irqfds_lock); <S2SV_EndVul> list_for_each_entry(kirqfd, &irqfds_list, list) { if (kirqfd->eventfd == eventfd) { irqfd_deactivate(kirqfd); break; } } <S2SV_StartVul> mutex_unlock(&irqfds_lock); <S2SV_EndVul> eventfd_ctx_put(eventfd); flush_workqueue(irqfd_cleanup_wq); return 0; }","- mutex_lock(&irqfds_lock);
- mutex_unlock(&irqfds_lock);
+ unsigned long flags;
+ spin_lock_irqsave(&irqfds_lock, flags);
+ spin_unlock_irqrestore(&irqfds_lock, flags);","static int privcmd_irqfd_deassign(struct privcmd_irqfd *irqfd) { struct privcmd_kernel_irqfd *kirqfd; struct eventfd_ctx *eventfd; unsigned long flags; eventfd = eventfd_ctx_fdget(irqfd->fd); if (IS_ERR(eventfd)) return PTR_ERR(eventfd); spin_lock_irqsave(&irqfds_lock, flags); list_for_each_entry(kirqfd, &irqfds_list, list) { if (kirqfd->eventfd == eventfd) { irqfd_deactivate(kirqfd); break; } } spin_unlock_irqrestore(&irqfds_lock, flags); eventfd_ctx_put(eventfd); flush_workqueue(irqfd_cleanup_wq); return 0; }"
328----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46793/bad/bxt_rt298.c----broxton_audio_probe,"static int broxton_audio_probe(struct platform_device *pdev) { struct bxt_rt286_private *ctx; struct snd_soc_card *card = (struct snd_soc_card *)pdev->id_entry->driver_data; struct snd_soc_acpi_mach *mach; const char *platform_name; int ret; int i; for (i = 0; i < ARRAY_SIZE(broxton_rt298_dais); i++) { <S2SV_StartVul> if (card->dai_link[i].codecs->name && <S2SV_EndVul> !strncmp(card->dai_link[i].codecs->name, ""i2c-INT343A:00"", I2C_NAME_SIZE)) { if (!strncmp(card->name, ""broxton-rt298"", PLATFORM_NAME_SIZE)) { card->dai_link[i].name = ""SSP5-Codec""; card->dai_link[i].cpus->dai_name = ""SSP5 Pin""; } else if (!strncmp(card->name, ""geminilake-rt298"", PLATFORM_NAME_SIZE)) { card->dai_link[i].name = ""SSP2-Codec""; card->dai_link[i].cpus->dai_name = ""SSP2 Pin""; } } } ctx = devm_kzalloc(&pdev->dev, sizeof(*ctx), GFP_KERNEL); if (!ctx) return -ENOMEM; INIT_LIST_HEAD(&ctx->hdmi_pcm_list); card->dev = &pdev->dev; snd_soc_card_set_drvdata(card, ctx); mach = pdev->dev.platform_data; platform_name = mach->mach_params.platform; ret = snd_soc_fixup_dai_links_platform_name(card, platform_name); if (ret) return ret; ctx->common_hdmi_codec_drv = mach->mach_params.common_hdmi_codec_drv; return devm_snd_soc_register_card(&pdev->dev, card); }","- if (card->dai_link[i].codecs->name &&
+ if (card->dai_link[i].num_codecs &&","static int broxton_audio_probe(struct platform_device *pdev) { struct bxt_rt286_private *ctx; struct snd_soc_card *card = (struct snd_soc_card *)pdev->id_entry->driver_data; struct snd_soc_acpi_mach *mach; const char *platform_name; int ret; int i; for (i = 0; i < ARRAY_SIZE(broxton_rt298_dais); i++) { if (card->dai_link[i].num_codecs && !strncmp(card->dai_link[i].codecs->name, ""i2c-INT343A:00"", I2C_NAME_SIZE)) { if (!strncmp(card->name, ""broxton-rt298"", PLATFORM_NAME_SIZE)) { card->dai_link[i].name = ""SSP5-Codec""; card->dai_link[i].cpus->dai_name = ""SSP5 Pin""; } else if (!strncmp(card->name, ""geminilake-rt298"", PLATFORM_NAME_SIZE)) { card->dai_link[i].name = ""SSP2-Codec""; card->dai_link[i].cpus->dai_name = ""SSP2 Pin""; } } } ctx = devm_kzalloc(&pdev->dev, sizeof(*ctx), GFP_KERNEL); if (!ctx) return -ENOMEM; INIT_LIST_HEAD(&ctx->hdmi_pcm_list); card->dev = &pdev->dev; snd_soc_card_set_drvdata(card, ctx); mach = pdev->dev.platform_data; platform_name = mach->mach_params.platform; ret = snd_soc_fixup_dai_links_platform_name(card, platform_name); if (ret) return ret; ctx->common_hdmi_codec_drv = mach->mach_params.common_hdmi_codec_drv; return devm_snd_soc_register_card(&pdev->dev, card); }"
807----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50082/bad/blk-rq-qos.c----rq_qos_wake_function,"static int rq_qos_wake_function(struct wait_queue_entry *curr, unsigned int mode, int wake_flags, void *key) { struct rq_qos_wait_data *data = container_of(curr, struct rq_qos_wait_data, wq); if (!data->cb(data->rqw, data->private_data)) return -1; data->got_token = true; smp_wmb(); <S2SV_StartVul> list_del_init(&curr->entry); <S2SV_EndVul> wake_up_process(data->task); return 1; }","- list_del_init(&curr->entry);
+ list_del_init_careful(&curr->entry);","static int rq_qos_wake_function(struct wait_queue_entry *curr, unsigned int mode, int wake_flags, void *key) { struct rq_qos_wait_data *data = container_of(curr, struct rq_qos_wait_data, wq); if (!data->cb(data->rqw, data->private_data)) return -1; data->got_token = true; smp_wmb(); wake_up_process(data->task); list_del_init_careful(&curr->entry); return 1; }"
1247----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-54683/bad/xt_IDLETIMER.c----idletimer_tg_destroy,"static void idletimer_tg_destroy(const struct xt_tgdtor_param *par) { const struct idletimer_tg_info *info = par->targinfo; pr_debug(""destroy targinfo %s\n"", info->label); mutex_lock(&list_mutex); <S2SV_StartVul> if (--info->timer->refcnt == 0) { <S2SV_EndVul> <S2SV_StartVul> pr_debug(""deleting timer %s\n"", info->label); <S2SV_EndVul> <S2SV_StartVul> list_del(&info->timer->entry); <S2SV_EndVul> <S2SV_StartVul> timer_shutdown_sync(&info->timer->timer); <S2SV_EndVul> <S2SV_StartVul> cancel_work_sync(&info->timer->work); <S2SV_EndVul> <S2SV_StartVul> sysfs_remove_file(idletimer_tg_kobj, &info->timer->attr.attr); <S2SV_EndVul> <S2SV_StartVul> kfree(info->timer->attr.attr.name); <S2SV_EndVul> <S2SV_StartVul> kfree(info->timer); <S2SV_EndVul> <S2SV_StartVul> } else { <S2SV_EndVul> pr_debug(""decreased refcnt of timer %s to %u\n"", info->label, info->timer->refcnt); <S2SV_StartVul> } <S2SV_EndVul> mutex_unlock(&list_mutex); <S2SV_StartVul> } <S2SV_EndVul>","- if (--info->timer->refcnt == 0) {
- pr_debug(""deleting timer %s\n"", info->label);
- list_del(&info->timer->entry);
- timer_shutdown_sync(&info->timer->timer);
- cancel_work_sync(&info->timer->work);
- sysfs_remove_file(idletimer_tg_kobj, &info->timer->attr.attr);
- kfree(info->timer->attr.attr.name);
- kfree(info->timer);
- } else {
- }
- }
+ if (--info->timer->refcnt > 0) {
+ mutex_unlock(&list_mutex);
+ return;
+ }
+ pr_debug(""deleting timer %s\n"", info->label);
+ list_del(&info->timer->entry);
+ mutex_unlock(&list_mutex);
+ timer_shutdown_sync(&info->timer->timer);
+ cancel_work_sync(&info->timer->work);
+ sysfs_remove_file(idletimer_tg_kobj, &info->timer->attr.attr);
+ kfree(info->timer->attr.attr.name);
+ kfree(info->timer);
+ }","static void idletimer_tg_destroy(const struct xt_tgdtor_param *par) { const struct idletimer_tg_info *info = par->targinfo; pr_debug(""destroy targinfo %s\n"", info->label); mutex_lock(&list_mutex); if (--info->timer->refcnt > 0) { pr_debug(""decreased refcnt of timer %s to %u\n"", info->label, info->timer->refcnt); mutex_unlock(&list_mutex); return; } pr_debug(""deleting timer %s\n"", info->label); list_del(&info->timer->entry); mutex_unlock(&list_mutex); timer_shutdown_sync(&info->timer->timer); cancel_work_sync(&info->timer->work); sysfs_remove_file(idletimer_tg_kobj, &info->timer->attr.attr); kfree(info->timer->attr.attr.name); kfree(info->timer); }"
441----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47678/bad/icmp.c----icmp6_send,"void icmp6_send(struct sk_buff *skb, u8 type, u8 code, __u32 info, const struct in6_addr *force_saddr, const struct inet6_skb_parm *parm) { struct inet6_dev *idev = NULL; struct ipv6hdr *hdr = ipv6_hdr(skb); struct sock *sk; struct net *net; struct ipv6_pinfo *np; const struct in6_addr *saddr = NULL; struct dst_entry *dst; struct icmp6hdr tmp_hdr; struct flowi6 fl6; struct icmpv6_msg msg; struct ipcm6_cookie ipc6; int iif = 0; int addr_type = 0; int len; u32 mark; if ((u8 *)hdr < skb->head || (skb_network_header(skb) + sizeof(*hdr)) > skb_tail_pointer(skb)) return; if (!skb->dev) return; net = dev_net(skb->dev); mark = IP6_REPLY_MARK(net, skb->mark); addr_type = ipv6_addr_type(&hdr->daddr); if (ipv6_chk_addr(net, &hdr->daddr, skb->dev, 0) || ipv6_chk_acast_addr_src(net, skb->dev, &hdr->daddr)) saddr = &hdr->daddr; if (addr_type & IPV6_ADDR_MULTICAST || skb->pkt_type != PACKET_HOST) { if (type != ICMPV6_PKT_TOOBIG && !(type == ICMPV6_PARAMPROB && code == ICMPV6_UNK_OPTION && (opt_unrec(skb, info)))) return; saddr = NULL; } addr_type = ipv6_addr_type(&hdr->saddr); if (__ipv6_addr_needs_scope_id(addr_type)) { iif = icmp6_iif(skb); } else { iif = l3mdev_master_ifindex(skb->dev); } if ((addr_type == IPV6_ADDR_ANY) || (addr_type & IPV6_ADDR_MULTICAST)) { net_dbg_ratelimited(""icmp6_send: addr_any/mcast source [%pI6c > %pI6c]\n"", &hdr->saddr, &hdr->daddr); return; } if (is_ineligible(skb)) { net_dbg_ratelimited(""icmp6_send: no reply to icmp error [%pI6c > %pI6c]\n"", &hdr->saddr, &hdr->daddr); return; } local_bh_disable(); <S2SV_StartVul> if (!(skb->dev->flags & IFF_LOOPBACK) && !icmpv6_global_allow(net, type)) <S2SV_EndVul> goto out_bh_enable; mip6_addr_swap(skb, parm); sk = icmpv6_xmit_lock(net); if (!sk) goto out_bh_enable; memset(&fl6, 0, sizeof(fl6)); fl6.flowi6_proto = IPPROTO_ICMPV6; fl6.daddr = hdr->saddr; if (force_saddr) saddr = force_saddr; if (saddr) { fl6.saddr = *saddr; } else if (!icmpv6_rt_has_prefsrc(sk, type, &fl6)) { struct net_device *in_netdev; in_netdev = dev_get_by_index(net, parm->iif); if (in_netdev) { ipv6_dev_get_saddr(net, in_netdev, &fl6.daddr, inet6_sk(sk)->srcprefs, &fl6.saddr); dev_put(in_netdev); } } fl6.flowi6_mark = mark; fl6.flowi6_oif = iif; fl6.fl6_icmp_type = type; fl6.fl6_icmp_code = code; fl6.flowi6_uid = sock_net_uid(net, NULL); fl6.mp_hash = rt6_multipath_hash(net, &fl6, skb, NULL); security_skb_classify_flow(skb, flowi6_to_flowi_common(&fl6)); np = inet6_sk(sk); <S2SV_StartVul> if (!icmpv6_xrlim_allow(sk, type, &fl6)) <S2SV_EndVul> goto out; tmp_hdr.icmp6_type = type; tmp_hdr.icmp6_code = code; tmp_hdr.icmp6_cksum = 0; tmp_hdr.icmp6_pointer = htonl(info); if (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr)) fl6.flowi6_oif = READ_ONCE(np->mcast_oif); else if (!fl6.flowi6_oif) fl6.flowi6_oif = READ_ONCE(np->ucast_oif); ipcm6_init_sk(&ipc6, sk); ipc6.sockc.mark = mark; fl6.flowlabel = ip6_make_flowinfo(ipc6.tclass, fl6.flowlabel); dst = icmpv6_route_lookup(net, skb, sk, &fl6); if (IS_ERR(dst)) goto out; ipc6.hlimit = ip6_sk_dst_hoplimit(np, &fl6, dst); msg.skb = skb; msg.offset = skb_network_offset(skb); msg.type = type; len = skb->len - msg.offset; len = min_t(unsigned int, len, IPV6_MIN_MTU - sizeof(struct ipv6hdr) - sizeof(struct icmp6hdr)); if (len < 0) { net_dbg_ratelimited(""icmp: len problem [%pI6c > %pI6c]\n"", &hdr->saddr, &hdr->daddr); goto out_dst_release; } rcu_read_lock(); idev = __in6_dev_get(skb->dev); if (ip6_append_data(sk, icmpv6_getfrag, &msg, len + sizeof(struct icmp6hdr), sizeof(struct icmp6hdr), &ipc6, &fl6, dst_rt6_info(dst), MSG_DONTWAIT)) { ICMP6_INC_STATS(net, idev, ICMP6_MIB_OUTERRORS); ip6_flush_pending_frames(sk); } else { icmpv6_push_pending_frames(sk, &fl6, &tmp_hdr, len + sizeof(struct icmp6hdr)); } rcu_read_unlock(); out_dst_release: dst_release(dst); out: icmpv6_xmit_unlock(sk); out_bh_enable: local_bh_enable(); }","- if (!(skb->dev->flags & IFF_LOOPBACK) && !icmpv6_global_allow(net, type))
- if (!icmpv6_xrlim_allow(sk, type, &fl6))
+ bool apply_ratelimit = false;
+ }
+ if (!(skb->dev->flags & IFF_LOOPBACK) &&
+ !icmpv6_global_allow(net, type, &apply_ratelimit))
+ if (!icmpv6_xrlim_allow(sk, type, &fl6, apply_ratelimit))","void icmp6_send(struct sk_buff *skb, u8 type, u8 code, __u32 info, const struct in6_addr *force_saddr, const struct inet6_skb_parm *parm) { struct inet6_dev *idev = NULL; struct ipv6hdr *hdr = ipv6_hdr(skb); struct sock *sk; struct net *net; struct ipv6_pinfo *np; const struct in6_addr *saddr = NULL; bool apply_ratelimit = false; struct dst_entry *dst; struct icmp6hdr tmp_hdr; struct flowi6 fl6; struct icmpv6_msg msg; struct ipcm6_cookie ipc6; int iif = 0; int addr_type = 0; int len; u32 mark; if ((u8 *)hdr < skb->head || (skb_network_header(skb) + sizeof(*hdr)) > skb_tail_pointer(skb)) return; if (!skb->dev) return; net = dev_net(skb->dev); mark = IP6_REPLY_MARK(net, skb->mark); addr_type = ipv6_addr_type(&hdr->daddr); if (ipv6_chk_addr(net, &hdr->daddr, skb->dev, 0) || ipv6_chk_acast_addr_src(net, skb->dev, &hdr->daddr)) saddr = &hdr->daddr; if (addr_type & IPV6_ADDR_MULTICAST || skb->pkt_type != PACKET_HOST) { if (type != ICMPV6_PKT_TOOBIG && !(type == ICMPV6_PARAMPROB && code == ICMPV6_UNK_OPTION && (opt_unrec(skb, info)))) return; saddr = NULL; } addr_type = ipv6_addr_type(&hdr->saddr); if (__ipv6_addr_needs_scope_id(addr_type)) { iif = icmp6_iif(skb); } else { iif = l3mdev_master_ifindex(skb->dev); } if ((addr_type == IPV6_ADDR_ANY) || (addr_type & IPV6_ADDR_MULTICAST)) { net_dbg_ratelimited(""icmp6_send: addr_any/mcast source [%pI6c > %pI6c]\n"", &hdr->saddr, &hdr->daddr); return; } if (is_ineligible(skb)) { net_dbg_ratelimited(""icmp6_send: no reply to icmp error [%pI6c > %pI6c]\n"", &hdr->saddr, &hdr->daddr); return; } local_bh_disable(); if (!(skb->dev->flags & IFF_LOOPBACK) && !icmpv6_global_allow(net, type, &apply_ratelimit)) goto out_bh_enable; mip6_addr_swap(skb, parm); sk = icmpv6_xmit_lock(net); if (!sk) goto out_bh_enable; memset(&fl6, 0, sizeof(fl6)); fl6.flowi6_proto = IPPROTO_ICMPV6; fl6.daddr = hdr->saddr; if (force_saddr) saddr = force_saddr; if (saddr) { fl6.saddr = *saddr; } else if (!icmpv6_rt_has_prefsrc(sk, type, &fl6)) { struct net_device *in_netdev; in_netdev = dev_get_by_index(net, parm->iif); if (in_netdev) { ipv6_dev_get_saddr(net, in_netdev, &fl6.daddr, inet6_sk(sk)->srcprefs, &fl6.saddr); dev_put(in_netdev); } } fl6.flowi6_mark = mark; fl6.flowi6_oif = iif; fl6.fl6_icmp_type = type; fl6.fl6_icmp_code = code; fl6.flowi6_uid = sock_net_uid(net, NULL); fl6.mp_hash = rt6_multipath_hash(net, &fl6, skb, NULL); security_skb_classify_flow(skb, flowi6_to_flowi_common(&fl6)); np = inet6_sk(sk); if (!icmpv6_xrlim_allow(sk, type, &fl6, apply_ratelimit)) goto out; tmp_hdr.icmp6_type = type; tmp_hdr.icmp6_code = code; tmp_hdr.icmp6_cksum = 0; tmp_hdr.icmp6_pointer = htonl(info); if (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr)) fl6.flowi6_oif = READ_ONCE(np->mcast_oif); else if (!fl6.flowi6_oif) fl6.flowi6_oif = READ_ONCE(np->ucast_oif); ipcm6_init_sk(&ipc6, sk); ipc6.sockc.mark = mark; fl6.flowlabel = ip6_make_flowinfo(ipc6.tclass, fl6.flowlabel); dst = icmpv6_route_lookup(net, skb, sk, &fl6); if (IS_ERR(dst)) goto out; ipc6.hlimit = ip6_sk_dst_hoplimit(np, &fl6, dst); msg.skb = skb; msg.offset = skb_network_offset(skb); msg.type = type; len = skb->len - msg.offset; len = min_t(unsigned int, len, IPV6_MIN_MTU - sizeof(struct ipv6hdr) - sizeof(struct icmp6hdr)); if (len < 0) { net_dbg_ratelimited(""icmp: len problem [%pI6c > %pI6c]\n"", &hdr->saddr, &hdr->daddr); goto out_dst_release; } rcu_read_lock(); idev = __in6_dev_get(skb->dev); if (ip6_append_data(sk, icmpv6_getfrag, &msg, len + sizeof(struct icmp6hdr), sizeof(struct icmp6hdr), &ipc6, &fl6, dst_rt6_info(dst), MSG_DONTWAIT)) { ICMP6_INC_STATS(net, idev, ICMP6_MIB_OUTERRORS); ip6_flush_pending_frames(sk); } else { icmpv6_push_pending_frames(sk, &fl6, &tmp_hdr, len + sizeof(struct icmp6hdr)); } rcu_read_unlock(); out_dst_release: dst_release(dst); out: icmpv6_xmit_unlock(sk); out_bh_enable: local_bh_enable(); }"
1058----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50299/bad/sm_statefuns.c----sctp_sf_ootb,"enum sctp_disposition sctp_sf_ootb(struct net *net, const struct sctp_endpoint *ep, const struct sctp_association *asoc, const union sctp_subtype type, void *arg, struct sctp_cmd_seq *commands) { struct sctp_chunk *chunk = arg; struct sk_buff *skb = chunk->skb; struct sctp_chunkhdr *ch; struct sctp_errhdr *err; int ootb_cookie_ack = 0; int ootb_shut_ack = 0; __u8 *ch_end; SCTP_INC_STATS(net, SCTP_MIB_OUTOFBLUES); if (asoc && !sctp_vtag_verify(chunk, asoc)) asoc = NULL; ch = (struct sctp_chunkhdr *)chunk->chunk_hdr; do { if (ntohs(ch->length) < sizeof(*ch)) return sctp_sf_violation_chunklen(net, ep, asoc, type, arg, commands); ch_end = ((__u8 *)ch) + SCTP_PAD4(ntohs(ch->length)); if (ch_end > skb_tail_pointer(skb)) return sctp_sf_violation_chunklen(net, ep, asoc, type, arg, commands); if (SCTP_CID_SHUTDOWN_ACK == ch->type) ootb_shut_ack = 1; if (SCTP_CID_ABORT == ch->type) return sctp_sf_pdiscard(net, ep, asoc, type, arg, commands); if (SCTP_CID_COOKIE_ACK == ch->type) ootb_cookie_ack = 1; if (SCTP_CID_ERROR == ch->type) { sctp_walk_errors(err, ch) { if (SCTP_ERROR_STALE_COOKIE == err->cause) { ootb_cookie_ack = 1; break; } } } ch = (struct sctp_chunkhdr *)ch_end; <S2SV_StartVul> } while (ch_end < skb_tail_pointer(skb)); <S2SV_EndVul> if (ootb_shut_ack) return sctp_sf_shut_8_4_5(net, ep, asoc, type, arg, commands); else if (ootb_cookie_ack) return sctp_sf_pdiscard(net, ep, asoc, type, arg, commands); else return sctp_sf_tabort_8_4_8(net, ep, asoc, type, arg, commands); }","- } while (ch_end < skb_tail_pointer(skb));
+ } while (ch_end + sizeof(*ch) < skb_tail_pointer(skb));","enum sctp_disposition sctp_sf_ootb(struct net *net, const struct sctp_endpoint *ep, const struct sctp_association *asoc, const union sctp_subtype type, void *arg, struct sctp_cmd_seq *commands) { struct sctp_chunk *chunk = arg; struct sk_buff *skb = chunk->skb; struct sctp_chunkhdr *ch; struct sctp_errhdr *err; int ootb_cookie_ack = 0; int ootb_shut_ack = 0; __u8 *ch_end; SCTP_INC_STATS(net, SCTP_MIB_OUTOFBLUES); if (asoc && !sctp_vtag_verify(chunk, asoc)) asoc = NULL; ch = (struct sctp_chunkhdr *)chunk->chunk_hdr; do { if (ntohs(ch->length) < sizeof(*ch)) return sctp_sf_violation_chunklen(net, ep, asoc, type, arg, commands); ch_end = ((__u8 *)ch) + SCTP_PAD4(ntohs(ch->length)); if (ch_end > skb_tail_pointer(skb)) return sctp_sf_violation_chunklen(net, ep, asoc, type, arg, commands); if (SCTP_CID_SHUTDOWN_ACK == ch->type) ootb_shut_ack = 1; if (SCTP_CID_ABORT == ch->type) return sctp_sf_pdiscard(net, ep, asoc, type, arg, commands); if (SCTP_CID_COOKIE_ACK == ch->type) ootb_cookie_ack = 1; if (SCTP_CID_ERROR == ch->type) { sctp_walk_errors(err, ch) { if (SCTP_ERROR_STALE_COOKIE == err->cause) { ootb_cookie_ack = 1; break; } } } ch = (struct sctp_chunkhdr *)ch_end; } while (ch_end + sizeof(*ch) < skb_tail_pointer(skb)); if (ootb_shut_ack) return sctp_sf_shut_8_4_5(net, ep, asoc, type, arg, commands); else if (ootb_cookie_ack) return sctp_sf_pdiscard(net, ep, asoc, type, arg, commands); else return sctp_sf_tabort_8_4_8(net, ep, asoc, type, arg, commands); }"
332----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46793/bad/bytcr_rt5640.c----snd_byt_rt5640_mc_probe,"static int snd_byt_rt5640_mc_probe(struct platform_device *pdev) { struct device *dev = &pdev->dev; static const char * const map_name[] = { ""dmic1"", ""dmic2"", ""in1"", ""in3"", ""none"" }; struct snd_soc_acpi_mach *mach = dev_get_platdata(dev); __maybe_unused const char *spk_type; const struct dmi_system_id *dmi_id; const char *headset2_string = """"; const char *lineout_string = """"; struct byt_rt5640_private *priv; const char *platform_name; struct acpi_device *adev; struct device *codec_dev; const char *cfg_spk; bool sof_parent; int ret_val = 0; int dai_index = 0; int i, aif; is_bytcr = false; priv = devm_kzalloc(dev, sizeof(*priv), GFP_KERNEL); if (!priv) return -ENOMEM; byt_rt5640_card.dev = dev; snd_soc_card_set_drvdata(&byt_rt5640_card, priv); for (i = 0; i < ARRAY_SIZE(byt_rt5640_dais); i++) { <S2SV_StartVul> if (byt_rt5640_dais[i].codecs->name && <S2SV_EndVul> !strcmp(byt_rt5640_dais[i].codecs->name, ""i2c-10EC5640:00"")) { dai_index = i; break; } } adev = acpi_dev_get_first_match_dev(mach->id, NULL, -1); if (adev) { snprintf(byt_rt5640_codec_name, sizeof(byt_rt5640_codec_name), ""i2c-%s"", acpi_dev_name(adev)); byt_rt5640_dais[dai_index].codecs->name = byt_rt5640_codec_name; } else { dev_err(dev, ""Error cannot find '%s' dev\n"", mach->id); return -ENXIO; } codec_dev = acpi_get_first_physical_node(adev); acpi_dev_put(adev); if (!codec_dev) return -EPROBE_DEFER; priv->codec_dev = get_device(codec_dev); if (soc_intel_is_byt()) { if (mach->mach_params.acpi_ipc_irq_index == 0) is_bytcr = true; } if (is_bytcr) { struct acpi_chan_package chan_package = { 0 }; struct acpi_buffer format = {sizeof(""NN""), ""NN""}; struct acpi_buffer state = {0, NULL}; struct snd_soc_acpi_package_context pkg_ctx; bool pkg_found = false; state.length = sizeof(chan_package); state.pointer = &chan_package; pkg_ctx.name = ""CHAN""; pkg_ctx.length = 2; pkg_ctx.format = &format; pkg_ctx.state = &state; pkg_ctx.data_valid = false; pkg_found = snd_soc_acpi_find_package_from_hid(mach->id, &pkg_ctx); if (pkg_found) { if (chan_package.aif_value == 1) { dev_info(dev, ""BIOS Routing: AIF1 connected\n""); byt_rt5640_quirk |= BYT_RT5640_SSP0_AIF1; } else if (chan_package.aif_value == 2) { dev_info(dev, ""BIOS Routing: AIF2 connected\n""); byt_rt5640_quirk |= BYT_RT5640_SSP0_AIF2; } else { dev_info(dev, ""BIOS Routing isn't valid, ignored\n""); pkg_found = false; } } if (!pkg_found) { byt_rt5640_quirk |= BYT_RT5640_SSP0_AIF2; } byt_rt5640_quirk |= BYTCR_INPUT_DEFAULTS; } else { byt_rt5640_quirk |= BYT_RT5640_DMIC1_MAP | BYT_RT5640_JD_SRC_JD2_IN4N | BYT_RT5640_OVCD_TH_2000UA | BYT_RT5640_OVCD_SF_0P75; } dmi_id = dmi_first_match(byt_rt5640_quirk_table); if (dmi_id) byt_rt5640_quirk = (unsigned long)dmi_id->driver_data; if (quirk_override != -1) { dev_info(dev, ""Overriding quirk 0x%lx => 0x%x\n"", byt_rt5640_quirk, quirk_override); byt_rt5640_quirk = quirk_override; } if (byt_rt5640_quirk & BYT_RT5640_JD_HP_ELITEP_1000G2) { acpi_dev_add_driver_gpios(ACPI_COMPANION(priv->codec_dev), byt_rt5640_hp_elitepad_1000g2_gpios); priv->hsmic_detect = devm_fwnode_gpiod_get(dev, codec_dev->fwnode, ""headset-mic-detect"", GPIOD_IN, ""headset-mic-detect""); if (IS_ERR(priv->hsmic_detect)) { ret_val = dev_err_probe(dev, PTR_ERR(priv->hsmic_detect), ""getting hsmic-detect GPIO\n""); goto err_device; } } ret_val = byt_rt5640_add_codec_device_props(codec_dev, priv); if (ret_val) goto err_remove_gpios; log_quirks(dev); if ((byt_rt5640_quirk & BYT_RT5640_SSP2_AIF2) || (byt_rt5640_quirk & BYT_RT5640_SSP0_AIF2)) { byt_rt5640_dais[dai_index].codecs->dai_name = ""rt5640-aif2""; aif = 2; } else { aif = 1; } if ((byt_rt5640_quirk & BYT_RT5640_SSP0_AIF1) || (byt_rt5640_quirk & BYT_RT5640_SSP0_AIF2)) byt_rt5640_dais[dai_index].cpus->dai_name = ""ssp0-port""; if (byt_rt5640_quirk & BYT_RT5640_MCLK_EN) { priv->mclk = devm_clk_get_optional(dev, ""pmc_plt_clk_3""); if (IS_ERR(priv->mclk)) { ret_val = dev_err_probe(dev, PTR_ERR(priv->mclk), ""Failed to get MCLK from pmc_plt_clk_3\n""); goto err; } if (!priv->mclk) byt_rt5640_quirk &= ~BYT_RT5640_MCLK_EN; } if (byt_rt5640_quirk & BYT_RT5640_NO_SPEAKERS) { cfg_spk = ""0""; spk_type = ""none""; } else if (byt_rt5640_quirk & BYT_RT5640_MONO_SPEAKER) { cfg_spk = ""1""; spk_type = ""mono""; } else if (byt_rt5640_quirk & BYT_RT5640_SWAPPED_SPEAKERS) { cfg_spk = ""swapped""; spk_type = ""swapped""; } else { cfg_spk = ""2""; spk_type = ""stereo""; } if (byt_rt5640_quirk & BYT_RT5640_LINEOUT) { if (byt_rt5640_quirk & BYT_RT5640_LINEOUT_AS_HP2) lineout_string = "" cfg-hp2:lineout""; else lineout_string = "" cfg-lineout:2""; } if (byt_rt5640_quirk & BYT_RT5640_HSMIC2_ON_IN1) headset2_string = "" cfg-hs2:in1""; snprintf(byt_rt5640_components, sizeof(byt_rt5640_components), ""cfg-spk:%s cfg-mic:%s aif:%d%s%s"", cfg_spk, map_name[BYT_RT5640_MAP(byt_rt5640_quirk)], aif, lineout_string, headset2_string); byt_rt5640_card.components = byt_rt5640_components; #if !IS_ENABLED(CONFIG_SND_SOC_INTEL_USER_FRIENDLY_LONG_NAMES) snprintf(byt_rt5640_long_name, sizeof(byt_rt5640_long_name), ""bytcr-rt5640-%s-spk-%s-mic"", spk_type, map_name[BYT_RT5640_MAP(byt_rt5640_quirk)]); byt_rt5640_card.long_name = byt_rt5640_long_name; #endif platform_name = mach->mach_params.platform; ret_val = snd_soc_fixup_dai_links_platform_name(&byt_rt5640_card, platform_name); if (ret_val) goto err; sof_parent = snd_soc_acpi_sof_parent(dev); if (sof_parent) { byt_rt5640_card.name = SOF_CARD_NAME; byt_rt5640_card.driver_name = SOF_DRIVER_NAME; } else { byt_rt5640_card.name = CARD_NAME; byt_rt5640_card.driver_name = DRIVER_NAME; } if (sof_parent) dev->driver->pm = &snd_soc_pm_ops; ret_val = devm_snd_soc_register_card(dev, &byt_rt5640_card); if (ret_val) { dev_err(dev, ""devm_snd_soc_register_card failed %d\n"", ret_val); goto err; } platform_set_drvdata(pdev, &byt_rt5640_card); return ret_val; err: device_remove_software_node(priv->codec_dev); err_remove_gpios: if (byt_rt5640_quirk & BYT_RT5640_JD_HP_ELITEP_1000G2) acpi_dev_remove_driver_gpios(ACPI_COMPANION(priv->codec_dev)); err_device: put_device(priv->codec_dev); return ret_val; }","- if (byt_rt5640_dais[i].codecs->name &&
+ if (byt_rt5640_dais[i].num_codecs &&","static int snd_byt_rt5640_mc_probe(struct platform_device *pdev) { struct device *dev = &pdev->dev; static const char * const map_name[] = { ""dmic1"", ""dmic2"", ""in1"", ""in3"", ""none"" }; struct snd_soc_acpi_mach *mach = dev_get_platdata(dev); __maybe_unused const char *spk_type; const struct dmi_system_id *dmi_id; const char *headset2_string = """"; const char *lineout_string = """"; struct byt_rt5640_private *priv; const char *platform_name; struct acpi_device *adev; struct device *codec_dev; const char *cfg_spk; bool sof_parent; int ret_val = 0; int dai_index = 0; int i, aif; is_bytcr = false; priv = devm_kzalloc(dev, sizeof(*priv), GFP_KERNEL); if (!priv) return -ENOMEM; byt_rt5640_card.dev = dev; snd_soc_card_set_drvdata(&byt_rt5640_card, priv); for (i = 0; i < ARRAY_SIZE(byt_rt5640_dais); i++) { if (byt_rt5640_dais[i].num_codecs && !strcmp(byt_rt5640_dais[i].codecs->name, ""i2c-10EC5640:00"")) { dai_index = i; break; } } adev = acpi_dev_get_first_match_dev(mach->id, NULL, -1); if (adev) { snprintf(byt_rt5640_codec_name, sizeof(byt_rt5640_codec_name), ""i2c-%s"", acpi_dev_name(adev)); byt_rt5640_dais[dai_index].codecs->name = byt_rt5640_codec_name; } else { dev_err(dev, ""Error cannot find '%s' dev\n"", mach->id); return -ENXIO; } codec_dev = acpi_get_first_physical_node(adev); acpi_dev_put(adev); if (!codec_dev) return -EPROBE_DEFER; priv->codec_dev = get_device(codec_dev); if (soc_intel_is_byt()) { if (mach->mach_params.acpi_ipc_irq_index == 0) is_bytcr = true; } if (is_bytcr) { struct acpi_chan_package chan_package = { 0 }; struct acpi_buffer format = {sizeof(""NN""), ""NN""}; struct acpi_buffer state = {0, NULL}; struct snd_soc_acpi_package_context pkg_ctx; bool pkg_found = false; state.length = sizeof(chan_package); state.pointer = &chan_package; pkg_ctx.name = ""CHAN""; pkg_ctx.length = 2; pkg_ctx.format = &format; pkg_ctx.state = &state; pkg_ctx.data_valid = false; pkg_found = snd_soc_acpi_find_package_from_hid(mach->id, &pkg_ctx); if (pkg_found) { if (chan_package.aif_value == 1) { dev_info(dev, ""BIOS Routing: AIF1 connected\n""); byt_rt5640_quirk |= BYT_RT5640_SSP0_AIF1; } else if (chan_package.aif_value == 2) { dev_info(dev, ""BIOS Routing: AIF2 connected\n""); byt_rt5640_quirk |= BYT_RT5640_SSP0_AIF2; } else { dev_info(dev, ""BIOS Routing isn't valid, ignored\n""); pkg_found = false; } } if (!pkg_found) { byt_rt5640_quirk |= BYT_RT5640_SSP0_AIF2; } byt_rt5640_quirk |= BYTCR_INPUT_DEFAULTS; } else { byt_rt5640_quirk |= BYT_RT5640_DMIC1_MAP | BYT_RT5640_JD_SRC_JD2_IN4N | BYT_RT5640_OVCD_TH_2000UA | BYT_RT5640_OVCD_SF_0P75; } dmi_id = dmi_first_match(byt_rt5640_quirk_table); if (dmi_id) byt_rt5640_quirk = (unsigned long)dmi_id->driver_data; if (quirk_override != -1) { dev_info(dev, ""Overriding quirk 0x%lx => 0x%x\n"", byt_rt5640_quirk, quirk_override); byt_rt5640_quirk = quirk_override; } if (byt_rt5640_quirk & BYT_RT5640_JD_HP_ELITEP_1000G2) { acpi_dev_add_driver_gpios(ACPI_COMPANION(priv->codec_dev), byt_rt5640_hp_elitepad_1000g2_gpios); priv->hsmic_detect = devm_fwnode_gpiod_get(dev, codec_dev->fwnode, ""headset-mic-detect"", GPIOD_IN, ""headset-mic-detect""); if (IS_ERR(priv->hsmic_detect)) { ret_val = dev_err_probe(dev, PTR_ERR(priv->hsmic_detect), ""getting hsmic-detect GPIO\n""); goto err_device; } } ret_val = byt_rt5640_add_codec_device_props(codec_dev, priv); if (ret_val) goto err_remove_gpios; log_quirks(dev); if ((byt_rt5640_quirk & BYT_RT5640_SSP2_AIF2) || (byt_rt5640_quirk & BYT_RT5640_SSP0_AIF2)) { byt_rt5640_dais[dai_index].codecs->dai_name = ""rt5640-aif2""; aif = 2; } else { aif = 1; } if ((byt_rt5640_quirk & BYT_RT5640_SSP0_AIF1) || (byt_rt5640_quirk & BYT_RT5640_SSP0_AIF2)) byt_rt5640_dais[dai_index].cpus->dai_name = ""ssp0-port""; if (byt_rt5640_quirk & BYT_RT5640_MCLK_EN) { priv->mclk = devm_clk_get_optional(dev, ""pmc_plt_clk_3""); if (IS_ERR(priv->mclk)) { ret_val = dev_err_probe(dev, PTR_ERR(priv->mclk), ""Failed to get MCLK from pmc_plt_clk_3\n""); goto err; } if (!priv->mclk) byt_rt5640_quirk &= ~BYT_RT5640_MCLK_EN; } if (byt_rt5640_quirk & BYT_RT5640_NO_SPEAKERS) { cfg_spk = ""0""; spk_type = ""none""; } else if (byt_rt5640_quirk & BYT_RT5640_MONO_SPEAKER) { cfg_spk = ""1""; spk_type = ""mono""; } else if (byt_rt5640_quirk & BYT_RT5640_SWAPPED_SPEAKERS) { cfg_spk = ""swapped""; spk_type = ""swapped""; } else { cfg_spk = ""2""; spk_type = ""stereo""; } if (byt_rt5640_quirk & BYT_RT5640_LINEOUT) { if (byt_rt5640_quirk & BYT_RT5640_LINEOUT_AS_HP2) lineout_string = "" cfg-hp2:lineout""; else lineout_string = "" cfg-lineout:2""; } if (byt_rt5640_quirk & BYT_RT5640_HSMIC2_ON_IN1) headset2_string = "" cfg-hs2:in1""; snprintf(byt_rt5640_components, sizeof(byt_rt5640_components), ""cfg-spk:%s cfg-mic:%s aif:%d%s%s"", cfg_spk, map_name[BYT_RT5640_MAP(byt_rt5640_quirk)], aif, lineout_string, headset2_string); byt_rt5640_card.components = byt_rt5640_components; #if !IS_ENABLED(CONFIG_SND_SOC_INTEL_USER_FRIENDLY_LONG_NAMES) snprintf(byt_rt5640_long_name, sizeof(byt_rt5640_long_name), ""bytcr-rt5640-%s-spk-%s-mic"", spk_type, map_name[BYT_RT5640_MAP(byt_rt5640_quirk)]); byt_rt5640_card.long_name = byt_rt5640_long_name; #endif platform_name = mach->mach_params.platform; ret_val = snd_soc_fixup_dai_links_platform_name(&byt_rt5640_card, platform_name); if (ret_val) goto err; sof_parent = snd_soc_acpi_sof_parent(dev); if (sof_parent) { byt_rt5640_card.name = SOF_CARD_NAME; byt_rt5640_card.driver_name = SOF_DRIVER_NAME; } else { byt_rt5640_card.name = CARD_NAME; byt_rt5640_card.driver_name = DRIVER_NAME; } if (sof_parent) dev->driver->pm = &snd_soc_pm_ops; ret_val = devm_snd_soc_register_card(dev, &byt_rt5640_card); if (ret_val) { dev_err(dev, ""devm_snd_soc_register_card failed %d\n"", ret_val); goto err; } platform_set_drvdata(pdev, &byt_rt5640_card); return ret_val; err: device_remove_software_node(priv->codec_dev); err_remove_gpios: if (byt_rt5640_quirk & BYT_RT5640_JD_HP_ELITEP_1000G2) acpi_dev_remove_driver_gpios(ACPI_COMPANION(priv->codec_dev)); err_device: put_device(priv->codec_dev); return ret_val; }"
800----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50074/bad/procfs.c----do_hardware_irq,"static int do_hardware_irq(const struct ctl_table *table, int write, void *result, size_t *lenp, loff_t *ppos) { struct parport *port = (struct parport *)table->extra1; char buffer[20]; int len = 0; if (*ppos) { *lenp = 0; return 0; } if (write) return -EACCES; <S2SV_StartVul> len += snprintf (buffer, sizeof(buffer), ""%d\n"", port->irq); <S2SV_EndVul> if (len > *lenp) len = *lenp; else *lenp = len; *ppos += len; memcpy(result, buffer, len); return 0; }","- len += snprintf (buffer, sizeof(buffer), ""%d\n"", port->irq);
+ len += scnprintf (buffer, sizeof(buffer), ""%d\n"", port->irq);","static int do_hardware_irq(const struct ctl_table *table, int write, void *result, size_t *lenp, loff_t *ppos) { struct parport *port = (struct parport *)table->extra1; char buffer[20]; int len = 0; if (*ppos) { *lenp = 0; return 0; } if (write) return -EACCES; len += scnprintf (buffer, sizeof(buffer), ""%d\n"", port->irq); if (len > *lenp) len = *lenp; else *lenp = len; *ppos += len; memcpy(result, buffer, len); return 0; }"
477----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47723/bad/jfs_dmap.c----dbNextAG,"int dbNextAG(struct inode *ipbmap) { s64 avgfree; int agpref; s64 hwm = 0; int i; int next_best = -1; struct bmap *bmp = JFS_SBI(ipbmap->i_sb)->bmap; BMAP_LOCK(bmp); avgfree = (u32)bmp->db_nfree / bmp->db_numag; agpref = bmp->db_agpref; if ((atomic_read(&bmp->db_active[agpref]) == 0) && (bmp->db_agfree[agpref] >= avgfree)) goto unlock; for (i = 0 ; i < bmp->db_numag; i++, agpref++) { <S2SV_StartVul> if (agpref == bmp->db_numag) <S2SV_EndVul> agpref = 0; if (atomic_read(&bmp->db_active[agpref])) continue; if (bmp->db_agfree[agpref] >= avgfree) { bmp->db_agpref = agpref; goto unlock; } else if (bmp->db_agfree[agpref] > hwm) { hwm = bmp->db_agfree[agpref]; next_best = agpref; } } if (next_best != -1) bmp->db_agpref = next_best; unlock: BMAP_UNLOCK(bmp); return (bmp->db_agpref); }","- if (agpref == bmp->db_numag)
+ if (agpref >= bmp->db_numag)","int dbNextAG(struct inode *ipbmap) { s64 avgfree; int agpref; s64 hwm = 0; int i; int next_best = -1; struct bmap *bmp = JFS_SBI(ipbmap->i_sb)->bmap; BMAP_LOCK(bmp); avgfree = (u32)bmp->db_nfree / bmp->db_numag; agpref = bmp->db_agpref; if ((atomic_read(&bmp->db_active[agpref]) == 0) && (bmp->db_agfree[agpref] >= avgfree)) goto unlock; for (i = 0 ; i < bmp->db_numag; i++, agpref++) { if (agpref >= bmp->db_numag) agpref = 0; if (atomic_read(&bmp->db_active[agpref])) continue; if (bmp->db_agfree[agpref] >= avgfree) { bmp->db_agpref = agpref; goto unlock; } else if (bmp->db_agfree[agpref] > hwm) { hwm = bmp->db_agfree[agpref]; next_best = agpref; } } if (next_best != -1) bmp->db_agpref = next_best; unlock: BMAP_UNLOCK(bmp); return (bmp->db_agpref); }"
115----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44974/bad/pm_netlink.c----select_local_address,"select_local_address(const struct pm_nl_pernet *pernet, const struct mptcp_sock *msk) { struct mptcp_pm_addr_entry *entry, *ret = NULL; msk_owned_by_me(msk); rcu_read_lock(); list_for_each_entry_rcu(entry, &pernet->local_addr_list, list) { if (!(entry->flags & MPTCP_PM_ADDR_FLAG_SUBFLOW)) continue; if (!test_bit(entry->addr.id, msk->pm.id_avail_bitmap)) continue; <S2SV_StartVul> ret = entry; <S2SV_EndVul> <S2SV_StartVul> break; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> rcu_read_unlock(); <S2SV_StartVul> return ret; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul>","- ret = entry;
- break;
- }
- return ret;
- }
+ found = true;
+ break;
+ return found;","select_local_address(const struct pm_nl_pernet *pernet, const struct mptcp_sock *msk, struct mptcp_pm_addr_entry *new_entry) { struct mptcp_pm_addr_entry *entry; bool found = false; msk_owned_by_me(msk); rcu_read_lock(); list_for_each_entry_rcu(entry, &pernet->local_addr_list, list) { if (!(entry->flags & MPTCP_PM_ADDR_FLAG_SUBFLOW)) continue; if (!test_bit(entry->addr.id, msk->pm.id_avail_bitmap)) continue; *new_entry = *entry; found = true; break; } rcu_read_unlock(); return found; }"
633----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49938/bad/hif_usb.c----ath9k_hif_usb_reg_in_cb,"static void ath9k_hif_usb_reg_in_cb(struct urb *urb) { struct rx_buf *rx_buf = (struct rx_buf *)urb->context; struct hif_device_usb *hif_dev = rx_buf->hif_dev; struct sk_buff *skb = rx_buf->skb; int ret; if (!skb) return; if (!hif_dev) goto free_skb; switch (urb->status) { case 0: break; case -ENOENT: case -ECONNRESET: case -ENODEV: case -ESHUTDOWN: goto free_skb; default: <S2SV_StartVul> skb_reset_tail_pointer(skb); <S2SV_EndVul> <S2SV_StartVul> skb_trim(skb, 0); <S2SV_EndVul> goto resubmit; } if (likely(urb->actual_length != 0)) { skb_put(skb, urb->actual_length); ath9k_htc_rx_msg(hif_dev->htc_handle, skb, skb->len, USB_REG_IN_PIPE); skb = alloc_skb(MAX_REG_IN_BUF_SIZE, GFP_ATOMIC); if (!skb) { dev_err(&hif_dev->udev->dev, ""ath9k_htc: REG_IN memory allocation failure\n""); goto free_rx_buf; } rx_buf->skb = skb; usb_fill_int_urb(urb, hif_dev->udev, usb_rcvintpipe(hif_dev->udev, USB_REG_IN_PIPE), skb->data, MAX_REG_IN_BUF_SIZE, ath9k_hif_usb_reg_in_cb, rx_buf, 1); } resubmit: usb_anchor_urb(urb, &hif_dev->reg_in_submitted); ret = usb_submit_urb(urb, GFP_ATOMIC); if (ret) { usb_unanchor_urb(urb); goto free_skb; } return; free_skb: kfree_skb(skb); free_rx_buf: kfree(rx_buf); urb->context = NULL; }","- skb_reset_tail_pointer(skb);
- skb_trim(skb, 0);
+ __skb_set_length(skb, 0);","static void ath9k_hif_usb_reg_in_cb(struct urb *urb) { struct rx_buf *rx_buf = (struct rx_buf *)urb->context; struct hif_device_usb *hif_dev = rx_buf->hif_dev; struct sk_buff *skb = rx_buf->skb; int ret; if (!skb) return; if (!hif_dev) goto free_skb; switch (urb->status) { case 0: break; case -ENOENT: case -ECONNRESET: case -ENODEV: case -ESHUTDOWN: goto free_skb; default: __skb_set_length(skb, 0); goto resubmit; } if (likely(urb->actual_length != 0)) { skb_put(skb, urb->actual_length); ath9k_htc_rx_msg(hif_dev->htc_handle, skb, skb->len, USB_REG_IN_PIPE); skb = alloc_skb(MAX_REG_IN_BUF_SIZE, GFP_ATOMIC); if (!skb) { dev_err(&hif_dev->udev->dev, ""ath9k_htc: REG_IN memory allocation failure\n""); goto free_rx_buf; } rx_buf->skb = skb; usb_fill_int_urb(urb, hif_dev->udev, usb_rcvintpipe(hif_dev->udev, USB_REG_IN_PIPE), skb->data, MAX_REG_IN_BUF_SIZE, ath9k_hif_usb_reg_in_cb, rx_buf, 1); } resubmit: usb_anchor_urb(urb, &hif_dev->reg_in_submitted); ret = usb_submit_urb(urb, GFP_ATOMIC); if (ret) { usb_unanchor_urb(urb); goto free_skb; } return; free_skb: kfree_skb(skb); free_rx_buf: kfree(rx_buf); urb->context = NULL; }"
228----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46705/bad/xe_mmio.c----xe_mmio_probe_vram,"int xe_mmio_probe_vram(struct xe_device *xe) { struct xe_tile *tile; resource_size_t io_size; u64 available_size = 0; u64 total_size = 0; u64 tile_offset; u64 tile_size; u64 vram_size; int err; u8 id; if (!IS_DGFX(xe)) return 0; tile = xe_device_get_root_tile(xe); err = xe_mmio_tile_vram_size(tile, &vram_size, &tile_size, &tile_offset); if (err) return err; err = xe_determine_lmem_bar_size(xe); if (err) return err; drm_info(&xe->drm, ""VISIBLE VRAM: %pa, %pa\n"", &xe->mem.vram.io_start, &xe->mem.vram.io_size); io_size = xe->mem.vram.io_size; for_each_tile(tile, xe, id) { err = xe_mmio_tile_vram_size(tile, &vram_size, &tile_size, &tile_offset); if (err) return err; tile->mem.vram.actual_physical_size = tile_size; tile->mem.vram.io_start = xe->mem.vram.io_start + tile_offset; tile->mem.vram.io_size = min_t(u64, vram_size, io_size); if (!tile->mem.vram.io_size) { drm_err(&xe->drm, ""Tile without any CPU visible VRAM. Aborting.\n""); return -ENODEV; } tile->mem.vram.dpa_base = xe->mem.vram.dpa_base + tile_offset; tile->mem.vram.usable_size = vram_size; tile->mem.vram.mapping = xe->mem.vram.mapping + tile_offset; if (tile->mem.vram.io_size < tile->mem.vram.usable_size) drm_info(&xe->drm, ""Small BAR device\n""); drm_info(&xe->drm, ""VRAM[%u, %u]: Actual physical size %pa, usable size exclude stolen %pa, CPU accessible size %pa\n"", id, tile->id, &tile->mem.vram.actual_physical_size, &tile->mem.vram.usable_size, &tile->mem.vram.io_size); drm_info(&xe->drm, ""VRAM[%u, %u]: DPA range: [%pa-%llx], io range: [%pa-%llx]\n"", id, tile->id, &tile->mem.vram.dpa_base, tile->mem.vram.dpa_base + (u64)tile->mem.vram.actual_physical_size, &tile->mem.vram.io_start, tile->mem.vram.io_start + (u64)tile->mem.vram.io_size); total_size += tile_size; available_size += vram_size; if (total_size > xe->mem.vram.io_size) { drm_info(&xe->drm, ""VRAM: %pa is larger than resource %pa\n"", &total_size, &xe->mem.vram.io_size); } io_size -= min_t(u64, tile_size, io_size); } xe->mem.vram.actual_physical_size = total_size; drm_info(&xe->drm, ""Total VRAM: %pa, %pa\n"", &xe->mem.vram.io_start, &xe->mem.vram.actual_physical_size); drm_info(&xe->drm, ""Available VRAM: %pa, %pa\n"", &xe->mem.vram.io_start, &available_size); <S2SV_StartVul> return 0; <S2SV_EndVul> }","- return 0;
+ return devm_add_action_or_reset(xe->drm.dev, vram_fini, xe);
+ }","int xe_mmio_probe_vram(struct xe_device *xe) { struct xe_tile *tile; resource_size_t io_size; u64 available_size = 0; u64 total_size = 0; u64 tile_offset; u64 tile_size; u64 vram_size; int err; u8 id; if (!IS_DGFX(xe)) return 0; tile = xe_device_get_root_tile(xe); err = xe_mmio_tile_vram_size(tile, &vram_size, &tile_size, &tile_offset); if (err) return err; err = xe_determine_lmem_bar_size(xe); if (err) return err; drm_info(&xe->drm, ""VISIBLE VRAM: %pa, %pa\n"", &xe->mem.vram.io_start, &xe->mem.vram.io_size); io_size = xe->mem.vram.io_size; for_each_tile(tile, xe, id) { err = xe_mmio_tile_vram_size(tile, &vram_size, &tile_size, &tile_offset); if (err) return err; tile->mem.vram.actual_physical_size = tile_size; tile->mem.vram.io_start = xe->mem.vram.io_start + tile_offset; tile->mem.vram.io_size = min_t(u64, vram_size, io_size); if (!tile->mem.vram.io_size) { drm_err(&xe->drm, ""Tile without any CPU visible VRAM. Aborting.\n""); return -ENODEV; } tile->mem.vram.dpa_base = xe->mem.vram.dpa_base + tile_offset; tile->mem.vram.usable_size = vram_size; tile->mem.vram.mapping = xe->mem.vram.mapping + tile_offset; if (tile->mem.vram.io_size < tile->mem.vram.usable_size) drm_info(&xe->drm, ""Small BAR device\n""); drm_info(&xe->drm, ""VRAM[%u, %u]: Actual physical size %pa, usable size exclude stolen %pa, CPU accessible size %pa\n"", id, tile->id, &tile->mem.vram.actual_physical_size, &tile->mem.vram.usable_size, &tile->mem.vram.io_size); drm_info(&xe->drm, ""VRAM[%u, %u]: DPA range: [%pa-%llx], io range: [%pa-%llx]\n"", id, tile->id, &tile->mem.vram.dpa_base, tile->mem.vram.dpa_base + (u64)tile->mem.vram.actual_physical_size, &tile->mem.vram.io_start, tile->mem.vram.io_start + (u64)tile->mem.vram.io_size); total_size += tile_size; available_size += vram_size; if (total_size > xe->mem.vram.io_size) { drm_info(&xe->drm, ""VRAM: %pa is larger than resource %pa\n"", &total_size, &xe->mem.vram.io_size); } io_size -= min_t(u64, tile_size, io_size); } xe->mem.vram.actual_physical_size = total_size; drm_info(&xe->drm, ""Total VRAM: %pa, %pa\n"", &xe->mem.vram.io_start, &xe->mem.vram.actual_physical_size); drm_info(&xe->drm, ""Available VRAM: %pa, %pa\n"", &xe->mem.vram.io_start, &available_size); return devm_add_action_or_reset(xe->drm.dev, vram_fini, xe); }"
568----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49891/bad/lpfc_scsi.c----lpfc_abort_handler,"lpfc_abort_handler(struct scsi_cmnd *cmnd) { struct Scsi_Host *shost = cmnd->device->host; struct fc_rport *rport = starget_to_rport(scsi_target(cmnd->device)); struct lpfc_vport *vport = (struct lpfc_vport *) shost->hostdata; struct lpfc_hba *phba = vport->phba; struct lpfc_iocbq *iocb; struct lpfc_io_buf *lpfc_cmd; int ret = SUCCESS, status = 0; struct lpfc_sli_ring *pring_s4 = NULL; struct lpfc_sli_ring *pring = NULL; int ret_val; unsigned long flags; DECLARE_WAIT_QUEUE_HEAD_ONSTACK(waitq); status = fc_block_rport(rport); if (status != 0 && status != SUCCESS) return status; lpfc_cmd = (struct lpfc_io_buf *)cmnd->host_scribble; if (!lpfc_cmd) return ret; spin_lock_irqsave(&lpfc_cmd->buf_lock, flags); spin_lock(&phba->hbalock); if (test_bit(HBA_IOQ_FLUSH, &phba->hba_flag)) { lpfc_printf_vlog(vport, KERN_WARNING, LOG_FCP, ""3168 SCSI Layer abort requested I/O has been "" ""flushed by LLD.\n""); ret = FAILED; goto out_unlock_hba; } if (!lpfc_cmd->pCmd) { lpfc_printf_vlog(vport, KERN_WARNING, LOG_FCP, ""2873 SCSI Layer I/O Abort Request IO CMPL Status "" ""x%x ID %d LUN %llu\n"", SUCCESS, cmnd->device->id, cmnd->device->lun); goto out_unlock_hba; } iocb = &lpfc_cmd->cur_iocbq; if (phba->sli_rev == LPFC_SLI_REV4) { <S2SV_StartVul> pring_s4 = phba->sli4_hba.hdwq[iocb->hba_wqidx].io_wq->pring; <S2SV_EndVul> <S2SV_StartVul> if (!pring_s4) { <S2SV_EndVul> ret = FAILED; goto out_unlock_hba; } spin_lock(&pring_s4->ring_lock); } if (!(iocb->cmd_flag & LPFC_IO_ON_TXCMPLQ)) { lpfc_printf_vlog(vport, KERN_WARNING, LOG_FCP, ""3169 SCSI Layer abort requested I/O has been "" ""cancelled by LLD.\n""); ret = FAILED; goto out_unlock_ring; } if (lpfc_cmd->pCmd != cmnd) { lpfc_printf_vlog(vport, KERN_WARNING, LOG_FCP, ""3170 SCSI Layer abort requested I/O has been "" ""completed by LLD.\n""); goto out_unlock_ring; } WARN_ON(iocb->io_buf != lpfc_cmd); if (iocb->cmd_flag & LPFC_DRIVER_ABORTED) { lpfc_printf_vlog(vport, KERN_WARNING, LOG_FCP, ""3389 SCSI Layer I/O Abort Request is pending\n""); if (phba->sli_rev == LPFC_SLI_REV4) spin_unlock(&pring_s4->ring_lock); spin_unlock(&phba->hbalock); spin_unlock_irqrestore(&lpfc_cmd->buf_lock, flags); goto wait_for_cmpl; } lpfc_cmd->waitq = &waitq; if (phba->sli_rev == LPFC_SLI_REV4) { spin_unlock(&pring_s4->ring_lock); ret_val = lpfc_sli4_issue_abort_iotag(phba, iocb, lpfc_sli_abort_fcp_cmpl); } else { pring = &phba->sli.sli3_ring[LPFC_FCP_RING]; ret_val = lpfc_sli_issue_abort_iotag(phba, pring, iocb, lpfc_sli_abort_fcp_cmpl); } lpfc_issue_hb_tmo(phba); if (ret_val != IOCB_SUCCESS) { lpfc_cmd->waitq = NULL; ret = FAILED; goto out_unlock_hba; } spin_unlock(&phba->hbalock); spin_unlock_irqrestore(&lpfc_cmd->buf_lock, flags); if (phba->cfg_poll & DISABLE_FCP_RING_INT) lpfc_sli_handle_fast_ring_event(phba, &phba->sli.sli3_ring[LPFC_FCP_RING], HA_R0RE_REQ); wait_for_cmpl: wait_event_timeout(waitq, (lpfc_cmd->pCmd != cmnd), msecs_to_jiffies(2*vport->cfg_devloss_tmo*1000)); spin_lock(&lpfc_cmd->buf_lock); if (lpfc_cmd->pCmd == cmnd) { ret = FAILED; lpfc_printf_vlog(vport, KERN_ERR, LOG_TRACE_EVENT, ""0748 abort handler timed out waiting "" ""for aborting I/O (xri:x%x) to complete: "" ""ret %#x, ID %d, LUN %llu\n"", iocb->sli4_xritag, ret, cmnd->device->id, cmnd->device->lun); } lpfc_cmd->waitq = NULL; spin_unlock(&lpfc_cmd->buf_lock); goto out; out_unlock_ring: if (phba->sli_rev == LPFC_SLI_REV4) spin_unlock(&pring_s4->ring_lock); out_unlock_hba: spin_unlock(&phba->hbalock); spin_unlock_irqrestore(&lpfc_cmd->buf_lock, flags); out: lpfc_printf_vlog(vport, KERN_WARNING, LOG_FCP, ""0749 SCSI Layer I/O Abort Request Status x%x ID %d "" ""LUN %llu\n"", ret, cmnd->device->id, cmnd->device->lun); return ret; }","- pring_s4 = phba->sli4_hba.hdwq[iocb->hba_wqidx].io_wq->pring;
- if (!pring_s4) {
+ if (!phba->sli4_hba.hdwq[iocb->hba_wqidx].io_wq ||
+ !phba->sli4_hba.hdwq[iocb->hba_wqidx].io_wq->pring) {
+ lpfc_printf_vlog(vport, KERN_WARNING, LOG_FCP,
+ ""2877 SCSI Layer I/O Abort Request ""
+ ""IO CMPL Status x%x ID %d LUN %llu ""
+ ""HBA_SETUP %d\n"", FAILED,
+ cmnd->device->id,
+ (u64)cmnd->device->lun,
+ test_bit(HBA_SETUP, &phba->hba_flag));
+ pring_s4 = phba->sli4_hba.hdwq[iocb->hba_wqidx].io_wq->pring;","lpfc_abort_handler(struct scsi_cmnd *cmnd) { struct Scsi_Host *shost = cmnd->device->host; struct fc_rport *rport = starget_to_rport(scsi_target(cmnd->device)); struct lpfc_vport *vport = (struct lpfc_vport *) shost->hostdata; struct lpfc_hba *phba = vport->phba; struct lpfc_iocbq *iocb; struct lpfc_io_buf *lpfc_cmd; int ret = SUCCESS, status = 0; struct lpfc_sli_ring *pring_s4 = NULL; struct lpfc_sli_ring *pring = NULL; int ret_val; unsigned long flags; DECLARE_WAIT_QUEUE_HEAD_ONSTACK(waitq); status = fc_block_rport(rport); if (status != 0 && status != SUCCESS) return status; lpfc_cmd = (struct lpfc_io_buf *)cmnd->host_scribble; if (!lpfc_cmd) return ret; spin_lock_irqsave(&lpfc_cmd->buf_lock, flags); spin_lock(&phba->hbalock); if (test_bit(HBA_IOQ_FLUSH, &phba->hba_flag)) { lpfc_printf_vlog(vport, KERN_WARNING, LOG_FCP, ""3168 SCSI Layer abort requested I/O has been "" ""flushed by LLD.\n""); ret = FAILED; goto out_unlock_hba; } if (!lpfc_cmd->pCmd) { lpfc_printf_vlog(vport, KERN_WARNING, LOG_FCP, ""2873 SCSI Layer I/O Abort Request IO CMPL Status "" ""x%x ID %d LUN %llu\n"", SUCCESS, cmnd->device->id, cmnd->device->lun); goto out_unlock_hba; } iocb = &lpfc_cmd->cur_iocbq; if (phba->sli_rev == LPFC_SLI_REV4) { if (!phba->sli4_hba.hdwq[iocb->hba_wqidx].io_wq || !phba->sli4_hba.hdwq[iocb->hba_wqidx].io_wq->pring) { lpfc_printf_vlog(vport, KERN_WARNING, LOG_FCP, ""2877 SCSI Layer I/O Abort Request "" ""IO CMPL Status x%x ID %d LUN %llu "" ""HBA_SETUP %d\n"", FAILED, cmnd->device->id, (u64)cmnd->device->lun, test_bit(HBA_SETUP, &phba->hba_flag)); ret = FAILED; goto out_unlock_hba; } pring_s4 = phba->sli4_hba.hdwq[iocb->hba_wqidx].io_wq->pring; spin_lock(&pring_s4->ring_lock); } if (!(iocb->cmd_flag & LPFC_IO_ON_TXCMPLQ)) { lpfc_printf_vlog(vport, KERN_WARNING, LOG_FCP, ""3169 SCSI Layer abort requested I/O has been "" ""cancelled by LLD.\n""); ret = FAILED; goto out_unlock_ring; } if (lpfc_cmd->pCmd != cmnd) { lpfc_printf_vlog(vport, KERN_WARNING, LOG_FCP, ""3170 SCSI Layer abort requested I/O has been "" ""completed by LLD.\n""); goto out_unlock_ring; } WARN_ON(iocb->io_buf != lpfc_cmd); if (iocb->cmd_flag & LPFC_DRIVER_ABORTED) { lpfc_printf_vlog(vport, KERN_WARNING, LOG_FCP, ""3389 SCSI Layer I/O Abort Request is pending\n""); if (phba->sli_rev == LPFC_SLI_REV4) spin_unlock(&pring_s4->ring_lock); spin_unlock(&phba->hbalock); spin_unlock_irqrestore(&lpfc_cmd->buf_lock, flags); goto wait_for_cmpl; } lpfc_cmd->waitq = &waitq; if (phba->sli_rev == LPFC_SLI_REV4) { spin_unlock(&pring_s4->ring_lock); ret_val = lpfc_sli4_issue_abort_iotag(phba, iocb, lpfc_sli_abort_fcp_cmpl); } else { pring = &phba->sli.sli3_ring[LPFC_FCP_RING]; ret_val = lpfc_sli_issue_abort_iotag(phba, pring, iocb, lpfc_sli_abort_fcp_cmpl); } lpfc_issue_hb_tmo(phba); if (ret_val != IOCB_SUCCESS) { lpfc_cmd->waitq = NULL; ret = FAILED; goto out_unlock_hba; } spin_unlock(&phba->hbalock); spin_unlock_irqrestore(&lpfc_cmd->buf_lock, flags); if (phba->cfg_poll & DISABLE_FCP_RING_INT) lpfc_sli_handle_fast_ring_event(phba, &phba->sli.sli3_ring[LPFC_FCP_RING], HA_R0RE_REQ); wait_for_cmpl: wait_event_timeout(waitq, (lpfc_cmd->pCmd != cmnd), msecs_to_jiffies(2*vport->cfg_devloss_tmo*1000)); spin_lock(&lpfc_cmd->buf_lock); if (lpfc_cmd->pCmd == cmnd) { ret = FAILED; lpfc_printf_vlog(vport, KERN_ERR, LOG_TRACE_EVENT, ""0748 abort handler timed out waiting "" ""for aborting I/O (xri:x%x) to complete: "" ""ret %#x, ID %d, LUN %llu\n"", iocb->sli4_xritag, ret, cmnd->device->id, cmnd->device->lun); } lpfc_cmd->waitq = NULL; spin_unlock(&lpfc_cmd->buf_lock); goto out; out_unlock_ring: if (phba->sli_rev == LPFC_SLI_REV4) spin_unlock(&pring_s4->ring_lock); out_unlock_hba: spin_unlock(&phba->hbalock); spin_unlock_irqrestore(&lpfc_cmd->buf_lock, flags); out: lpfc_printf_vlog(vport, KERN_WARNING, LOG_FCP, ""0749 SCSI Layer I/O Abort Request Status x%x ID %d "" ""LUN %llu\n"", ret, cmnd->device->id, cmnd->device->lun); return ret; }"
1180----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53165/bad/core.c----register_intc_controller,"int __init register_intc_controller(struct intc_desc *desc) { unsigned int i, k, smp; struct intc_hw_desc *hw = &desc->hw; struct intc_desc_int *d; struct resource *res; pr_info(""Registered controller '%s' with %u IRQs\n"", desc->name, hw->nr_vectors); d = kzalloc(sizeof(*d), GFP_NOWAIT); if (!d) goto err0; INIT_LIST_HEAD(&d->list); <S2SV_StartVul> list_add_tail(&d->list, &intc_list); <S2SV_EndVul> raw_spin_lock_init(&d->lock); INIT_RADIX_TREE(&d->tree, GFP_ATOMIC); d->index = nr_intc_controllers; if (desc->num_resources) { d->nr_windows = desc->num_resources; d->window = kcalloc(d->nr_windows, sizeof(*d->window), GFP_NOWAIT); if (!d->window) goto err1; for (k = 0; k < d->nr_windows; k++) { res = desc->resource + k; WARN_ON(resource_type(res) != IORESOURCE_MEM); d->window[k].phys = res->start; d->window[k].size = resource_size(res); d->window[k].virt = ioremap_nocache(res->start, resource_size(res)); if (!d->window[k].virt) goto err2; } } d->nr_reg = hw->mask_regs ? hw->nr_mask_regs * 2 : 0; #ifdef CONFIG_INTC_BALANCING if (d->nr_reg) d->nr_reg += hw->nr_mask_regs; #endif d->nr_reg += hw->prio_regs ? hw->nr_prio_regs * 2 : 0; d->nr_reg += hw->sense_regs ? hw->nr_sense_regs : 0; d->nr_reg += hw->ack_regs ? hw->nr_ack_regs : 0; d->nr_reg += hw->subgroups ? hw->nr_subgroups : 0; d->reg = kcalloc(d->nr_reg, sizeof(*d->reg), GFP_NOWAIT); if (!d->reg) goto err2; #ifdef CONFIG_SMP d->smp = kcalloc(d->nr_reg, sizeof(*d->smp), GFP_NOWAIT); if (!d->smp) goto err3; #endif k = 0; if (hw->mask_regs) { for (i = 0; i < hw->nr_mask_regs; i++) { smp = IS_SMP(hw->mask_regs[i]); k += save_reg(d, k, hw->mask_regs[i].set_reg, smp); k += save_reg(d, k, hw->mask_regs[i].clr_reg, smp); #ifdef CONFIG_INTC_BALANCING k += save_reg(d, k, hw->mask_regs[i].dist_reg, 0); #endif } } if (hw->prio_regs) { d->prio = kcalloc(hw->nr_vectors, sizeof(*d->prio), GFP_NOWAIT); if (!d->prio) goto err4; for (i = 0; i < hw->nr_prio_regs; i++) { smp = IS_SMP(hw->prio_regs[i]); k += save_reg(d, k, hw->prio_regs[i].set_reg, smp); k += save_reg(d, k, hw->prio_regs[i].clr_reg, smp); } sort(d->prio, hw->nr_prio_regs, sizeof(*d->prio), intc_handle_int_cmp, NULL); } if (hw->sense_regs) { d->sense = kcalloc(hw->nr_vectors, sizeof(*d->sense), GFP_NOWAIT); if (!d->sense) goto err5; for (i = 0; i < hw->nr_sense_regs; i++) k += save_reg(d, k, hw->sense_regs[i].reg, 0); sort(d->sense, hw->nr_sense_regs, sizeof(*d->sense), intc_handle_int_cmp, NULL); } if (hw->subgroups) for (i = 0; i < hw->nr_subgroups; i++) if (hw->subgroups[i].reg) k+= save_reg(d, k, hw->subgroups[i].reg, 0); memcpy(&d->chip, &intc_irq_chip, sizeof(struct irq_chip)); d->chip.name = desc->name; if (hw->ack_regs) for (i = 0; i < hw->nr_ack_regs; i++) k += save_reg(d, k, hw->ack_regs[i].set_reg, 0); else d->chip.irq_mask_ack = d->chip.irq_disable; if (desc->force_disable) intc_enable_disable_enum(desc, d, desc->force_disable, 0); if (desc->force_enable) intc_enable_disable_enum(desc, d, desc->force_enable, 0); BUG_ON(k > 256); intc_irq_domain_init(d, hw); for (i = 0; i < hw->nr_vectors; i++) { struct intc_vect *vect = hw->vectors + i; unsigned int irq = evt2irq(vect->vect); int res; if (!vect->enum_id) continue; res = irq_create_identity_mapping(d->domain, irq); if (unlikely(res)) { if (res == -EEXIST) { res = irq_domain_associate(d->domain, irq, irq); if (unlikely(res)) { pr_err(""domain association failure\n""); continue; } } else { pr_err(""can't identity map IRQ %d\n"", irq); continue; } } intc_irq_xlate_set(irq, vect->enum_id, d); intc_register_irq(desc, d, vect->enum_id, irq); for (k = i + 1; k < hw->nr_vectors; k++) { struct intc_vect *vect2 = hw->vectors + k; unsigned int irq2 = evt2irq(vect2->vect); if (vect->enum_id != vect2->enum_id) continue; res = irq_create_identity_mapping(d->domain, irq2); if (unlikely(res)) { if (res == -EEXIST) { res = irq_domain_associate(d->domain, irq2, irq2); if (unlikely(res)) { pr_err(""domain association "" ""failure\n""); continue; } } else { pr_err(""can't identity map IRQ %d\n"", irq); continue; } } vect2->enum_id = 0; irq_set_chip(irq2, &dummy_irq_chip); irq_set_chained_handler_and_data(irq2, intc_redirect_irq, (void *)irq); } } intc_subgroup_init(desc, d); if (desc->force_enable) intc_enable_disable_enum(desc, d, desc->force_enable, 1); d->skip_suspend = desc->skip_syscore_suspend; nr_intc_controllers++; return 0; err5: kfree(d->prio); err4: #ifdef CONFIG_SMP kfree(d->smp); err3: #endif kfree(d->reg); err2: for (k = 0; k < d->nr_windows; k++) if (d->window[k].virt) iounmap(d->window[k].virt); kfree(d->window); err1: kfree(d); err0: pr_err(""unable to allocate INTC memory\n""); return -ENOMEM; }","- list_add_tail(&d->list, &intc_list);
+ list_add_tail(&d->list, &intc_list);","int __init register_intc_controller(struct intc_desc *desc) { unsigned int i, k, smp; struct intc_hw_desc *hw = &desc->hw; struct intc_desc_int *d; struct resource *res; pr_info(""Registered controller '%s' with %u IRQs\n"", desc->name, hw->nr_vectors); d = kzalloc(sizeof(*d), GFP_NOWAIT); if (!d) goto err0; INIT_LIST_HEAD(&d->list); raw_spin_lock_init(&d->lock); INIT_RADIX_TREE(&d->tree, GFP_ATOMIC); d->index = nr_intc_controllers; if (desc->num_resources) { d->nr_windows = desc->num_resources; d->window = kcalloc(d->nr_windows, sizeof(*d->window), GFP_NOWAIT); if (!d->window) goto err1; for (k = 0; k < d->nr_windows; k++) { res = desc->resource + k; WARN_ON(resource_type(res) != IORESOURCE_MEM); d->window[k].phys = res->start; d->window[k].size = resource_size(res); d->window[k].virt = ioremap_nocache(res->start, resource_size(res)); if (!d->window[k].virt) goto err2; } } d->nr_reg = hw->mask_regs ? hw->nr_mask_regs * 2 : 0; #ifdef CONFIG_INTC_BALANCING if (d->nr_reg) d->nr_reg += hw->nr_mask_regs; #endif d->nr_reg += hw->prio_regs ? hw->nr_prio_regs * 2 : 0; d->nr_reg += hw->sense_regs ? hw->nr_sense_regs : 0; d->nr_reg += hw->ack_regs ? hw->nr_ack_regs : 0; d->nr_reg += hw->subgroups ? hw->nr_subgroups : 0; d->reg = kcalloc(d->nr_reg, sizeof(*d->reg), GFP_NOWAIT); if (!d->reg) goto err2; #ifdef CONFIG_SMP d->smp = kcalloc(d->nr_reg, sizeof(*d->smp), GFP_NOWAIT); if (!d->smp) goto err3; #endif k = 0; if (hw->mask_regs) { for (i = 0; i < hw->nr_mask_regs; i++) { smp = IS_SMP(hw->mask_regs[i]); k += save_reg(d, k, hw->mask_regs[i].set_reg, smp); k += save_reg(d, k, hw->mask_regs[i].clr_reg, smp); #ifdef CONFIG_INTC_BALANCING k += save_reg(d, k, hw->mask_regs[i].dist_reg, 0); #endif } } if (hw->prio_regs) { d->prio = kcalloc(hw->nr_vectors, sizeof(*d->prio), GFP_NOWAIT); if (!d->prio) goto err4; for (i = 0; i < hw->nr_prio_regs; i++) { smp = IS_SMP(hw->prio_regs[i]); k += save_reg(d, k, hw->prio_regs[i].set_reg, smp); k += save_reg(d, k, hw->prio_regs[i].clr_reg, smp); } sort(d->prio, hw->nr_prio_regs, sizeof(*d->prio), intc_handle_int_cmp, NULL); } if (hw->sense_regs) { d->sense = kcalloc(hw->nr_vectors, sizeof(*d->sense), GFP_NOWAIT); if (!d->sense) goto err5; for (i = 0; i < hw->nr_sense_regs; i++) k += save_reg(d, k, hw->sense_regs[i].reg, 0); sort(d->sense, hw->nr_sense_regs, sizeof(*d->sense), intc_handle_int_cmp, NULL); } if (hw->subgroups) for (i = 0; i < hw->nr_subgroups; i++) if (hw->subgroups[i].reg) k+= save_reg(d, k, hw->subgroups[i].reg, 0); memcpy(&d->chip, &intc_irq_chip, sizeof(struct irq_chip)); d->chip.name = desc->name; if (hw->ack_regs) for (i = 0; i < hw->nr_ack_regs; i++) k += save_reg(d, k, hw->ack_regs[i].set_reg, 0); else d->chip.irq_mask_ack = d->chip.irq_disable; if (desc->force_disable) intc_enable_disable_enum(desc, d, desc->force_disable, 0); if (desc->force_enable) intc_enable_disable_enum(desc, d, desc->force_enable, 0); BUG_ON(k > 256); intc_irq_domain_init(d, hw); for (i = 0; i < hw->nr_vectors; i++) { struct intc_vect *vect = hw->vectors + i; unsigned int irq = evt2irq(vect->vect); int res; if (!vect->enum_id) continue; res = irq_create_identity_mapping(d->domain, irq); if (unlikely(res)) { if (res == -EEXIST) { res = irq_domain_associate(d->domain, irq, irq); if (unlikely(res)) { pr_err(""domain association failure\n""); continue; } } else { pr_err(""can't identity map IRQ %d\n"", irq); continue; } } intc_irq_xlate_set(irq, vect->enum_id, d); intc_register_irq(desc, d, vect->enum_id, irq); for (k = i + 1; k < hw->nr_vectors; k++) { struct intc_vect *vect2 = hw->vectors + k; unsigned int irq2 = evt2irq(vect2->vect); if (vect->enum_id != vect2->enum_id) continue; res = irq_create_identity_mapping(d->domain, irq2); if (unlikely(res)) { if (res == -EEXIST) { res = irq_domain_associate(d->domain, irq2, irq2); if (unlikely(res)) { pr_err(""domain association "" ""failure\n""); continue; } } else { pr_err(""can't identity map IRQ %d\n"", irq); continue; } } vect2->enum_id = 0; irq_set_chip(irq2, &dummy_irq_chip); irq_set_chained_handler_and_data(irq2, intc_redirect_irq, (void *)irq); } } intc_subgroup_init(desc, d); if (desc->force_enable) intc_enable_disable_enum(desc, d, desc->force_enable, 1); d->skip_suspend = desc->skip_syscore_suspend; list_add_tail(&d->list, &intc_list); nr_intc_controllers++; return 0; err5: kfree(d->prio); err4: #ifdef CONFIG_SMP kfree(d->smp); err3: #endif kfree(d->reg); err2: for (k = 0; k < d->nr_windows; k++) if (d->window[k].virt) iounmap(d->window[k].virt); kfree(d->window); err1: kfree(d); err0: pr_err(""unable to allocate INTC memory\n""); return -ENOMEM; }"
743----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50020/bad/ice_sriov.c----ice_sriov_set_msix_vec_count,"int ice_sriov_set_msix_vec_count(struct pci_dev *vf_dev, int msix_vec_count) { struct pci_dev *pdev = pci_physfn(vf_dev); struct ice_pf *pf = pci_get_drvdata(pdev); u16 prev_msix, prev_queues, queues; bool needs_rebuild = false; struct ice_vsi *vsi; struct ice_vf *vf; int id; if (!ice_get_num_vfs(pf)) return -ENOENT; if (!msix_vec_count) return 0; queues = msix_vec_count; msix_vec_count += 1; if (queues > min(ice_get_avail_txq_count(pf), ice_get_avail_rxq_count(pf))) return -EINVAL; if (msix_vec_count < ICE_MIN_INTR_PER_VF) return -EINVAL; for (id = 0; id < pci_num_vf(pdev); id++) { if (vf_dev->devfn == pci_iov_virtfn_devfn(pdev, id)) break; } if (id == pci_num_vf(pdev)) return -ENOENT; vf = ice_get_vf_by_id(pf, id); if (!vf) return -ENOENT; vsi = ice_get_vf_vsi(vf); <S2SV_StartVul> if (!vsi) <S2SV_EndVul> return -ENOENT; prev_msix = vf->num_msix; prev_queues = vf->num_vf_qs; if (ice_sriov_move_base_vector(pf, msix_vec_count - prev_msix)) { ice_put_vf(vf); return -ENOSPC; } ice_dis_vf_mappings(vf); ice_sriov_free_irqs(pf, vf); ice_sriov_remap_vectors(pf, vf->vf_id); vf->num_msix = msix_vec_count; vf->num_vf_qs = queues; vf->first_vector_idx = ice_sriov_get_irqs(pf, vf->num_msix); if (vf->first_vector_idx < 0) goto unroll; if (ice_vf_reconfig_vsi(vf) || ice_vf_init_host_cfg(vf, vsi)) { needs_rebuild = true; goto unroll; } dev_info(ice_pf_to_dev(pf), ""Changing VF %d resources to %d vectors and %d queues\n"", vf->vf_id, vf->num_msix, vf->num_vf_qs); ice_ena_vf_mappings(vf); ice_put_vf(vf); return 0; unroll: dev_info(ice_pf_to_dev(pf), ""Can't set %d vectors on VF %d, falling back to %d\n"", vf->num_msix, vf->vf_id, prev_msix); vf->num_msix = prev_msix; vf->num_vf_qs = prev_queues; vf->first_vector_idx = ice_sriov_get_irqs(pf, vf->num_msix); <S2SV_StartVul> if (vf->first_vector_idx < 0) <S2SV_EndVul> return -EINVAL; if (needs_rebuild) { ice_vf_reconfig_vsi(vf); ice_vf_init_host_cfg(vf, vsi); } ice_ena_vf_mappings(vf); ice_put_vf(vf); return -EINVAL; }","- if (!vsi)
- if (vf->first_vector_idx < 0)
+ if (!vsi) {
+ ice_put_vf(vf);
+ }
+ if (vf->first_vector_idx < 0) {
+ ice_put_vf(vf);
+ }","int ice_sriov_set_msix_vec_count(struct pci_dev *vf_dev, int msix_vec_count) { struct pci_dev *pdev = pci_physfn(vf_dev); struct ice_pf *pf = pci_get_drvdata(pdev); u16 prev_msix, prev_queues, queues; bool needs_rebuild = false; struct ice_vsi *vsi; struct ice_vf *vf; int id; if (!ice_get_num_vfs(pf)) return -ENOENT; if (!msix_vec_count) return 0; queues = msix_vec_count; msix_vec_count += 1; if (queues > min(ice_get_avail_txq_count(pf), ice_get_avail_rxq_count(pf))) return -EINVAL; if (msix_vec_count < ICE_MIN_INTR_PER_VF) return -EINVAL; for (id = 0; id < pci_num_vf(pdev); id++) { if (vf_dev->devfn == pci_iov_virtfn_devfn(pdev, id)) break; } if (id == pci_num_vf(pdev)) return -ENOENT; vf = ice_get_vf_by_id(pf, id); if (!vf) return -ENOENT; vsi = ice_get_vf_vsi(vf); if (!vsi) { ice_put_vf(vf); return -ENOENT; } prev_msix = vf->num_msix; prev_queues = vf->num_vf_qs; if (ice_sriov_move_base_vector(pf, msix_vec_count - prev_msix)) { ice_put_vf(vf); return -ENOSPC; } ice_dis_vf_mappings(vf); ice_sriov_free_irqs(pf, vf); ice_sriov_remap_vectors(pf, vf->vf_id); vf->num_msix = msix_vec_count; vf->num_vf_qs = queues; vf->first_vector_idx = ice_sriov_get_irqs(pf, vf->num_msix); if (vf->first_vector_idx < 0) goto unroll; if (ice_vf_reconfig_vsi(vf) || ice_vf_init_host_cfg(vf, vsi)) { needs_rebuild = true; goto unroll; } dev_info(ice_pf_to_dev(pf), ""Changing VF %d resources to %d vectors and %d queues\n"", vf->vf_id, vf->num_msix, vf->num_vf_qs); ice_ena_vf_mappings(vf); ice_put_vf(vf); return 0; unroll: dev_info(ice_pf_to_dev(pf), ""Can't set %d vectors on VF %d, falling back to %d\n"", vf->num_msix, vf->vf_id, prev_msix); vf->num_msix = prev_msix; vf->num_vf_qs = prev_queues; vf->first_vector_idx = ice_sriov_get_irqs(pf, vf->num_msix); if (vf->first_vector_idx < 0) { ice_put_vf(vf); return -EINVAL; } if (needs_rebuild) { ice_vf_reconfig_vsi(vf); ice_vf_init_host_cfg(vf, vsi); } ice_ena_vf_mappings(vf); ice_put_vf(vf); return -EINVAL; }"
882----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50135/bad/pci.c----nvme_reset_work,"static void nvme_reset_work(struct work_struct *work) { struct nvme_dev *dev = container_of(work, struct nvme_dev, ctrl.reset_work); bool was_suspend = !!(dev->ctrl.ctrl_config & NVME_CC_SHN_NORMAL); int result; if (nvme_ctrl_state(&dev->ctrl) != NVME_CTRL_RESETTING) { dev_warn(dev->ctrl.device, ""ctrl state %d is not RESETTING\n"", dev->ctrl.state); result = -ENODEV; goto out; } if (dev->ctrl.ctrl_config & NVME_CC_ENABLE) nvme_dev_disable(dev, false); nvme_sync_queues(&dev->ctrl); mutex_lock(&dev->shutdown_lock); result = nvme_pci_enable(dev); if (result) goto out_unlock; nvme_unquiesce_admin_queue(&dev->ctrl); mutex_unlock(&dev->shutdown_lock); if (!nvme_change_ctrl_state(&dev->ctrl, NVME_CTRL_CONNECTING)) { dev_warn(dev->ctrl.device, ""failed to mark controller CONNECTING\n""); result = -EBUSY; goto out; } result = nvme_init_ctrl_finish(&dev->ctrl, was_suspend); if (result) goto out; nvme_dbbuf_dma_alloc(dev); result = nvme_setup_host_mem(dev); if (result < 0) goto out; result = nvme_setup_io_queues(dev); if (result) goto out; if (dev->online_queues > 1) { nvme_dbbuf_set(dev); nvme_unquiesce_io_queues(&dev->ctrl); nvme_wait_freeze(&dev->ctrl); <S2SV_StartVul> nvme_pci_update_nr_queues(dev); <S2SV_EndVul> nvme_unfreeze(&dev->ctrl); } else { dev_warn(dev->ctrl.device, ""IO queues lost\n""); nvme_mark_namespaces_dead(&dev->ctrl); nvme_unquiesce_io_queues(&dev->ctrl); nvme_remove_namespaces(&dev->ctrl); nvme_free_tagset(dev); } if (!nvme_change_ctrl_state(&dev->ctrl, NVME_CTRL_LIVE)) { dev_warn(dev->ctrl.device, ""failed to mark controller live state\n""); result = -ENODEV; goto out; } nvme_start_ctrl(&dev->ctrl); return; out_unlock: mutex_unlock(&dev->shutdown_lock); out: dev_warn(dev->ctrl.device, ""Disabling device after reset failure: %d\n"", result); nvme_change_ctrl_state(&dev->ctrl, NVME_CTRL_DELETING); nvme_dev_disable(dev, true); nvme_sync_queues(&dev->ctrl); nvme_mark_namespaces_dead(&dev->ctrl); nvme_unquiesce_io_queues(&dev->ctrl); nvme_change_ctrl_state(&dev->ctrl, NVME_CTRL_DEAD); }","- nvme_pci_update_nr_queues(dev);
+ if (!nvme_pci_update_nr_queues(dev))
+ goto out;","static void nvme_reset_work(struct work_struct *work) { struct nvme_dev *dev = container_of(work, struct nvme_dev, ctrl.reset_work); bool was_suspend = !!(dev->ctrl.ctrl_config & NVME_CC_SHN_NORMAL); int result; if (nvme_ctrl_state(&dev->ctrl) != NVME_CTRL_RESETTING) { dev_warn(dev->ctrl.device, ""ctrl state %d is not RESETTING\n"", dev->ctrl.state); result = -ENODEV; goto out; } if (dev->ctrl.ctrl_config & NVME_CC_ENABLE) nvme_dev_disable(dev, false); nvme_sync_queues(&dev->ctrl); mutex_lock(&dev->shutdown_lock); result = nvme_pci_enable(dev); if (result) goto out_unlock; nvme_unquiesce_admin_queue(&dev->ctrl); mutex_unlock(&dev->shutdown_lock); if (!nvme_change_ctrl_state(&dev->ctrl, NVME_CTRL_CONNECTING)) { dev_warn(dev->ctrl.device, ""failed to mark controller CONNECTING\n""); result = -EBUSY; goto out; } result = nvme_init_ctrl_finish(&dev->ctrl, was_suspend); if (result) goto out; nvme_dbbuf_dma_alloc(dev); result = nvme_setup_host_mem(dev); if (result < 0) goto out; result = nvme_setup_io_queues(dev); if (result) goto out; if (dev->online_queues > 1) { nvme_dbbuf_set(dev); nvme_unquiesce_io_queues(&dev->ctrl); nvme_wait_freeze(&dev->ctrl); if (!nvme_pci_update_nr_queues(dev)) goto out; nvme_unfreeze(&dev->ctrl); } else { dev_warn(dev->ctrl.device, ""IO queues lost\n""); nvme_mark_namespaces_dead(&dev->ctrl); nvme_unquiesce_io_queues(&dev->ctrl); nvme_remove_namespaces(&dev->ctrl); nvme_free_tagset(dev); } if (!nvme_change_ctrl_state(&dev->ctrl, NVME_CTRL_LIVE)) { dev_warn(dev->ctrl.device, ""failed to mark controller live state\n""); result = -ENODEV; goto out; } nvme_start_ctrl(&dev->ctrl); return; out_unlock: mutex_unlock(&dev->shutdown_lock); out: dev_warn(dev->ctrl.device, ""Disabling device after reset failure: %d\n"", result); nvme_change_ctrl_state(&dev->ctrl, NVME_CTRL_DELETING); nvme_dev_disable(dev, true); nvme_sync_queues(&dev->ctrl); nvme_mark_namespaces_dead(&dev->ctrl); nvme_unquiesce_io_queues(&dev->ctrl); nvme_change_ctrl_state(&dev->ctrl, NVME_CTRL_DEAD); }"
1294----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56588/bad/hisi_sas_v3_hw.c----debugfs_create_files_v3_hw,"static void debugfs_create_files_v3_hw(struct hisi_hba *hisi_hba) { u64 *debugfs_timestamp; int dump_index = hisi_hba->debugfs_dump_index; struct dentry *dump_dentry; struct dentry *dentry; char name[256]; int p; int c; int d; <S2SV_StartVul> snprintf(name, 256, ""%d"", dump_index); <S2SV_EndVul> dump_dentry = debugfs_create_dir(name, hisi_hba->debugfs_dump_dentry); <S2SV_StartVul> debugfs_timestamp = &hisi_hba->debugfs_timestamp[dump_index]; <S2SV_EndVul> debugfs_create_u64(""timestamp"", 0400, dump_dentry, debugfs_timestamp); debugfs_create_file(""global"", 0400, dump_dentry, <S2SV_StartVul> &hisi_hba->debugfs_regs[dump_index][DEBUGFS_GLOBAL], <S2SV_EndVul> &debugfs_global_v3_hw_fops); dentry = debugfs_create_dir(""port"", dump_dentry); for (p = 0; p < hisi_hba->n_phy; p++) { snprintf(name, 256, ""%d"", p); debugfs_create_file(name, 0400, dentry, <S2SV_StartVul> &hisi_hba->debugfs_port_reg[dump_index][p], <S2SV_EndVul> &debugfs_port_v3_hw_fops); } dentry = debugfs_create_dir(""cq"", dump_dentry); for (c = 0; c < hisi_hba->queue_count; c++) { snprintf(name, 256, ""%d"", c); debugfs_create_file(name, 0400, dentry, <S2SV_StartVul> &hisi_hba->debugfs_cq[dump_index][c], <S2SV_EndVul> &debugfs_cq_v3_hw_fops); } dentry = debugfs_create_dir(""dq"", dump_dentry); for (d = 0; d < hisi_hba->queue_count; d++) { snprintf(name, 256, ""%d"", d); debugfs_create_file(name, 0400, dentry, <S2SV_StartVul> &hisi_hba->debugfs_dq[dump_index][d], <S2SV_EndVul> &debugfs_dq_v3_hw_fops); } debugfs_create_file(""iost"", 0400, dump_dentry, <S2SV_StartVul> &hisi_hba->debugfs_iost[dump_index], <S2SV_EndVul> &debugfs_iost_v3_hw_fops); debugfs_create_file(""iost_cache"", 0400, dump_dentry, <S2SV_StartVul> &hisi_hba->debugfs_iost_cache[dump_index], <S2SV_EndVul> &debugfs_iost_cache_v3_hw_fops); debugfs_create_file(""itct"", 0400, dump_dentry, <S2SV_StartVul> &hisi_hba->debugfs_itct[dump_index], <S2SV_EndVul> &debugfs_itct_v3_hw_fops); debugfs_create_file(""itct_cache"", 0400, dump_dentry, <S2SV_StartVul> &hisi_hba->debugfs_itct_cache[dump_index], <S2SV_EndVul> &debugfs_itct_cache_v3_hw_fops); debugfs_create_file(""axi"", 0400, dump_dentry, <S2SV_StartVul> &hisi_hba->debugfs_regs[dump_index][DEBUGFS_AXI], <S2SV_EndVul> &debugfs_axi_v3_hw_fops); debugfs_create_file(""ras"", 0400, dump_dentry, <S2SV_StartVul> &hisi_hba->debugfs_regs[dump_index][DEBUGFS_RAS], <S2SV_EndVul> &debugfs_ras_v3_hw_fops); }","- snprintf(name, 256, ""%d"", dump_index);
- debugfs_timestamp = &hisi_hba->debugfs_timestamp[dump_index];
- &hisi_hba->debugfs_regs[dump_index][DEBUGFS_GLOBAL],
- &hisi_hba->debugfs_port_reg[dump_index][p],
- &hisi_hba->debugfs_cq[dump_index][c],
- &hisi_hba->debugfs_dq[dump_index][d],
- &hisi_hba->debugfs_iost[dump_index],
- &hisi_hba->debugfs_iost_cache[dump_index],
- &hisi_hba->debugfs_itct[dump_index],
- &hisi_hba->debugfs_itct_cache[dump_index],
- &hisi_hba->debugfs_regs[dump_index][DEBUGFS_AXI],
- &hisi_hba->debugfs_regs[dump_index][DEBUGFS_RAS],
+ #define DLVRY_QUEUE_ENABLE 0x0
+ #define ITCT_BASE_ADDR_HI 0x14
+ #define PHY_CONN_RATE 0x30
+ #define SATA_INITI_D2H_STORE_ADDR_LO 0x60
+ #define CFG_MAX_TAG 0x68
+ #define MAX_CON_TIME_LIMIT_TIME 0xa4
+ #define REJECT_TO_OPEN_LIMIT_TIME 0xac
+ #define CFG_SET_ABORTED_EN_OFF 12
+ #define CFG_ABT_SET_IPTT_DONE_OFF 0
+ #define HGC_LM_DFX_STATUS2_IOSTLIST_OFF 0
+ #define HGC_LM_DFX_STATUS2_ITCTLIST_MSK (0x7ff << \
+ #define HGC_CQE_ECC_1B_ADDR_MSK (0x3f << HGC_CQE_ECC_1B_ADDR_OFF)
+ #define HGC_IOST_ECC_1B_ADDR_OFF 0
+ #define HGC_DQE_ECC_ADDR 0x144
+ #define HGC_DQE_ECC_MB_ADDR_MSK (0xfff << HGC_DQE_ECC_MB_ADDR_OFF)
+ #define TAB_DFX 0x14c","#include <linux/sched/clock.h> #include ""hisi_sas.h"" #define DRV_NAME ""hisi_sas_v3_hw"" #define DLVRY_QUEUE_ENABLE 0x0 #define IOST_BASE_ADDR_LO 0x8 #define IOST_BASE_ADDR_HI 0xc #define ITCT_BASE_ADDR_LO 0x10 #define ITCT_BASE_ADDR_HI 0x14 #define IO_BROKEN_MSG_ADDR_LO 0x18 #define IO_BROKEN_MSG_ADDR_HI 0x1c #define PHY_CONTEXT 0x20 #define PHY_STATE 0x24 #define PHY_PORT_NUM_MA 0x28 #define PHY_CONN_RATE 0x30 #define ITCT_CLR 0x44 #define ITCT_CLR_EN_OFF 16 #define ITCT_CLR_EN_MSK (0x1 << ITCT_CLR_EN_OFF) #define ITCT_DEV_OFF 0 #define ITCT_DEV_MSK (0x7ff << ITCT_DEV_OFF) #define SAS_AXI_USER3 0x50 #define IO_SATA_BROKEN_MSG_ADDR_LO 0x58 #define IO_SATA_BROKEN_MSG_ADDR_HI 0x5c #define SATA_INITI_D2H_STORE_ADDR_LO 0x60 #define SATA_INITI_D2H_STORE_ADDR_HI 0x64 #define CFG_MAX_TAG 0x68 #define TRANS_LOCK_ICT_TIME 0X70 #define HGC_SAS_TX_OPEN_FAIL_RETRY_CTRL 0x84 #define HGC_SAS_TXFAIL_RETRY_CTRL 0x88 #define HGC_GET_ITV_TIME 0x90 #define DEVICE_MSG_WORK_MODE 0x94 #define OPENA_WT_CONTI_TIME 0x9c #define I_T_NEXUS_LOSS_TIME 0xa0 #define MAX_CON_TIME_LIMIT_TIME 0xa4 #define BUS_INACTIVE_LIMIT_TIME 0xa8 #define REJECT_TO_OPEN_LIMIT_TIME 0xac #define CQ_INT_CONVERGE_EN 0xb0 #define CFG_AGING_TIME 0xbc #define HGC_DFX_CFG2 0xc0 #define CFG_ICT_TIMER_STEP_TRSH 0xc8 #define CFG_ABT_SET_QUERY_IPTT 0xd4 #define CFG_SET_ABORTED_IPTT_OFF 0 #define CFG_SET_ABORTED_IPTT_MSK (0xfff << CFG_SET_ABORTED_IPTT_OFF) #define CFG_SET_ABORTED_EN_OFF 12 #define CFG_ABT_SET_IPTT_DONE 0xd8 #define CFG_ABT_SET_IPTT_DONE_OFF 0 #define HGC_IOMB_PROC1_STATUS 0x104 #define HGC_LM_DFX_STATUS2 0x128 #define HGC_LM_DFX_STATUS2_IOSTLIST_OFF 0 #define HGC_LM_DFX_STATUS2_IOSTLIST_MSK (0xfff << \ HGC_LM_DFX_STATUS2_IOSTLIST_OFF) #define HGC_LM_DFX_STATUS2_ITCTLIST_OFF 12 #define HGC_LM_DFX_STATUS2_ITCTLIST_MSK (0x7ff << \ HGC_LM_DFX_STATUS2_ITCTLIST_OFF) #define HGC_CQE_ECC_ADDR 0x13c #define HGC_CQE_ECC_1B_ADDR_OFF 0 #define HGC_CQE_ECC_1B_ADDR_MSK (0x3f << HGC_CQE_ECC_1B_ADDR_OFF) #define HGC_CQE_ECC_MB_ADDR_OFF 8 #define HGC_CQE_ECC_MB_ADDR_MSK (0x3f << HGC_CQE_ECC_MB_ADDR_OFF) #define HGC_IOST_ECC_ADDR 0x140 #define HGC_IOST_ECC_1B_ADDR_OFF 0 #define HGC_IOST_ECC_1B_ADDR_MSK (0x3ff << HGC_IOST_ECC_1B_ADDR_OFF) #define HGC_IOST_ECC_MB_ADDR_OFF 16 #define HGC_IOST_ECC_MB_ADDR_MSK (0x3ff << HGC_IOST_ECC_MB_ADDR_OFF) #define HGC_DQE_ECC_ADDR 0x144 #define HGC_DQE_ECC_1B_ADDR_OFF 0 #define HGC_DQE_ECC_1B_ADDR_MSK (0xfff << HGC_DQE_ECC_1B_ADDR_OFF) #define HGC_DQE_ECC_MB_ADDR_OFF 16 #define HGC_DQE_ECC_MB_ADDR_MSK (0xfff << HGC_DQE_ECC_MB_ADDR_OFF) #define CHNL_INT_STATUS 0x148 #define TAB_DFX 0x14c #define HGC_ITCT_ECC_ADDR 0x150 #define HGC_ITCT_ECC_1B_ADDR_OFF 0 #define HGC_ITCT_ECC_1B_ADDR_MSK (0x3ff << \ HGC_ITCT_ECC_1B_ADDR_OFF) #define HGC_ITCT_ECC_MB_ADDR_OFF 16 #define HGC_ITCT_ECC_MB_ADDR_MSK (0x3ff << \ HGC_ITCT_ECC_MB_ADDR_OFF) #define HGC_AXI_FIFO_ERR_INFO 0x154 #define AXI_ERR_INFO_OFF 0 #define AXI_ERR_INFO_MSK (0xff << AXI_ERR_INFO_OFF) #define FIFO_ERR_INFO_OFF 8 #define FIFO_ERR_INFO_MSK (0xff << FIFO_ERR_INFO_OFF) #define TAB_RD_TYPE 0x15c #define INT_COAL_EN 0x19c #define OQ_INT_COAL_TIME 0x1a0 #define OQ_INT_COAL_CNT 0x1a4 #define ENT_INT_COAL_TIME 0x1a8 #define ENT_INT_COAL_CNT 0x1ac #define OQ_INT_SRC 0x1b0 #define OQ_INT_SRC_MSK 0x1b4 #define ENT_INT_SRC1 0x1b8 #define ENT_INT_SRC1_D2H_FIS_CH0_OFF 0 #define ENT_INT_SRC1_D2H_FIS_CH0_MSK (0x1 << ENT_INT_SRC1_D2H_FIS_CH0_OFF) #define ENT_INT_SRC1_D2H_FIS_CH1_OFF 8 #define ENT_INT_SRC1_D2H_FIS_CH1_MSK (0x1 << ENT_INT_SRC1_D2H_FIS_CH1_OFF) #define ENT_INT_SRC2 0x1bc #define ENT_INT_SRC3 0x1c0 #define ENT_INT_SRC3_WP_DEPTH_OFF 8 #define ENT_INT_SRC3_IPTT_SLOT_NOMATCH_OFF 9 #define ENT_INT_SRC3_RP_DEPTH_OFF 10 #define ENT_INT_SRC3_AXI_OFF 11 #define ENT_INT_SRC3_FIFO_OFF 12 #define ENT_INT_SRC3_LM_OFF 14 #define ENT_INT_SRC3_ITC_INT_OFF 15 #define ENT_INT_SRC3_ITC_INT_MSK (0x1 << ENT_INT_SRC3_ITC_INT_OFF) #define ENT_INT_SRC3_ABT_OFF 16 #define ENT_INT_SRC3_DQE_POISON_OFF 18 #define ENT_INT_SRC3_IOST_POISON_OFF 19 #define ENT_INT_SRC3_ITCT_POISON_OFF 20 #define ENT_INT_SRC3_ITCT_NCQ_POISON_OFF 21 #define ENT_INT_SRC_MSK1 0x1c4 #define ENT_INT_SRC_MSK2 0x1c8 #define ENT_INT_SRC_MSK3 0x1cc #define ENT_INT_SRC_MSK3_ENT95_MSK_OFF 31 #define CHNL_PHYUPDOWN_INT_MSK 0x1d0 #define CHNL_ENT_INT_MSK 0x1d4 #define HGC_COM_INT_MSK 0x1d8 #define ENT_INT_SRC_MSK3_ENT95_MSK_MSK (0x1 << ENT_INT_SRC_MSK3_ENT95_MSK_OFF) #define SAS_ECC_INTR 0x1e8 #define SAS_ECC_INTR_DQE_ECC_1B_OFF 0 #define SAS_ECC_INTR_DQE_ECC_MB_OFF 1 #define SAS_ECC_INTR_IOST_ECC_1B_OFF 2 #define SAS_ECC_INTR_IOST_ECC_MB_OFF 3 #define SAS_ECC_INTR_ITCT_ECC_1B_OFF 4 #define SAS_ECC_INTR_ITCT_ECC_MB_OFF 5 #define SAS_ECC_INTR_ITCTLIST_ECC_1B_OFF 6 #define SAS_ECC_INTR_ITCTLIST_ECC_MB_OFF 7 #define SAS_ECC_INTR_IOSTLIST_ECC_1B_OFF 8 #define SAS_ECC_INTR_IOSTLIST_ECC_MB_OFF 9 #define SAS_ECC_INTR_CQE_ECC_1B_OFF 10 #define SAS_ECC_INTR_CQE_ECC_MB_OFF 11 #define SAS_ECC_INTR_NCQ_MEM0_ECC_1B_OFF 12 #define SAS_ECC_INTR_NCQ_MEM0_ECC_MB_OFF 13 #define SAS_ECC_INTR_NCQ_MEM1_ECC_1B_OFF 14 #define SAS_ECC_INTR_NCQ_MEM1_ECC_MB_OFF 15 #define SAS_ECC_INTR_NCQ_MEM2_ECC_1B_OFF 16 #define SAS_ECC_INTR_NCQ_MEM2_ECC_MB_OFF 17 #define SAS_ECC_INTR_NCQ_MEM3_ECC_1B_OFF 18 #define SAS_ECC_INTR_NCQ_MEM3_ECC_MB_OFF 19 #define SAS_ECC_INTR_OOO_RAM_ECC_1B_OFF 20 #define SAS_ECC_INTR_OOO_RAM_ECC_MB_OFF 21 #define SAS_ECC_INTR_MSK 0x1ec #define HGC_ERR_STAT_EN 0x238 #define CQE_SEND_CNT 0x248 #define DLVRY_Q_0_BASE_ADDR_LO 0x260 #define DLVRY_Q_0_BASE_ADDR_HI 0x264 #define DLVRY_Q_0_DEPTH 0x268 #define DLVRY_Q_0_WR_PTR 0x26c #define DLVRY_Q_0_RD_PTR 0x270 #define HYPER_STREAM_ID_EN_CFG 0xc80 #define OQ0_INT_SRC_MSK 0xc90 #define COMPL_Q_0_BASE_ADDR_LO 0x4e0 #define COMPL_Q_0_BASE_ADDR_HI 0x4e4 #define COMPL_Q_0_DEPTH 0x4e8 #define COMPL_Q_0_WR_PTR 0x4ec #define COMPL_Q_0_RD_PTR 0x4f0 #define HGC_RXM_DFX_STATUS14 0xae8 #define HGC_RXM_DFX_STATUS14_MEM0_OFF 0 #define HGC_RXM_DFX_STATUS14_MEM0_MSK (0x1ff << \ HGC_RXM_DFX_STATUS14_MEM0_OFF) #define HGC_RXM_DFX_STATUS14_MEM1_OFF 9 #define HGC_RXM_DFX_STATUS14_MEM1_MSK (0x1ff << \ HGC_RXM_DFX_STATUS14_MEM1_OFF) #define HGC_RXM_DFX_STATUS14_MEM2_OFF 18 #define HGC_RXM_DFX_STATUS14_MEM2_MSK (0x1ff << \ HGC_RXM_DFX_STATUS14_MEM2_OFF) #define HGC_RXM_DFX_STATUS15 0xaec #define HGC_RXM_DFX_STATUS15_MEM3_OFF 0 #define HGC_RXM_DFX_STATUS15_MEM3_MSK (0x1ff << \ HGC_RXM_DFX_STATUS15_MEM3_OFF) #define AWQOS_AWCACHE_CFG 0xc84 #define ARQOS_ARCACHE_CFG 0xc88 #define HILINK_ERR_DFX 0xe04 #define SAS_GPIO_CFG_0 0x1000 #define SAS_GPIO_CFG_1 0x1004 #define SAS_GPIO_TX_0_1 0x1040 #define SAS_CFG_DRIVE_VLD 0x1070 #define PORT_BASE (0x2000) #define PHY_CFG (PORT_BASE + 0x0) #define HARD_PHY_LINKRATE (PORT_BASE + 0x4) #define PHY_CFG_ENA_OFF 0 #define PHY_CFG_ENA_MSK (0x1 << PHY_CFG_ENA_OFF) #define PHY_CFG_DC_OPT_OFF 2 #define PHY_CFG_DC_OPT_MSK (0x1 << PHY_CFG_DC_OPT_OFF) #define PHY_CFG_PHY_RST_OFF 3 #define PHY_CFG_PHY_RST_MSK (0x1 << PHY_CFG_PHY_RST_OFF) #define PROG_PHY_LINK_RATE (PORT_BASE + 0x8) #define CFG_PROG_PHY_LINK_RATE_OFF 0 #define CFG_PROG_PHY_LINK_RATE_MSK (0xff << CFG_PROG_PHY_LINK_RATE_OFF) #define CFG_PROG_OOB_PHY_LINK_RATE_OFF 8 #define CFG_PROG_OOB_PHY_LINK_RATE_MSK (0xf << CFG_PROG_OOB_PHY_LINK_RATE_OFF) #define PHY_CTRL (PORT_BASE + 0x14) #define PHY_CTRL_RESET_OFF 0 #define PHY_CTRL_RESET_MSK (0x1 << PHY_CTRL_RESET_OFF) #define CMD_HDR_PIR_OFF 8 #define CMD_HDR_PIR_MSK (0x1 << CMD_HDR_PIR_OFF) #define SERDES_CFG (PORT_BASE + 0x1c) #define CFG_ALOS_CHK_DISABLE_OFF 9 #define CFG_ALOS_CHK_DISABLE_MSK (0x1 << CFG_ALOS_CHK_DISABLE_OFF) #define SAS_PHY_BIST_CTRL (PORT_BASE + 0x2c) #define CFG_BIST_MODE_SEL_OFF 0 #define CFG_BIST_MODE_SEL_MSK (0xf << CFG_BIST_MODE_SEL_OFF) #define CFG_LOOP_TEST_MODE_OFF 14 #define CFG_LOOP_TEST_MODE_MSK (0x3 << CFG_LOOP_TEST_MODE_OFF) #define CFG_RX_BIST_EN_OFF 16 #define CFG_RX_BIST_EN_MSK (0x1 << CFG_RX_BIST_EN_OFF) #define CFG_TX_BIST_EN_OFF 17 #define CFG_TX_BIST_EN_MSK (0x1 << CFG_TX_BIST_EN_OFF) #define CFG_BIST_TEST_OFF 18 #define CFG_BIST_TEST_MSK (0x1 << CFG_BIST_TEST_OFF) #define SAS_PHY_BIST_CODE (PORT_BASE + 0x30) #define SAS_PHY_BIST_CODE1 (PORT_BASE + 0x34) #define SAS_BIST_ERR_CNT (PORT_BASE + 0x38) #define SL_CFG (PORT_BASE + 0x84) #define AIP_LIMIT (PORT_BASE + 0x90) #define SL_CONTROL (PORT_BASE + 0x94) #define SL_CONTROL_NOTIFY_EN_OFF 0 #define SL_CONTROL_NOTIFY_EN_MSK (0x1 << SL_CONTROL_NOTIFY_EN_OFF) #define SL_CTA_OFF 17 #define SL_CTA_MSK (0x1 << SL_CTA_OFF) #define RX_PRIMS_STATUS (PORT_BASE + 0x98) #define RX_BCAST_CHG_OFF 1 #define RX_BCAST_CHG_MSK (0x1 << RX_BCAST_CHG_OFF) #define TX_ID_DWORD0 (PORT_BASE + 0x9c) #define TX_ID_DWORD1 (PORT_BASE + 0xa0) #define TX_ID_DWORD2 (PORT_BASE + 0xa4) #define TX_ID_DWORD3 (PORT_BASE + 0xa8) #define TX_ID_DWORD4 (PORT_BASE + 0xaC) #define TX_ID_DWORD5 (PORT_BASE + 0xb0) #define TX_ID_DWORD6 (PORT_BASE + 0xb4) #define TXID_AUTO (PORT_BASE + 0xb8) #define CT3_OFF 1 #define CT3_MSK (0x1 << CT3_OFF) #define TX_HARDRST_OFF 2 #define TX_HARDRST_MSK (0x1 << TX_HARDRST_OFF) #define RX_IDAF_DWORD0 (PORT_BASE + 0xc4) #define RXOP_CHECK_CFG_H (PORT_BASE + 0xfc) #define STP_LINK_TIMER (PORT_BASE + 0x120) #define STP_LINK_TIMEOUT_STATE (PORT_BASE + 0x124) #define CON_CFG_DRIVER (PORT_BASE + 0x130) #define SAS_SSP_CON_TIMER_CFG (PORT_BASE + 0x134) #define SAS_SMP_CON_TIMER_CFG (PORT_BASE + 0x138) #define SAS_STP_CON_TIMER_CFG (PORT_BASE + 0x13c) #define CHL_INT0 (PORT_BASE + 0x1b4) #define CHL_INT0_HOTPLUG_TOUT_OFF 0 #define CHL_INT0_HOTPLUG_TOUT_MSK (0x1 << CHL_INT0_HOTPLUG_TOUT_OFF) #define CHL_INT0_SL_RX_BCST_ACK_OFF 1 #define CHL_INT0_SL_RX_BCST_ACK_MSK (0x1 << CHL_INT0_SL_RX_BCST_ACK_OFF) #define CHL_INT0_SL_PHY_ENABLE_OFF 2 #define CHL_INT0_SL_PHY_ENABLE_MSK (0x1 << CHL_INT0_SL_PHY_ENABLE_OFF) #define CHL_INT0_NOT_RDY_OFF 4 #define CHL_INT0_NOT_RDY_MSK (0x1 << CHL_INT0_NOT_RDY_OFF) #define CHL_INT0_PHY_RDY_OFF 5 #define CHL_INT0_PHY_RDY_MSK (0x1 << CHL_INT0_PHY_RDY_OFF) #define CHL_INT1 (PORT_BASE + 0x1b8) #define CHL_INT1_DMAC_TX_ECC_MB_ERR_OFF 15 #define CHL_INT1_DMAC_TX_ECC_1B_ERR_OFF 16 #define CHL_INT1_DMAC_RX_ECC_MB_ERR_OFF 17 #define CHL_INT1_DMAC_RX_ECC_1B_ERR_OFF 18 #define CHL_INT1_DMAC_TX_AXI_WR_ERR_OFF 19 #define CHL_INT1_DMAC_TX_AXI_RD_ERR_OFF 20 #define CHL_INT1_DMAC_RX_AXI_WR_ERR_OFF 21 #define CHL_INT1_DMAC_RX_AXI_RD_ERR_OFF 22 #define CHL_INT1_DMAC_TX_FIFO_ERR_OFF 23 #define CHL_INT1_DMAC_RX_FIFO_ERR_OFF 24 #define CHL_INT1_DMAC_TX_AXI_RUSER_ERR_OFF 26 #define CHL_INT1_DMAC_RX_AXI_RUSER_ERR_OFF 27 #define CHL_INT2 (PORT_BASE + 0x1bc) #define CHL_INT2_SL_IDAF_TOUT_CONF_OFF 0 #define CHL_INT2_RX_DISP_ERR_OFF 28 #define CHL_INT2_RX_CODE_ERR_OFF 29 #define CHL_INT2_RX_INVLD_DW_OFF 30 #define CHL_INT2_STP_LINK_TIMEOUT_OFF 31 #define CHL_INT0_MSK (PORT_BASE + 0x1c0) #define CHL_INT1_MSK (PORT_BASE + 0x1c4) #define CHL_INT2_MSK (PORT_BASE + 0x1c8) #define SAS_EC_INT_COAL_TIME (PORT_BASE + 0x1cc) #define CHL_INT_COAL_EN (PORT_BASE + 0x1d0) #define SAS_RX_TRAIN_TIMER (PORT_BASE + 0x2a4) #define PHY_CTRL_RDY_MSK (PORT_BASE + 0x2b0) #define PHYCTRL_NOT_RDY_MSK (PORT_BASE + 0x2b4) #define PHYCTRL_DWS_RESET_MSK (PORT_BASE + 0x2b8) #define PHYCTRL_PHY_ENA_MSK (PORT_BASE + 0x2bc) #define SL_RX_BCAST_CHK_MSK (PORT_BASE + 0x2c0) #define PHYCTRL_OOB_RESTART_MSK (PORT_BASE + 0x2c4) #define DMA_TX_STATUS (PORT_BASE + 0x2d0) #define DMA_TX_STATUS_BUSY_OFF 0 #define DMA_TX_STATUS_BUSY_MSK (0x1 << DMA_TX_STATUS_BUSY_OFF) #define DMA_RX_STATUS (PORT_BASE + 0x2e8) #define DMA_RX_STATUS_BUSY_OFF 0 #define DMA_RX_STATUS_BUSY_MSK (0x1 << DMA_RX_STATUS_BUSY_OFF) #define COARSETUNE_TIME (PORT_BASE + 0x304) #define TXDEEMPH_G1 (PORT_BASE + 0x350) #define ERR_CNT_DWS_LOST (PORT_BASE + 0x380) #define ERR_CNT_RESET_PROB (PORT_BASE + 0x384) #define ERR_CNT_INVLD_DW (PORT_BASE + 0x390) #define ERR_CNT_CODE_ERR (PORT_BASE + 0x394) #define ERR_CNT_DISP_ERR (PORT_BASE + 0x398) #define DFX_FIFO_CTRL (PORT_BASE + 0x3a0) #define DFX_FIFO_CTRL_TRIGGER_MODE_OFF 0 #define DFX_FIFO_CTRL_TRIGGER_MODE_MSK (0x7 << DFX_FIFO_CTRL_TRIGGER_MODE_OFF) #define DFX_FIFO_CTRL_DUMP_MODE_OFF 3 #define DFX_FIFO_CTRL_DUMP_MODE_MSK (0x7 << DFX_FIFO_CTRL_DUMP_MODE_OFF) #define DFX_FIFO_CTRL_SIGNAL_SEL_OFF 6 #define DFX_FIFO_CTRL_SIGNAL_SEL_MSK (0xF << DFX_FIFO_CTRL_SIGNAL_SEL_OFF) #define DFX_FIFO_CTRL_DUMP_DISABLE_OFF 10 #define DFX_FIFO_CTRL_DUMP_DISABLE_MSK (0x1 << DFX_FIFO_CTRL_DUMP_DISABLE_OFF) #define DFX_FIFO_TRIGGER (PORT_BASE + 0x3a4) #define DFX_FIFO_TRIGGER_MSK (PORT_BASE + 0x3a8) #define DFX_FIFO_DUMP_MSK (PORT_BASE + 0x3aC) #define DFX_FIFO_RD_DATA (PORT_BASE + 0x3b0) #define DEFAULT_ITCT_HW 2048 #if (HISI_SAS_MAX_DEVICES > DEFAULT_ITCT_HW) #error Max ITCT exceeded #endif #define AXI_MASTER_CFG_BASE (0x5000) #define AM_CTRL_GLOBAL (0x0) #define AM_CTRL_SHUTDOWN_REQ_OFF 0 #define AM_CTRL_SHUTDOWN_REQ_MSK (0x1 << AM_CTRL_SHUTDOWN_REQ_OFF) #define AM_CURR_TRANS_RETURN (0x150) #define AM_CFG_MAX_TRANS (0x5010) #define AM_CFG_SINGLE_PORT_MAX_TRANS (0x5014) #define AXI_CFG (0x5100) #define AM_ROB_ECC_ERR_ADDR (0x510c) #define AM_ROB_ECC_ERR_ADDR_OFF 0 #define AM_ROB_ECC_ERR_ADDR_MSK 0xffffffff #define RAS_BASE (0x6000) #define SAS_RAS_INTR0 (RAS_BASE) #define SAS_RAS_INTR1 (RAS_BASE + 0x04) #define SAS_RAS_INTR0_MASK (RAS_BASE + 0x08) #define SAS_RAS_INTR1_MASK (RAS_BASE + 0x0c) #define CFG_SAS_RAS_INTR_MASK (RAS_BASE + 0x1c) #define SAS_RAS_INTR2 (RAS_BASE + 0x20) #define SAS_RAS_INTR2_MASK (RAS_BASE + 0x24) #define CMD_HDR_ABORT_FLAG_OFF 0 #define CMD_HDR_ABORT_FLAG_MSK (0x3 << CMD_HDR_ABORT_FLAG_OFF) #define CMD_HDR_ABORT_DEVICE_TYPE_OFF 2 #define CMD_HDR_ABORT_DEVICE_TYPE_MSK (0x1 << CMD_HDR_ABORT_DEVICE_TYPE_OFF) #define CMD_HDR_RESP_REPORT_OFF 5 #define CMD_HDR_RESP_REPORT_MSK (0x1 << CMD_HDR_RESP_REPORT_OFF) #define CMD_HDR_TLR_CTRL_OFF 6 #define CMD_HDR_TLR_CTRL_MSK (0x3 << CMD_HDR_TLR_CTRL_OFF) #define CMD_HDR_PORT_OFF 18 #define CMD_HDR_PORT_MSK (0xf << CMD_HDR_PORT_OFF) #define CMD_HDR_PRIORITY_OFF 27 #define CMD_HDR_PRIORITY_MSK (0x1 << CMD_HDR_PRIORITY_OFF) #define CMD_HDR_CMD_OFF 29 #define CMD_HDR_CMD_MSK (0x7 << CMD_HDR_CMD_OFF) #define CMD_HDR_UNCON_CMD_OFF 3 #define CMD_HDR_DIR_OFF 5 #define CMD_HDR_DIR_MSK (0x3 << CMD_HDR_DIR_OFF) #define CMD_HDR_RESET_OFF 7 #define CMD_HDR_RESET_MSK (0x1 << CMD_HDR_RESET_OFF) #define CMD_HDR_VDTL_OFF 10 #define CMD_HDR_VDTL_MSK (0x1 << CMD_HDR_VDTL_OFF) #define CMD_HDR_FRAME_TYPE_OFF 11 #define CMD_HDR_FRAME_TYPE_MSK (0x1f << CMD_HDR_FRAME_TYPE_OFF) #define CMD_HDR_DEV_ID_OFF 16 #define CMD_HDR_DEV_ID_MSK (0xffff << CMD_HDR_DEV_ID_OFF) #define CMD_HDR_CFL_OFF 0 #define CMD_HDR_CFL_MSK (0x1ff << CMD_HDR_CFL_OFF) #define CMD_HDR_NCQ_TAG_OFF 10 #define CMD_HDR_NCQ_TAG_MSK (0x1f << CMD_HDR_NCQ_TAG_OFF) #define CMD_HDR_MRFL_OFF 15 #define CMD_HDR_MRFL_MSK (0x1ff << CMD_HDR_MRFL_OFF) #define CMD_HDR_SG_MOD_OFF 24 #define CMD_HDR_SG_MOD_MSK (0x3 << CMD_HDR_SG_MOD_OFF) #define CMD_HDR_IPTT_OFF 0 #define CMD_HDR_IPTT_MSK (0xffff << CMD_HDR_IPTT_OFF) #define CMD_HDR_DIF_SGL_LEN_OFF 0 #define CMD_HDR_DIF_SGL_LEN_MSK (0xffff << CMD_HDR_DIF_SGL_LEN_OFF) #define CMD_HDR_DATA_SGL_LEN_OFF 16 #define CMD_HDR_DATA_SGL_LEN_MSK (0xffff << CMD_HDR_DATA_SGL_LEN_OFF) #define CMD_HDR_ADDR_MODE_SEL_OFF 15 #define CMD_HDR_ADDR_MODE_SEL_MSK (1 << CMD_HDR_ADDR_MODE_SEL_OFF) #define CMD_HDR_ABORT_IPTT_OFF 16 #define CMD_HDR_ABORT_IPTT_MSK (0xffff << CMD_HDR_ABORT_IPTT_OFF) #define CMPLT_HDR_CMPLT_OFF 0 #define CMPLT_HDR_CMPLT_MSK (0x3 << CMPLT_HDR_CMPLT_OFF) #define CMPLT_HDR_ERROR_PHASE_OFF 2 #define CMPLT_HDR_ERROR_PHASE_MSK (0xff << CMPLT_HDR_ERROR_PHASE_OFF) #define ERR_PHASE_RESPONSE_FRAME_REV_STAGE_OFF \ 8 #define ERR_PHASE_RESPONSE_FRAME_REV_STAGE_MSK \ (0x1 << ERR_PHASE_RESPONSE_FRAME_REV_STAGE_OFF) #define CMPLT_HDR_RSPNS_XFRD_OFF 10 #define CMPLT_HDR_RSPNS_XFRD_MSK (0x1 << CMPLT_HDR_RSPNS_XFRD_OFF) #define CMPLT_HDR_RSPNS_GOOD_OFF 11 #define CMPLT_HDR_RSPNS_GOOD_MSK (0x1 << CMPLT_HDR_RSPNS_GOOD_OFF) #define CMPLT_HDR_ERX_OFF 12 #define CMPLT_HDR_ERX_MSK (0x1 << CMPLT_HDR_ERX_OFF) #define CMPLT_HDR_ABORT_STAT_OFF 13 #define CMPLT_HDR_ABORT_STAT_MSK (0x7 << CMPLT_HDR_ABORT_STAT_OFF) #define STAT_IO_NOT_VALID 0x1 #define STAT_IO_NO_DEVICE 0x2 #define STAT_IO_COMPLETE 0x3 #define STAT_IO_ABORTED 0x4 #define CMPLT_HDR_IPTT_OFF 0 #define CMPLT_HDR_IPTT_MSK (0xffff << CMPLT_HDR_IPTT_OFF) #define CMPLT_HDR_DEV_ID_OFF 16 #define CMPLT_HDR_DEV_ID_MSK (0xffff << CMPLT_HDR_DEV_ID_OFF) #define SATA_DISK_IN_ERROR_STATUS_OFF 8 #define SATA_DISK_IN_ERROR_STATUS_MSK (0x1 << SATA_DISK_IN_ERROR_STATUS_OFF) #define CMPLT_HDR_SATA_DISK_ERR_OFF 16 #define CMPLT_HDR_SATA_DISK_ERR_MSK (0x1 << CMPLT_HDR_SATA_DISK_ERR_OFF) #define CMPLT_HDR_IO_IN_TARGET_OFF 17 #define CMPLT_HDR_IO_IN_TARGET_MSK (0x1 << CMPLT_HDR_IO_IN_TARGET_OFF) #define FIS_ATA_STATUS_ERR_OFF 18 #define FIS_ATA_STATUS_ERR_MSK (0x1 << FIS_ATA_STATUS_ERR_OFF) #define FIS_TYPE_SDB_OFF 31 #define FIS_TYPE_SDB_MSK (0x1 << FIS_TYPE_SDB_OFF) #define ITCT_HDR_DEV_TYPE_OFF 0 #define ITCT_HDR_DEV_TYPE_MSK (0x3 << ITCT_HDR_DEV_TYPE_OFF) #define ITCT_HDR_VALID_OFF 2 #define ITCT_HDR_VALID_MSK (0x1 << ITCT_HDR_VALID_OFF) #define ITCT_HDR_MCR_OFF 5 #define ITCT_HDR_MCR_MSK (0xf << ITCT_HDR_MCR_OFF) #define ITCT_HDR_VLN_OFF 9 #define ITCT_HDR_VLN_MSK (0xf << ITCT_HDR_VLN_OFF) #define ITCT_HDR_SMP_TIMEOUT_OFF 16 #define ITCT_HDR_AWT_CONTINUE_OFF 25 #define ITCT_HDR_PORT_ID_OFF 28 #define ITCT_HDR_PORT_ID_MSK (0xf << ITCT_HDR_PORT_ID_OFF) #define ITCT_HDR_INLT_OFF 0 #define ITCT_HDR_INLT_MSK (0xffffULL << ITCT_HDR_INLT_OFF) #define ITCT_HDR_RTOLT_OFF 48 #define ITCT_HDR_RTOLT_MSK (0xffffULL << ITCT_HDR_RTOLT_OFF) struct hisi_sas_protect_iu_v3_hw { u32 dw0; u32 lbrtcv; u32 lbrtgv; u32 dw3; u32 dw4; u32 dw5; u32 rsv; };"
1518----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2025-21674/bad/ipsec.c----mlx5e_xfrm_add_state,"static int mlx5e_xfrm_add_state(struct xfrm_state *x, struct netlink_ext_ack *extack) { struct mlx5e_ipsec_sa_entry *sa_entry = NULL; struct net_device *netdev = x->xso.real_dev; struct mlx5e_ipsec *ipsec; struct mlx5e_priv *priv; gfp_t gfp; int err; priv = netdev_priv(netdev); if (!priv->ipsec) return -EOPNOTSUPP; ipsec = priv->ipsec; gfp = (x->xso.flags & XFRM_DEV_OFFLOAD_FLAG_ACQ) ? GFP_ATOMIC : GFP_KERNEL; sa_entry = kzalloc(sizeof(*sa_entry), gfp); if (!sa_entry) return -ENOMEM; sa_entry->x = x; sa_entry->ipsec = ipsec; if (x->xso.flags & XFRM_DEV_OFFLOAD_FLAG_ACQ) goto out; err = mlx5e_xfrm_validate_state(priv->mdev, x, extack); if (err) goto err_xfrm; if (!mlx5_eswitch_block_ipsec(priv->mdev)) { err = -EBUSY; goto err_xfrm; } if (x->props.flags & XFRM_STATE_ESN) mlx5e_ipsec_update_esn_state(sa_entry); mlx5e_ipsec_build_accel_xfrm_attrs(sa_entry, &sa_entry->attrs); err = mlx5_ipsec_create_work(sa_entry); if (err) goto unblock_ipsec; err = mlx5e_ipsec_create_dwork(sa_entry); if (err) goto release_work; err = mlx5_ipsec_create_sa_ctx(sa_entry); if (err) goto release_dwork; err = mlx5e_accel_ipsec_fs_add_rule(sa_entry); if (err) goto err_hw_ctx; if (x->props.mode == XFRM_MODE_TUNNEL && x->xso.type == XFRM_DEV_OFFLOAD_PACKET && !mlx5e_ipsec_fs_tunnel_enabled(sa_entry)) { NL_SET_ERR_MSG_MOD(extack, ""Packet offload tunnel mode is disabled due to encap settings""); err = -EINVAL; goto err_add_rule; } err = xa_insert_bh(&ipsec->sadb, sa_entry->ipsec_obj_id, sa_entry, GFP_KERNEL); if (err) goto err_add_rule; mlx5e_ipsec_set_esn_ops(sa_entry); if (sa_entry->dwork) queue_delayed_work(ipsec->wq, &sa_entry->dwork->dwork, MLX5_IPSEC_RESCHED); if (x->xso.type == XFRM_DEV_OFFLOAD_PACKET && <S2SV_StartVul> x->props.mode == XFRM_MODE_TUNNEL) <S2SV_EndVul> <S2SV_StartVul> xa_set_mark(&ipsec->sadb, sa_entry->ipsec_obj_id, <S2SV_EndVul> <S2SV_StartVul> MLX5E_IPSEC_TUNNEL_SA); <S2SV_EndVul> out: x->xso.offload_handle = (unsigned long)sa_entry; return 0; err_add_rule: mlx5e_accel_ipsec_fs_del_rule(sa_entry); err_hw_ctx: mlx5_ipsec_free_sa_ctx(sa_entry); release_dwork: kfree(sa_entry->dwork); release_work: if (sa_entry->work) kfree(sa_entry->work->data); kfree(sa_entry->work); unblock_ipsec: mlx5_eswitch_unblock_ipsec(priv->mdev); err_xfrm: kfree(sa_entry); NL_SET_ERR_MSG_WEAK_MOD(extack, ""Device failed to offload this state""); return err; }","- x->props.mode == XFRM_MODE_TUNNEL)
- xa_set_mark(&ipsec->sadb, sa_entry->ipsec_obj_id,
- MLX5E_IPSEC_TUNNEL_SA);
+ x->props.mode == XFRM_MODE_TUNNEL) {
+ xa_lock_bh(&ipsec->sadb);
+ __xa_set_mark(&ipsec->sadb, sa_entry->ipsec_obj_id,
+ MLX5E_IPSEC_TUNNEL_SA);
+ xa_unlock_bh(&ipsec->sadb);
+ }","static int mlx5e_xfrm_add_state(struct xfrm_state *x, struct netlink_ext_ack *extack) { struct mlx5e_ipsec_sa_entry *sa_entry = NULL; struct net_device *netdev = x->xso.real_dev; struct mlx5e_ipsec *ipsec; struct mlx5e_priv *priv; gfp_t gfp; int err; priv = netdev_priv(netdev); if (!priv->ipsec) return -EOPNOTSUPP; ipsec = priv->ipsec; gfp = (x->xso.flags & XFRM_DEV_OFFLOAD_FLAG_ACQ) ? GFP_ATOMIC : GFP_KERNEL; sa_entry = kzalloc(sizeof(*sa_entry), gfp); if (!sa_entry) return -ENOMEM; sa_entry->x = x; sa_entry->ipsec = ipsec; if (x->xso.flags & XFRM_DEV_OFFLOAD_FLAG_ACQ) goto out; err = mlx5e_xfrm_validate_state(priv->mdev, x, extack); if (err) goto err_xfrm; if (!mlx5_eswitch_block_ipsec(priv->mdev)) { err = -EBUSY; goto err_xfrm; } if (x->props.flags & XFRM_STATE_ESN) mlx5e_ipsec_update_esn_state(sa_entry); mlx5e_ipsec_build_accel_xfrm_attrs(sa_entry, &sa_entry->attrs); err = mlx5_ipsec_create_work(sa_entry); if (err) goto unblock_ipsec; err = mlx5e_ipsec_create_dwork(sa_entry); if (err) goto release_work; err = mlx5_ipsec_create_sa_ctx(sa_entry); if (err) goto release_dwork; err = mlx5e_accel_ipsec_fs_add_rule(sa_entry); if (err) goto err_hw_ctx; if (x->props.mode == XFRM_MODE_TUNNEL && x->xso.type == XFRM_DEV_OFFLOAD_PACKET && !mlx5e_ipsec_fs_tunnel_enabled(sa_entry)) { NL_SET_ERR_MSG_MOD(extack, ""Packet offload tunnel mode is disabled due to encap settings""); err = -EINVAL; goto err_add_rule; } err = xa_insert_bh(&ipsec->sadb, sa_entry->ipsec_obj_id, sa_entry, GFP_KERNEL); if (err) goto err_add_rule; mlx5e_ipsec_set_esn_ops(sa_entry); if (sa_entry->dwork) queue_delayed_work(ipsec->wq, &sa_entry->dwork->dwork, MLX5_IPSEC_RESCHED); if (x->xso.type == XFRM_DEV_OFFLOAD_PACKET && x->props.mode == XFRM_MODE_TUNNEL) { xa_lock_bh(&ipsec->sadb); __xa_set_mark(&ipsec->sadb, sa_entry->ipsec_obj_id, MLX5E_IPSEC_TUNNEL_SA); xa_unlock_bh(&ipsec->sadb); } out: x->xso.offload_handle = (unsigned long)sa_entry; return 0; err_add_rule: mlx5e_accel_ipsec_fs_del_rule(sa_entry); err_hw_ctx: mlx5_ipsec_free_sa_ctx(sa_entry); release_dwork: kfree(sa_entry->dwork); release_work: if (sa_entry->work) kfree(sa_entry->work->data); kfree(sa_entry->work); unblock_ipsec: mlx5_eswitch_unblock_ipsec(priv->mdev); err_xfrm: kfree(sa_entry); NL_SET_ERR_MSG_WEAK_MOD(extack, ""Device failed to offload this state""); return err; }"
1348----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56669/bad/iommu.c----device_block_translation,"void device_block_translation(struct device *dev) { struct device_domain_info *info = dev_iommu_priv_get(dev); struct intel_iommu *iommu = info->iommu; unsigned long flags; iommu_disable_pci_caps(info); if (!dev_is_real_dma_subdevice(dev)) { if (sm_supported(iommu)) intel_pasid_tear_down_entry(iommu, dev, IOMMU_NO_PASID, false); else domain_context_clear(info); } if (!info->domain) return; spin_lock_irqsave(&info->domain->lock, flags); list_del(&info->link); spin_unlock_irqrestore(&info->domain->lock, flags); <S2SV_StartVul> cache_tag_unassign_domain(info->domain, dev, IOMMU_NO_PASID); <S2SV_EndVul> domain_detach_iommu(info->domain, iommu); info->domain = NULL; }","- cache_tag_unassign_domain(info->domain, dev, IOMMU_NO_PASID);
+ if (info->domain)
+ cache_tag_unassign_domain(info->domain, dev, IOMMU_NO_PASID);","void device_block_translation(struct device *dev) { struct device_domain_info *info = dev_iommu_priv_get(dev); struct intel_iommu *iommu = info->iommu; unsigned long flags; if (info->domain) cache_tag_unassign_domain(info->domain, dev, IOMMU_NO_PASID); iommu_disable_pci_caps(info); if (!dev_is_real_dma_subdevice(dev)) { if (sm_supported(iommu)) intel_pasid_tear_down_entry(iommu, dev, IOMMU_NO_PASID, false); else domain_context_clear(info); } if (!info->domain) return; spin_lock_irqsave(&info->domain->lock, flags); list_del(&info->link); spin_unlock_irqrestore(&info->domain->lock, flags); domain_detach_iommu(info->domain, iommu); info->domain = NULL; }"
183----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46672/bad/cfg80211.c----brcmf_pmksa_v3_op,"brcmf_pmksa_v3_op(struct brcmf_if *ifp, struct cfg80211_pmksa *pmksa, bool alive) { struct brcmf_pmk_op_v3_le *pmk_op; int length = offsetof(struct brcmf_pmk_op_v3_le, pmk); int ret; pmk_op = kzalloc(sizeof(*pmk_op), GFP_KERNEL); if (!pmk_op) return -ENOMEM; pmk_op->version = cpu_to_le16(BRCMF_PMKSA_VER_3); if (!pmksa) { pmk_op->count = cpu_to_le16(0); } else { pmk_op->count = cpu_to_le16(1); length += sizeof(struct brcmf_pmksa_v3); <S2SV_StartVul> memcpy(pmk_op->pmk[0].bssid, pmksa->bssid, ETH_ALEN); <S2SV_EndVul> <S2SV_StartVul> memcpy(pmk_op->pmk[0].pmkid, pmksa->pmkid, WLAN_PMKID_LEN); <S2SV_EndVul> <S2SV_StartVul> pmk_op->pmk[0].pmkid_len = WLAN_PMKID_LEN; <S2SV_EndVul> pmk_op->pmk[0].time_left = cpu_to_le32(alive ? BRCMF_PMKSA_NO_EXPIRY : 0); } pmk_op->length = cpu_to_le16(length); ret = brcmf_fil_iovar_data_set(ifp, ""pmkid_info"", pmk_op, sizeof(*pmk_op)); kfree(pmk_op); return ret; }","- memcpy(pmk_op->pmk[0].bssid, pmksa->bssid, ETH_ALEN);
- memcpy(pmk_op->pmk[0].pmkid, pmksa->pmkid, WLAN_PMKID_LEN);
- pmk_op->pmk[0].pmkid_len = WLAN_PMKID_LEN;
+ if (pmksa->bssid)
+ memcpy(pmk_op->pmk[0].bssid, pmksa->bssid, ETH_ALEN);
+ if (pmksa->pmkid) {
+ memcpy(pmk_op->pmk[0].pmkid, pmksa->pmkid, WLAN_PMKID_LEN);
+ pmk_op->pmk[0].pmkid_len = WLAN_PMKID_LEN;
+ }
+ if (pmksa->ssid && pmksa->ssid_len) {
+ memcpy(pmk_op->pmk[0].ssid.SSID, pmksa->ssid, pmksa->ssid_len);
+ pmk_op->pmk[0].ssid.SSID_len = pmksa->ssid_len;
+ }
+ }","brcmf_pmksa_v3_op(struct brcmf_if *ifp, struct cfg80211_pmksa *pmksa, bool alive) { struct brcmf_pmk_op_v3_le *pmk_op; int length = offsetof(struct brcmf_pmk_op_v3_le, pmk); int ret; pmk_op = kzalloc(sizeof(*pmk_op), GFP_KERNEL); if (!pmk_op) return -ENOMEM; pmk_op->version = cpu_to_le16(BRCMF_PMKSA_VER_3); if (!pmksa) { pmk_op->count = cpu_to_le16(0); } else { pmk_op->count = cpu_to_le16(1); length += sizeof(struct brcmf_pmksa_v3); if (pmksa->bssid) memcpy(pmk_op->pmk[0].bssid, pmksa->bssid, ETH_ALEN); if (pmksa->pmkid) { memcpy(pmk_op->pmk[0].pmkid, pmksa->pmkid, WLAN_PMKID_LEN); pmk_op->pmk[0].pmkid_len = WLAN_PMKID_LEN; } if (pmksa->ssid && pmksa->ssid_len) { memcpy(pmk_op->pmk[0].ssid.SSID, pmksa->ssid, pmksa->ssid_len); pmk_op->pmk[0].ssid.SSID_len = pmksa->ssid_len; } pmk_op->pmk[0].time_left = cpu_to_le32(alive ? BRCMF_PMKSA_NO_EXPIRY : 0); } pmk_op->length = cpu_to_le16(length); ret = brcmf_fil_iovar_data_set(ifp, ""pmkid_info"", pmk_op, sizeof(*pmk_op)); kfree(pmk_op); return ret; }"
712----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49991/bad/kfd_mqd_manager.c----kfd_free_mqd_cp,"void kfd_free_mqd_cp(struct mqd_manager *mm, void *mqd, struct kfd_mem_obj *mqd_mem_obj) { if (mqd_mem_obj->gtt_mem) { <S2SV_StartVul> amdgpu_amdkfd_free_gtt_mem(mm->dev->adev, mqd_mem_obj->gtt_mem); <S2SV_EndVul> kfree(mqd_mem_obj); } else { kfd_gtt_sa_free(mm->dev, mqd_mem_obj); } }","- amdgpu_amdkfd_free_gtt_mem(mm->dev->adev, mqd_mem_obj->gtt_mem);
+ amdgpu_amdkfd_free_gtt_mem(mm->dev->adev, &mqd_mem_obj->gtt_mem);","void kfd_free_mqd_cp(struct mqd_manager *mm, void *mqd, struct kfd_mem_obj *mqd_mem_obj) { if (mqd_mem_obj->gtt_mem) { amdgpu_amdkfd_free_gtt_mem(mm->dev->adev, &mqd_mem_obj->gtt_mem); kfree(mqd_mem_obj); } else { kfd_gtt_sa_free(mm->dev, mqd_mem_obj); } }"
1016----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50255/bad/hci_sync.c----__hci_cmd_sync_status_sk,"int __hci_cmd_sync_status_sk(struct hci_dev *hdev, u16 opcode, u32 plen, const void *param, u8 event, u32 timeout, struct sock *sk) { struct sk_buff *skb; u8 status; skb = __hci_cmd_sync_sk(hdev, opcode, plen, param, event, timeout, sk); if (IS_ERR(skb)) { if (!event) bt_dev_err(hdev, ""Opcode 0x%4.4x failed: %ld"", opcode, PTR_ERR(skb)); return PTR_ERR(skb); } <S2SV_StartVul> if (!skb) <S2SV_EndVul> <S2SV_StartVul> return 0; <S2SV_EndVul> status = skb->data[0]; kfree_skb(skb); return status; }","- if (!skb)
- return 0;
+ if (skb == ERR_PTR(-ENODATA))
+ return 0;","int __hci_cmd_sync_status_sk(struct hci_dev *hdev, u16 opcode, u32 plen, const void *param, u8 event, u32 timeout, struct sock *sk) { struct sk_buff *skb; u8 status; skb = __hci_cmd_sync_sk(hdev, opcode, plen, param, event, timeout, sk); if (skb == ERR_PTR(-ENODATA)) return 0; if (IS_ERR(skb)) { if (!event) bt_dev_err(hdev, ""Opcode 0x%4.4x failed: %ld"", opcode, PTR_ERR(skb)); return PTR_ERR(skb); } status = skb->data[0]; kfree_skb(skb); return status; }"
551----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49870/bad/namei.c----cachefiles_look_up_object,"bool cachefiles_look_up_object(struct cachefiles_object *object) { struct cachefiles_volume *volume = object->volume; struct dentry *dentry, *fan = volume->fanout[(u8)object->cookie->key_hash]; int ret; _enter(""OBJ%x,%s,"", object->debug_id, object->d_name); ret = cachefiles_inject_read_error(); if (ret == 0) dentry = lookup_positive_unlocked(object->d_name, fan, object->d_name_len); else dentry = ERR_PTR(ret); trace_cachefiles_lookup(object, fan, dentry); if (IS_ERR(dentry)) { if (dentry == ERR_PTR(-ENOENT)) goto new_file; if (dentry == ERR_PTR(-EIO)) cachefiles_io_error_obj(object, ""Lookup failed""); return false; } if (!d_is_reg(dentry)) { pr_err(""%pd is not a file\n"", dentry); inode_lock_nested(d_inode(fan), I_MUTEX_PARENT); ret = cachefiles_bury_object(volume->cache, object, fan, dentry, FSCACHE_OBJECT_IS_WEIRD); dput(dentry); if (ret < 0) return false; goto new_file; } <S2SV_StartVul> if (!cachefiles_open_file(object, dentry)) <S2SV_EndVul> return false; _leave("" = t [%lu]"", file_inode(object->file)->i_ino); return true; new_file: fscache_cookie_lookup_negative(object->cookie); return cachefiles_create_file(object); }","- if (!cachefiles_open_file(object, dentry))
+ ret = cachefiles_open_file(object, dentry);
+ dput(dentry);
+ if (!ret)","bool cachefiles_look_up_object(struct cachefiles_object *object) { struct cachefiles_volume *volume = object->volume; struct dentry *dentry, *fan = volume->fanout[(u8)object->cookie->key_hash]; int ret; _enter(""OBJ%x,%s,"", object->debug_id, object->d_name); ret = cachefiles_inject_read_error(); if (ret == 0) dentry = lookup_positive_unlocked(object->d_name, fan, object->d_name_len); else dentry = ERR_PTR(ret); trace_cachefiles_lookup(object, fan, dentry); if (IS_ERR(dentry)) { if (dentry == ERR_PTR(-ENOENT)) goto new_file; if (dentry == ERR_PTR(-EIO)) cachefiles_io_error_obj(object, ""Lookup failed""); return false; } if (!d_is_reg(dentry)) { pr_err(""%pd is not a file\n"", dentry); inode_lock_nested(d_inode(fan), I_MUTEX_PARENT); ret = cachefiles_bury_object(volume->cache, object, fan, dentry, FSCACHE_OBJECT_IS_WEIRD); dput(dentry); if (ret < 0) return false; goto new_file; } ret = cachefiles_open_file(object, dentry); dput(dentry); if (!ret) return false; _leave("" = t [%lu]"", file_inode(object->file)->i_ino); return true; new_file: fscache_cookie_lookup_negative(object->cookie); return cachefiles_create_file(object); }"
1111----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53088/bad/i40e_main.c----i40e_sync_vsi_filters,"int i40e_sync_vsi_filters(struct i40e_vsi *vsi) { struct hlist_head tmp_add_list, tmp_del_list; struct i40e_mac_filter *f; struct i40e_new_mac_filter *new, *add_head = NULL; struct i40e_hw *hw = &vsi->back->hw; bool old_overflow, new_overflow; unsigned int failed_filters = 0; unsigned int vlan_filters = 0; char vsi_name[16] = ""PF""; int filter_list_len = 0; u32 changed_flags = 0; struct hlist_node *h; struct i40e_pf *pf; int num_add = 0; int num_del = 0; int aq_ret = 0; int retval = 0; u16 cmd_flags; int list_size; int bkt; struct i40e_aqc_add_macvlan_element_data *add_list; struct i40e_aqc_remove_macvlan_element_data *del_list; while (test_and_set_bit(__I40E_VSI_SYNCING_FILTERS, vsi->state)) usleep_range(1000, 2000); pf = vsi->back; old_overflow = test_bit(__I40E_VSI_OVERFLOW_PROMISC, vsi->state); if (vsi->netdev) { changed_flags = vsi->current_netdev_flags ^ vsi->netdev->flags; vsi->current_netdev_flags = vsi->netdev->flags; } INIT_HLIST_HEAD(&tmp_add_list); INIT_HLIST_HEAD(&tmp_del_list); if (vsi->type == I40E_VSI_SRIOV) snprintf(vsi_name, sizeof(vsi_name) - 1, ""VF %d"", vsi->vf_id); else if (vsi->type != I40E_VSI_MAIN) snprintf(vsi_name, sizeof(vsi_name) - 1, ""vsi %d"", vsi->seid); if (vsi->flags & I40E_VSI_FLAG_FILTER_CHANGED) { vsi->flags &= ~I40E_VSI_FLAG_FILTER_CHANGED; spin_lock_bh(&vsi->mac_filter_hash_lock); hash_for_each_safe(vsi->mac_filter_hash, bkt, h, f, hlist) { if (f->state == I40E_FILTER_REMOVE) { hash_del(&f->hlist); hlist_add_head(&f->hlist, &tmp_del_list); continue; } if (f->state == I40E_FILTER_NEW) { new = kzalloc(sizeof(*new), GFP_ATOMIC); if (!new) goto err_no_memory_locked; new->f = f; new->state = f->state; hlist_add_head(&new->hlist, &tmp_add_list); } if (f->vlan > 0) vlan_filters++; } if (vsi->type != I40E_VSI_SRIOV) retval = i40e_correct_mac_vlan_filters (vsi, &tmp_add_list, &tmp_del_list, vlan_filters); else if (pf->vf) retval = i40e_correct_vf_mac_vlan_filters (vsi, &tmp_add_list, &tmp_del_list, vlan_filters, pf->vf[vsi->vf_id].trusted); hlist_for_each_entry(new, &tmp_add_list, hlist) netdev_hw_addr_refcnt(new->f, vsi->netdev, 1); if (retval) goto err_no_memory_locked; spin_unlock_bh(&vsi->mac_filter_hash_lock); } if (!hlist_empty(&tmp_del_list)) { filter_list_len = hw->aq.asq_buf_size / sizeof(struct i40e_aqc_remove_macvlan_element_data); list_size = filter_list_len * sizeof(struct i40e_aqc_remove_macvlan_element_data); del_list = kzalloc(list_size, GFP_ATOMIC); if (!del_list) goto err_no_memory; hlist_for_each_entry_safe(f, h, &tmp_del_list, hlist) { cmd_flags = 0; if (is_broadcast_ether_addr(f->macaddr)) { i40e_aqc_broadcast_filter(vsi, vsi_name, f); hlist_del(&f->hlist); kfree(f); continue; } ether_addr_copy(del_list[num_del].mac_addr, f->macaddr); if (f->vlan == I40E_VLAN_ANY) { del_list[num_del].vlan_tag = 0; cmd_flags |= I40E_AQC_MACVLAN_DEL_IGNORE_VLAN; } else { del_list[num_del].vlan_tag = cpu_to_le16((u16)(f->vlan)); } cmd_flags |= I40E_AQC_MACVLAN_DEL_PERFECT_MATCH; del_list[num_del].flags = cmd_flags; num_del++; if (num_del == filter_list_len) { i40e_aqc_del_filters(vsi, vsi_name, del_list, num_del, &retval); memset(del_list, 0, list_size); num_del = 0; } hlist_del(&f->hlist); kfree(f); } if (num_del) { i40e_aqc_del_filters(vsi, vsi_name, del_list, num_del, &retval); } kfree(del_list); del_list = NULL; } if (!hlist_empty(&tmp_add_list)) { filter_list_len = hw->aq.asq_buf_size / sizeof(struct i40e_aqc_add_macvlan_element_data); list_size = filter_list_len * sizeof(struct i40e_aqc_add_macvlan_element_data); add_list = kzalloc(list_size, GFP_ATOMIC); if (!add_list) goto err_no_memory; num_add = 0; hlist_for_each_entry_safe(new, h, &tmp_add_list, hlist) { if (is_broadcast_ether_addr(new->f->macaddr)) { if (i40e_aqc_broadcast_filter(vsi, vsi_name, new->f)) new->state = I40E_FILTER_FAILED; else new->state = I40E_FILTER_ACTIVE; continue; } if (num_add == 0) add_head = new; cmd_flags = 0; ether_addr_copy(add_list[num_add].mac_addr, new->f->macaddr); if (new->f->vlan == I40E_VLAN_ANY) { add_list[num_add].vlan_tag = 0; cmd_flags |= I40E_AQC_MACVLAN_ADD_IGNORE_VLAN; } else { add_list[num_add].vlan_tag = cpu_to_le16((u16)(new->f->vlan)); } add_list[num_add].queue_number = 0; add_list[num_add].match_method = I40E_AQC_MM_ERR_NO_RES; cmd_flags |= I40E_AQC_MACVLAN_ADD_PERFECT_MATCH; add_list[num_add].flags = cpu_to_le16(cmd_flags); num_add++; if (num_add == filter_list_len) { i40e_aqc_add_filters(vsi, vsi_name, add_list, add_head, num_add); memset(add_list, 0, list_size); num_add = 0; } } if (num_add) { i40e_aqc_add_filters(vsi, vsi_name, add_list, add_head, num_add); } spin_lock_bh(&vsi->mac_filter_hash_lock); hlist_for_each_entry_safe(new, h, &tmp_add_list, hlist) { <S2SV_StartVul> if (new->f->state == I40E_FILTER_NEW) <S2SV_EndVul> new->f->state = new->state; hlist_del(&new->hlist); netdev_hw_addr_refcnt(new->f, vsi->netdev, -1); kfree(new); } spin_unlock_bh(&vsi->mac_filter_hash_lock); kfree(add_list); add_list = NULL; } spin_lock_bh(&vsi->mac_filter_hash_lock); vsi->active_filters = 0; hash_for_each(vsi->mac_filter_hash, bkt, f, hlist) { if (f->state == I40E_FILTER_ACTIVE) vsi->active_filters++; else if (f->state == I40E_FILTER_FAILED) failed_filters++; } spin_unlock_bh(&vsi->mac_filter_hash_lock); if (old_overflow && !failed_filters && vsi->active_filters < vsi->promisc_threshold) { dev_info(&pf->pdev->dev, ""filter logjam cleared on %s, leaving overflow promiscuous mode\n"", vsi_name); clear_bit(__I40E_VSI_OVERFLOW_PROMISC, vsi->state); vsi->promisc_threshold = 0; } if (vsi->type == I40E_VSI_SRIOV && pf->vf && !pf->vf[vsi->vf_id].trusted) { clear_bit(__I40E_VSI_OVERFLOW_PROMISC, vsi->state); goto out; } new_overflow = test_bit(__I40E_VSI_OVERFLOW_PROMISC, vsi->state); if (!old_overflow && new_overflow) vsi->promisc_threshold = (vsi->active_filters * 3) / 4; if (changed_flags & IFF_ALLMULTI) { bool cur_multipromisc; cur_multipromisc = !!(vsi->current_netdev_flags & IFF_ALLMULTI); aq_ret = i40e_aq_set_vsi_multicast_promiscuous(&vsi->back->hw, vsi->seid, cur_multipromisc, NULL); if (aq_ret) { retval = i40e_aq_rc_to_posix(aq_ret, hw->aq.asq_last_status); dev_info(&pf->pdev->dev, ""set multi promisc failed on %s, err %pe aq_err %s\n"", vsi_name, ERR_PTR(aq_ret), i40e_aq_str(hw, hw->aq.asq_last_status)); } else { dev_info(&pf->pdev->dev, ""%s allmulti mode.\n"", cur_multipromisc ? ""entering"" : ""leaving""); } } if ((changed_flags & IFF_PROMISC) || old_overflow != new_overflow) { bool cur_promisc; cur_promisc = (!!(vsi->current_netdev_flags & IFF_PROMISC) || new_overflow); aq_ret = i40e_set_promiscuous(pf, cur_promisc); if (aq_ret) { retval = i40e_aq_rc_to_posix(aq_ret, hw->aq.asq_last_status); dev_info(&pf->pdev->dev, ""Setting promiscuous %s failed on %s, err %pe aq_err %s\n"", cur_promisc ? ""on"" : ""off"", vsi_name, ERR_PTR(aq_ret), i40e_aq_str(hw, hw->aq.asq_last_status)); } } out: if (retval) vsi->flags |= I40E_VSI_FLAG_FILTER_CHANGED; clear_bit(__I40E_VSI_SYNCING_FILTERS, vsi->state); return retval; err_no_memory: spin_lock_bh(&vsi->mac_filter_hash_lock); err_no_memory_locked: i40e_undo_del_filter_entries(vsi, &tmp_del_list); i40e_undo_add_filter_entries(vsi, &tmp_add_list); spin_unlock_bh(&vsi->mac_filter_hash_lock); vsi->flags |= I40E_VSI_FLAG_FILTER_CHANGED; clear_bit(__I40E_VSI_SYNCING_FILTERS, vsi->state); return -ENOMEM; }","- if (new->f->state == I40E_FILTER_NEW)
+ f->state = I40E_FILTER_NEW_SYNC;
+ if (new->f->state == I40E_FILTER_NEW ||
+ new->f->state == I40E_FILTER_NEW_SYNC)","int i40e_sync_vsi_filters(struct i40e_vsi *vsi) { struct hlist_head tmp_add_list, tmp_del_list; struct i40e_mac_filter *f; struct i40e_new_mac_filter *new, *add_head = NULL; struct i40e_hw *hw = &vsi->back->hw; bool old_overflow, new_overflow; unsigned int failed_filters = 0; unsigned int vlan_filters = 0; char vsi_name[16] = ""PF""; int filter_list_len = 0; u32 changed_flags = 0; struct hlist_node *h; struct i40e_pf *pf; int num_add = 0; int num_del = 0; int aq_ret = 0; int retval = 0; u16 cmd_flags; int list_size; int bkt; struct i40e_aqc_add_macvlan_element_data *add_list; struct i40e_aqc_remove_macvlan_element_data *del_list; while (test_and_set_bit(__I40E_VSI_SYNCING_FILTERS, vsi->state)) usleep_range(1000, 2000); pf = vsi->back; old_overflow = test_bit(__I40E_VSI_OVERFLOW_PROMISC, vsi->state); if (vsi->netdev) { changed_flags = vsi->current_netdev_flags ^ vsi->netdev->flags; vsi->current_netdev_flags = vsi->netdev->flags; } INIT_HLIST_HEAD(&tmp_add_list); INIT_HLIST_HEAD(&tmp_del_list); if (vsi->type == I40E_VSI_SRIOV) snprintf(vsi_name, sizeof(vsi_name) - 1, ""VF %d"", vsi->vf_id); else if (vsi->type != I40E_VSI_MAIN) snprintf(vsi_name, sizeof(vsi_name) - 1, ""vsi %d"", vsi->seid); if (vsi->flags & I40E_VSI_FLAG_FILTER_CHANGED) { vsi->flags &= ~I40E_VSI_FLAG_FILTER_CHANGED; spin_lock_bh(&vsi->mac_filter_hash_lock); hash_for_each_safe(vsi->mac_filter_hash, bkt, h, f, hlist) { if (f->state == I40E_FILTER_REMOVE) { hash_del(&f->hlist); hlist_add_head(&f->hlist, &tmp_del_list); continue; } if (f->state == I40E_FILTER_NEW) { new = kzalloc(sizeof(*new), GFP_ATOMIC); if (!new) goto err_no_memory_locked; new->f = f; new->state = f->state; hlist_add_head(&new->hlist, &tmp_add_list); f->state = I40E_FILTER_NEW_SYNC; } if (f->vlan > 0) vlan_filters++; } if (vsi->type != I40E_VSI_SRIOV) retval = i40e_correct_mac_vlan_filters (vsi, &tmp_add_list, &tmp_del_list, vlan_filters); else if (pf->vf) retval = i40e_correct_vf_mac_vlan_filters (vsi, &tmp_add_list, &tmp_del_list, vlan_filters, pf->vf[vsi->vf_id].trusted); hlist_for_each_entry(new, &tmp_add_list, hlist) netdev_hw_addr_refcnt(new->f, vsi->netdev, 1); if (retval) goto err_no_memory_locked; spin_unlock_bh(&vsi->mac_filter_hash_lock); } if (!hlist_empty(&tmp_del_list)) { filter_list_len = hw->aq.asq_buf_size / sizeof(struct i40e_aqc_remove_macvlan_element_data); list_size = filter_list_len * sizeof(struct i40e_aqc_remove_macvlan_element_data); del_list = kzalloc(list_size, GFP_ATOMIC); if (!del_list) goto err_no_memory; hlist_for_each_entry_safe(f, h, &tmp_del_list, hlist) { cmd_flags = 0; if (is_broadcast_ether_addr(f->macaddr)) { i40e_aqc_broadcast_filter(vsi, vsi_name, f); hlist_del(&f->hlist); kfree(f); continue; } ether_addr_copy(del_list[num_del].mac_addr, f->macaddr); if (f->vlan == I40E_VLAN_ANY) { del_list[num_del].vlan_tag = 0; cmd_flags |= I40E_AQC_MACVLAN_DEL_IGNORE_VLAN; } else { del_list[num_del].vlan_tag = cpu_to_le16((u16)(f->vlan)); } cmd_flags |= I40E_AQC_MACVLAN_DEL_PERFECT_MATCH; del_list[num_del].flags = cmd_flags; num_del++; if (num_del == filter_list_len) { i40e_aqc_del_filters(vsi, vsi_name, del_list, num_del, &retval); memset(del_list, 0, list_size); num_del = 0; } hlist_del(&f->hlist); kfree(f); } if (num_del) { i40e_aqc_del_filters(vsi, vsi_name, del_list, num_del, &retval); } kfree(del_list); del_list = NULL; } if (!hlist_empty(&tmp_add_list)) { filter_list_len = hw->aq.asq_buf_size / sizeof(struct i40e_aqc_add_macvlan_element_data); list_size = filter_list_len * sizeof(struct i40e_aqc_add_macvlan_element_data); add_list = kzalloc(list_size, GFP_ATOMIC); if (!add_list) goto err_no_memory; num_add = 0; hlist_for_each_entry_safe(new, h, &tmp_add_list, hlist) { if (is_broadcast_ether_addr(new->f->macaddr)) { if (i40e_aqc_broadcast_filter(vsi, vsi_name, new->f)) new->state = I40E_FILTER_FAILED; else new->state = I40E_FILTER_ACTIVE; continue; } if (num_add == 0) add_head = new; cmd_flags = 0; ether_addr_copy(add_list[num_add].mac_addr, new->f->macaddr); if (new->f->vlan == I40E_VLAN_ANY) { add_list[num_add].vlan_tag = 0; cmd_flags |= I40E_AQC_MACVLAN_ADD_IGNORE_VLAN; } else { add_list[num_add].vlan_tag = cpu_to_le16((u16)(new->f->vlan)); } add_list[num_add].queue_number = 0; add_list[num_add].match_method = I40E_AQC_MM_ERR_NO_RES; cmd_flags |= I40E_AQC_MACVLAN_ADD_PERFECT_MATCH; add_list[num_add].flags = cpu_to_le16(cmd_flags); num_add++; if (num_add == filter_list_len) { i40e_aqc_add_filters(vsi, vsi_name, add_list, add_head, num_add); memset(add_list, 0, list_size); num_add = 0; } } if (num_add) { i40e_aqc_add_filters(vsi, vsi_name, add_list, add_head, num_add); } spin_lock_bh(&vsi->mac_filter_hash_lock); hlist_for_each_entry_safe(new, h, &tmp_add_list, hlist) { if (new->f->state == I40E_FILTER_NEW || new->f->state == I40E_FILTER_NEW_SYNC) new->f->state = new->state; hlist_del(&new->hlist); netdev_hw_addr_refcnt(new->f, vsi->netdev, -1); kfree(new); } spin_unlock_bh(&vsi->mac_filter_hash_lock); kfree(add_list); add_list = NULL; } spin_lock_bh(&vsi->mac_filter_hash_lock); vsi->active_filters = 0; hash_for_each(vsi->mac_filter_hash, bkt, f, hlist) { if (f->state == I40E_FILTER_ACTIVE) vsi->active_filters++; else if (f->state == I40E_FILTER_FAILED) failed_filters++; } spin_unlock_bh(&vsi->mac_filter_hash_lock); if (old_overflow && !failed_filters && vsi->active_filters < vsi->promisc_threshold) { dev_info(&pf->pdev->dev, ""filter logjam cleared on %s, leaving overflow promiscuous mode\n"", vsi_name); clear_bit(__I40E_VSI_OVERFLOW_PROMISC, vsi->state); vsi->promisc_threshold = 0; } if (vsi->type == I40E_VSI_SRIOV && pf->vf && !pf->vf[vsi->vf_id].trusted) { clear_bit(__I40E_VSI_OVERFLOW_PROMISC, vsi->state); goto out; } new_overflow = test_bit(__I40E_VSI_OVERFLOW_PROMISC, vsi->state); if (!old_overflow && new_overflow) vsi->promisc_threshold = (vsi->active_filters * 3) / 4; if (changed_flags & IFF_ALLMULTI) { bool cur_multipromisc; cur_multipromisc = !!(vsi->current_netdev_flags & IFF_ALLMULTI); aq_ret = i40e_aq_set_vsi_multicast_promiscuous(&vsi->back->hw, vsi->seid, cur_multipromisc, NULL); if (aq_ret) { retval = i40e_aq_rc_to_posix(aq_ret, hw->aq.asq_last_status); dev_info(&pf->pdev->dev, ""set multi promisc failed on %s, err %pe aq_err %s\n"", vsi_name, ERR_PTR(aq_ret), i40e_aq_str(hw, hw->aq.asq_last_status)); } else { dev_info(&pf->pdev->dev, ""%s allmulti mode.\n"", cur_multipromisc ? ""entering"" : ""leaving""); } } if ((changed_flags & IFF_PROMISC) || old_overflow != new_overflow) { bool cur_promisc; cur_promisc = (!!(vsi->current_netdev_flags & IFF_PROMISC) || new_overflow); aq_ret = i40e_set_promiscuous(pf, cur_promisc); if (aq_ret) { retval = i40e_aq_rc_to_posix(aq_ret, hw->aq.asq_last_status); dev_info(&pf->pdev->dev, ""Setting promiscuous %s failed on %s, err %pe aq_err %s\n"", cur_promisc ? ""on"" : ""off"", vsi_name, ERR_PTR(aq_ret), i40e_aq_str(hw, hw->aq.asq_last_status)); } } out: if (retval) vsi->flags |= I40E_VSI_FLAG_FILTER_CHANGED; clear_bit(__I40E_VSI_SYNCING_FILTERS, vsi->state); return retval; err_no_memory: spin_lock_bh(&vsi->mac_filter_hash_lock); err_no_memory_locked: i40e_undo_del_filter_entries(vsi, &tmp_del_list); i40e_undo_add_filter_entries(vsi, &tmp_add_list); spin_unlock_bh(&vsi->mac_filter_hash_lock); vsi->flags |= I40E_VSI_FLAG_FILTER_CHANGED; clear_bit(__I40E_VSI_SYNCING_FILTERS, vsi->state); return -ENOMEM; }"
9----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-43891/bad/trace_events.c----event_filter_write,"event_filter_write(struct file *filp, const char __user *ubuf, size_t cnt, loff_t *ppos) { struct trace_event_file *file; char *buf; int err = -ENODEV; if (cnt >= PAGE_SIZE) return -EINVAL; buf = memdup_user_nul(ubuf, cnt); if (IS_ERR(buf)) return PTR_ERR(buf); mutex_lock(&event_mutex); <S2SV_StartVul> file = event_file_data(filp); <S2SV_EndVul> <S2SV_StartVul> if (file) <S2SV_EndVul> <S2SV_StartVul> err = apply_event_filter(file, buf); <S2SV_EndVul> mutex_unlock(&event_mutex); kfree(buf); if (err < 0) return err; *ppos += cnt; return cnt; }","- file = event_file_data(filp);
- if (file)
- err = apply_event_filter(file, buf);
+ file = event_file_file(filp);
+ if (file) {
+ if (file->flags & EVENT_FILE_FL_FREED)
+ err = -ENODEV;
+ else
+ err = apply_event_filter(file, buf);
+ }","event_filter_write(struct file *filp, const char __user *ubuf, size_t cnt, loff_t *ppos) { struct trace_event_file *file; char *buf; int err = -ENODEV; if (cnt >= PAGE_SIZE) return -EINVAL; buf = memdup_user_nul(ubuf, cnt); if (IS_ERR(buf)) return PTR_ERR(buf); mutex_lock(&event_mutex); file = event_file_file(filp); if (file) { if (file->flags & EVENT_FILE_FL_FREED) err = -ENODEV; else err = apply_event_filter(file, buf); } mutex_unlock(&event_mutex); kfree(buf); if (err < 0) return err; *ppos += cnt; return cnt; }"
1191----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53185/bad/smb2pdu.c----SMB2_negotiate,"SMB2_negotiate(const unsigned int xid, struct cifs_ses *ses, struct TCP_Server_Info *server) { struct smb_rqst rqst; struct smb2_negotiate_req *req; struct smb2_negotiate_rsp *rsp; struct kvec iov[1]; struct kvec rsp_iov; int rc; int resp_buftype; int blob_offset, blob_length; char *security_blob; int flags = CIFS_NEG_OP; unsigned int total_len; cifs_dbg(FYI, ""Negotiate protocol\n""); if (!server) { WARN(1, ""%s: server is NULL!\n"", __func__); return -EIO; } rc = smb2_plain_req_init(SMB2_NEGOTIATE, NULL, server, (void **) &req, &total_len); if (rc) return rc; req->hdr.SessionId = 0; memset(server->preauth_sha_hash, 0, SMB2_PREAUTH_HASH_SIZE); memset(ses->preauth_sha_hash, 0, SMB2_PREAUTH_HASH_SIZE); if (strcmp(server->vals->version_string, SMB3ANY_VERSION_STRING) == 0) { req->Dialects[0] = cpu_to_le16(SMB30_PROT_ID); req->Dialects[1] = cpu_to_le16(SMB302_PROT_ID); req->Dialects[2] = cpu_to_le16(SMB311_PROT_ID); req->DialectCount = cpu_to_le16(3); total_len += 6; } else if (strcmp(server->vals->version_string, SMBDEFAULT_VERSION_STRING) == 0) { req->Dialects[0] = cpu_to_le16(SMB21_PROT_ID); req->Dialects[1] = cpu_to_le16(SMB30_PROT_ID); req->Dialects[2] = cpu_to_le16(SMB302_PROT_ID); req->Dialects[3] = cpu_to_le16(SMB311_PROT_ID); req->DialectCount = cpu_to_le16(4); total_len += 8; } else { req->Dialects[0] = cpu_to_le16(server->vals->protocol_id); req->DialectCount = cpu_to_le16(1); total_len += 2; } if (ses->sign) req->SecurityMode = cpu_to_le16(SMB2_NEGOTIATE_SIGNING_REQUIRED); else if (global_secflags & CIFSSEC_MAY_SIGN) req->SecurityMode = cpu_to_le16(SMB2_NEGOTIATE_SIGNING_ENABLED); else req->SecurityMode = 0; req->Capabilities = cpu_to_le32(server->vals->req_capabilities); if (ses->chan_max > 1) req->Capabilities |= cpu_to_le32(SMB2_GLOBAL_CAP_MULTI_CHANNEL); if (server->vals->protocol_id == SMB20_PROT_ID) memset(req->ClientGUID, 0, SMB2_CLIENT_GUID_SIZE); else { memcpy(req->ClientGUID, server->client_guid, SMB2_CLIENT_GUID_SIZE); if ((server->vals->protocol_id == SMB311_PROT_ID) || (strcmp(server->vals->version_string, SMB3ANY_VERSION_STRING) == 0) || (strcmp(server->vals->version_string, SMBDEFAULT_VERSION_STRING) == 0)) assemble_neg_contexts(req, server, &total_len); } iov[0].iov_base = (char *)req; iov[0].iov_len = total_len; memset(&rqst, 0, sizeof(struct smb_rqst)); rqst.rq_iov = iov; rqst.rq_nvec = 1; rc = cifs_send_recv(xid, ses, server, &rqst, &resp_buftype, flags, &rsp_iov); cifs_small_buf_release(req); rsp = (struct smb2_negotiate_rsp *)rsp_iov.iov_base; if (rc == -EOPNOTSUPP) { cifs_server_dbg(VFS, ""Dialect not supported by server. Consider specifying vers=1.0 or vers=2.0 on mount for accessing older servers\n""); goto neg_exit; } else if (rc != 0) goto neg_exit; rc = -EIO; if (strcmp(server->vals->version_string, SMB3ANY_VERSION_STRING) == 0) { if (rsp->DialectRevision == cpu_to_le16(SMB20_PROT_ID)) { cifs_server_dbg(VFS, ""SMB2 dialect returned but not requested\n""); goto neg_exit; } else if (rsp->DialectRevision == cpu_to_le16(SMB21_PROT_ID)) { cifs_server_dbg(VFS, ""SMB2.1 dialect returned but not requested\n""); goto neg_exit; } else if (rsp->DialectRevision == cpu_to_le16(SMB311_PROT_ID)) { server->ops = &smb311_operations; server->vals = &smb311_values; } } else if (strcmp(server->vals->version_string, SMBDEFAULT_VERSION_STRING) == 0) { if (rsp->DialectRevision == cpu_to_le16(SMB20_PROT_ID)) { cifs_server_dbg(VFS, ""SMB2 dialect returned but not requested\n""); goto neg_exit; } else if (rsp->DialectRevision == cpu_to_le16(SMB21_PROT_ID)) { server->ops = &smb21_operations; server->vals = &smb21_values; } else if (rsp->DialectRevision == cpu_to_le16(SMB311_PROT_ID)) { server->ops = &smb311_operations; server->vals = &smb311_values; } } else if (le16_to_cpu(rsp->DialectRevision) != server->vals->protocol_id) { cifs_server_dbg(VFS, ""Invalid 0x%x dialect returned: not requested\n"", le16_to_cpu(rsp->DialectRevision)); goto neg_exit; } cifs_dbg(FYI, ""mode 0x%x\n"", rsp->SecurityMode); if (rsp->DialectRevision == cpu_to_le16(SMB20_PROT_ID)) cifs_dbg(FYI, ""negotiated smb2.0 dialect\n""); else if (rsp->DialectRevision == cpu_to_le16(SMB21_PROT_ID)) cifs_dbg(FYI, ""negotiated smb2.1 dialect\n""); else if (rsp->DialectRevision == cpu_to_le16(SMB30_PROT_ID)) cifs_dbg(FYI, ""negotiated smb3.0 dialect\n""); else if (rsp->DialectRevision == cpu_to_le16(SMB302_PROT_ID)) cifs_dbg(FYI, ""negotiated smb3.02 dialect\n""); else if (rsp->DialectRevision == cpu_to_le16(SMB311_PROT_ID)) cifs_dbg(FYI, ""negotiated smb3.1.1 dialect\n""); else { cifs_server_dbg(VFS, ""Invalid dialect returned by server 0x%x\n"", le16_to_cpu(rsp->DialectRevision)); goto neg_exit; } rc = 0; server->dialect = le16_to_cpu(rsp->DialectRevision); memcpy(server->preauth_sha_hash, ses->preauth_sha_hash, SMB2_PREAUTH_HASH_SIZE); server->negflavor = CIFS_NEGFLAVOR_EXTENDED; server->maxBuf = min_t(unsigned int, le32_to_cpu(rsp->MaxTransactSize), SMB2_MAX_BUFFER_SIZE); server->max_read = le32_to_cpu(rsp->MaxReadSize); server->max_write = le32_to_cpu(rsp->MaxWriteSize); server->sec_mode = le16_to_cpu(rsp->SecurityMode); if ((server->sec_mode & SMB2_SEC_MODE_FLAGS_ALL) != server->sec_mode) cifs_dbg(FYI, ""Server returned unexpected security mode 0x%x\n"", server->sec_mode); server->capabilities = le32_to_cpu(rsp->Capabilities); server->capabilities |= SMB2_NT_FIND | SMB2_LARGE_FILES; <S2SV_StartVul> if (server->dialect == SMB30_PROT_ID && (server->capabilities & SMB2_GLOBAL_CAP_ENCRYPTION)) <S2SV_EndVul> server->cipher_type = SMB2_ENCRYPTION_AES128_CCM; security_blob = smb2_get_data_area_len(&blob_offset, &blob_length, (struct smb2_hdr *)rsp); if (blob_length == 0) { cifs_dbg(FYI, ""missing security blob on negprot\n""); server->sec_ntlmssp = true; } rc = cifs_enable_signing(server, ses->sign); if (rc) goto neg_exit; if (blob_length) { rc = decode_negTokenInit(security_blob, blob_length, server); if (rc == 1) rc = 0; else if (rc == 0) rc = -EIO; } if (rsp->DialectRevision == cpu_to_le16(SMB311_PROT_ID)) { if (rsp->NegotiateContextCount) rc = smb311_decode_neg_context(rsp, server, rsp_iov.iov_len); else cifs_server_dbg(VFS, ""Missing expected negotiate contexts\n""); } if (server->cipher_type && !rc) { rc = smb3_crypto_aead_allocate(server); if (rc) cifs_server_dbg(VFS, ""%s: crypto alloc failed, rc=%d\n"", __func__, rc); } neg_exit: free_rsp_buf(resp_buftype, rsp); return rc; }","- if (server->dialect == SMB30_PROT_ID && (server->capabilities & SMB2_GLOBAL_CAP_ENCRYPTION))
+ if ((server->dialect == SMB30_PROT_ID ||
+ server->dialect == SMB302_PROT_ID) &&
+ (server->capabilities & SMB2_GLOBAL_CAP_ENCRYPTION))","SMB2_negotiate(const unsigned int xid, struct cifs_ses *ses, struct TCP_Server_Info *server) { struct smb_rqst rqst; struct smb2_negotiate_req *req; struct smb2_negotiate_rsp *rsp; struct kvec iov[1]; struct kvec rsp_iov; int rc; int resp_buftype; int blob_offset, blob_length; char *security_blob; int flags = CIFS_NEG_OP; unsigned int total_len; cifs_dbg(FYI, ""Negotiate protocol\n""); if (!server) { WARN(1, ""%s: server is NULL!\n"", __func__); return -EIO; } rc = smb2_plain_req_init(SMB2_NEGOTIATE, NULL, server, (void **) &req, &total_len); if (rc) return rc; req->hdr.SessionId = 0; memset(server->preauth_sha_hash, 0, SMB2_PREAUTH_HASH_SIZE); memset(ses->preauth_sha_hash, 0, SMB2_PREAUTH_HASH_SIZE); if (strcmp(server->vals->version_string, SMB3ANY_VERSION_STRING) == 0) { req->Dialects[0] = cpu_to_le16(SMB30_PROT_ID); req->Dialects[1] = cpu_to_le16(SMB302_PROT_ID); req->Dialects[2] = cpu_to_le16(SMB311_PROT_ID); req->DialectCount = cpu_to_le16(3); total_len += 6; } else if (strcmp(server->vals->version_string, SMBDEFAULT_VERSION_STRING) == 0) { req->Dialects[0] = cpu_to_le16(SMB21_PROT_ID); req->Dialects[1] = cpu_to_le16(SMB30_PROT_ID); req->Dialects[2] = cpu_to_le16(SMB302_PROT_ID); req->Dialects[3] = cpu_to_le16(SMB311_PROT_ID); req->DialectCount = cpu_to_le16(4); total_len += 8; } else { req->Dialects[0] = cpu_to_le16(server->vals->protocol_id); req->DialectCount = cpu_to_le16(1); total_len += 2; } if (ses->sign) req->SecurityMode = cpu_to_le16(SMB2_NEGOTIATE_SIGNING_REQUIRED); else if (global_secflags & CIFSSEC_MAY_SIGN) req->SecurityMode = cpu_to_le16(SMB2_NEGOTIATE_SIGNING_ENABLED); else req->SecurityMode = 0; req->Capabilities = cpu_to_le32(server->vals->req_capabilities); if (ses->chan_max > 1) req->Capabilities |= cpu_to_le32(SMB2_GLOBAL_CAP_MULTI_CHANNEL); if (server->vals->protocol_id == SMB20_PROT_ID) memset(req->ClientGUID, 0, SMB2_CLIENT_GUID_SIZE); else { memcpy(req->ClientGUID, server->client_guid, SMB2_CLIENT_GUID_SIZE); if ((server->vals->protocol_id == SMB311_PROT_ID) || (strcmp(server->vals->version_string, SMB3ANY_VERSION_STRING) == 0) || (strcmp(server->vals->version_string, SMBDEFAULT_VERSION_STRING) == 0)) assemble_neg_contexts(req, server, &total_len); } iov[0].iov_base = (char *)req; iov[0].iov_len = total_len; memset(&rqst, 0, sizeof(struct smb_rqst)); rqst.rq_iov = iov; rqst.rq_nvec = 1; rc = cifs_send_recv(xid, ses, server, &rqst, &resp_buftype, flags, &rsp_iov); cifs_small_buf_release(req); rsp = (struct smb2_negotiate_rsp *)rsp_iov.iov_base; if (rc == -EOPNOTSUPP) { cifs_server_dbg(VFS, ""Dialect not supported by server. Consider specifying vers=1.0 or vers=2.0 on mount for accessing older servers\n""); goto neg_exit; } else if (rc != 0) goto neg_exit; rc = -EIO; if (strcmp(server->vals->version_string, SMB3ANY_VERSION_STRING) == 0) { if (rsp->DialectRevision == cpu_to_le16(SMB20_PROT_ID)) { cifs_server_dbg(VFS, ""SMB2 dialect returned but not requested\n""); goto neg_exit; } else if (rsp->DialectRevision == cpu_to_le16(SMB21_PROT_ID)) { cifs_server_dbg(VFS, ""SMB2.1 dialect returned but not requested\n""); goto neg_exit; } else if (rsp->DialectRevision == cpu_to_le16(SMB311_PROT_ID)) { server->ops = &smb311_operations; server->vals = &smb311_values; } } else if (strcmp(server->vals->version_string, SMBDEFAULT_VERSION_STRING) == 0) { if (rsp->DialectRevision == cpu_to_le16(SMB20_PROT_ID)) { cifs_server_dbg(VFS, ""SMB2 dialect returned but not requested\n""); goto neg_exit; } else if (rsp->DialectRevision == cpu_to_le16(SMB21_PROT_ID)) { server->ops = &smb21_operations; server->vals = &smb21_values; } else if (rsp->DialectRevision == cpu_to_le16(SMB311_PROT_ID)) { server->ops = &smb311_operations; server->vals = &smb311_values; } } else if (le16_to_cpu(rsp->DialectRevision) != server->vals->protocol_id) { cifs_server_dbg(VFS, ""Invalid 0x%x dialect returned: not requested\n"", le16_to_cpu(rsp->DialectRevision)); goto neg_exit; } cifs_dbg(FYI, ""mode 0x%x\n"", rsp->SecurityMode); if (rsp->DialectRevision == cpu_to_le16(SMB20_PROT_ID)) cifs_dbg(FYI, ""negotiated smb2.0 dialect\n""); else if (rsp->DialectRevision == cpu_to_le16(SMB21_PROT_ID)) cifs_dbg(FYI, ""negotiated smb2.1 dialect\n""); else if (rsp->DialectRevision == cpu_to_le16(SMB30_PROT_ID)) cifs_dbg(FYI, ""negotiated smb3.0 dialect\n""); else if (rsp->DialectRevision == cpu_to_le16(SMB302_PROT_ID)) cifs_dbg(FYI, ""negotiated smb3.02 dialect\n""); else if (rsp->DialectRevision == cpu_to_le16(SMB311_PROT_ID)) cifs_dbg(FYI, ""negotiated smb3.1.1 dialect\n""); else { cifs_server_dbg(VFS, ""Invalid dialect returned by server 0x%x\n"", le16_to_cpu(rsp->DialectRevision)); goto neg_exit; } rc = 0; server->dialect = le16_to_cpu(rsp->DialectRevision); memcpy(server->preauth_sha_hash, ses->preauth_sha_hash, SMB2_PREAUTH_HASH_SIZE); server->negflavor = CIFS_NEGFLAVOR_EXTENDED; server->maxBuf = min_t(unsigned int, le32_to_cpu(rsp->MaxTransactSize), SMB2_MAX_BUFFER_SIZE); server->max_read = le32_to_cpu(rsp->MaxReadSize); server->max_write = le32_to_cpu(rsp->MaxWriteSize); server->sec_mode = le16_to_cpu(rsp->SecurityMode); if ((server->sec_mode & SMB2_SEC_MODE_FLAGS_ALL) != server->sec_mode) cifs_dbg(FYI, ""Server returned unexpected security mode 0x%x\n"", server->sec_mode); server->capabilities = le32_to_cpu(rsp->Capabilities); server->capabilities |= SMB2_NT_FIND | SMB2_LARGE_FILES; if ((server->dialect == SMB30_PROT_ID || server->dialect == SMB302_PROT_ID) && (server->capabilities & SMB2_GLOBAL_CAP_ENCRYPTION)) server->cipher_type = SMB2_ENCRYPTION_AES128_CCM; security_blob = smb2_get_data_area_len(&blob_offset, &blob_length, (struct smb2_hdr *)rsp); if (blob_length == 0) { cifs_dbg(FYI, ""missing security blob on negprot\n""); server->sec_ntlmssp = true; } rc = cifs_enable_signing(server, ses->sign); if (rc) goto neg_exit; if (blob_length) { rc = decode_negTokenInit(security_blob, blob_length, server); if (rc == 1) rc = 0; else if (rc == 0) rc = -EIO; } if (rsp->DialectRevision == cpu_to_le16(SMB311_PROT_ID)) { if (rsp->NegotiateContextCount) rc = smb311_decode_neg_context(rsp, server, rsp_iov.iov_len); else cifs_server_dbg(VFS, ""Missing expected negotiate contexts\n""); } if (server->cipher_type && !rc) { rc = smb3_crypto_aead_allocate(server); if (rc) cifs_server_dbg(VFS, ""%s: crypto alloc failed, rc=%d\n"", __func__, rc); } neg_exit: free_rsp_buf(resp_buftype, rsp); return rc; }"
791----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50067/bad/trace_uprobe.c----__uprobe_trace_func,"static void __uprobe_trace_func(struct trace_uprobe *tu, unsigned long func, struct pt_regs *regs, struct uprobe_cpu_buffer *ucb, struct trace_event_file *trace_file) { struct uprobe_trace_entry_head *entry; struct trace_event_buffer fbuffer; void *data; int size, esize; struct trace_event_call *call = trace_probe_event_call(&tu->tp); WARN_ON(call != trace_file->event_call); <S2SV_StartVul> if (WARN_ON_ONCE(ucb->dsize > PAGE_SIZE)) <S2SV_EndVul> <S2SV_StartVul> return; <S2SV_EndVul> if (trace_trigger_soft_disabled(trace_file)) <S2SV_StartVul> return; <S2SV_EndVul> esize = SIZEOF_TRACE_ENTRY(is_ret_probe(tu)); size = esize + ucb->dsize; entry = trace_event_buffer_reserve(&fbuffer, trace_file, size); if (!entry) return; if (is_ret_probe(tu)) { entry->vaddr[0] = func; entry->vaddr[1] = instruction_pointer(regs); data = DATAOF_TRACE_ENTRY(entry, true); } else { entry->vaddr[0] = instruction_pointer(regs); data = DATAOF_TRACE_ENTRY(entry, false); } memcpy(data, ucb->buf, ucb->dsize); trace_event_buffer_commit(&fbuffer); }","- if (WARN_ON_ONCE(ucb->dsize > PAGE_SIZE))
- return;
- return;","static void __uprobe_trace_func(struct trace_uprobe *tu, unsigned long func, struct pt_regs *regs, struct uprobe_cpu_buffer *ucb, struct trace_event_file *trace_file) { struct uprobe_trace_entry_head *entry; struct trace_event_buffer fbuffer; void *data; int size, esize; struct trace_event_call *call = trace_probe_event_call(&tu->tp); WARN_ON(call != trace_file->event_call); if (trace_trigger_soft_disabled(trace_file)) return; esize = SIZEOF_TRACE_ENTRY(is_ret_probe(tu)); size = esize + ucb->dsize; entry = trace_event_buffer_reserve(&fbuffer, trace_file, size); if (!entry) return; if (is_ret_probe(tu)) { entry->vaddr[0] = func; entry->vaddr[1] = instruction_pointer(regs); data = DATAOF_TRACE_ENTRY(entry, true); } else { entry->vaddr[0] = instruction_pointer(regs); data = DATAOF_TRACE_ENTRY(entry, false); } memcpy(data, ucb->buf, ucb->dsize); trace_event_buffer_commit(&fbuffer); }"
956----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50201/bad/radeon_encoders.c----radeon_encoder_clones,"static uint32_t radeon_encoder_clones(struct drm_encoder *encoder) { struct drm_device *dev = encoder->dev; struct radeon_device *rdev = dev->dev_private; struct radeon_encoder *radeon_encoder = to_radeon_encoder(encoder); struct drm_encoder *clone_encoder; <S2SV_StartVul> uint32_t index_mask = 0; <S2SV_EndVul> int count; if (rdev->family >= CHIP_R600) return index_mask; if (radeon_encoder->devices & ATOM_DEVICE_LCD_SUPPORT) return index_mask; if (radeon_encoder->devices & ATOM_DEVICE_DFP2_SUPPORT) return index_mask; count = -1; list_for_each_entry(clone_encoder, &dev->mode_config.encoder_list, head) { struct radeon_encoder *radeon_clone = to_radeon_encoder(clone_encoder); count++; if (clone_encoder == encoder) continue; if (radeon_clone->devices & (ATOM_DEVICE_LCD_SUPPORT)) continue; if (radeon_clone->devices & ATOM_DEVICE_DFP2_SUPPORT) continue; else index_mask |= (1 << count); } return index_mask; }","- uint32_t index_mask = 0;
+ uint32_t index_mask = drm_encoder_mask(encoder);","static uint32_t radeon_encoder_clones(struct drm_encoder *encoder) { struct drm_device *dev = encoder->dev; struct radeon_device *rdev = dev->dev_private; struct radeon_encoder *radeon_encoder = to_radeon_encoder(encoder); struct drm_encoder *clone_encoder; uint32_t index_mask = drm_encoder_mask(encoder); int count; if (rdev->family >= CHIP_R600) return index_mask; if (radeon_encoder->devices & ATOM_DEVICE_LCD_SUPPORT) return index_mask; if (radeon_encoder->devices & ATOM_DEVICE_DFP2_SUPPORT) return index_mask; count = -1; list_for_each_entry(clone_encoder, &dev->mode_config.encoder_list, head) { struct radeon_encoder *radeon_clone = to_radeon_encoder(clone_encoder); count++; if (clone_encoder == encoder) continue; if (radeon_clone->devices & (ATOM_DEVICE_LCD_SUPPORT)) continue; if (radeon_clone->devices & ATOM_DEVICE_DFP2_SUPPORT) continue; else index_mask |= (1 << count); } return index_mask; }"
1528----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2025-21684/bad/gpio-xilinx.c----xgpio_dir_out,"static int xgpio_dir_out(struct gpio_chip *gc, unsigned int gpio, int val) { unsigned long flags; struct xgpio_instance *chip = gpiochip_get_data(gc); int bit = xgpio_to_bit(chip, gpio); <S2SV_StartVul> spin_lock_irqsave(&chip->gpio_lock, flags); <S2SV_EndVul> __assign_bit(bit, chip->state, val); xgpio_write_ch(chip, XGPIO_DATA_OFFSET, bit, chip->state); __clear_bit(bit, chip->dir); xgpio_write_ch(chip, XGPIO_TRI_OFFSET, bit, chip->dir); <S2SV_StartVul> spin_unlock_irqrestore(&chip->gpio_lock, flags); <S2SV_EndVul> return 0; }","- spin_lock_irqsave(&chip->gpio_lock, flags);
- spin_unlock_irqrestore(&chip->gpio_lock, flags);
+ raw_spin_lock_irqsave(&chip->gpio_lock, flags);
+ raw_spin_unlock_irqrestore(&chip->gpio_lock, flags);","static int xgpio_dir_out(struct gpio_chip *gc, unsigned int gpio, int val) { unsigned long flags; struct xgpio_instance *chip = gpiochip_get_data(gc); int bit = xgpio_to_bit(chip, gpio); raw_spin_lock_irqsave(&chip->gpio_lock, flags); __assign_bit(bit, chip->state, val); xgpio_write_ch(chip, XGPIO_DATA_OFFSET, bit, chip->state); __clear_bit(bit, chip->dir); xgpio_write_ch(chip, XGPIO_TRI_OFFSET, bit, chip->dir); raw_spin_unlock_irqrestore(&chip->gpio_lock, flags); return 0; }"
1418----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56746/bad/sh7760fb.c----sh7760fb_alloc_mem,"static int sh7760fb_alloc_mem(struct fb_info *info) { struct sh7760fb_par *par = info->par; void *fbmem; unsigned long vram; int ret, bpp; if (info->screen_base) return 0; ret = sh7760fb_get_color_info(info, par->pd->lddfr, &bpp, NULL); if (ret) { printk(KERN_ERR ""colinfo\n""); return ret; } vram = info->var.xres * info->var.yres; if (info->var.grayscale) { if (bpp == 1) vram >>= 3; else if (bpp == 2) vram >>= 2; else if (bpp == 4) vram >>= 1; } else if (bpp > 8) vram *= 2; if ((vram < 1) || (vram > 1024 * 2048)) { fb_dbg(info, ""too much VRAM required. Check settings\n""); return -ENODEV; } if (vram < PAGE_SIZE) vram = PAGE_SIZE; fbmem = dma_alloc_coherent(info->device, vram, &par->fbdma, GFP_KERNEL); if (!fbmem) return -ENOMEM; if ((par->fbdma & SH7760FB_DMA_MASK) != SH7760FB_DMA_MASK) { <S2SV_StartVul> sh7760fb_free_mem(info); <S2SV_EndVul> dev_err(info->device, ""kernel gave me memory at 0x%08lx, which is"" ""unusable for the LCDC\n"", (unsigned long)par->fbdma); return -ENOMEM; } info->screen_base = fbmem; info->screen_size = vram; info->fix.smem_start = (unsigned long)info->screen_base; info->fix.smem_len = info->screen_size; return 0; }","- sh7760fb_free_mem(info);
+ dma_free_coherent(info->device, vram, fbmem, par->fbdma);","static int sh7760fb_alloc_mem(struct fb_info *info) { struct sh7760fb_par *par = info->par; void *fbmem; unsigned long vram; int ret, bpp; if (info->screen_base) return 0; ret = sh7760fb_get_color_info(info, par->pd->lddfr, &bpp, NULL); if (ret) { printk(KERN_ERR ""colinfo\n""); return ret; } vram = info->var.xres * info->var.yres; if (info->var.grayscale) { if (bpp == 1) vram >>= 3; else if (bpp == 2) vram >>= 2; else if (bpp == 4) vram >>= 1; } else if (bpp > 8) vram *= 2; if ((vram < 1) || (vram > 1024 * 2048)) { fb_dbg(info, ""too much VRAM required. Check settings\n""); return -ENODEV; } if (vram < PAGE_SIZE) vram = PAGE_SIZE; fbmem = dma_alloc_coherent(info->device, vram, &par->fbdma, GFP_KERNEL); if (!fbmem) return -ENOMEM; if ((par->fbdma & SH7760FB_DMA_MASK) != SH7760FB_DMA_MASK) { dma_free_coherent(info->device, vram, fbmem, par->fbdma); dev_err(info->device, ""kernel gave me memory at 0x%08lx, which is"" ""unusable for the LCDC\n"", (unsigned long)par->fbdma); return -ENOMEM; } info->screen_base = fbmem; info->screen_size = vram; info->fix.smem_start = (unsigned long)info->screen_base; info->fix.smem_len = info->screen_size; return 0; }"
305----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46775/bad/dcn20_hubbub.c----hubbub2_program_watermarks,"static bool hubbub2_program_watermarks( struct hubbub *hubbub, union dcn_watermark_set *watermarks, unsigned int refclk_mhz, bool safe_to_lower) { struct dcn20_hubbub *hubbub1 = TO_DCN20_HUBBUB(hubbub); bool wm_pending = false; if (hubbub1_program_urgent_watermarks(hubbub, watermarks, refclk_mhz, safe_to_lower)) wm_pending = true; if (hubbub1_program_stutter_watermarks(hubbub, watermarks, refclk_mhz, safe_to_lower)) wm_pending = true; if (hubbub1->base.ctx->dc->clk_mgr->clks.prev_p_state_change_support == true && hubbub1->base.ctx->dc->clk_mgr->clks.p_state_change_support == false) safe_to_lower = true; <S2SV_StartVul> hubbub1_program_pstate_watermarks(hubbub, watermarks, refclk_mhz, safe_to_lower); <S2SV_EndVul> REG_SET(DCHUBBUB_ARB_SAT_LEVEL, 0, DCHUBBUB_ARB_SAT_LEVEL, 60 * refclk_mhz); REG_UPDATE(DCHUBBUB_ARB_DF_REQ_OUTSTAND, DCHUBBUB_ARB_MIN_REQ_OUTSTAND, 180); hubbub->funcs->allow_self_refresh_control(hubbub, !hubbub->ctx->dc->debug.disable_stutter); return wm_pending; }","- hubbub1_program_pstate_watermarks(hubbub, watermarks, refclk_mhz, safe_to_lower);
+ if (hubbub1_program_pstate_watermarks(hubbub, watermarks, refclk_mhz, safe_to_lower))
+ wm_pending = true;","static bool hubbub2_program_watermarks( struct hubbub *hubbub, union dcn_watermark_set *watermarks, unsigned int refclk_mhz, bool safe_to_lower) { struct dcn20_hubbub *hubbub1 = TO_DCN20_HUBBUB(hubbub); bool wm_pending = false; if (hubbub1_program_urgent_watermarks(hubbub, watermarks, refclk_mhz, safe_to_lower)) wm_pending = true; if (hubbub1_program_stutter_watermarks(hubbub, watermarks, refclk_mhz, safe_to_lower)) wm_pending = true; if (hubbub1->base.ctx->dc->clk_mgr->clks.prev_p_state_change_support == true && hubbub1->base.ctx->dc->clk_mgr->clks.p_state_change_support == false) safe_to_lower = true; if (hubbub1_program_pstate_watermarks(hubbub, watermarks, refclk_mhz, safe_to_lower)) wm_pending = true; REG_SET(DCHUBBUB_ARB_SAT_LEVEL, 0, DCHUBBUB_ARB_SAT_LEVEL, 60 * refclk_mhz); REG_UPDATE(DCHUBBUB_ARB_DF_REQ_OUTSTAND, DCHUBBUB_ARB_MIN_REQ_OUTSTAND, 180); hubbub->funcs->allow_self_refresh_control(hubbub, !hubbub->ctx->dc->debug.disable_stutter); return wm_pending; }"
1238----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53689/bad/blk-sysfs.c----queue_attr_store,"queue_attr_store(struct kobject *kobj, struct attribute *attr, const char *page, size_t length) { struct queue_sysfs_entry *entry = to_queue(attr); struct gendisk *disk = container_of(kobj, struct gendisk, queue_kobj); struct request_queue *q = disk->queue; ssize_t res; if (!entry->store) return -EIO; if (entry->load_module) entry->load_module(disk, page, length); <S2SV_StartVul> blk_mq_freeze_queue(q); <S2SV_EndVul> mutex_lock(&q->sysfs_lock); res = entry->store(disk, page, length); <S2SV_StartVul> mutex_unlock(&q->sysfs_lock); <S2SV_EndVul> blk_mq_unfreeze_queue(q); return res; }","- blk_mq_freeze_queue(q);
- mutex_unlock(&q->sysfs_lock);
+ blk_mq_freeze_queue(q);
+ mutex_unlock(&q->sysfs_lock);","queue_attr_store(struct kobject *kobj, struct attribute *attr, const char *page, size_t length) { struct queue_sysfs_entry *entry = to_queue(attr); struct gendisk *disk = container_of(kobj, struct gendisk, queue_kobj); struct request_queue *q = disk->queue; ssize_t res; if (!entry->store) return -EIO; if (entry->load_module) entry->load_module(disk, page, length); mutex_lock(&q->sysfs_lock); blk_mq_freeze_queue(q); res = entry->store(disk, page, length); blk_mq_unfreeze_queue(q); mutex_unlock(&q->sysfs_lock); return res; }"
973----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50211/bad/truncate.c----udf_truncate_extents,"int udf_truncate_extents(struct inode *inode) { struct extent_position epos; struct kernel_lb_addr eloc, neloc = {}; uint32_t elen, nelen = 0, indirect_ext_len = 0, lenalloc; int8_t etype; struct super_block *sb = inode->i_sb; sector_t first_block = inode->i_size >> sb->s_blocksize_bits, offset; loff_t byte_offset; int adsize; struct udf_inode_info *iinfo = UDF_I(inode); int ret = 0; if (iinfo->i_alloc_type == ICBTAG_FLAG_AD_SHORT) adsize = sizeof(struct short_ad); else if (iinfo->i_alloc_type == ICBTAG_FLAG_AD_LONG) adsize = sizeof(struct long_ad); else BUG(); <S2SV_StartVul> etype = inode_bmap(inode, first_block, &epos, &eloc, &elen, &offset); <S2SV_EndVul> byte_offset = (offset << sb->s_blocksize_bits) + (inode->i_size & (sb->s_blocksize - 1)); <S2SV_StartVul> if (etype == -1) { <S2SV_EndVul> WARN_ON(byte_offset); return 0; } epos.offset -= adsize; extent_trunc(inode, &epos, &eloc, etype, elen, byte_offset); epos.offset += adsize; if (byte_offset) lenalloc = epos.offset; else lenalloc = epos.offset - adsize; if (!epos.bh) lenalloc -= udf_file_entry_alloc_offset(inode); else lenalloc -= sizeof(struct allocExtDesc); while ((ret = udf_current_aext(inode, &epos, &eloc, &elen, &etype, 0)) > 0) { if (etype == (EXT_NEXT_EXTENT_ALLOCDESCS >> 30)) { udf_write_aext(inode, &epos, &neloc, nelen, 0); if (indirect_ext_len) { BUG_ON(!epos.bh); udf_free_blocks(sb, NULL, &epos.block, 0, indirect_ext_len); } else if (!epos.bh) { iinfo->i_lenAlloc = lenalloc; mark_inode_dirty(inode); } else udf_update_alloc_ext_desc(inode, &epos, lenalloc); brelse(epos.bh); epos.offset = sizeof(struct allocExtDesc); epos.block = eloc; epos.bh = sb_bread(sb, udf_get_lb_pblock(sb, &eloc, 0)); if (!epos.bh) return -EIO; if (elen) indirect_ext_len = (elen + sb->s_blocksize - 1) >> sb->s_blocksize_bits; else indirect_ext_len = 1; } else { extent_trunc(inode, &epos, &eloc, etype, elen, 0); epos.offset += adsize; } } if (ret < 0) { brelse(epos.bh); return ret; } if (indirect_ext_len) { BUG_ON(!epos.bh); udf_free_blocks(sb, NULL, &epos.block, 0, indirect_ext_len); } else if (!epos.bh) { iinfo->i_lenAlloc = lenalloc; mark_inode_dirty(inode); } else udf_update_alloc_ext_desc(inode, &epos, lenalloc); iinfo->i_lenExtents = inode->i_size; brelse(epos.bh); return 0; }","- etype = inode_bmap(inode, first_block, &epos, &eloc, &elen, &offset);
- if (etype == -1) {
+ ret = inode_bmap(inode, first_block, &epos, &eloc, &elen, &offset, &etype);
+ if (ret < 0)
+ return ret;
+ if (ret == 0) {","int udf_truncate_extents(struct inode *inode) { struct extent_position epos; struct kernel_lb_addr eloc, neloc = {}; uint32_t elen, nelen = 0, indirect_ext_len = 0, lenalloc; int8_t etype; struct super_block *sb = inode->i_sb; sector_t first_block = inode->i_size >> sb->s_blocksize_bits, offset; loff_t byte_offset; int adsize; struct udf_inode_info *iinfo = UDF_I(inode); int ret = 0; if (iinfo->i_alloc_type == ICBTAG_FLAG_AD_SHORT) adsize = sizeof(struct short_ad); else if (iinfo->i_alloc_type == ICBTAG_FLAG_AD_LONG) adsize = sizeof(struct long_ad); else BUG(); ret = inode_bmap(inode, first_block, &epos, &eloc, &elen, &offset, &etype); if (ret < 0) return ret; byte_offset = (offset << sb->s_blocksize_bits) + (inode->i_size & (sb->s_blocksize - 1)); if (ret == 0) { WARN_ON(byte_offset); return 0; } epos.offset -= adsize; extent_trunc(inode, &epos, &eloc, etype, elen, byte_offset); epos.offset += adsize; if (byte_offset) lenalloc = epos.offset; else lenalloc = epos.offset - adsize; if (!epos.bh) lenalloc -= udf_file_entry_alloc_offset(inode); else lenalloc -= sizeof(struct allocExtDesc); while ((ret = udf_current_aext(inode, &epos, &eloc, &elen, &etype, 0)) > 0) { if (etype == (EXT_NEXT_EXTENT_ALLOCDESCS >> 30)) { udf_write_aext(inode, &epos, &neloc, nelen, 0); if (indirect_ext_len) { BUG_ON(!epos.bh); udf_free_blocks(sb, NULL, &epos.block, 0, indirect_ext_len); } else if (!epos.bh) { iinfo->i_lenAlloc = lenalloc; mark_inode_dirty(inode); } else udf_update_alloc_ext_desc(inode, &epos, lenalloc); brelse(epos.bh); epos.offset = sizeof(struct allocExtDesc); epos.block = eloc; epos.bh = sb_bread(sb, udf_get_lb_pblock(sb, &eloc, 0)); if (!epos.bh) return -EIO; if (elen) indirect_ext_len = (elen + sb->s_blocksize - 1) >> sb->s_blocksize_bits; else indirect_ext_len = 1; } else { extent_trunc(inode, &epos, &eloc, etype, elen, 0); epos.offset += adsize; } } if (ret < 0) { brelse(epos.bh); return ret; } if (indirect_ext_len) { BUG_ON(!epos.bh); udf_free_blocks(sb, NULL, &epos.block, 0, indirect_ext_len); } else if (!epos.bh) { iinfo->i_lenAlloc = lenalloc; mark_inode_dirty(inode); } else udf_update_alloc_ext_desc(inode, &epos, lenalloc); iinfo->i_lenExtents = inode->i_size; brelse(epos.bh); return 0; }"
720----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49994/bad/ioctl.c----blk_ioctl_discard,"static int blk_ioctl_discard(struct block_device *bdev, blk_mode_t mode, unsigned long arg) { uint64_t range[2]; uint64_t start, len, end; struct inode *inode = bdev->bd_inode; int err; if (!(mode & BLK_OPEN_WRITE)) return -EBADF; if (!bdev_max_discard_sectors(bdev)) return -EOPNOTSUPP; if (copy_from_user(range, (void __user *)arg, sizeof(range))) return -EFAULT; start = range[0]; len = range[1]; if (start & 511) return -EINVAL; if (len & 511) return -EINVAL; if (check_add_overflow(start, len, &end) || end > bdev_nr_bytes(bdev)) return -EINVAL; filemap_invalidate_lock(inode->i_mapping); <S2SV_StartVul> err = truncate_bdev_range(bdev, mode, start, start + len - 1); <S2SV_EndVul> if (err) goto fail; err = blkdev_issue_discard(bdev, start >> 9, len >> 9, GFP_KERNEL); fail: filemap_invalidate_unlock(inode->i_mapping); return err; }","- err = truncate_bdev_range(bdev, mode, start, start + len - 1);
+ err = truncate_bdev_range(bdev, mode, start, end - 1);","static int blk_ioctl_discard(struct block_device *bdev, blk_mode_t mode, unsigned long arg) { uint64_t range[2]; uint64_t start, len, end; struct inode *inode = bdev->bd_inode; int err; if (!(mode & BLK_OPEN_WRITE)) return -EBADF; if (!bdev_max_discard_sectors(bdev)) return -EOPNOTSUPP; if (copy_from_user(range, (void __user *)arg, sizeof(range))) return -EFAULT; start = range[0]; len = range[1]; if (start & 511) return -EINVAL; if (len & 511) return -EINVAL; if (check_add_overflow(start, len, &end) || end > bdev_nr_bytes(bdev)) return -EINVAL; filemap_invalidate_lock(inode->i_mapping); err = truncate_bdev_range(bdev, mode, start, end - 1); if (err) goto fail; err = blkdev_issue_discard(bdev, start >> 9, len >> 9, GFP_KERNEL); fail: filemap_invalidate_unlock(inode->i_mapping); return err; }"
1046----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50286/bad/user_session.c----ksmbd_sessions_deregister,"void ksmbd_sessions_deregister(struct ksmbd_conn *conn) { struct ksmbd_session *sess; unsigned long id; down_write(&sessions_table_lock); if (conn->binding) { int bkt; struct hlist_node *tmp; hash_for_each_safe(sessions_table, bkt, tmp, sess, hlist) { if (!ksmbd_chann_del(conn, sess) && xa_empty(&sess->ksmbd_chann_list)) { hash_del(&sess->hlist); ksmbd_session_destroy(sess); } } } <S2SV_StartVul> up_write(&sessions_table_lock); <S2SV_EndVul> down_write(&conn->session_lock); xa_for_each(&conn->sessions, id, sess) { unsigned long chann_id; struct channel *chann; xa_for_each(&sess->ksmbd_chann_list, chann_id, chann) { if (chann->conn != conn) ksmbd_conn_set_exiting(chann->conn); } ksmbd_chann_del(conn, sess); if (xa_empty(&sess->ksmbd_chann_list)) { xa_erase(&conn->sessions, sess->id); hash_del(&sess->hlist); ksmbd_session_destroy(sess); } } up_write(&conn->session_lock); }","- up_write(&sessions_table_lock);
+ up_write(&sessions_table_lock);","void ksmbd_sessions_deregister(struct ksmbd_conn *conn) { struct ksmbd_session *sess; unsigned long id; down_write(&sessions_table_lock); if (conn->binding) { int bkt; struct hlist_node *tmp; hash_for_each_safe(sessions_table, bkt, tmp, sess, hlist) { if (!ksmbd_chann_del(conn, sess) && xa_empty(&sess->ksmbd_chann_list)) { hash_del(&sess->hlist); ksmbd_session_destroy(sess); } } } down_write(&conn->session_lock); xa_for_each(&conn->sessions, id, sess) { unsigned long chann_id; struct channel *chann; xa_for_each(&sess->ksmbd_chann_list, chann_id, chann) { if (chann->conn != conn) ksmbd_conn_set_exiting(chann->conn); } ksmbd_chann_del(conn, sess); if (xa_empty(&sess->ksmbd_chann_list)) { xa_erase(&conn->sessions, sess->id); hash_del(&sess->hlist); ksmbd_session_destroy(sess); } } up_write(&conn->session_lock); up_write(&sessions_table_lock); }"
1476----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-57892/bad/quota_global.c----ocfs2_get_next_id,"static int ocfs2_get_next_id(struct super_block *sb, struct kqid *qid) { int type = qid->type; struct ocfs2_mem_dqinfo *info = sb_dqinfo(sb, type)->dqi_priv; int status = 0; trace_ocfs2_get_next_id(from_kqid(&init_user_ns, *qid), type); <S2SV_StartVul> if (!sb_has_quota_loaded(sb, type)) { <S2SV_EndVul> status = -ESRCH; goto out; } status = ocfs2_lock_global_qf(info, 0); if (status < 0) goto out; status = ocfs2_qinfo_lock(info, 0); if (status < 0) goto out_global; status = qtree_get_next_id(&info->dqi_gi, qid); ocfs2_qinfo_unlock(info, 0); out_global: ocfs2_unlock_global_qf(info, 0); out: if (status && status != -ENOENT && status != -ESRCH) mlog_errno(status); return status; }","- if (!sb_has_quota_loaded(sb, type)) {
+ if (!sb_has_quota_active(sb, type)) {","static int ocfs2_get_next_id(struct super_block *sb, struct kqid *qid) { int type = qid->type; struct ocfs2_mem_dqinfo *info = sb_dqinfo(sb, type)->dqi_priv; int status = 0; trace_ocfs2_get_next_id(from_kqid(&init_user_ns, *qid), type); if (!sb_has_quota_active(sb, type)) { status = -ESRCH; goto out; } status = ocfs2_lock_global_qf(info, 0); if (status < 0) goto out; status = ocfs2_qinfo_lock(info, 0); if (status < 0) goto out_global; status = qtree_get_next_id(&info->dqi_gi, qid); ocfs2_qinfo_unlock(info, 0); out_global: ocfs2_unlock_global_qf(info, 0); out: if (status && status != -ENOENT && status != -ESRCH) mlog_errno(status); return status; }"
383----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46845/bad/trace_osnoise.c----timerlat_fd_release,"static int timerlat_fd_release(struct inode *inode, struct file *file) { struct osnoise_variables *osn_var; struct timerlat_variables *tlat_var; long cpu = (long) file->private_data; migrate_disable(); mutex_lock(&interface_lock); osn_var = per_cpu_ptr(&per_cpu_osnoise_var, cpu); tlat_var = per_cpu_ptr(&per_cpu_timerlat_var, cpu); <S2SV_StartVul> hrtimer_cancel(&tlat_var->timer); <S2SV_EndVul> memset(tlat_var, 0, sizeof(*tlat_var)); osn_var->sampling = 0; osn_var->pid = 0; if (osn_var->kthread) { put_task_struct(osn_var->kthread); osn_var->kthread = NULL; } mutex_unlock(&interface_lock); migrate_enable(); return 0; }","- hrtimer_cancel(&tlat_var->timer);
+ if (tlat_var->kthread)
+ hrtimer_cancel(&tlat_var->timer);","static int timerlat_fd_release(struct inode *inode, struct file *file) { struct osnoise_variables *osn_var; struct timerlat_variables *tlat_var; long cpu = (long) file->private_data; migrate_disable(); mutex_lock(&interface_lock); osn_var = per_cpu_ptr(&per_cpu_osnoise_var, cpu); tlat_var = per_cpu_ptr(&per_cpu_timerlat_var, cpu); if (tlat_var->kthread) hrtimer_cancel(&tlat_var->timer); memset(tlat_var, 0, sizeof(*tlat_var)); osn_var->sampling = 0; osn_var->pid = 0; if (osn_var->kthread) { put_task_struct(osn_var->kthread); osn_var->kthread = NULL; } mutex_unlock(&interface_lock); migrate_enable(); return 0; }"
1236----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53689/bad/blk-mq-sysfs.c----blk_mq_sysfs_unregister_hctxs,"void blk_mq_sysfs_unregister_hctxs(struct request_queue *q) { struct blk_mq_hw_ctx *hctx; unsigned long i; <S2SV_StartVul> mutex_lock(&q->sysfs_dir_lock); <S2SV_EndVul> if (!q->mq_sysfs_init_done) <S2SV_StartVul> goto unlock; <S2SV_EndVul> queue_for_each_hw_ctx(q, hctx, i) blk_mq_unregister_hctx(hctx); <S2SV_StartVul> unlock: <S2SV_EndVul> <S2SV_StartVul> mutex_unlock(&q->sysfs_dir_lock); <S2SV_EndVul> }","- mutex_lock(&q->sysfs_dir_lock);
- goto unlock;
- unlock:
- mutex_unlock(&q->sysfs_dir_lock);
+ lockdep_assert_held(&q->sysfs_dir_lock);
+ return;","void blk_mq_sysfs_unregister_hctxs(struct request_queue *q) { struct blk_mq_hw_ctx *hctx; unsigned long i; lockdep_assert_held(&q->sysfs_dir_lock); if (!q->mq_sysfs_init_done) return; queue_for_each_hw_ctx(q, hctx, i) blk_mq_unregister_hctx(hctx); }"
1496----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-57938/bad/associola.c----*sctp_association_init,"static struct sctp_association *sctp_association_init( struct sctp_association *asoc, const struct sctp_endpoint *ep, const struct sock *sk, enum sctp_scope scope, gfp_t gfp) { struct sctp_sock *sp; struct sctp_paramhdr *p; int i; sp = sctp_sk((struct sock *)sk); asoc->ep = (struct sctp_endpoint *)ep; asoc->base.sk = (struct sock *)sk; asoc->base.net = sock_net(sk); sctp_endpoint_hold(asoc->ep); sock_hold(asoc->base.sk); asoc->base.type = SCTP_EP_TYPE_ASSOCIATION; refcount_set(&asoc->base.refcnt, 1); sctp_bind_addr_init(&asoc->base.bind_addr, ep->base.bind_addr.port); asoc->state = SCTP_STATE_CLOSED; asoc->cookie_life = ms_to_ktime(sp->assocparams.sasoc_cookie_life); asoc->user_frag = sp->user_frag; asoc->max_retrans = sp->assocparams.sasoc_asocmaxrxt; asoc->pf_retrans = sp->pf_retrans; asoc->ps_retrans = sp->ps_retrans; asoc->pf_expose = sp->pf_expose; asoc->rto_initial = msecs_to_jiffies(sp->rtoinfo.srto_initial); asoc->rto_max = msecs_to_jiffies(sp->rtoinfo.srto_max); asoc->rto_min = msecs_to_jiffies(sp->rtoinfo.srto_min); asoc->hbinterval = msecs_to_jiffies(sp->hbinterval); asoc->probe_interval = msecs_to_jiffies(sp->probe_interval); asoc->encap_port = sp->encap_port; asoc->pathmaxrxt = sp->pathmaxrxt; asoc->flowlabel = sp->flowlabel; asoc->dscp = sp->dscp; asoc->sackdelay = msecs_to_jiffies(sp->sackdelay); asoc->sackfreq = sp->sackfreq; asoc->param_flags = sp->param_flags; asoc->max_burst = sp->max_burst; asoc->subscribe = sp->subscribe; asoc->timeouts[SCTP_EVENT_TIMEOUT_T1_COOKIE] = asoc->rto_initial; asoc->timeouts[SCTP_EVENT_TIMEOUT_T1_INIT] = asoc->rto_initial; asoc->timeouts[SCTP_EVENT_TIMEOUT_T2_SHUTDOWN] = asoc->rto_initial; asoc->timeouts[SCTP_EVENT_TIMEOUT_T5_SHUTDOWN_GUARD] = 5 * asoc->rto_max; asoc->timeouts[SCTP_EVENT_TIMEOUT_SACK] = asoc->sackdelay; <S2SV_StartVul> asoc->timeouts[SCTP_EVENT_TIMEOUT_AUTOCLOSE] = sp->autoclose * HZ; <S2SV_EndVul> for (i = SCTP_EVENT_TIMEOUT_NONE; i < SCTP_NUM_TIMEOUT_TYPES; ++i) timer_setup(&asoc->timers[i], sctp_timer_events[i], 0); asoc->c.sinit_max_instreams = sp->initmsg.sinit_max_instreams; asoc->c.sinit_num_ostreams = sp->initmsg.sinit_num_ostreams; asoc->max_init_attempts = sp->initmsg.sinit_max_attempts; asoc->max_init_timeo = msecs_to_jiffies(sp->initmsg.sinit_max_init_timeo); if ((sk->sk_rcvbuf/2) < SCTP_DEFAULT_MINWINDOW) asoc->rwnd = SCTP_DEFAULT_MINWINDOW; else asoc->rwnd = sk->sk_rcvbuf/2; asoc->a_rwnd = asoc->rwnd; asoc->peer.rwnd = SCTP_DEFAULT_MAXWINDOW; atomic_set(&asoc->rmem_alloc, 0); init_waitqueue_head(&asoc->wait); asoc->c.my_vtag = sctp_generate_tag(ep); asoc->c.my_port = ep->base.bind_addr.port; asoc->c.initial_tsn = sctp_generate_tsn(ep); asoc->next_tsn = asoc->c.initial_tsn; asoc->ctsn_ack_point = asoc->next_tsn - 1; asoc->adv_peer_ack_point = asoc->ctsn_ack_point; asoc->highest_sacked = asoc->ctsn_ack_point; asoc->last_cwr_tsn = asoc->ctsn_ack_point; asoc->addip_serial = asoc->c.initial_tsn; asoc->strreset_outseq = asoc->c.initial_tsn; INIT_LIST_HEAD(&asoc->addip_chunk_list); INIT_LIST_HEAD(&asoc->asconf_ack_list); INIT_LIST_HEAD(&asoc->peer.transport_addr_list); asoc->peer.sack_needed = 1; asoc->peer.sack_generation = 1; sctp_inq_init(&asoc->base.inqueue); sctp_inq_set_th_handler(&asoc->base.inqueue, sctp_assoc_bh_rcv); sctp_outq_init(asoc, &asoc->outqueue); sctp_ulpq_init(&asoc->ulpq, asoc); if (sctp_stream_init(&asoc->stream, asoc->c.sinit_num_ostreams, 0, gfp)) goto stream_free; asoc->pathmtu = sp->pathmtu; sctp_assoc_update_frag_point(asoc); asoc->peer.ipv4_address = 1; if (asoc->base.sk->sk_family == PF_INET6) asoc->peer.ipv6_address = 1; INIT_LIST_HEAD(&asoc->asocs); asoc->default_stream = sp->default_stream; asoc->default_ppid = sp->default_ppid; asoc->default_flags = sp->default_flags; asoc->default_context = sp->default_context; asoc->default_timetolive = sp->default_timetolive; asoc->default_rcv_context = sp->default_rcv_context; INIT_LIST_HEAD(&asoc->endpoint_shared_keys); if (sctp_auth_asoc_copy_shkeys(ep, asoc, gfp)) goto stream_free; asoc->active_key_id = ep->active_key_id; asoc->strreset_enable = ep->strreset_enable; if (ep->auth_hmacs_list) memcpy(asoc->c.auth_hmacs, ep->auth_hmacs_list, ntohs(ep->auth_hmacs_list->param_hdr.length)); if (ep->auth_chunk_list) memcpy(asoc->c.auth_chunks, ep->auth_chunk_list, ntohs(ep->auth_chunk_list->param_hdr.length)); p = (struct sctp_paramhdr *)asoc->c.auth_random; p->type = SCTP_PARAM_RANDOM; p->length = htons(sizeof(*p) + SCTP_AUTH_RANDOM_LENGTH); get_random_bytes(p+1, SCTP_AUTH_RANDOM_LENGTH); return asoc; stream_free: sctp_stream_free(&asoc->stream); sock_put(asoc->base.sk); sctp_endpoint_put(asoc->ep); return NULL; }","- asoc->timeouts[SCTP_EVENT_TIMEOUT_AUTOCLOSE] = sp->autoclose * HZ;
+ asoc->timeouts[SCTP_EVENT_TIMEOUT_AUTOCLOSE] =
+ (unsigned long)sp->autoclose * HZ;","static struct sctp_association *sctp_association_init( struct sctp_association *asoc, const struct sctp_endpoint *ep, const struct sock *sk, enum sctp_scope scope, gfp_t gfp) { struct sctp_sock *sp; struct sctp_paramhdr *p; int i; sp = sctp_sk((struct sock *)sk); asoc->ep = (struct sctp_endpoint *)ep; asoc->base.sk = (struct sock *)sk; asoc->base.net = sock_net(sk); sctp_endpoint_hold(asoc->ep); sock_hold(asoc->base.sk); asoc->base.type = SCTP_EP_TYPE_ASSOCIATION; refcount_set(&asoc->base.refcnt, 1); sctp_bind_addr_init(&asoc->base.bind_addr, ep->base.bind_addr.port); asoc->state = SCTP_STATE_CLOSED; asoc->cookie_life = ms_to_ktime(sp->assocparams.sasoc_cookie_life); asoc->user_frag = sp->user_frag; asoc->max_retrans = sp->assocparams.sasoc_asocmaxrxt; asoc->pf_retrans = sp->pf_retrans; asoc->ps_retrans = sp->ps_retrans; asoc->pf_expose = sp->pf_expose; asoc->rto_initial = msecs_to_jiffies(sp->rtoinfo.srto_initial); asoc->rto_max = msecs_to_jiffies(sp->rtoinfo.srto_max); asoc->rto_min = msecs_to_jiffies(sp->rtoinfo.srto_min); asoc->hbinterval = msecs_to_jiffies(sp->hbinterval); asoc->probe_interval = msecs_to_jiffies(sp->probe_interval); asoc->encap_port = sp->encap_port; asoc->pathmaxrxt = sp->pathmaxrxt; asoc->flowlabel = sp->flowlabel; asoc->dscp = sp->dscp; asoc->sackdelay = msecs_to_jiffies(sp->sackdelay); asoc->sackfreq = sp->sackfreq; asoc->param_flags = sp->param_flags; asoc->max_burst = sp->max_burst; asoc->subscribe = sp->subscribe; asoc->timeouts[SCTP_EVENT_TIMEOUT_T1_COOKIE] = asoc->rto_initial; asoc->timeouts[SCTP_EVENT_TIMEOUT_T1_INIT] = asoc->rto_initial; asoc->timeouts[SCTP_EVENT_TIMEOUT_T2_SHUTDOWN] = asoc->rto_initial; asoc->timeouts[SCTP_EVENT_TIMEOUT_T5_SHUTDOWN_GUARD] = 5 * asoc->rto_max; asoc->timeouts[SCTP_EVENT_TIMEOUT_SACK] = asoc->sackdelay; asoc->timeouts[SCTP_EVENT_TIMEOUT_AUTOCLOSE] = (unsigned long)sp->autoclose * HZ; for (i = SCTP_EVENT_TIMEOUT_NONE; i < SCTP_NUM_TIMEOUT_TYPES; ++i) timer_setup(&asoc->timers[i], sctp_timer_events[i], 0); asoc->c.sinit_max_instreams = sp->initmsg.sinit_max_instreams; asoc->c.sinit_num_ostreams = sp->initmsg.sinit_num_ostreams; asoc->max_init_attempts = sp->initmsg.sinit_max_attempts; asoc->max_init_timeo = msecs_to_jiffies(sp->initmsg.sinit_max_init_timeo); if ((sk->sk_rcvbuf/2) < SCTP_DEFAULT_MINWINDOW) asoc->rwnd = SCTP_DEFAULT_MINWINDOW; else asoc->rwnd = sk->sk_rcvbuf/2; asoc->a_rwnd = asoc->rwnd; asoc->peer.rwnd = SCTP_DEFAULT_MAXWINDOW; atomic_set(&asoc->rmem_alloc, 0); init_waitqueue_head(&asoc->wait); asoc->c.my_vtag = sctp_generate_tag(ep); asoc->c.my_port = ep->base.bind_addr.port; asoc->c.initial_tsn = sctp_generate_tsn(ep); asoc->next_tsn = asoc->c.initial_tsn; asoc->ctsn_ack_point = asoc->next_tsn - 1; asoc->adv_peer_ack_point = asoc->ctsn_ack_point; asoc->highest_sacked = asoc->ctsn_ack_point; asoc->last_cwr_tsn = asoc->ctsn_ack_point; asoc->addip_serial = asoc->c.initial_tsn; asoc->strreset_outseq = asoc->c.initial_tsn; INIT_LIST_HEAD(&asoc->addip_chunk_list); INIT_LIST_HEAD(&asoc->asconf_ack_list); INIT_LIST_HEAD(&asoc->peer.transport_addr_list); asoc->peer.sack_needed = 1; asoc->peer.sack_generation = 1; sctp_inq_init(&asoc->base.inqueue); sctp_inq_set_th_handler(&asoc->base.inqueue, sctp_assoc_bh_rcv); sctp_outq_init(asoc, &asoc->outqueue); sctp_ulpq_init(&asoc->ulpq, asoc); if (sctp_stream_init(&asoc->stream, asoc->c.sinit_num_ostreams, 0, gfp)) goto stream_free; asoc->pathmtu = sp->pathmtu; sctp_assoc_update_frag_point(asoc); asoc->peer.ipv4_address = 1; if (asoc->base.sk->sk_family == PF_INET6) asoc->peer.ipv6_address = 1; INIT_LIST_HEAD(&asoc->asocs); asoc->default_stream = sp->default_stream; asoc->default_ppid = sp->default_ppid; asoc->default_flags = sp->default_flags; asoc->default_context = sp->default_context; asoc->default_timetolive = sp->default_timetolive; asoc->default_rcv_context = sp->default_rcv_context; INIT_LIST_HEAD(&asoc->endpoint_shared_keys); if (sctp_auth_asoc_copy_shkeys(ep, asoc, gfp)) goto stream_free; asoc->active_key_id = ep->active_key_id; asoc->strreset_enable = ep->strreset_enable; if (ep->auth_hmacs_list) memcpy(asoc->c.auth_hmacs, ep->auth_hmacs_list, ntohs(ep->auth_hmacs_list->param_hdr.length)); if (ep->auth_chunk_list) memcpy(asoc->c.auth_chunks, ep->auth_chunk_list, ntohs(ep->auth_chunk_list->param_hdr.length)); p = (struct sctp_paramhdr *)asoc->c.auth_random; p->type = SCTP_PARAM_RANDOM; p->length = htons(sizeof(*p) + SCTP_AUTH_RANDOM_LENGTH); get_random_bytes(p+1, SCTP_AUTH_RANDOM_LENGTH); return asoc; stream_free: sctp_stream_free(&asoc->stream); sock_put(asoc->base.sk); sctp_endpoint_put(asoc->ep); return NULL; }"
363----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46824/bad/hw_pagetable.c----iommufd_hwpt_nested_alloc,"iommufd_hwpt_nested_alloc(struct iommufd_ctx *ictx, struct iommufd_hwpt_paging *parent, struct iommufd_device *idev, u32 flags, const struct iommu_user_data *user_data) { const struct iommu_ops *ops = dev_iommu_ops(idev->dev); struct iommufd_hwpt_nested *hwpt_nested; struct iommufd_hw_pagetable *hwpt; int rc; if (flags || !user_data->len || !ops->domain_alloc_user) return ERR_PTR(-EOPNOTSUPP); if (parent->auto_domain || !parent->nest_parent) return ERR_PTR(-EINVAL); hwpt_nested = __iommufd_object_alloc( ictx, hwpt_nested, IOMMUFD_OBJ_HWPT_NESTED, common.obj); if (IS_ERR(hwpt_nested)) return ERR_CAST(hwpt_nested); hwpt = &hwpt_nested->common; refcount_inc(&parent->common.obj.users); hwpt_nested->parent = parent; hwpt->domain = ops->domain_alloc_user(idev->dev, flags, parent->common.domain, user_data); if (IS_ERR(hwpt->domain)) { rc = PTR_ERR(hwpt->domain); hwpt->domain = NULL; goto out_abort; } hwpt->domain->owner = ops; <S2SV_StartVul> if (WARN_ON_ONCE(hwpt->domain->type != IOMMU_DOMAIN_NESTED)) { <S2SV_EndVul> rc = -EINVAL; goto out_abort; } return hwpt_nested; out_abort: iommufd_object_abort_and_destroy(ictx, &hwpt->obj); return ERR_PTR(rc); }","- if (WARN_ON_ONCE(hwpt->domain->type != IOMMU_DOMAIN_NESTED)) {
+ if (WARN_ON_ONCE(hwpt->domain->type != IOMMU_DOMAIN_NESTED ||
+ !hwpt->domain->ops->cache_invalidate_user)) {","iommufd_hwpt_nested_alloc(struct iommufd_ctx *ictx, struct iommufd_hwpt_paging *parent, struct iommufd_device *idev, u32 flags, const struct iommu_user_data *user_data) { const struct iommu_ops *ops = dev_iommu_ops(idev->dev); struct iommufd_hwpt_nested *hwpt_nested; struct iommufd_hw_pagetable *hwpt; int rc; if (flags || !user_data->len || !ops->domain_alloc_user) return ERR_PTR(-EOPNOTSUPP); if (parent->auto_domain || !parent->nest_parent) return ERR_PTR(-EINVAL); hwpt_nested = __iommufd_object_alloc( ictx, hwpt_nested, IOMMUFD_OBJ_HWPT_NESTED, common.obj); if (IS_ERR(hwpt_nested)) return ERR_CAST(hwpt_nested); hwpt = &hwpt_nested->common; refcount_inc(&parent->common.obj.users); hwpt_nested->parent = parent; hwpt->domain = ops->domain_alloc_user(idev->dev, flags, parent->common.domain, user_data); if (IS_ERR(hwpt->domain)) { rc = PTR_ERR(hwpt->domain); hwpt->domain = NULL; goto out_abort; } hwpt->domain->owner = ops; if (WARN_ON_ONCE(hwpt->domain->type != IOMMU_DOMAIN_NESTED || !hwpt->domain->ops->cache_invalidate_user)) { rc = -EINVAL; goto out_abort; } return hwpt_nested; out_abort: iommufd_object_abort_and_destroy(ictx, &hwpt->obj); return ERR_PTR(rc); }"
13----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-43891/bad/trace_events_trigger.c----event_trigger_regex_open,"static int event_trigger_regex_open(struct inode *inode, struct file *file) { int ret; ret = security_locked_down(LOCKDOWN_TRACEFS); if (ret) return ret; mutex_lock(&event_mutex); <S2SV_StartVul> if (unlikely(!event_file_data(file))) { <S2SV_EndVul> mutex_unlock(&event_mutex); return -ENODEV; } if ((file->f_mode & FMODE_WRITE) && (file->f_flags & O_TRUNC)) { struct trace_event_file *event_file; struct event_command *p; event_file = event_file_data(file); list_for_each_entry(p, &trigger_commands, list) { if (p->unreg_all) p->unreg_all(event_file); } } if (file->f_mode & FMODE_READ) { ret = seq_open(file, &event_triggers_seq_ops); if (!ret) { struct seq_file *m = file->private_data; m->private = file; } } mutex_unlock(&event_mutex); return ret; }","- if (unlikely(!event_file_data(file))) {
+ if (unlikely(!event_file_file(file))) {","static int event_trigger_regex_open(struct inode *inode, struct file *file) { int ret; ret = security_locked_down(LOCKDOWN_TRACEFS); if (ret) return ret; mutex_lock(&event_mutex); if (unlikely(!event_file_file(file))) { mutex_unlock(&event_mutex); return -ENODEV; } if ((file->f_mode & FMODE_WRITE) && (file->f_flags & O_TRUNC)) { struct trace_event_file *event_file; struct event_command *p; event_file = event_file_data(file); list_for_each_entry(p, &trigger_commands, list) { if (p->unreg_all) p->unreg_all(event_file); } } if (file->f_mode & FMODE_READ) { ret = seq_open(file, &event_triggers_seq_ops); if (!ret) { struct seq_file *m = file->private_data; m->private = file; } } mutex_unlock(&event_mutex); return ret; }"
42----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-43910/bad/verifier.c----btf_check_func_arg_match,"static int btf_check_func_arg_match(struct bpf_verifier_env *env, int subprog, const struct btf *btf, struct bpf_reg_state *regs) { struct bpf_subprog_info *sub = subprog_info(env, subprog); struct bpf_verifier_log *log = &env->log; u32 i; int ret; ret = btf_prepare_func_args(env, subprog); if (ret) return ret; for (i = 0; i < sub->arg_cnt; i++) { u32 regno = i + 1; struct bpf_reg_state *reg = &regs[regno]; struct bpf_subprog_arg_info *arg = &sub->args[i]; if (arg->arg_type == ARG_ANYTHING) { if (reg->type != SCALAR_VALUE) { bpf_log(log, ""R%d is not a scalar\n"", regno); return -EINVAL; } } else if (arg->arg_type == ARG_PTR_TO_CTX) { ret = check_func_arg_reg_off(env, reg, regno, ARG_DONTCARE); if (ret < 0) return ret; if (reg->type != PTR_TO_CTX) { bpf_log(log, ""arg#%d expects pointer to ctx\n"", i); return -EINVAL; } } else if (base_type(arg->arg_type) == ARG_PTR_TO_MEM) { ret = check_func_arg_reg_off(env, reg, regno, ARG_DONTCARE); if (ret < 0) return ret; if (check_mem_reg(env, reg, regno, arg->mem_size)) return -EINVAL; if (!(arg->arg_type & PTR_MAYBE_NULL) && (reg->type & PTR_MAYBE_NULL)) { bpf_log(log, ""arg#%d is expected to be non-NULL\n"", i); return -EINVAL; } } else if (base_type(arg->arg_type) == ARG_PTR_TO_ARENA) { if (reg->type != PTR_TO_ARENA && reg->type != SCALAR_VALUE) { bpf_log(log, ""R%d is not a pointer to arena or scalar.\n"", regno); return -EINVAL; <S2SV_StartVul> } <S2SV_EndVul> } else if (arg->arg_type == (ARG_PTR_TO_DYNPTR | MEM_RDONLY)) { ret = process_dynptr_func(env, regno, -1, arg->arg_type, 0); if (ret) return ret; } else if (base_type(arg->arg_type) == ARG_PTR_TO_BTF_ID) { struct bpf_call_arg_meta meta; int err; if (register_is_null(reg) && type_may_be_null(arg->arg_type)) continue; memset(&meta, 0, sizeof(meta)); err = check_reg_type(env, regno, arg->arg_type, &arg->btf_id, &meta); err = err ?: check_func_arg_reg_off(env, reg, regno, arg->arg_type); if (err) return err; } else { bpf_log(log, ""verifier bug: unrecognized arg#%d type %d\n"", i, arg->arg_type); return -EFAULT; } } return 0; }","- }
+ }
+ ret = check_func_arg_reg_off(env, reg, regno, ARG_PTR_TO_DYNPTR);
+ if (ret)
+ return ret;
+ if (ret)
+ return ret;","static int btf_check_func_arg_match(struct bpf_verifier_env *env, int subprog, const struct btf *btf, struct bpf_reg_state *regs) { struct bpf_subprog_info *sub = subprog_info(env, subprog); struct bpf_verifier_log *log = &env->log; u32 i; int ret; ret = btf_prepare_func_args(env, subprog); if (ret) return ret; for (i = 0; i < sub->arg_cnt; i++) { u32 regno = i + 1; struct bpf_reg_state *reg = &regs[regno]; struct bpf_subprog_arg_info *arg = &sub->args[i]; if (arg->arg_type == ARG_ANYTHING) { if (reg->type != SCALAR_VALUE) { bpf_log(log, ""R%d is not a scalar\n"", regno); return -EINVAL; } } else if (arg->arg_type == ARG_PTR_TO_CTX) { ret = check_func_arg_reg_off(env, reg, regno, ARG_DONTCARE); if (ret < 0) return ret; if (reg->type != PTR_TO_CTX) { bpf_log(log, ""arg#%d expects pointer to ctx\n"", i); return -EINVAL; } } else if (base_type(arg->arg_type) == ARG_PTR_TO_MEM) { ret = check_func_arg_reg_off(env, reg, regno, ARG_DONTCARE); if (ret < 0) return ret; if (check_mem_reg(env, reg, regno, arg->mem_size)) return -EINVAL; if (!(arg->arg_type & PTR_MAYBE_NULL) && (reg->type & PTR_MAYBE_NULL)) { bpf_log(log, ""arg#%d is expected to be non-NULL\n"", i); return -EINVAL; } } else if (base_type(arg->arg_type) == ARG_PTR_TO_ARENA) { if (reg->type != PTR_TO_ARENA && reg->type != SCALAR_VALUE) { bpf_log(log, ""R%d is not a pointer to arena or scalar.\n"", regno); return -EINVAL; } } else if (arg->arg_type == (ARG_PTR_TO_DYNPTR | MEM_RDONLY)) { ret = check_func_arg_reg_off(env, reg, regno, ARG_PTR_TO_DYNPTR); if (ret) return ret; ret = process_dynptr_func(env, regno, -1, arg->arg_type, 0); if (ret) return ret; } else if (base_type(arg->arg_type) == ARG_PTR_TO_BTF_ID) { struct bpf_call_arg_meta meta; int err; if (register_is_null(reg) && type_may_be_null(arg->arg_type)) continue; memset(&meta, 0, sizeof(meta)); err = check_reg_type(env, regno, arg->arg_type, &arg->btf_id, &meta); err = err ?: check_func_arg_reg_off(env, reg, regno, arg->arg_type); if (err) return err; } else { bpf_log(log, ""verifier bug: unrecognized arg#%d type %d\n"", i, arg->arg_type); return -EFAULT; } } return 0; }"
179----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-45027/bad/xhci-mem.c----xhci_mem_cleanup,"void xhci_mem_cleanup(struct xhci_hcd *xhci) { struct device *dev = xhci_to_hcd(xhci)->self.sysdev; int i, j, num_ports; cancel_delayed_work_sync(&xhci->cmd_timer); <S2SV_StartVul> for (i = 0; i < xhci->max_interrupters; i++) { <S2SV_EndVul> if (xhci->interrupters[i]) { xhci_remove_interrupter(xhci, xhci->interrupters[i]); xhci_free_interrupter(xhci, xhci->interrupters[i]); xhci->interrupters[i] = NULL; } } xhci_dbg_trace(xhci, trace_xhci_dbg_init, ""Freed interrupters""); if (xhci->cmd_ring) xhci_ring_free(xhci, xhci->cmd_ring); xhci->cmd_ring = NULL; xhci_dbg_trace(xhci, trace_xhci_dbg_init, ""Freed command ring""); xhci_cleanup_command_queue(xhci); num_ports = HCS_MAX_PORTS(xhci->hcs_params1); for (i = 0; i < num_ports && xhci->rh_bw; i++) { struct xhci_interval_bw_table *bwt = &xhci->rh_bw[i].bw_table; for (j = 0; j < XHCI_MAX_INTERVAL; j++) { struct list_head *ep = &bwt->interval_bw[j].endpoints; while (!list_empty(ep)) list_del_init(ep->next); } } for (i = HCS_MAX_SLOTS(xhci->hcs_params1); i > 0; i--) xhci_free_virt_devices_depth_first(xhci, i); dma_pool_destroy(xhci->segment_pool); xhci->segment_pool = NULL; xhci_dbg_trace(xhci, trace_xhci_dbg_init, ""Freed segment pool""); dma_pool_destroy(xhci->device_pool); xhci->device_pool = NULL; xhci_dbg_trace(xhci, trace_xhci_dbg_init, ""Freed device context pool""); dma_pool_destroy(xhci->small_streams_pool); xhci->small_streams_pool = NULL; xhci_dbg_trace(xhci, trace_xhci_dbg_init, ""Freed small stream array pool""); dma_pool_destroy(xhci->medium_streams_pool); xhci->medium_streams_pool = NULL; xhci_dbg_trace(xhci, trace_xhci_dbg_init, ""Freed medium stream array pool""); if (xhci->dcbaa) dma_free_coherent(dev, sizeof(*xhci->dcbaa), xhci->dcbaa, xhci->dcbaa->dma); xhci->dcbaa = NULL; scratchpad_free(xhci); if (!xhci->rh_bw) goto no_bw; for (i = 0; i < num_ports; i++) { struct xhci_tt_bw_info *tt, *n; list_for_each_entry_safe(tt, n, &xhci->rh_bw[i].tts, tt_list) { list_del(&tt->tt_list); kfree(tt); } } no_bw: xhci->cmd_ring_reserved_trbs = 0; xhci->usb2_rhub.num_ports = 0; xhci->usb3_rhub.num_ports = 0; xhci->num_active_eps = 0; kfree(xhci->usb2_rhub.ports); kfree(xhci->usb3_rhub.ports); kfree(xhci->hw_ports); kfree(xhci->rh_bw); for (i = 0; i < xhci->num_port_caps; i++) kfree(xhci->port_caps[i].psi); kfree(xhci->port_caps); kfree(xhci->interrupters); xhci->num_port_caps = 0; xhci->usb2_rhub.ports = NULL; xhci->usb3_rhub.ports = NULL; xhci->hw_ports = NULL; xhci->rh_bw = NULL; xhci->port_caps = NULL; xhci->interrupters = NULL; xhci->page_size = 0; xhci->page_shift = 0; xhci->usb2_rhub.bus_state.bus_suspended = 0; xhci->usb3_rhub.bus_state.bus_suspended = 0; }","- for (i = 0; i < xhci->max_interrupters; i++) {
+ for (i = 0; xhci->interrupters && i < xhci->max_interrupters; i++) {","void xhci_mem_cleanup(struct xhci_hcd *xhci) { struct device *dev = xhci_to_hcd(xhci)->self.sysdev; int i, j, num_ports; cancel_delayed_work_sync(&xhci->cmd_timer); for (i = 0; xhci->interrupters && i < xhci->max_interrupters; i++) { if (xhci->interrupters[i]) { xhci_remove_interrupter(xhci, xhci->interrupters[i]); xhci_free_interrupter(xhci, xhci->interrupters[i]); xhci->interrupters[i] = NULL; } } xhci_dbg_trace(xhci, trace_xhci_dbg_init, ""Freed interrupters""); if (xhci->cmd_ring) xhci_ring_free(xhci, xhci->cmd_ring); xhci->cmd_ring = NULL; xhci_dbg_trace(xhci, trace_xhci_dbg_init, ""Freed command ring""); xhci_cleanup_command_queue(xhci); num_ports = HCS_MAX_PORTS(xhci->hcs_params1); for (i = 0; i < num_ports && xhci->rh_bw; i++) { struct xhci_interval_bw_table *bwt = &xhci->rh_bw[i].bw_table; for (j = 0; j < XHCI_MAX_INTERVAL; j++) { struct list_head *ep = &bwt->interval_bw[j].endpoints; while (!list_empty(ep)) list_del_init(ep->next); } } for (i = HCS_MAX_SLOTS(xhci->hcs_params1); i > 0; i--) xhci_free_virt_devices_depth_first(xhci, i); dma_pool_destroy(xhci->segment_pool); xhci->segment_pool = NULL; xhci_dbg_trace(xhci, trace_xhci_dbg_init, ""Freed segment pool""); dma_pool_destroy(xhci->device_pool); xhci->device_pool = NULL; xhci_dbg_trace(xhci, trace_xhci_dbg_init, ""Freed device context pool""); dma_pool_destroy(xhci->small_streams_pool); xhci->small_streams_pool = NULL; xhci_dbg_trace(xhci, trace_xhci_dbg_init, ""Freed small stream array pool""); dma_pool_destroy(xhci->medium_streams_pool); xhci->medium_streams_pool = NULL; xhci_dbg_trace(xhci, trace_xhci_dbg_init, ""Freed medium stream array pool""); if (xhci->dcbaa) dma_free_coherent(dev, sizeof(*xhci->dcbaa), xhci->dcbaa, xhci->dcbaa->dma); xhci->dcbaa = NULL; scratchpad_free(xhci); if (!xhci->rh_bw) goto no_bw; for (i = 0; i < num_ports; i++) { struct xhci_tt_bw_info *tt, *n; list_for_each_entry_safe(tt, n, &xhci->rh_bw[i].tts, tt_list) { list_del(&tt->tt_list); kfree(tt); } } no_bw: xhci->cmd_ring_reserved_trbs = 0; xhci->usb2_rhub.num_ports = 0; xhci->usb3_rhub.num_ports = 0; xhci->num_active_eps = 0; kfree(xhci->usb2_rhub.ports); kfree(xhci->usb3_rhub.ports); kfree(xhci->hw_ports); kfree(xhci->rh_bw); for (i = 0; i < xhci->num_port_caps; i++) kfree(xhci->port_caps[i].psi); kfree(xhci->port_caps); kfree(xhci->interrupters); xhci->num_port_caps = 0; xhci->usb2_rhub.ports = NULL; xhci->usb3_rhub.ports = NULL; xhci->hw_ports = NULL; xhci->rh_bw = NULL; xhci->port_caps = NULL; xhci->interrupters = NULL; xhci->page_size = 0; xhci->page_shift = 0; xhci->usb2_rhub.bus_state.bus_suspended = 0; xhci->usb3_rhub.bus_state.bus_suspended = 0; }"
645----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49948/bad/dev.c----qdisc_pkt_len_init,"static void qdisc_pkt_len_init(struct sk_buff *skb) { const struct skb_shared_info *shinfo = skb_shinfo(skb); qdisc_skb_cb(skb)->pkt_len = skb->len; if (shinfo->gso_size && skb_transport_header_was_set(skb)) { unsigned int hdr_len; u16 gso_segs = shinfo->gso_segs; hdr_len = skb_transport_header(skb) - skb_mac_header(skb); if (likely(shinfo->gso_type & (SKB_GSO_TCPV4 | SKB_GSO_TCPV6))) { const struct tcphdr *th; struct tcphdr _tcphdr; th = skb_header_pointer(skb, skb_transport_offset(skb), sizeof(_tcphdr), &_tcphdr); if (likely(th)) hdr_len += __tcp_hdrlen(th); } else if (shinfo->gso_type & SKB_GSO_UDP_L4) { struct udphdr _udphdr; if (skb_header_pointer(skb, skb_transport_offset(skb), sizeof(_udphdr), &_udphdr)) hdr_len += sizeof(struct udphdr); } <S2SV_StartVul> if (shinfo->gso_type & SKB_GSO_DODGY) <S2SV_EndVul> <S2SV_StartVul> gso_segs = DIV_ROUND_UP(skb->len - hdr_len, <S2SV_EndVul> <S2SV_StartVul> shinfo->gso_size); <S2SV_EndVul> qdisc_skb_cb(skb)->pkt_len += (gso_segs - 1) * hdr_len; } }","- if (shinfo->gso_type & SKB_GSO_DODGY)
- gso_segs = DIV_ROUND_UP(skb->len - hdr_len,
- shinfo->gso_size);
+ }
+ if (unlikely(shinfo->gso_type & SKB_GSO_DODGY)) {
+ int payload = skb->len - hdr_len;
+ if (payload <= 0)
+ return;
+ gso_segs = DIV_ROUND_UP(payload, shinfo->gso_size);
+ }
+ }
+ }","static void qdisc_pkt_len_init(struct sk_buff *skb) { const struct skb_shared_info *shinfo = skb_shinfo(skb); qdisc_skb_cb(skb)->pkt_len = skb->len; if (shinfo->gso_size && skb_transport_header_was_set(skb)) { unsigned int hdr_len; u16 gso_segs = shinfo->gso_segs; hdr_len = skb_transport_header(skb) - skb_mac_header(skb); if (likely(shinfo->gso_type & (SKB_GSO_TCPV4 | SKB_GSO_TCPV6))) { const struct tcphdr *th; struct tcphdr _tcphdr; th = skb_header_pointer(skb, skb_transport_offset(skb), sizeof(_tcphdr), &_tcphdr); if (likely(th)) hdr_len += __tcp_hdrlen(th); } else if (shinfo->gso_type & SKB_GSO_UDP_L4) { struct udphdr _udphdr; if (skb_header_pointer(skb, skb_transport_offset(skb), sizeof(_udphdr), &_udphdr)) hdr_len += sizeof(struct udphdr); } if (unlikely(shinfo->gso_type & SKB_GSO_DODGY)) { int payload = skb->len - hdr_len; if (payload <= 0) return; gso_segs = DIV_ROUND_UP(payload, shinfo->gso_size); } qdisc_skb_cb(skb)->pkt_len += (gso_segs - 1) * hdr_len; } }"
236----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46711/bad/pm_netlink.c----mptcp_pm_nl_add_addr_received,"static void mptcp_pm_nl_add_addr_received(struct mptcp_sock *msk) { struct mptcp_addr_info addrs[MPTCP_PM_ADDR_MAX]; struct sock *sk = (struct sock *)msk; unsigned int add_addr_accept_max; struct mptcp_addr_info remote; unsigned int subflows_max; bool sf_created = false; int i, nr; add_addr_accept_max = mptcp_pm_get_add_addr_accept_max(msk); subflows_max = mptcp_pm_get_subflows_max(msk); pr_debug(""accepted %d:%d remote family %d\n"", msk->pm.add_addr_accepted, add_addr_accept_max, msk->pm.remote.family); remote = msk->pm.remote; mptcp_pm_announce_addr(msk, &remote, true); mptcp_pm_nl_addr_send_ack(msk); if (lookup_subflow_by_daddr(&msk->conn_list, &remote)) return; if (!remote.port) remote.port = sk->sk_dport; nr = fill_local_addresses_vec(msk, &remote, addrs); if (nr == 0) return; spin_unlock_bh(&msk->pm.lock); for (i = 0; i < nr; i++) if (__mptcp_subflow_connect(sk, &addrs[i], &remote) == 0) sf_created = true; spin_lock_bh(&msk->pm.lock); if (sf_created) { <S2SV_StartVul> msk->pm.add_addr_accepted++; <S2SV_EndVul> if (msk->pm.add_addr_accepted >= add_addr_accept_max || msk->pm.subflows >= subflows_max) WRITE_ONCE(msk->pm.accept_addr, false); } }","- msk->pm.add_addr_accepted++;
+ if (remote.id)
+ msk->pm.add_addr_accepted++;","static void mptcp_pm_nl_add_addr_received(struct mptcp_sock *msk) { struct mptcp_addr_info addrs[MPTCP_PM_ADDR_MAX]; struct sock *sk = (struct sock *)msk; unsigned int add_addr_accept_max; struct mptcp_addr_info remote; unsigned int subflows_max; bool sf_created = false; int i, nr; add_addr_accept_max = mptcp_pm_get_add_addr_accept_max(msk); subflows_max = mptcp_pm_get_subflows_max(msk); pr_debug(""accepted %d:%d remote family %d\n"", msk->pm.add_addr_accepted, add_addr_accept_max, msk->pm.remote.family); remote = msk->pm.remote; mptcp_pm_announce_addr(msk, &remote, true); mptcp_pm_nl_addr_send_ack(msk); if (lookup_subflow_by_daddr(&msk->conn_list, &remote)) return; if (!remote.port) remote.port = sk->sk_dport; nr = fill_local_addresses_vec(msk, &remote, addrs); if (nr == 0) return; spin_unlock_bh(&msk->pm.lock); for (i = 0; i < nr; i++) if (__mptcp_subflow_connect(sk, &addrs[i], &remote) == 0) sf_created = true; spin_lock_bh(&msk->pm.lock); if (sf_created) { if (remote.id) msk->pm.add_addr_accepted++; if (msk->pm.add_addr_accepted >= add_addr_accept_max || msk->pm.subflows >= subflows_max) WRITE_ONCE(msk->pm.accept_addr, false); } }"
45----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-43913/bad/apple.c----apple_nvme_probe,"static int apple_nvme_probe(struct platform_device *pdev) { struct device *dev = &pdev->dev; struct apple_nvme *anv; int ret; anv = devm_kzalloc(dev, sizeof(*anv), GFP_KERNEL); if (!anv) <S2SV_StartVul> return -ENOMEM; <S2SV_EndVul> anv->dev = get_device(dev); anv->adminq.is_adminq = true; platform_set_drvdata(pdev, anv); ret = apple_nvme_attach_genpd(anv); if (ret < 0) { dev_err_probe(dev, ret, ""Failed to attach power domains""); goto put_dev; } if (dma_set_mask_and_coherent(dev, DMA_BIT_MASK(64))) { ret = -ENXIO; goto put_dev; } anv->irq = platform_get_irq(pdev, 0); if (anv->irq < 0) { ret = anv->irq; goto put_dev; } if (!anv->irq) { ret = -ENXIO; goto put_dev; } anv->mmio_coproc = devm_platform_ioremap_resource_byname(pdev, ""ans""); if (IS_ERR(anv->mmio_coproc)) { ret = PTR_ERR(anv->mmio_coproc); goto put_dev; } anv->mmio_nvme = devm_platform_ioremap_resource_byname(pdev, ""nvme""); if (IS_ERR(anv->mmio_nvme)) { ret = PTR_ERR(anv->mmio_nvme); goto put_dev; } anv->adminq.sq_db = anv->mmio_nvme + APPLE_ANS_LINEAR_ASQ_DB; anv->adminq.cq_db = anv->mmio_nvme + APPLE_ANS_ACQ_DB; anv->ioq.sq_db = anv->mmio_nvme + APPLE_ANS_LINEAR_IOSQ_DB; anv->ioq.cq_db = anv->mmio_nvme + APPLE_ANS_IOCQ_DB; anv->sart = devm_apple_sart_get(dev); if (IS_ERR(anv->sart)) { ret = dev_err_probe(dev, PTR_ERR(anv->sart), ""Failed to initialize SART""); goto put_dev; } anv->reset = devm_reset_control_array_get_exclusive(anv->dev); if (IS_ERR(anv->reset)) { ret = dev_err_probe(dev, PTR_ERR(anv->reset), ""Failed to get reset control""); goto put_dev; } INIT_WORK(&anv->ctrl.reset_work, apple_nvme_reset_work); INIT_WORK(&anv->remove_work, apple_nvme_remove_dead_ctrl_work); spin_lock_init(&anv->lock); ret = apple_nvme_queue_alloc(anv, &anv->adminq); if (ret) goto put_dev; ret = apple_nvme_queue_alloc(anv, &anv->ioq); if (ret) goto put_dev; anv->prp_page_pool = dmam_pool_create(""prp list page"", anv->dev, NVME_CTRL_PAGE_SIZE, NVME_CTRL_PAGE_SIZE, 0); if (!anv->prp_page_pool) { ret = -ENOMEM; goto put_dev; } anv->prp_small_pool = dmam_pool_create(""prp list 256"", anv->dev, 256, 256, 0); if (!anv->prp_small_pool) { ret = -ENOMEM; goto put_dev; } WARN_ON_ONCE(apple_nvme_iod_alloc_size() > PAGE_SIZE); anv->iod_mempool = mempool_create_kmalloc_pool(1, apple_nvme_iod_alloc_size()); if (!anv->iod_mempool) { ret = -ENOMEM; goto put_dev; } ret = devm_add_action_or_reset(anv->dev, devm_apple_nvme_mempool_destroy, anv->iod_mempool); if (ret) goto put_dev; ret = apple_nvme_alloc_tagsets(anv); if (ret) goto put_dev; ret = devm_request_irq(anv->dev, anv->irq, apple_nvme_irq, 0, ""nvme-apple"", anv); if (ret) { dev_err_probe(dev, ret, ""Failed to request IRQ""); goto put_dev; } anv->rtk = devm_apple_rtkit_init(dev, anv, NULL, 0, &apple_nvme_rtkit_ops); if (IS_ERR(anv->rtk)) { ret = dev_err_probe(dev, PTR_ERR(anv->rtk), ""Failed to initialize RTKit""); goto put_dev; } ret = nvme_init_ctrl(&anv->ctrl, anv->dev, &nvme_ctrl_ops, NVME_QUIRK_SKIP_CID_GEN | NVME_QUIRK_IDENTIFY_CNS); if (ret) { dev_err_probe(dev, ret, ""Failed to initialize nvme_ctrl""); goto put_dev; } anv->ctrl.admin_q = blk_mq_alloc_queue(&anv->admin_tagset, NULL, NULL); if (IS_ERR(anv->ctrl.admin_q)) { ret = -ENOMEM; <S2SV_StartVul> goto put_dev; <S2SV_EndVul> } nvme_reset_ctrl(&anv->ctrl); async_schedule(apple_nvme_async_probe, anv); return 0; <S2SV_StartVul> put_dev: <S2SV_EndVul> <S2SV_StartVul> put_device(anv->dev); <S2SV_EndVul> return ret; }","- return -ENOMEM;
- goto put_dev;
- put_dev:
- put_device(anv->dev);
+ static int apple_nvme_probe(struct platform_device *pdev)
+ {
+ struct apple_nvme *anv;
+ int ret;
+ anv = apple_nvme_alloc(pdev);
+ if (IS_ERR(anv))
+ return PTR_ERR(anv);
+ anv->ctrl.admin_q = NULL;
+ goto out_uninit_ctrl;
+ }
+ out_uninit_ctrl:
+ nvme_uninit_ctrl(&anv->ctrl);
+ nvme_put_ctrl(&anv->ctrl);
+ }","static int apple_nvme_probe(struct platform_device *pdev) { struct apple_nvme *anv; int ret; anv = apple_nvme_alloc(pdev); if (IS_ERR(anv)) return PTR_ERR(anv); anv->ctrl.admin_q = blk_mq_alloc_queue(&anv->admin_tagset, NULL, NULL); if (IS_ERR(anv->ctrl.admin_q)) { ret = -ENOMEM; anv->ctrl.admin_q = NULL; goto out_uninit_ctrl; } nvme_reset_ctrl(&anv->ctrl); async_schedule(apple_nvme_async_probe, anv); return 0; out_uninit_ctrl: nvme_uninit_ctrl(&anv->ctrl); nvme_put_ctrl(&anv->ctrl); return ret; }"
1060----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50302/bad/hid-core.c----*hid_alloc_report_buf,"u8 *hid_alloc_report_buf(struct hid_report *report, gfp_t flags) { u32 len = hid_report_len(report) + 7; <S2SV_StartVul> return kmalloc(len, flags); <S2SV_EndVul> }","- return kmalloc(len, flags);
+ return kzalloc(len, flags);","u8 *hid_alloc_report_buf(struct hid_report *report, gfp_t flags) { u32 len = hid_report_len(report) + 7; return kzalloc(len, flags); }"
292----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46766/bad/ice_lib.c----ice_vsi_cfg_def,"static int ice_vsi_cfg_def(struct ice_vsi *vsi) { struct device *dev = ice_pf_to_dev(vsi->back); struct ice_pf *pf = vsi->back; int ret; vsi->vsw = pf->first_sw; ret = ice_vsi_alloc_def(vsi, vsi->ch); if (ret) return ret; ret = ice_vsi_alloc_stat_arrays(vsi); if (ret) goto unroll_vsi_alloc; ice_alloc_fd_res(vsi); ret = ice_vsi_get_qs(vsi); if (ret) { dev_err(dev, ""Failed to allocate queues. vsi->idx = %d\n"", vsi->idx); goto unroll_vsi_alloc_stat; } ice_vsi_set_rss_params(vsi); ice_vsi_set_tc_cfg(vsi); ret = ice_vsi_init(vsi, vsi->flags); if (ret) goto unroll_get_qs; ice_vsi_init_vlan_ops(vsi); switch (vsi->type) { case ICE_VSI_CTRL: case ICE_VSI_PF: ret = ice_vsi_alloc_q_vectors(vsi); if (ret) goto unroll_vsi_init; ret = ice_vsi_alloc_rings(vsi); if (ret) goto unroll_vector_base; ret = ice_vsi_alloc_ring_stats(vsi); if (ret) goto unroll_vector_base; if (ice_is_xdp_ena_vsi(vsi)) { ret = ice_vsi_determine_xdp_res(vsi); if (ret) goto unroll_vector_base; ret = ice_prepare_xdp_rings(vsi, vsi->xdp_prog, ICE_XDP_CFG_PART); if (ret) goto unroll_vector_base; } ice_vsi_map_rings_to_vectors(vsi); <S2SV_StartVul> ice_vsi_set_napi_queues(vsi); <S2SV_EndVul> vsi->stat_offsets_loaded = false; if (vsi->type != ICE_VSI_CTRL) if (test_bit(ICE_FLAG_RSS_ENA, pf->flags)) { ice_vsi_cfg_rss_lut_key(vsi); ice_vsi_set_rss_flow_fld(vsi); } ice_init_arfs(vsi); break; case ICE_VSI_CHNL: if (test_bit(ICE_FLAG_RSS_ENA, pf->flags)) { ice_vsi_cfg_rss_lut_key(vsi); ice_vsi_set_rss_flow_fld(vsi); } break; case ICE_VSI_VF: ret = ice_vsi_alloc_q_vectors(vsi); if (ret) goto unroll_vsi_init; ret = ice_vsi_alloc_rings(vsi); if (ret) goto unroll_alloc_q_vector; ret = ice_vsi_alloc_ring_stats(vsi); if (ret) goto unroll_vector_base; vsi->stat_offsets_loaded = false; if (test_bit(ICE_FLAG_RSS_ENA, pf->flags)) { ice_vsi_cfg_rss_lut_key(vsi); ice_vsi_set_vf_rss_flow_fld(vsi); } break; case ICE_VSI_LB: ret = ice_vsi_alloc_rings(vsi); if (ret) goto unroll_vsi_init; ret = ice_vsi_alloc_ring_stats(vsi); if (ret) goto unroll_vector_base; break; default: ret = -EINVAL; goto unroll_vsi_init; } return 0; unroll_vector_base: unroll_alloc_q_vector: ice_vsi_free_q_vectors(vsi); unroll_vsi_init: ice_vsi_delete_from_hw(vsi); unroll_get_qs: ice_vsi_put_qs(vsi); unroll_vsi_alloc_stat: ice_vsi_free_stats(vsi); unroll_vsi_alloc: ice_vsi_free_arrays(vsi); return ret; }",- ice_vsi_set_napi_queues(vsi);,"static int ice_vsi_cfg_def(struct ice_vsi *vsi) { struct device *dev = ice_pf_to_dev(vsi->back); struct ice_pf *pf = vsi->back; int ret; vsi->vsw = pf->first_sw; ret = ice_vsi_alloc_def(vsi, vsi->ch); if (ret) return ret; ret = ice_vsi_alloc_stat_arrays(vsi); if (ret) goto unroll_vsi_alloc; ice_alloc_fd_res(vsi); ret = ice_vsi_get_qs(vsi); if (ret) { dev_err(dev, ""Failed to allocate queues. vsi->idx = %d\n"", vsi->idx); goto unroll_vsi_alloc_stat; } ice_vsi_set_rss_params(vsi); ice_vsi_set_tc_cfg(vsi); ret = ice_vsi_init(vsi, vsi->flags); if (ret) goto unroll_get_qs; ice_vsi_init_vlan_ops(vsi); switch (vsi->type) { case ICE_VSI_CTRL: case ICE_VSI_PF: ret = ice_vsi_alloc_q_vectors(vsi); if (ret) goto unroll_vsi_init; ret = ice_vsi_alloc_rings(vsi); if (ret) goto unroll_vector_base; ret = ice_vsi_alloc_ring_stats(vsi); if (ret) goto unroll_vector_base; if (ice_is_xdp_ena_vsi(vsi)) { ret = ice_vsi_determine_xdp_res(vsi); if (ret) goto unroll_vector_base; ret = ice_prepare_xdp_rings(vsi, vsi->xdp_prog, ICE_XDP_CFG_PART); if (ret) goto unroll_vector_base; } ice_vsi_map_rings_to_vectors(vsi); vsi->stat_offsets_loaded = false; if (vsi->type != ICE_VSI_CTRL) if (test_bit(ICE_FLAG_RSS_ENA, pf->flags)) { ice_vsi_cfg_rss_lut_key(vsi); ice_vsi_set_rss_flow_fld(vsi); } ice_init_arfs(vsi); break; case ICE_VSI_CHNL: if (test_bit(ICE_FLAG_RSS_ENA, pf->flags)) { ice_vsi_cfg_rss_lut_key(vsi); ice_vsi_set_rss_flow_fld(vsi); } break; case ICE_VSI_VF: ret = ice_vsi_alloc_q_vectors(vsi); if (ret) goto unroll_vsi_init; ret = ice_vsi_alloc_rings(vsi); if (ret) goto unroll_alloc_q_vector; ret = ice_vsi_alloc_ring_stats(vsi); if (ret) goto unroll_vector_base; vsi->stat_offsets_loaded = false; if (test_bit(ICE_FLAG_RSS_ENA, pf->flags)) { ice_vsi_cfg_rss_lut_key(vsi); ice_vsi_set_vf_rss_flow_fld(vsi); } break; case ICE_VSI_LB: ret = ice_vsi_alloc_rings(vsi); if (ret) goto unroll_vsi_init; ret = ice_vsi_alloc_ring_stats(vsi); if (ret) goto unroll_vector_base; break; default: ret = -EINVAL; goto unroll_vsi_init; } return 0; unroll_vector_base: unroll_alloc_q_vector: ice_vsi_free_q_vectors(vsi); unroll_vsi_init: ice_vsi_delete_from_hw(vsi); unroll_get_qs: ice_vsi_put_qs(vsi); unroll_vsi_alloc_stat: ice_vsi_free_stats(vsi); unroll_vsi_alloc: ice_vsi_free_arrays(vsi); return ret; }"
425----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47666/bad/pm8001_sas.c----pm8001_phy_control,"int pm8001_phy_control(struct asd_sas_phy *sas_phy, enum phy_func func, void *funcdata) { int rc = 0, phy_id = sas_phy->id; struct pm8001_hba_info *pm8001_ha = NULL; struct sas_phy_linkrates *rates; struct pm8001_phy *phy; DECLARE_COMPLETION_ONSTACK(completion); unsigned long flags; pm8001_ha = sas_phy->ha->lldd_ha; phy = &pm8001_ha->phy[phy_id]; <S2SV_StartVul> pm8001_ha->phy[phy_id].enable_completion = &completion; <S2SV_EndVul> if (PM8001_CHIP_DISP->fatal_errors(pm8001_ha)) { pm8001_dbg(pm8001_ha, FAIL, ""Phy control failed due to fatal errors\n""); return -EFAULT; } switch (func) { case PHY_FUNC_SET_LINK_RATE: rates = funcdata; if (rates->minimum_linkrate) { pm8001_ha->phy[phy_id].minimum_linkrate = rates->minimum_linkrate; } if (rates->maximum_linkrate) { pm8001_ha->phy[phy_id].maximum_linkrate = rates->maximum_linkrate; } if (pm8001_ha->phy[phy_id].phy_state == PHY_LINK_DISABLE) { PM8001_CHIP_DISP->phy_start_req(pm8001_ha, phy_id); wait_for_completion(&completion); } PM8001_CHIP_DISP->phy_ctl_req(pm8001_ha, phy_id, PHY_LINK_RESET); break; case PHY_FUNC_HARD_RESET: if (pm8001_ha->phy[phy_id].phy_state == PHY_LINK_DISABLE) { PM8001_CHIP_DISP->phy_start_req(pm8001_ha, phy_id); wait_for_completion(&completion); } PM8001_CHIP_DISP->phy_ctl_req(pm8001_ha, phy_id, PHY_HARD_RESET); break; case PHY_FUNC_LINK_RESET: if (pm8001_ha->phy[phy_id].phy_state == PHY_LINK_DISABLE) { PM8001_CHIP_DISP->phy_start_req(pm8001_ha, phy_id); wait_for_completion(&completion); } PM8001_CHIP_DISP->phy_ctl_req(pm8001_ha, phy_id, PHY_LINK_RESET); break; case PHY_FUNC_RELEASE_SPINUP_HOLD: PM8001_CHIP_DISP->phy_ctl_req(pm8001_ha, phy_id, PHY_LINK_RESET); break; case PHY_FUNC_DISABLE: if (pm8001_ha->chip_id != chip_8001) { if (pm8001_ha->phy[phy_id].phy_state == PHY_STATE_LINK_UP_SPCV) { sas_phy_disconnected(&phy->sas_phy); sas_notify_phy_event(&phy->sas_phy, PHYE_LOSS_OF_SIGNAL, GFP_KERNEL); phy->phy_attached = 0; } } else { if (pm8001_ha->phy[phy_id].phy_state == PHY_STATE_LINK_UP_SPC) { sas_phy_disconnected(&phy->sas_phy); sas_notify_phy_event(&phy->sas_phy, PHYE_LOSS_OF_SIGNAL, GFP_KERNEL); phy->phy_attached = 0; } } PM8001_CHIP_DISP->phy_stop_req(pm8001_ha, phy_id); break; case PHY_FUNC_GET_EVENTS: spin_lock_irqsave(&pm8001_ha->lock, flags); if (pm8001_ha->chip_id == chip_8001) { if (-1 == pm8001_bar4_shift(pm8001_ha, (phy_id < 4) ? 0x30000 : 0x40000)) { spin_unlock_irqrestore(&pm8001_ha->lock, flags); return -EINVAL; } } { struct sas_phy *phy = sas_phy->phy; u32 __iomem *qp = pm8001_ha->io_mem[2].memvirtaddr + 0x1034 + (0x4000 * (phy_id & 3)); phy->invalid_dword_count = readl(qp); phy->running_disparity_error_count = readl(&qp[1]); phy->loss_of_dword_sync_count = readl(&qp[3]); phy->phy_reset_problem_count = readl(&qp[4]); } if (pm8001_ha->chip_id == chip_8001) pm8001_bar4_shift(pm8001_ha, 0); spin_unlock_irqrestore(&pm8001_ha->lock, flags); return 0; default: pm8001_dbg(pm8001_ha, DEVIO, ""func 0x%x\n"", func); rc = -EOPNOTSUPP; } msleep(300); return rc; }","- pm8001_ha->phy[phy_id].enable_completion = &completion;
+ pm8001_ha->phy[phy_id].enable_completion = &completion;
+ pm8001_ha->phy[phy_id].enable_completion = &completion;
+ pm8001_ha->phy[phy_id].enable_completion = &completion;","int pm8001_phy_control(struct asd_sas_phy *sas_phy, enum phy_func func, void *funcdata) { int rc = 0, phy_id = sas_phy->id; struct pm8001_hba_info *pm8001_ha = NULL; struct sas_phy_linkrates *rates; struct pm8001_phy *phy; DECLARE_COMPLETION_ONSTACK(completion); unsigned long flags; pm8001_ha = sas_phy->ha->lldd_ha; phy = &pm8001_ha->phy[phy_id]; if (PM8001_CHIP_DISP->fatal_errors(pm8001_ha)) { pm8001_dbg(pm8001_ha, FAIL, ""Phy control failed due to fatal errors\n""); return -EFAULT; } switch (func) { case PHY_FUNC_SET_LINK_RATE: rates = funcdata; if (rates->minimum_linkrate) { pm8001_ha->phy[phy_id].minimum_linkrate = rates->minimum_linkrate; } if (rates->maximum_linkrate) { pm8001_ha->phy[phy_id].maximum_linkrate = rates->maximum_linkrate; } if (pm8001_ha->phy[phy_id].phy_state == PHY_LINK_DISABLE) { pm8001_ha->phy[phy_id].enable_completion = &completion; PM8001_CHIP_DISP->phy_start_req(pm8001_ha, phy_id); wait_for_completion(&completion); } PM8001_CHIP_DISP->phy_ctl_req(pm8001_ha, phy_id, PHY_LINK_RESET); break; case PHY_FUNC_HARD_RESET: if (pm8001_ha->phy[phy_id].phy_state == PHY_LINK_DISABLE) { pm8001_ha->phy[phy_id].enable_completion = &completion; PM8001_CHIP_DISP->phy_start_req(pm8001_ha, phy_id); wait_for_completion(&completion); } PM8001_CHIP_DISP->phy_ctl_req(pm8001_ha, phy_id, PHY_HARD_RESET); break; case PHY_FUNC_LINK_RESET: if (pm8001_ha->phy[phy_id].phy_state == PHY_LINK_DISABLE) { pm8001_ha->phy[phy_id].enable_completion = &completion; PM8001_CHIP_DISP->phy_start_req(pm8001_ha, phy_id); wait_for_completion(&completion); } PM8001_CHIP_DISP->phy_ctl_req(pm8001_ha, phy_id, PHY_LINK_RESET); break; case PHY_FUNC_RELEASE_SPINUP_HOLD: PM8001_CHIP_DISP->phy_ctl_req(pm8001_ha, phy_id, PHY_LINK_RESET); break; case PHY_FUNC_DISABLE: if (pm8001_ha->chip_id != chip_8001) { if (pm8001_ha->phy[phy_id].phy_state == PHY_STATE_LINK_UP_SPCV) { sas_phy_disconnected(&phy->sas_phy); sas_notify_phy_event(&phy->sas_phy, PHYE_LOSS_OF_SIGNAL, GFP_KERNEL); phy->phy_attached = 0; } } else { if (pm8001_ha->phy[phy_id].phy_state == PHY_STATE_LINK_UP_SPC) { sas_phy_disconnected(&phy->sas_phy); sas_notify_phy_event(&phy->sas_phy, PHYE_LOSS_OF_SIGNAL, GFP_KERNEL); phy->phy_attached = 0; } } PM8001_CHIP_DISP->phy_stop_req(pm8001_ha, phy_id); break; case PHY_FUNC_GET_EVENTS: spin_lock_irqsave(&pm8001_ha->lock, flags); if (pm8001_ha->chip_id == chip_8001) { if (-1 == pm8001_bar4_shift(pm8001_ha, (phy_id < 4) ? 0x30000 : 0x40000)) { spin_unlock_irqrestore(&pm8001_ha->lock, flags); return -EINVAL; } } { struct sas_phy *phy = sas_phy->phy; u32 __iomem *qp = pm8001_ha->io_mem[2].memvirtaddr + 0x1034 + (0x4000 * (phy_id & 3)); phy->invalid_dword_count = readl(qp); phy->running_disparity_error_count = readl(&qp[1]); phy->loss_of_dword_sync_count = readl(&qp[3]); phy->phy_reset_problem_count = readl(&qp[4]); } if (pm8001_ha->chip_id == chip_8001) pm8001_bar4_shift(pm8001_ha, 0); spin_unlock_irqrestore(&pm8001_ha->lock, flags); return 0; default: pm8001_dbg(pm8001_ha, DEVIO, ""func 0x%x\n"", func); rc = -EOPNOTSUPP; } msleep(300); return rc; }"
771----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50047/bad/smb2ops.c----crypt_message,"crypt_message(struct TCP_Server_Info *server, int num_rqst, struct smb_rqst *rqst, int enc) { struct smb2_transform_hdr *tr_hdr = (struct smb2_transform_hdr *)rqst[0].rq_iov[0].iov_base; unsigned int assoc_data_len = sizeof(struct smb2_transform_hdr) - 20; int rc = 0; struct scatterlist *sg; u8 sign[SMB2_SIGNATURE_SIZE] = {}; u8 key[SMB3_ENC_DEC_KEY_SIZE]; struct aead_request *req; u8 *iv; <S2SV_StartVul> DECLARE_CRYPTO_WAIT(wait); <S2SV_EndVul> <S2SV_StartVul> struct crypto_aead *tfm; <S2SV_EndVul> unsigned int crypt_len = le32_to_cpu(tr_hdr->OriginalMessageSize); void *creq; size_t sensitive_size; rc = smb2_get_enc_key(server, le64_to_cpu(tr_hdr->SessionId), enc, key); if (rc) { cifs_server_dbg(FYI, ""%s: Could not get %scryption key. sid: 0x%llx\n"", __func__, enc ? ""en"" : ""de"", le64_to_cpu(tr_hdr->SessionId)); return rc; <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> rc = smb3_crypto_aead_allocate(server); <S2SV_EndVul> <S2SV_StartVul> if (rc) { <S2SV_EndVul> <S2SV_StartVul> cifs_server_dbg(VFS, ""%s: crypto alloc failed\n"", __func__); <S2SV_EndVul> <S2SV_StartVul> return rc; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> tfm = enc ? server->secmech.enc : server->secmech.dec; <S2SV_EndVul> if ((server->cipher_type == SMB2_ENCRYPTION_AES256_CCM) || (server->cipher_type == SMB2_ENCRYPTION_AES256_GCM)) rc = crypto_aead_setkey(tfm, key, SMB3_GCM256_CRYPTKEY_SIZE); else rc = crypto_aead_setkey(tfm, key, SMB3_GCM128_CRYPTKEY_SIZE); if (rc) { cifs_server_dbg(VFS, ""%s: Failed to set aead key %d\n"", __func__, rc); return rc; } rc = crypto_aead_setauthsize(tfm, SMB2_SIGNATURE_SIZE); if (rc) { cifs_server_dbg(VFS, ""%s: Failed to set authsize %d\n"", __func__, rc); return rc; } creq = smb2_get_aead_req(tfm, rqst, num_rqst, sign, &iv, &req, &sg, &sensitive_size); if (IS_ERR(creq)) return PTR_ERR(creq); if (!enc) { memcpy(sign, &tr_hdr->Signature, SMB2_SIGNATURE_SIZE); crypt_len += SMB2_SIGNATURE_SIZE; } if ((server->cipher_type == SMB2_ENCRYPTION_AES128_GCM) || (server->cipher_type == SMB2_ENCRYPTION_AES256_GCM)) memcpy(iv, (char *)tr_hdr->Nonce, SMB3_AES_GCM_NONCE); else { iv[0] = 3; memcpy(iv + 1, (char *)tr_hdr->Nonce, SMB3_AES_CCM_NONCE); } aead_request_set_tfm(req, tfm); aead_request_set_crypt(req, sg, sg, crypt_len, iv); aead_request_set_ad(req, assoc_data_len); <S2SV_StartVul> aead_request_set_callback(req, CRYPTO_TFM_REQ_MAY_BACKLOG, <S2SV_EndVul> <S2SV_StartVul> crypto_req_done, &wait); <S2SV_EndVul> <S2SV_StartVul> rc = crypto_wait_req(enc ? crypto_aead_encrypt(req) <S2SV_EndVul> <S2SV_StartVul> : crypto_aead_decrypt(req), &wait); <S2SV_EndVul> if (!rc && enc) memcpy(&tr_hdr->Signature, sign, SMB2_SIGNATURE_SIZE); kvfree_sensitive(creq, sensitive_size); return rc; }","- DECLARE_CRYPTO_WAIT(wait);
- struct crypto_aead *tfm;
- }
- rc = smb3_crypto_aead_allocate(server);
- if (rc) {
- cifs_server_dbg(VFS, ""%s: crypto alloc failed\n"", __func__);
- return rc;
- }
- tfm = enc ? server->secmech.enc : server->secmech.dec;
- aead_request_set_callback(req, CRYPTO_TFM_REQ_MAY_BACKLOG,
- crypto_req_done, &wait);
- rc = crypto_wait_req(enc ? crypto_aead_encrypt(req)
- : crypto_aead_decrypt(req), &wait);
+ }
+ (server->cipher_type == SMB2_ENCRYPTION_AES256_GCM))
+ rc = enc ? crypto_aead_encrypt(req) : crypto_aead_decrypt(req);","crypt_message(struct TCP_Server_Info *server, int num_rqst, struct smb_rqst *rqst, int enc, struct crypto_aead *tfm) { struct smb2_transform_hdr *tr_hdr = (struct smb2_transform_hdr *)rqst[0].rq_iov[0].iov_base; unsigned int assoc_data_len = sizeof(struct smb2_transform_hdr) - 20; int rc = 0; struct scatterlist *sg; u8 sign[SMB2_SIGNATURE_SIZE] = {}; u8 key[SMB3_ENC_DEC_KEY_SIZE]; struct aead_request *req; u8 *iv; unsigned int crypt_len = le32_to_cpu(tr_hdr->OriginalMessageSize); void *creq; size_t sensitive_size; rc = smb2_get_enc_key(server, le64_to_cpu(tr_hdr->SessionId), enc, key); if (rc) { cifs_server_dbg(FYI, ""%s: Could not get %scryption key. sid: 0x%llx\n"", __func__, enc ? ""en"" : ""de"", le64_to_cpu(tr_hdr->SessionId)); return rc; } if ((server->cipher_type == SMB2_ENCRYPTION_AES256_CCM) || (server->cipher_type == SMB2_ENCRYPTION_AES256_GCM)) rc = crypto_aead_setkey(tfm, key, SMB3_GCM256_CRYPTKEY_SIZE); else rc = crypto_aead_setkey(tfm, key, SMB3_GCM128_CRYPTKEY_SIZE); if (rc) { cifs_server_dbg(VFS, ""%s: Failed to set aead key %d\n"", __func__, rc); return rc; } rc = crypto_aead_setauthsize(tfm, SMB2_SIGNATURE_SIZE); if (rc) { cifs_server_dbg(VFS, ""%s: Failed to set authsize %d\n"", __func__, rc); return rc; } creq = smb2_get_aead_req(tfm, rqst, num_rqst, sign, &iv, &req, &sg, &sensitive_size); if (IS_ERR(creq)) return PTR_ERR(creq); if (!enc) { memcpy(sign, &tr_hdr->Signature, SMB2_SIGNATURE_SIZE); crypt_len += SMB2_SIGNATURE_SIZE; } if ((server->cipher_type == SMB2_ENCRYPTION_AES128_GCM) || (server->cipher_type == SMB2_ENCRYPTION_AES256_GCM)) memcpy(iv, (char *)tr_hdr->Nonce, SMB3_AES_GCM_NONCE); else { iv[0] = 3; memcpy(iv + 1, (char *)tr_hdr->Nonce, SMB3_AES_CCM_NONCE); } aead_request_set_tfm(req, tfm); aead_request_set_crypt(req, sg, sg, crypt_len, iv); aead_request_set_ad(req, assoc_data_len); rc = enc ? crypto_aead_encrypt(req) : crypto_aead_decrypt(req); if (!rc && enc) memcpy(&tr_hdr->Signature, sign, SMB2_SIGNATURE_SIZE); kvfree_sensitive(creq, sensitive_size); return rc; }"
627----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49933/bad/blk-iocost.c----ioc_forgive_debts,"static void ioc_forgive_debts(struct ioc *ioc, u64 usage_us_sum, int nr_debtors, struct ioc_now *now) { struct ioc_gq *iocg; <S2SV_StartVul> u64 dur, usage_pct, nr_cycles; <S2SV_EndVul> if (!nr_debtors) { ioc->dfgv_period_at = now->now; ioc->dfgv_period_rem = 0; ioc->dfgv_usage_us_sum = 0; return; } if (ioc->busy_level > 0) usage_us_sum = max_t(u64, usage_us_sum, ioc->period_us); ioc->dfgv_usage_us_sum += usage_us_sum; if (time_before64(now->now, ioc->dfgv_period_at + DFGV_PERIOD)) return; dur = now->now - ioc->dfgv_period_at; usage_pct = div64_u64(100 * ioc->dfgv_usage_us_sum, dur); ioc->dfgv_period_at = now->now; ioc->dfgv_usage_us_sum = 0; if (usage_pct > DFGV_USAGE_PCT) { ioc->dfgv_period_rem = 0; return; } nr_cycles = dur + ioc->dfgv_period_rem; ioc->dfgv_period_rem = do_div(nr_cycles, DFGV_PERIOD); list_for_each_entry(iocg, &ioc->active_iocgs, active_list) { u64 __maybe_unused old_debt, __maybe_unused old_delay; if (!iocg->abs_vdebt && !iocg->delay) continue; spin_lock(&iocg->waitq.lock); old_debt = iocg->abs_vdebt; old_delay = iocg->delay; if (iocg->abs_vdebt) <S2SV_StartVul> iocg->abs_vdebt = iocg->abs_vdebt >> nr_cycles ?: 1; <S2SV_EndVul> if (iocg->delay) <S2SV_StartVul> iocg->delay = iocg->delay >> nr_cycles ?: 1; <S2SV_EndVul> iocg_kick_waitq(iocg, true, now); TRACE_IOCG_PATH(iocg_forgive_debt, iocg, now, usage_pct, old_debt, iocg->abs_vdebt, old_delay, iocg->delay); spin_unlock(&iocg->waitq.lock); } }","- u64 dur, usage_pct, nr_cycles;
- iocg->abs_vdebt = iocg->abs_vdebt >> nr_cycles ?: 1;
- iocg->delay = iocg->delay >> nr_cycles ?: 1;
+ u64 dur, usage_pct, nr_cycles, nr_cycles_shift;
+ nr_cycles_shift = min_t(u64, nr_cycles, BITS_PER_LONG - 1);
+ iocg->abs_vdebt = iocg->abs_vdebt >> nr_cycles_shift ?: 1;
+ iocg->delay = iocg->delay >> nr_cycles_shift ?: 1;","static void ioc_forgive_debts(struct ioc *ioc, u64 usage_us_sum, int nr_debtors, struct ioc_now *now) { struct ioc_gq *iocg; u64 dur, usage_pct, nr_cycles, nr_cycles_shift; if (!nr_debtors) { ioc->dfgv_period_at = now->now; ioc->dfgv_period_rem = 0; ioc->dfgv_usage_us_sum = 0; return; } if (ioc->busy_level > 0) usage_us_sum = max_t(u64, usage_us_sum, ioc->period_us); ioc->dfgv_usage_us_sum += usage_us_sum; if (time_before64(now->now, ioc->dfgv_period_at + DFGV_PERIOD)) return; dur = now->now - ioc->dfgv_period_at; usage_pct = div64_u64(100 * ioc->dfgv_usage_us_sum, dur); ioc->dfgv_period_at = now->now; ioc->dfgv_usage_us_sum = 0; if (usage_pct > DFGV_USAGE_PCT) { ioc->dfgv_period_rem = 0; return; } nr_cycles = dur + ioc->dfgv_period_rem; ioc->dfgv_period_rem = do_div(nr_cycles, DFGV_PERIOD); list_for_each_entry(iocg, &ioc->active_iocgs, active_list) { u64 __maybe_unused old_debt, __maybe_unused old_delay; if (!iocg->abs_vdebt && !iocg->delay) continue; spin_lock(&iocg->waitq.lock); old_debt = iocg->abs_vdebt; old_delay = iocg->delay; nr_cycles_shift = min_t(u64, nr_cycles, BITS_PER_LONG - 1); if (iocg->abs_vdebt) iocg->abs_vdebt = iocg->abs_vdebt >> nr_cycles_shift ?: 1; if (iocg->delay) iocg->delay = iocg->delay >> nr_cycles_shift ?: 1; iocg_kick_waitq(iocg, true, now); TRACE_IOCG_PATH(iocg_forgive_debt, iocg, now, usage_pct, old_debt, iocg->abs_vdebt, old_delay, iocg->delay); spin_unlock(&iocg->waitq.lock); } }"
357----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46818/bad/gpio_service.c----dal_gpio_service_lock,"enum gpio_result dal_gpio_service_lock( struct gpio_service *service, enum gpio_id id, uint32_t en) { <S2SV_StartVul> if (!service->busyness[id]) { <S2SV_EndVul> ASSERT_CRITICAL(false); return GPIO_RESULT_OPEN_FAILED; } set_pin_busy(service, id, en); return GPIO_RESULT_OK; }","- if (!service->busyness[id]) {
+ if (id != GPIO_ID_UNKNOWN && !service->busyness[id]) {","enum gpio_result dal_gpio_service_lock( struct gpio_service *service, enum gpio_id id, uint32_t en) { if (id != GPIO_ID_UNKNOWN && !service->busyness[id]) { ASSERT_CRITICAL(false); return GPIO_RESULT_OPEN_FAILED; } set_pin_busy(service, id, en); return GPIO_RESULT_OK; }"
1397----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56722/bad/hns_roce_srq.c----free_srqc,"static void free_srqc(struct hns_roce_dev *hr_dev, struct hns_roce_srq *srq) { struct hns_roce_srq_table *srq_table = &hr_dev->srq_table; int ret; ret = hns_roce_destroy_hw_ctx(hr_dev, HNS_ROCE_CMD_DESTROY_SRQ, srq->srqn); if (ret) <S2SV_StartVul> dev_err(hr_dev->dev, ""DESTROY_SRQ failed (%d) for SRQN %06lx\n"", <S2SV_EndVul> <S2SV_StartVul> ret, srq->srqn); <S2SV_EndVul> xa_erase_irq(&srq_table->xa, srq->srqn); if (refcount_dec_and_test(&srq->refcount)) complete(&srq->free); wait_for_completion(&srq->free); hns_roce_table_put(hr_dev, &srq_table->table, srq->srqn); }","- dev_err(hr_dev->dev, ""DESTROY_SRQ failed (%d) for SRQN %06lx\n"",
- ret, srq->srqn);
+ dev_err_ratelimited(hr_dev->dev, ""DESTROY_SRQ failed (%d) for SRQN %06lx\n"",
+ ret, srq->srqn);","static void free_srqc(struct hns_roce_dev *hr_dev, struct hns_roce_srq *srq) { struct hns_roce_srq_table *srq_table = &hr_dev->srq_table; int ret; ret = hns_roce_destroy_hw_ctx(hr_dev, HNS_ROCE_CMD_DESTROY_SRQ, srq->srqn); if (ret) dev_err_ratelimited(hr_dev->dev, ""DESTROY_SRQ failed (%d) for SRQN %06lx\n"", ret, srq->srqn); xa_erase_irq(&srq_table->xa, srq->srqn); if (refcount_dec_and_test(&srq->refcount)) complete(&srq->free); wait_for_completion(&srq->free); hns_roce_table_put(hr_dev, &srq_table->table, srq->srqn); }"
233----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46709/bad/vmwgfx_stdu.c----vmw_stdu_bo_populate_update_cpu,"vmw_stdu_bo_populate_update_cpu(struct vmw_du_update_plane *update, void *cmd, struct drm_rect *bb) { struct vmw_du_update_plane_buffer *bo_update; struct vmw_screen_target_display_unit *stdu; struct vmw_framebuffer_bo *vfbbo; struct vmw_diff_cpy diff = VMW_CPU_BLIT_DIFF_INITIALIZER(0); struct vmw_stdu_update_gb_image *cmd_img = cmd; struct vmw_stdu_update *cmd_update; <S2SV_StartVul> struct ttm_buffer_object *src_bo, *dst_bo; <S2SV_EndVul> u32 src_offset, dst_offset; s32 src_pitch, dst_pitch; s32 width, height; bo_update = container_of(update, typeof(*bo_update), base); stdu = container_of(update->du, typeof(*stdu), base); vfbbo = container_of(update->vfb, typeof(*vfbbo), base); width = bb->x2 - bb->x1; height = bb->y2 - bb->y1; diff.cpp = stdu->cpp; <S2SV_StartVul> dst_bo = &stdu->display_srf->res.guest_memory_bo->tbo; <S2SV_EndVul> dst_pitch = stdu->display_srf->metadata.base_size.width * stdu->cpp; dst_offset = bb->y1 * dst_pitch + bb->x1 * stdu->cpp; <S2SV_StartVul> src_bo = &vfbbo->buffer->tbo; <S2SV_EndVul> src_pitch = update->vfb->base.pitches[0]; src_offset = bo_update->fb_top * src_pitch + bo_update->fb_left * stdu->cpp; (void) vmw_bo_cpu_blit(dst_bo, dst_offset, dst_pitch, src_bo, src_offset, src_pitch, width * stdu->cpp, height, &diff); if (drm_rect_visible(&diff.rect)) { SVGA3dBox *box = &cmd_img->body.box; cmd_img->header.id = SVGA_3D_CMD_UPDATE_GB_IMAGE; cmd_img->header.size = sizeof(cmd_img->body); cmd_img->body.image.sid = stdu->display_srf->res.id; cmd_img->body.image.face = 0; cmd_img->body.image.mipmap = 0; box->x = diff.rect.x1; box->y = diff.rect.y1; box->z = 0; box->w = drm_rect_width(&diff.rect); box->h = drm_rect_height(&diff.rect); box->d = 1; cmd_update = (struct vmw_stdu_update *)&cmd_img[1]; vmw_stdu_populate_update(cmd_update, stdu->base.unit, diff.rect.x1, diff.rect.x2, diff.rect.y1, diff.rect.y2); return sizeof(*cmd_img) + sizeof(*cmd_update); } return 0; }","- struct ttm_buffer_object *src_bo, *dst_bo;
- dst_bo = &stdu->display_srf->res.guest_memory_bo->tbo;
- src_bo = &vfbbo->buffer->tbo;
+ struct vmw_bo *src_bo, *dst_bo;
+ dst_bo = stdu->display_srf->res.guest_memory_bo;
+ src_bo = vfbbo->buffer;","vmw_stdu_bo_populate_update_cpu(struct vmw_du_update_plane *update, void *cmd, struct drm_rect *bb) { struct vmw_du_update_plane_buffer *bo_update; struct vmw_screen_target_display_unit *stdu; struct vmw_framebuffer_bo *vfbbo; struct vmw_diff_cpy diff = VMW_CPU_BLIT_DIFF_INITIALIZER(0); struct vmw_stdu_update_gb_image *cmd_img = cmd; struct vmw_stdu_update *cmd_update; struct vmw_bo *src_bo, *dst_bo; u32 src_offset, dst_offset; s32 src_pitch, dst_pitch; s32 width, height; bo_update = container_of(update, typeof(*bo_update), base); stdu = container_of(update->du, typeof(*stdu), base); vfbbo = container_of(update->vfb, typeof(*vfbbo), base); width = bb->x2 - bb->x1; height = bb->y2 - bb->y1; diff.cpp = stdu->cpp; dst_bo = stdu->display_srf->res.guest_memory_bo; dst_pitch = stdu->display_srf->metadata.base_size.width * stdu->cpp; dst_offset = bb->y1 * dst_pitch + bb->x1 * stdu->cpp; src_bo = vfbbo->buffer; src_pitch = update->vfb->base.pitches[0]; src_offset = bo_update->fb_top * src_pitch + bo_update->fb_left * stdu->cpp; (void) vmw_bo_cpu_blit(dst_bo, dst_offset, dst_pitch, src_bo, src_offset, src_pitch, width * stdu->cpp, height, &diff); if (drm_rect_visible(&diff.rect)) { SVGA3dBox *box = &cmd_img->body.box; cmd_img->header.id = SVGA_3D_CMD_UPDATE_GB_IMAGE; cmd_img->header.size = sizeof(cmd_img->body); cmd_img->body.image.sid = stdu->display_srf->res.id; cmd_img->body.image.face = 0; cmd_img->body.image.mipmap = 0; box->x = diff.rect.x1; box->y = diff.rect.y1; box->z = 0; box->w = drm_rect_width(&diff.rect); box->h = drm_rect_height(&diff.rect); box->d = 1; cmd_update = (struct vmw_stdu_update *)&cmd_img[1]; vmw_stdu_populate_update(cmd_update, stdu->base.unit, diff.rect.x1, diff.rect.x2, diff.rect.y1, diff.rect.y2); return sizeof(*cmd_img) + sizeof(*cmd_update); } return 0; }"
1120----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53092/bad/virtio_pci_modern.c----vp_modern_avq_cleanup,"static void vp_modern_avq_cleanup(struct virtio_device *vdev) { struct virtio_pci_device *vp_dev = to_vp_device(vdev); struct virtio_admin_cmd *cmd; struct virtqueue *vq; if (!virtio_has_feature(vdev, VIRTIO_F_ADMIN_VQ)) return; <S2SV_StartVul> vq = vp_dev->vqs[vp_dev->admin_vq.vq_index]->vq; <S2SV_EndVul> if (!vq) return; while ((cmd = virtqueue_detach_unused_buf(vq))) { cmd->ret = -EIO; complete(&cmd->completion); } }","- vq = vp_dev->vqs[vp_dev->admin_vq.vq_index]->vq;
+ vq = vp_dev->admin_vq.info->vq;","static void vp_modern_avq_cleanup(struct virtio_device *vdev) { struct virtio_pci_device *vp_dev = to_vp_device(vdev); struct virtio_admin_cmd *cmd; struct virtqueue *vq; if (!virtio_has_feature(vdev, VIRTIO_F_ADMIN_VQ)) return; vq = vp_dev->admin_vq.info->vq; if (!vq) return; while ((cmd = virtqueue_detach_unused_buf(vq))) { cmd->ret = -EIO; complete(&cmd->completion); } }"
598----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49920/bad/dce112_resource.c----bw_calcs_data_update_from_pplib,"static void bw_calcs_data_update_from_pplib(struct dc *dc) { struct dm_pp_clock_levels_with_latency eng_clks = {0}; struct dm_pp_clock_levels_with_latency mem_clks = {0}; struct dm_pp_wm_sets_with_clock_ranges clk_ranges = {0}; struct dm_pp_clock_levels clks = {0}; int memory_type_multiplier = MEMORY_TYPE_MULTIPLIER_CZ; <S2SV_StartVul> if (dc->bw_vbios && dc->bw_vbios->memory_type == bw_def_hbm) <S2SV_EndVul> memory_type_multiplier = MEMORY_TYPE_HBM; if (!dm_pp_get_clock_levels_by_type_with_latency( dc->ctx, DM_PP_CLOCK_TYPE_ENGINE_CLK, &eng_clks)) { dm_pp_get_clock_levels_by_type( dc->ctx, DM_PP_CLOCK_TYPE_ENGINE_CLK, &clks); dc->bw_vbios->high_sclk = bw_frc_to_fixed( clks.clocks_in_khz[clks.num_levels-1], 1000); dc->bw_vbios->mid1_sclk = bw_frc_to_fixed( clks.clocks_in_khz[clks.num_levels/8], 1000); dc->bw_vbios->mid2_sclk = bw_frc_to_fixed( clks.clocks_in_khz[clks.num_levels*2/8], 1000); dc->bw_vbios->mid3_sclk = bw_frc_to_fixed( clks.clocks_in_khz[clks.num_levels*3/8], 1000); dc->bw_vbios->mid4_sclk = bw_frc_to_fixed( clks.clocks_in_khz[clks.num_levels*4/8], 1000); dc->bw_vbios->mid5_sclk = bw_frc_to_fixed( clks.clocks_in_khz[clks.num_levels*5/8], 1000); dc->bw_vbios->mid6_sclk = bw_frc_to_fixed( clks.clocks_in_khz[clks.num_levels*6/8], 1000); dc->bw_vbios->low_sclk = bw_frc_to_fixed( clks.clocks_in_khz[0], 1000); dm_pp_get_clock_levels_by_type( dc->ctx, DM_PP_CLOCK_TYPE_MEMORY_CLK, &clks); dc->bw_vbios->low_yclk = bw_frc_to_fixed( clks.clocks_in_khz[0] * memory_type_multiplier, 1000); dc->bw_vbios->mid_yclk = bw_frc_to_fixed( clks.clocks_in_khz[clks.num_levels>>1] * memory_type_multiplier, 1000); dc->bw_vbios->high_yclk = bw_frc_to_fixed( clks.clocks_in_khz[clks.num_levels-1] * memory_type_multiplier, 1000); return; } dc->bw_vbios->high_sclk = bw_frc_to_fixed( eng_clks.data[eng_clks.num_levels-1].clocks_in_khz, 1000); dc->bw_vbios->mid1_sclk = bw_frc_to_fixed( eng_clks.data[eng_clks.num_levels/8].clocks_in_khz, 1000); dc->bw_vbios->mid2_sclk = bw_frc_to_fixed( eng_clks.data[eng_clks.num_levels*2/8].clocks_in_khz, 1000); dc->bw_vbios->mid3_sclk = bw_frc_to_fixed( eng_clks.data[eng_clks.num_levels*3/8].clocks_in_khz, 1000); dc->bw_vbios->mid4_sclk = bw_frc_to_fixed( eng_clks.data[eng_clks.num_levels*4/8].clocks_in_khz, 1000); dc->bw_vbios->mid5_sclk = bw_frc_to_fixed( eng_clks.data[eng_clks.num_levels*5/8].clocks_in_khz, 1000); dc->bw_vbios->mid6_sclk = bw_frc_to_fixed( eng_clks.data[eng_clks.num_levels*6/8].clocks_in_khz, 1000); dc->bw_vbios->low_sclk = bw_frc_to_fixed( eng_clks.data[0].clocks_in_khz, 1000); dm_pp_get_clock_levels_by_type_with_latency( dc->ctx, DM_PP_CLOCK_TYPE_MEMORY_CLK, &mem_clks); dc->bw_vbios->low_yclk = bw_frc_to_fixed( mem_clks.data[0].clocks_in_khz * memory_type_multiplier, 1000); dc->bw_vbios->mid_yclk = bw_frc_to_fixed( mem_clks.data[mem_clks.num_levels>>1].clocks_in_khz * memory_type_multiplier, 1000); dc->bw_vbios->high_yclk = bw_frc_to_fixed( mem_clks.data[mem_clks.num_levels-1].clocks_in_khz * memory_type_multiplier, 1000); clk_ranges.num_wm_sets = 4; clk_ranges.wm_clk_ranges[0].wm_set_id = WM_SET_A; clk_ranges.wm_clk_ranges[0].wm_min_eng_clk_in_khz = eng_clks.data[0].clocks_in_khz; clk_ranges.wm_clk_ranges[0].wm_max_eng_clk_in_khz = eng_clks.data[eng_clks.num_levels*3/8].clocks_in_khz - 1; clk_ranges.wm_clk_ranges[0].wm_min_mem_clk_in_khz = mem_clks.data[0].clocks_in_khz; clk_ranges.wm_clk_ranges[0].wm_max_mem_clk_in_khz = mem_clks.data[mem_clks.num_levels>>1].clocks_in_khz - 1; clk_ranges.wm_clk_ranges[1].wm_set_id = WM_SET_B; clk_ranges.wm_clk_ranges[1].wm_min_eng_clk_in_khz = eng_clks.data[eng_clks.num_levels*3/8].clocks_in_khz; clk_ranges.wm_clk_ranges[1].wm_max_eng_clk_in_khz = 5000000; clk_ranges.wm_clk_ranges[1].wm_min_mem_clk_in_khz = mem_clks.data[0].clocks_in_khz; clk_ranges.wm_clk_ranges[1].wm_max_mem_clk_in_khz = mem_clks.data[mem_clks.num_levels>>1].clocks_in_khz - 1; clk_ranges.wm_clk_ranges[2].wm_set_id = WM_SET_C; clk_ranges.wm_clk_ranges[2].wm_min_eng_clk_in_khz = eng_clks.data[0].clocks_in_khz; clk_ranges.wm_clk_ranges[2].wm_max_eng_clk_in_khz = eng_clks.data[eng_clks.num_levels*3/8].clocks_in_khz - 1; clk_ranges.wm_clk_ranges[2].wm_min_mem_clk_in_khz = mem_clks.data[mem_clks.num_levels>>1].clocks_in_khz; clk_ranges.wm_clk_ranges[2].wm_max_mem_clk_in_khz = 5000000; clk_ranges.wm_clk_ranges[3].wm_set_id = WM_SET_D; clk_ranges.wm_clk_ranges[3].wm_min_eng_clk_in_khz = eng_clks.data[eng_clks.num_levels*3/8].clocks_in_khz; clk_ranges.wm_clk_ranges[3].wm_max_eng_clk_in_khz = 5000000; clk_ranges.wm_clk_ranges[3].wm_min_mem_clk_in_khz = mem_clks.data[mem_clks.num_levels>>1].clocks_in_khz; clk_ranges.wm_clk_ranges[3].wm_max_mem_clk_in_khz = 5000000; dm_pp_notify_wm_clock_changes(dc->ctx, &clk_ranges); }","- if (dc->bw_vbios && dc->bw_vbios->memory_type == bw_def_hbm)
+ if (!dc->bw_vbios)
+ return;
+ if (dc->bw_vbios->memory_type == bw_def_hbm)","static void bw_calcs_data_update_from_pplib(struct dc *dc) { struct dm_pp_clock_levels_with_latency eng_clks = {0}; struct dm_pp_clock_levels_with_latency mem_clks = {0}; struct dm_pp_wm_sets_with_clock_ranges clk_ranges = {0}; struct dm_pp_clock_levels clks = {0}; int memory_type_multiplier = MEMORY_TYPE_MULTIPLIER_CZ; if (!dc->bw_vbios) return; if (dc->bw_vbios->memory_type == bw_def_hbm) memory_type_multiplier = MEMORY_TYPE_HBM; if (!dm_pp_get_clock_levels_by_type_with_latency( dc->ctx, DM_PP_CLOCK_TYPE_ENGINE_CLK, &eng_clks)) { dm_pp_get_clock_levels_by_type( dc->ctx, DM_PP_CLOCK_TYPE_ENGINE_CLK, &clks); dc->bw_vbios->high_sclk = bw_frc_to_fixed( clks.clocks_in_khz[clks.num_levels-1], 1000); dc->bw_vbios->mid1_sclk = bw_frc_to_fixed( clks.clocks_in_khz[clks.num_levels/8], 1000); dc->bw_vbios->mid2_sclk = bw_frc_to_fixed( clks.clocks_in_khz[clks.num_levels*2/8], 1000); dc->bw_vbios->mid3_sclk = bw_frc_to_fixed( clks.clocks_in_khz[clks.num_levels*3/8], 1000); dc->bw_vbios->mid4_sclk = bw_frc_to_fixed( clks.clocks_in_khz[clks.num_levels*4/8], 1000); dc->bw_vbios->mid5_sclk = bw_frc_to_fixed( clks.clocks_in_khz[clks.num_levels*5/8], 1000); dc->bw_vbios->mid6_sclk = bw_frc_to_fixed( clks.clocks_in_khz[clks.num_levels*6/8], 1000); dc->bw_vbios->low_sclk = bw_frc_to_fixed( clks.clocks_in_khz[0], 1000); dm_pp_get_clock_levels_by_type( dc->ctx, DM_PP_CLOCK_TYPE_MEMORY_CLK, &clks); dc->bw_vbios->low_yclk = bw_frc_to_fixed( clks.clocks_in_khz[0] * memory_type_multiplier, 1000); dc->bw_vbios->mid_yclk = bw_frc_to_fixed( clks.clocks_in_khz[clks.num_levels>>1] * memory_type_multiplier, 1000); dc->bw_vbios->high_yclk = bw_frc_to_fixed( clks.clocks_in_khz[clks.num_levels-1] * memory_type_multiplier, 1000); return; } dc->bw_vbios->high_sclk = bw_frc_to_fixed( eng_clks.data[eng_clks.num_levels-1].clocks_in_khz, 1000); dc->bw_vbios->mid1_sclk = bw_frc_to_fixed( eng_clks.data[eng_clks.num_levels/8].clocks_in_khz, 1000); dc->bw_vbios->mid2_sclk = bw_frc_to_fixed( eng_clks.data[eng_clks.num_levels*2/8].clocks_in_khz, 1000); dc->bw_vbios->mid3_sclk = bw_frc_to_fixed( eng_clks.data[eng_clks.num_levels*3/8].clocks_in_khz, 1000); dc->bw_vbios->mid4_sclk = bw_frc_to_fixed( eng_clks.data[eng_clks.num_levels*4/8].clocks_in_khz, 1000); dc->bw_vbios->mid5_sclk = bw_frc_to_fixed( eng_clks.data[eng_clks.num_levels*5/8].clocks_in_khz, 1000); dc->bw_vbios->mid6_sclk = bw_frc_to_fixed( eng_clks.data[eng_clks.num_levels*6/8].clocks_in_khz, 1000); dc->bw_vbios->low_sclk = bw_frc_to_fixed( eng_clks.data[0].clocks_in_khz, 1000); dm_pp_get_clock_levels_by_type_with_latency( dc->ctx, DM_PP_CLOCK_TYPE_MEMORY_CLK, &mem_clks); dc->bw_vbios->low_yclk = bw_frc_to_fixed( mem_clks.data[0].clocks_in_khz * memory_type_multiplier, 1000); dc->bw_vbios->mid_yclk = bw_frc_to_fixed( mem_clks.data[mem_clks.num_levels>>1].clocks_in_khz * memory_type_multiplier, 1000); dc->bw_vbios->high_yclk = bw_frc_to_fixed( mem_clks.data[mem_clks.num_levels-1].clocks_in_khz * memory_type_multiplier, 1000); clk_ranges.num_wm_sets = 4; clk_ranges.wm_clk_ranges[0].wm_set_id = WM_SET_A; clk_ranges.wm_clk_ranges[0].wm_min_eng_clk_in_khz = eng_clks.data[0].clocks_in_khz; clk_ranges.wm_clk_ranges[0].wm_max_eng_clk_in_khz = eng_clks.data[eng_clks.num_levels*3/8].clocks_in_khz - 1; clk_ranges.wm_clk_ranges[0].wm_min_mem_clk_in_khz = mem_clks.data[0].clocks_in_khz; clk_ranges.wm_clk_ranges[0].wm_max_mem_clk_in_khz = mem_clks.data[mem_clks.num_levels>>1].clocks_in_khz - 1; clk_ranges.wm_clk_ranges[1].wm_set_id = WM_SET_B; clk_ranges.wm_clk_ranges[1].wm_min_eng_clk_in_khz = eng_clks.data[eng_clks.num_levels*3/8].clocks_in_khz; clk_ranges.wm_clk_ranges[1].wm_max_eng_clk_in_khz = 5000000; clk_ranges.wm_clk_ranges[1].wm_min_mem_clk_in_khz = mem_clks.data[0].clocks_in_khz; clk_ranges.wm_clk_ranges[1].wm_max_mem_clk_in_khz = mem_clks.data[mem_clks.num_levels>>1].clocks_in_khz - 1; clk_ranges.wm_clk_ranges[2].wm_set_id = WM_SET_C; clk_ranges.wm_clk_ranges[2].wm_min_eng_clk_in_khz = eng_clks.data[0].clocks_in_khz; clk_ranges.wm_clk_ranges[2].wm_max_eng_clk_in_khz = eng_clks.data[eng_clks.num_levels*3/8].clocks_in_khz - 1; clk_ranges.wm_clk_ranges[2].wm_min_mem_clk_in_khz = mem_clks.data[mem_clks.num_levels>>1].clocks_in_khz; clk_ranges.wm_clk_ranges[2].wm_max_mem_clk_in_khz = 5000000; clk_ranges.wm_clk_ranges[3].wm_set_id = WM_SET_D; clk_ranges.wm_clk_ranges[3].wm_min_eng_clk_in_khz = eng_clks.data[eng_clks.num_levels*3/8].clocks_in_khz; clk_ranges.wm_clk_ranges[3].wm_max_eng_clk_in_khz = 5000000; clk_ranges.wm_clk_ranges[3].wm_min_mem_clk_in_khz = mem_clks.data[mem_clks.num_levels>>1].clocks_in_khz; clk_ranges.wm_clk_ranges[3].wm_max_mem_clk_in_khz = 5000000; dm_pp_notify_wm_clock_changes(dc->ctx, &clk_ranges); }"
566----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49889/bad/extents.c----ext4_ext_handle_unwritten_extents,"ext4_ext_handle_unwritten_extents(handle_t *handle, struct inode *inode, struct ext4_map_blocks *map, struct ext4_ext_path **ppath, int flags, unsigned int allocated, ext4_fsblk_t newblock) { <S2SV_StartVul> struct ext4_ext_path __maybe_unused *path = *ppath; <S2SV_EndVul> int ret = 0; int err = 0; ext_debug(inode, ""logical block %llu, max_blocks %u, flags 0x%x, allocated %u\n"", (unsigned long long)map->m_lblk, map->m_len, flags, allocated); <S2SV_StartVul> ext4_ext_show_leaf(inode, path); <S2SV_EndVul> flags |= EXT4_GET_BLOCKS_METADATA_NOFAIL; trace_ext4_ext_handle_unwritten_extents(inode, map, flags, allocated, newblock); if (flags & EXT4_GET_BLOCKS_PRE_IO) { ret = ext4_split_convert_extents(handle, inode, map, ppath, flags | EXT4_GET_BLOCKS_CONVERT); if (ret < 0) { err = ret; goto out2; } if (unlikely(ret == 0)) { EXT4_ERROR_INODE(inode, ""unexpected ret == 0, m_len = %u"", map->m_len); err = -EFSCORRUPTED; goto out2; } map->m_flags |= EXT4_MAP_UNWRITTEN; goto out; } if (flags & EXT4_GET_BLOCKS_CONVERT) { err = ext4_convert_unwritten_extents_endio(handle, inode, map, ppath); if (err < 0) goto out2; ext4_update_inode_fsync_trans(handle, inode, 1); goto map_out; } if (flags & EXT4_GET_BLOCKS_UNWRIT_EXT) { map->m_flags |= EXT4_MAP_UNWRITTEN; goto map_out; } if ((flags & EXT4_GET_BLOCKS_CREATE) == 0) { map->m_flags |= EXT4_MAP_UNWRITTEN; goto out1; } ret = ext4_ext_convert_to_initialized(handle, inode, map, ppath, flags); if (ret < 0) { err = ret; goto out2; } ext4_update_inode_fsync_trans(handle, inode, 1); if (unlikely(ret == 0)) { EXT4_ERROR_INODE(inode, ""unexpected ret == 0, m_len = %u"", map->m_len); err = -EFSCORRUPTED; goto out2; } out: allocated = ret; map->m_flags |= EXT4_MAP_NEW; map_out: map->m_flags |= EXT4_MAP_MAPPED; out1: map->m_pblk = newblock; if (allocated > map->m_len) allocated = map->m_len; map->m_len = allocated; <S2SV_StartVul> ext4_ext_show_leaf(inode, path); <S2SV_EndVul> out2: return err ? err : allocated; }","- struct ext4_ext_path __maybe_unused *path = *ppath;
- ext4_ext_show_leaf(inode, path);
- ext4_ext_show_leaf(inode, path);
+ ext4_ext_show_leaf(inode, *ppath);
+ ext4_ext_show_leaf(inode, *ppath);","ext4_ext_handle_unwritten_extents(handle_t *handle, struct inode *inode, struct ext4_map_blocks *map, struct ext4_ext_path **ppath, int flags, unsigned int allocated, ext4_fsblk_t newblock) { int ret = 0; int err = 0; ext_debug(inode, ""logical block %llu, max_blocks %u, flags 0x%x, allocated %u\n"", (unsigned long long)map->m_lblk, map->m_len, flags, allocated); ext4_ext_show_leaf(inode, *ppath); flags |= EXT4_GET_BLOCKS_METADATA_NOFAIL; trace_ext4_ext_handle_unwritten_extents(inode, map, flags, allocated, newblock); if (flags & EXT4_GET_BLOCKS_PRE_IO) { ret = ext4_split_convert_extents(handle, inode, map, ppath, flags | EXT4_GET_BLOCKS_CONVERT); if (ret < 0) { err = ret; goto out2; } if (unlikely(ret == 0)) { EXT4_ERROR_INODE(inode, ""unexpected ret == 0, m_len = %u"", map->m_len); err = -EFSCORRUPTED; goto out2; } map->m_flags |= EXT4_MAP_UNWRITTEN; goto out; } if (flags & EXT4_GET_BLOCKS_CONVERT) { err = ext4_convert_unwritten_extents_endio(handle, inode, map, ppath); if (err < 0) goto out2; ext4_update_inode_fsync_trans(handle, inode, 1); goto map_out; } if (flags & EXT4_GET_BLOCKS_UNWRIT_EXT) { map->m_flags |= EXT4_MAP_UNWRITTEN; goto map_out; } if ((flags & EXT4_GET_BLOCKS_CREATE) == 0) { map->m_flags |= EXT4_MAP_UNWRITTEN; goto out1; } ret = ext4_ext_convert_to_initialized(handle, inode, map, ppath, flags); if (ret < 0) { err = ret; goto out2; } ext4_update_inode_fsync_trans(handle, inode, 1); if (unlikely(ret == 0)) { EXT4_ERROR_INODE(inode, ""unexpected ret == 0, m_len = %u"", map->m_len); err = -EFSCORRUPTED; goto out2; } out: allocated = ret; map->m_flags |= EXT4_MAP_NEW; map_out: map->m_flags |= EXT4_MAP_MAPPED; out1: map->m_pblk = newblock; if (allocated > map->m_len) allocated = map->m_len; map->m_len = allocated; ext4_ext_show_leaf(inode, *ppath); out2: return err ? err : allocated; }"
614----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49922/bad/amdgpu_dm.c----amdgpu_dm_commit_streams,"static void amdgpu_dm_commit_streams(struct drm_atomic_state *state, struct dc_state *dc_state) { struct drm_device *dev = state->dev; struct amdgpu_device *adev = drm_to_adev(dev); struct amdgpu_display_manager *dm = &adev->dm; struct drm_crtc *crtc; struct drm_crtc_state *old_crtc_state, *new_crtc_state; struct dm_crtc_state *dm_old_crtc_state, *dm_new_crtc_state; struct drm_connector_state *old_con_state; struct drm_connector *connector; bool mode_set_reset_required = false; u32 i; struct dc_commit_streams_params params = {dc_state->streams, dc_state->stream_count}; for_each_old_connector_in_state(state, connector, old_con_state, i) { struct dm_connector_state *dm_old_con_state; struct amdgpu_crtc *acrtc; if (connector->connector_type != DRM_MODE_CONNECTOR_WRITEBACK) continue; old_crtc_state = NULL; dm_old_con_state = to_dm_connector_state(old_con_state); if (!dm_old_con_state->base.crtc) continue; acrtc = to_amdgpu_crtc(dm_old_con_state->base.crtc); if (acrtc) old_crtc_state = drm_atomic_get_old_crtc_state(state, &acrtc->base); <S2SV_StartVul> if (!acrtc->wb_enabled) <S2SV_EndVul> continue; dm_old_crtc_state = to_dm_crtc_state(old_crtc_state); dm_clear_writeback(dm, dm_old_crtc_state); acrtc->wb_enabled = false; } for_each_oldnew_crtc_in_state(state, crtc, old_crtc_state, new_crtc_state, i) { struct amdgpu_crtc *acrtc = to_amdgpu_crtc(crtc); dm_old_crtc_state = to_dm_crtc_state(old_crtc_state); if (old_crtc_state->active && (!new_crtc_state->active || drm_atomic_crtc_needs_modeset(new_crtc_state))) { manage_dm_interrupts(adev, acrtc, false); dc_stream_release(dm_old_crtc_state->stream); } } drm_atomic_helper_calc_timestamping_constants(state); for_each_oldnew_crtc_in_state(state, crtc, old_crtc_state, new_crtc_state, i) { struct amdgpu_crtc *acrtc = to_amdgpu_crtc(crtc); dm_new_crtc_state = to_dm_crtc_state(new_crtc_state); dm_old_crtc_state = to_dm_crtc_state(old_crtc_state); drm_dbg_state(state->dev, ""amdgpu_crtc id:%d crtc_state_flags: enable:%d, active:%d, planes_changed:%d, mode_changed:%d,active_changed:%d,connectors_changed:%d\n"", acrtc->crtc_id, new_crtc_state->enable, new_crtc_state->active, new_crtc_state->planes_changed, new_crtc_state->mode_changed, new_crtc_state->active_changed, new_crtc_state->connectors_changed); if (old_crtc_state->active && !new_crtc_state->active) { struct dc_cursor_position position; memset(&position, 0, sizeof(position)); mutex_lock(&dm->dc_lock); dc_exit_ips_for_hw_access(dm->dc); dc_stream_program_cursor_position(dm_old_crtc_state->stream, &position); mutex_unlock(&dm->dc_lock); } if (dm_new_crtc_state->stream) { amdgpu_dm_crtc_copy_transient_flags(&dm_new_crtc_state->base, dm_new_crtc_state->stream); } if (amdgpu_dm_crtc_modeset_required(new_crtc_state, dm_new_crtc_state->stream, dm_old_crtc_state->stream)) { drm_dbg_atomic(dev, ""Atomic commit: SET crtc id %d: [%p]\n"", acrtc->crtc_id, acrtc); if (!dm_new_crtc_state->stream) { drm_dbg_atomic(dev, ""Failed to create new stream for crtc %d\n"", acrtc->base.base.id); continue; } if (dm_old_crtc_state->stream) remove_stream(adev, acrtc, dm_old_crtc_state->stream); pm_runtime_get_noresume(dev->dev); acrtc->enabled = true; acrtc->hw_mode = new_crtc_state->mode; crtc->hwmode = new_crtc_state->mode; mode_set_reset_required = true; } else if (modereset_required(new_crtc_state)) { drm_dbg_atomic(dev, ""Atomic commit: RESET. crtc id %d:[%p]\n"", acrtc->crtc_id, acrtc); if (dm_old_crtc_state->stream) remove_stream(adev, acrtc, dm_old_crtc_state->stream); mode_set_reset_required = true; } } if (mode_set_reset_required) { if (dm->vblank_control_workqueue) flush_workqueue(dm->vblank_control_workqueue); amdgpu_dm_replay_disable_all(dm); amdgpu_dm_psr_disable_all(dm); } dm_enable_per_frame_crtc_master_sync(dc_state); mutex_lock(&dm->dc_lock); dc_exit_ips_for_hw_access(dm->dc); WARN_ON(!dc_commit_streams(dm->dc, &params)); if (dm->active_vblank_irq_count == 0) dc_allow_idle_optimizations(dm->dc, true); mutex_unlock(&dm->dc_lock); for_each_new_crtc_in_state(state, crtc, new_crtc_state, i) { struct amdgpu_crtc *acrtc = to_amdgpu_crtc(crtc); dm_new_crtc_state = to_dm_crtc_state(new_crtc_state); if (dm_new_crtc_state->stream != NULL) { const struct dc_stream_status *status = dc_stream_get_status(dm_new_crtc_state->stream); if (!status) status = dc_state_get_stream_status(dc_state, dm_new_crtc_state->stream); if (!status) drm_err(dev, ""got no status for stream %p on acrtc%p\n"", dm_new_crtc_state->stream, acrtc); else acrtc->otg_inst = status->primary_otg_inst; } } }","- if (!acrtc->wb_enabled)
+ if (!acrtc || !acrtc->wb_enabled)","static void amdgpu_dm_commit_streams(struct drm_atomic_state *state, struct dc_state *dc_state) { struct drm_device *dev = state->dev; struct amdgpu_device *adev = drm_to_adev(dev); struct amdgpu_display_manager *dm = &adev->dm; struct drm_crtc *crtc; struct drm_crtc_state *old_crtc_state, *new_crtc_state; struct dm_crtc_state *dm_old_crtc_state, *dm_new_crtc_state; struct drm_connector_state *old_con_state; struct drm_connector *connector; bool mode_set_reset_required = false; u32 i; struct dc_commit_streams_params params = {dc_state->streams, dc_state->stream_count}; for_each_old_connector_in_state(state, connector, old_con_state, i) { struct dm_connector_state *dm_old_con_state; struct amdgpu_crtc *acrtc; if (connector->connector_type != DRM_MODE_CONNECTOR_WRITEBACK) continue; old_crtc_state = NULL; dm_old_con_state = to_dm_connector_state(old_con_state); if (!dm_old_con_state->base.crtc) continue; acrtc = to_amdgpu_crtc(dm_old_con_state->base.crtc); if (acrtc) old_crtc_state = drm_atomic_get_old_crtc_state(state, &acrtc->base); if (!acrtc || !acrtc->wb_enabled) continue; dm_old_crtc_state = to_dm_crtc_state(old_crtc_state); dm_clear_writeback(dm, dm_old_crtc_state); acrtc->wb_enabled = false; } for_each_oldnew_crtc_in_state(state, crtc, old_crtc_state, new_crtc_state, i) { struct amdgpu_crtc *acrtc = to_amdgpu_crtc(crtc); dm_old_crtc_state = to_dm_crtc_state(old_crtc_state); if (old_crtc_state->active && (!new_crtc_state->active || drm_atomic_crtc_needs_modeset(new_crtc_state))) { manage_dm_interrupts(adev, acrtc, false); dc_stream_release(dm_old_crtc_state->stream); } } drm_atomic_helper_calc_timestamping_constants(state); for_each_oldnew_crtc_in_state(state, crtc, old_crtc_state, new_crtc_state, i) { struct amdgpu_crtc *acrtc = to_amdgpu_crtc(crtc); dm_new_crtc_state = to_dm_crtc_state(new_crtc_state); dm_old_crtc_state = to_dm_crtc_state(old_crtc_state); drm_dbg_state(state->dev, ""amdgpu_crtc id:%d crtc_state_flags: enable:%d, active:%d, planes_changed:%d, mode_changed:%d,active_changed:%d,connectors_changed:%d\n"", acrtc->crtc_id, new_crtc_state->enable, new_crtc_state->active, new_crtc_state->planes_changed, new_crtc_state->mode_changed, new_crtc_state->active_changed, new_crtc_state->connectors_changed); if (old_crtc_state->active && !new_crtc_state->active) { struct dc_cursor_position position; memset(&position, 0, sizeof(position)); mutex_lock(&dm->dc_lock); dc_exit_ips_for_hw_access(dm->dc); dc_stream_program_cursor_position(dm_old_crtc_state->stream, &position); mutex_unlock(&dm->dc_lock); } if (dm_new_crtc_state->stream) { amdgpu_dm_crtc_copy_transient_flags(&dm_new_crtc_state->base, dm_new_crtc_state->stream); } if (amdgpu_dm_crtc_modeset_required(new_crtc_state, dm_new_crtc_state->stream, dm_old_crtc_state->stream)) { drm_dbg_atomic(dev, ""Atomic commit: SET crtc id %d: [%p]\n"", acrtc->crtc_id, acrtc); if (!dm_new_crtc_state->stream) { drm_dbg_atomic(dev, ""Failed to create new stream for crtc %d\n"", acrtc->base.base.id); continue; } if (dm_old_crtc_state->stream) remove_stream(adev, acrtc, dm_old_crtc_state->stream); pm_runtime_get_noresume(dev->dev); acrtc->enabled = true; acrtc->hw_mode = new_crtc_state->mode; crtc->hwmode = new_crtc_state->mode; mode_set_reset_required = true; } else if (modereset_required(new_crtc_state)) { drm_dbg_atomic(dev, ""Atomic commit: RESET. crtc id %d:[%p]\n"", acrtc->crtc_id, acrtc); if (dm_old_crtc_state->stream) remove_stream(adev, acrtc, dm_old_crtc_state->stream); mode_set_reset_required = true; } } if (mode_set_reset_required) { if (dm->vblank_control_workqueue) flush_workqueue(dm->vblank_control_workqueue); amdgpu_dm_replay_disable_all(dm); amdgpu_dm_psr_disable_all(dm); } dm_enable_per_frame_crtc_master_sync(dc_state); mutex_lock(&dm->dc_lock); dc_exit_ips_for_hw_access(dm->dc); WARN_ON(!dc_commit_streams(dm->dc, &params)); if (dm->active_vblank_irq_count == 0) dc_allow_idle_optimizations(dm->dc, true); mutex_unlock(&dm->dc_lock); for_each_new_crtc_in_state(state, crtc, new_crtc_state, i) { struct amdgpu_crtc *acrtc = to_amdgpu_crtc(crtc); dm_new_crtc_state = to_dm_crtc_state(new_crtc_state); if (dm_new_crtc_state->stream != NULL) { const struct dc_stream_status *status = dc_stream_get_status(dm_new_crtc_state->stream); if (!status) status = dc_state_get_stream_status(dc_state, dm_new_crtc_state->stream); if (!status) drm_err(dev, ""got no status for stream %p on acrtc%p\n"", dm_new_crtc_state->stream, acrtc); else acrtc->otg_inst = status->primary_otg_inst; } } }"
757----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50036/bad/dst.c----dst_release,"void dst_release(struct dst_entry *dst) { if (dst) { int newrefcnt; newrefcnt = atomic_dec_return(&dst->__refcnt); if (WARN_ONCE(newrefcnt < 0, ""dst_release underflow"")) net_warn_ratelimited(""%s: dst:%p refcnt:%d\n"", __func__, dst, newrefcnt); <S2SV_StartVul> if (!newrefcnt) <S2SV_EndVul> call_rcu(&dst->rcu_head, dst_destroy_rcu); } }","- if (!newrefcnt)
+ if (!newrefcnt){
+ dst_count_dec(dst);
+ }
+ }
+ }","void dst_release(struct dst_entry *dst) { if (dst) { int newrefcnt; newrefcnt = atomic_dec_return(&dst->__refcnt); if (WARN_ONCE(newrefcnt < 0, ""dst_release underflow"")) net_warn_ratelimited(""%s: dst:%p refcnt:%d\n"", __func__, dst, newrefcnt); if (!newrefcnt){ dst_count_dec(dst); call_rcu(&dst->rcu_head, dst_destroy_rcu); } } }"
944----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50190/bad/ice_ddp.c----ice_verify_pkg,"static enum ice_ddp_state ice_verify_pkg(struct ice_pkg_hdr *pkg, u32 len) { u32 seg_count; u32 i; if (len < struct_size(pkg, seg_offset, 1)) return ICE_DDP_PKG_INVALID_FILE; if (pkg->pkg_format_ver.major != ICE_PKG_FMT_VER_MAJ || pkg->pkg_format_ver.minor != ICE_PKG_FMT_VER_MNR || pkg->pkg_format_ver.update != ICE_PKG_FMT_VER_UPD || pkg->pkg_format_ver.draft != ICE_PKG_FMT_VER_DFT) return ICE_DDP_PKG_INVALID_FILE; seg_count = le32_to_cpu(pkg->seg_count); if (seg_count < 1) return ICE_DDP_PKG_INVALID_FILE; if (len < struct_size(pkg, seg_offset, seg_count)) return ICE_DDP_PKG_INVALID_FILE; for (i = 0; i < seg_count; i++) { u32 off = le32_to_cpu(pkg->seg_offset[i]); <S2SV_StartVul> struct ice_generic_seg_hdr *seg; <S2SV_EndVul> if (len < off + sizeof(*seg)) return ICE_DDP_PKG_INVALID_FILE; <S2SV_StartVul> seg = (struct ice_generic_seg_hdr *)((u8 *)pkg + off); <S2SV_EndVul> if (len < off + le32_to_cpu(seg->seg_size)) return ICE_DDP_PKG_INVALID_FILE; } return ICE_DDP_PKG_SUCCESS; }","- struct ice_generic_seg_hdr *seg;
- seg = (struct ice_generic_seg_hdr *)((u8 *)pkg + off);","#include ""ice_common.h"" #include ""ice.h"" #include ""ice_ddp.h"" #include ""ice_sched.h"" #define ICE_DVM_PRE ""BOOST_MAC_VLAN_DVM"" #define ICE_SVM_PRE ""BOOST_MAC_VLAN_SVM"" #define ICE_TNL_PRE ""TNL_"" static const struct ice_tunnel_type_scan tnls[] = { { TNL_VXLAN, ""TNL_VXLAN_PF"" }, { TNL_GENEVE, ""TNL_GENEVE_PF"" }, { TNL_LAST, """" } };"
145----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-45004/bad/trusted_dcp.c----trusted_dcp_seal,"static int trusted_dcp_seal(struct trusted_key_payload *p, char *datablob) { struct dcp_blob_fmt *b = (struct dcp_blob_fmt *)p->blob; int blen, ret; blen = calc_blob_len(p->key_len); if (blen > MAX_BLOB_SIZE) return -E2BIG; b->fmt_version = DCP_BLOB_VERSION; get_random_bytes(b->nonce, AES_KEYSIZE_128); <S2SV_StartVul> get_random_bytes(b->blob_key, AES_KEYSIZE_128); <S2SV_EndVul> <S2SV_StartVul> ret = do_aead_crypto(p->key, b->payload, p->key_len, b->blob_key, <S2SV_EndVul> b->nonce, true); if (ret) { pr_err(""Unable to encrypt blob payload: %i\n"", ret); <S2SV_StartVul> return ret; <S2SV_EndVul> } <S2SV_StartVul> ret = encrypt_blob_key(b->blob_key); <S2SV_EndVul> if (ret) { pr_err(""Unable to encrypt blob key: %i\n"", ret); <S2SV_StartVul> return ret; <S2SV_EndVul> } put_unaligned_le32(p->key_len, &b->payload_len); p->blob_len = blen; <S2SV_StartVul> return 0; <S2SV_EndVul> }","- get_random_bytes(b->blob_key, AES_KEYSIZE_128);
- ret = do_aead_crypto(p->key, b->payload, p->key_len, b->blob_key,
- return ret;
- ret = encrypt_blob_key(b->blob_key);
- return ret;
- return 0;
+ get_random_bytes(plain_blob_key, AES_KEYSIZE_128);
+ ret = do_aead_crypto(p->key, b->payload, p->key_len, plain_blob_key,
+ goto out;
+ ret = encrypt_blob_key(plain_blob_key, b->blob_key);
+ goto out;
+ ret = 0;
+ out:
+ memzero_explicit(plain_blob_key, sizeof(plain_blob_key));
+ return ret;","static int trusted_dcp_seal(struct trusted_key_payload *p, char *datablob) { struct dcp_blob_fmt *b = (struct dcp_blob_fmt *)p->blob; int blen, ret; u8 plain_blob_key[AES_KEYSIZE_128]; blen = calc_blob_len(p->key_len); if (blen > MAX_BLOB_SIZE) return -E2BIG; b->fmt_version = DCP_BLOB_VERSION; get_random_bytes(b->nonce, AES_KEYSIZE_128); get_random_bytes(plain_blob_key, AES_KEYSIZE_128); ret = do_aead_crypto(p->key, b->payload, p->key_len, plain_blob_key, b->nonce, true); if (ret) { pr_err(""Unable to encrypt blob payload: %i\n"", ret); goto out; } ret = encrypt_blob_key(plain_blob_key, b->blob_key); if (ret) { pr_err(""Unable to encrypt blob key: %i\n"", ret); goto out; } put_unaligned_le32(p->key_len, &b->payload_len); p->blob_len = blen; ret = 0; out: memzero_explicit(plain_blob_key, sizeof(plain_blob_key)); return ret; }"
1173----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53149/bad/ucsi_glink.c----pmic_glink_ucsi_connector_status,"static void pmic_glink_ucsi_connector_status(struct ucsi_connector *con) { struct pmic_glink_ucsi *ucsi = ucsi_get_drvdata(con->ucsi); int orientation; <S2SV_StartVul> if (con->num >= PMIC_GLINK_MAX_PORTS || <S2SV_EndVul> !ucsi->port_orientation[con->num - 1]) return; orientation = gpiod_get_value(ucsi->port_orientation[con->num - 1]); if (orientation >= 0) { typec_set_orientation(con->port, orientation ? TYPEC_ORIENTATION_REVERSE : TYPEC_ORIENTATION_NORMAL); } }","- if (con->num >= PMIC_GLINK_MAX_PORTS ||
+ if (con->num > PMIC_GLINK_MAX_PORTS ||","static void pmic_glink_ucsi_connector_status(struct ucsi_connector *con) { struct pmic_glink_ucsi *ucsi = ucsi_get_drvdata(con->ucsi); int orientation; if (con->num > PMIC_GLINK_MAX_PORTS || !ucsi->port_orientation[con->num - 1]) return; orientation = gpiod_get_value(ucsi->port_orientation[con->num - 1]); if (orientation >= 0) { typec_set_orientation(con->port, orientation ? TYPEC_ORIENTATION_REVERSE : TYPEC_ORIENTATION_NORMAL); } }"
1530----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2025-21684/bad/gpio-xilinx.c----xgpio_irq_mask,"static void xgpio_irq_mask(struct irq_data *irq_data) { unsigned long flags; struct xgpio_instance *chip = irq_data_get_irq_chip_data(irq_data); int irq_offset = irqd_to_hwirq(irq_data); int bit = xgpio_to_bit(chip, irq_offset); u32 mask = BIT(bit / 32), temp; <S2SV_StartVul> spin_lock_irqsave(&chip->gpio_lock, flags); <S2SV_EndVul> __clear_bit(bit, chip->enable); if (xgpio_get_value32(chip->enable, bit) == 0) { temp = xgpio_readreg(chip->regs + XGPIO_IPIER_OFFSET); temp &= ~mask; xgpio_writereg(chip->regs + XGPIO_IPIER_OFFSET, temp); } <S2SV_StartVul> spin_unlock_irqrestore(&chip->gpio_lock, flags); <S2SV_EndVul> gpiochip_disable_irq(&chip->gc, irq_offset); }","- spin_lock_irqsave(&chip->gpio_lock, flags);
- spin_unlock_irqrestore(&chip->gpio_lock, flags);
+ raw_spin_lock_irqsave(&chip->gpio_lock, flags);
+ raw_spin_unlock_irqrestore(&chip->gpio_lock, flags);","static void xgpio_irq_mask(struct irq_data *irq_data) { unsigned long flags; struct xgpio_instance *chip = irq_data_get_irq_chip_data(irq_data); int irq_offset = irqd_to_hwirq(irq_data); int bit = xgpio_to_bit(chip, irq_offset); u32 mask = BIT(bit / 32), temp; raw_spin_lock_irqsave(&chip->gpio_lock, flags); __clear_bit(bit, chip->enable); if (xgpio_get_value32(chip->enable, bit) == 0) { temp = xgpio_readreg(chip->regs + XGPIO_IPIER_OFFSET); temp &= ~mask; xgpio_writereg(chip->regs + XGPIO_IPIER_OFFSET, temp); } raw_spin_unlock_irqrestore(&chip->gpio_lock, flags); gpiochip_disable_irq(&chip->gc, irq_offset); }"
263----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46742/bad/smb2pdu.c----parse_durable_handle_context,"static int parse_durable_handle_context(struct ksmbd_work *work, struct smb2_create_req *req, struct lease_ctx_info *lc, struct durable_info *dh_info) { struct ksmbd_conn *conn = work->conn; struct create_context *context; int dh_idx, err = 0; u64 persistent_id = 0; int req_op_level; static const char * const durable_arr[] = {""DH2C"", ""DHnC"", ""DH2Q"", ""DHnQ""}; req_op_level = req->RequestedOplockLevel; for (dh_idx = DURABLE_RECONN_V2; dh_idx <= ARRAY_SIZE(durable_arr); dh_idx++) { context = smb2_find_context_vals(req, durable_arr[dh_idx - 1], 4); if (IS_ERR(context)) { err = PTR_ERR(context); goto out; } if (!context) continue; switch (dh_idx) { case DURABLE_RECONN_V2: { struct create_durable_reconn_v2_req *recon_v2; if (dh_info->type == DURABLE_RECONN || dh_info->type == DURABLE_REQ_V2) { err = -EINVAL; goto out; } recon_v2 = (struct create_durable_reconn_v2_req *)context; persistent_id = recon_v2->Fid.PersistentFileId; dh_info->fp = ksmbd_lookup_durable_fd(persistent_id); if (!dh_info->fp) { ksmbd_debug(SMB, ""Failed to get durable handle state\n""); err = -EBADF; goto out; } if (memcmp(dh_info->fp->create_guid, recon_v2->CreateGuid, SMB2_CREATE_GUID_SIZE)) { err = -EBADF; ksmbd_put_durable_fd(dh_info->fp); goto out; } dh_info->type = dh_idx; dh_info->reconnected = true; ksmbd_debug(SMB, ""reconnect v2 Persistent-id from reconnect = %llu\n"", persistent_id); break; } case DURABLE_RECONN: { struct create_durable_reconn_req *recon; if (dh_info->type == DURABLE_RECONN_V2 || dh_info->type == DURABLE_REQ_V2) { err = -EINVAL; goto out; } recon = (struct create_durable_reconn_req *)context; persistent_id = recon->Data.Fid.PersistentFileId; dh_info->fp = ksmbd_lookup_durable_fd(persistent_id); if (!dh_info->fp) { ksmbd_debug(SMB, ""Failed to get durable handle state\n""); err = -EBADF; goto out; } dh_info->type = dh_idx; dh_info->reconnected = true; ksmbd_debug(SMB, ""reconnect Persistent-id from reconnect = %llu\n"", persistent_id); break; } case DURABLE_REQ_V2: { struct create_durable_req_v2 *durable_v2_blob; if (dh_info->type == DURABLE_RECONN || dh_info->type == DURABLE_RECONN_V2) { err = -EINVAL; goto out; } durable_v2_blob = (struct create_durable_req_v2 *)context; ksmbd_debug(SMB, ""Request for durable v2 open\n""); dh_info->fp = ksmbd_lookup_fd_cguid(durable_v2_blob->CreateGuid); if (dh_info->fp) { if (!memcmp(conn->ClientGUID, dh_info->fp->client_guid, SMB2_CLIENT_GUID_SIZE)) { if (!(req->hdr.Flags & SMB2_FLAGS_REPLAY_OPERATION)) { err = -ENOEXEC; goto out; } dh_info->fp->conn = conn; dh_info->reconnected = true; goto out; } } <S2SV_StartVul> if (((lc && (lc->req_state & SMB2_LEASE_HANDLE_CACHING_LE)) || <S2SV_EndVul> <S2SV_StartVul> req_op_level == SMB2_OPLOCK_LEVEL_BATCH)) { <S2SV_EndVul> dh_info->CreateGuid = durable_v2_blob->CreateGuid; dh_info->persistent = le32_to_cpu(durable_v2_blob->Flags); dh_info->timeout = le32_to_cpu(durable_v2_blob->Timeout); dh_info->type = dh_idx; } break; } case DURABLE_REQ: if (dh_info->type == DURABLE_RECONN) goto out; if (dh_info->type == DURABLE_RECONN_V2 || dh_info->type == DURABLE_REQ_V2) { err = -EINVAL; goto out; } <S2SV_StartVul> if (((lc && (lc->req_state & SMB2_LEASE_HANDLE_CACHING_LE)) || <S2SV_EndVul> <S2SV_StartVul> req_op_level == SMB2_OPLOCK_LEVEL_BATCH)) { <S2SV_EndVul> ksmbd_debug(SMB, ""Request for durable open\n""); dh_info->type = dh_idx; } } } out: return err; }","- if (((lc && (lc->req_state & SMB2_LEASE_HANDLE_CACHING_LE)) ||
- req_op_level == SMB2_OPLOCK_LEVEL_BATCH)) {
- if (((lc && (lc->req_state & SMB2_LEASE_HANDLE_CACHING_LE)) ||
- req_op_level == SMB2_OPLOCK_LEVEL_BATCH)) {
+ if ((lc && (lc->req_state & SMB2_LEASE_HANDLE_CACHING_LE)) ||
+ req_op_level == SMB2_OPLOCK_LEVEL_BATCH) {
+ if ((lc && (lc->req_state & SMB2_LEASE_HANDLE_CACHING_LE)) ||
+ req_op_level == SMB2_OPLOCK_LEVEL_BATCH) {","static int parse_durable_handle_context(struct ksmbd_work *work, struct smb2_create_req *req, struct lease_ctx_info *lc, struct durable_info *dh_info) { struct ksmbd_conn *conn = work->conn; struct create_context *context; int dh_idx, err = 0; u64 persistent_id = 0; int req_op_level; static const char * const durable_arr[] = {""DH2C"", ""DHnC"", ""DH2Q"", ""DHnQ""}; req_op_level = req->RequestedOplockLevel; for (dh_idx = DURABLE_RECONN_V2; dh_idx <= ARRAY_SIZE(durable_arr); dh_idx++) { context = smb2_find_context_vals(req, durable_arr[dh_idx - 1], 4); if (IS_ERR(context)) { err = PTR_ERR(context); goto out; } if (!context) continue; switch (dh_idx) { case DURABLE_RECONN_V2: { struct create_durable_reconn_v2_req *recon_v2; if (dh_info->type == DURABLE_RECONN || dh_info->type == DURABLE_REQ_V2) { err = -EINVAL; goto out; } recon_v2 = (struct create_durable_reconn_v2_req *)context; persistent_id = recon_v2->Fid.PersistentFileId; dh_info->fp = ksmbd_lookup_durable_fd(persistent_id); if (!dh_info->fp) { ksmbd_debug(SMB, ""Failed to get durable handle state\n""); err = -EBADF; goto out; } if (memcmp(dh_info->fp->create_guid, recon_v2->CreateGuid, SMB2_CREATE_GUID_SIZE)) { err = -EBADF; ksmbd_put_durable_fd(dh_info->fp); goto out; } dh_info->type = dh_idx; dh_info->reconnected = true; ksmbd_debug(SMB, ""reconnect v2 Persistent-id from reconnect = %llu\n"", persistent_id); break; } case DURABLE_RECONN: { struct create_durable_reconn_req *recon; if (dh_info->type == DURABLE_RECONN_V2 || dh_info->type == DURABLE_REQ_V2) { err = -EINVAL; goto out; } recon = (struct create_durable_reconn_req *)context; persistent_id = recon->Data.Fid.PersistentFileId; dh_info->fp = ksmbd_lookup_durable_fd(persistent_id); if (!dh_info->fp) { ksmbd_debug(SMB, ""Failed to get durable handle state\n""); err = -EBADF; goto out; } dh_info->type = dh_idx; dh_info->reconnected = true; ksmbd_debug(SMB, ""reconnect Persistent-id from reconnect = %llu\n"", persistent_id); break; } case DURABLE_REQ_V2: { struct create_durable_req_v2 *durable_v2_blob; if (dh_info->type == DURABLE_RECONN || dh_info->type == DURABLE_RECONN_V2) { err = -EINVAL; goto out; } durable_v2_blob = (struct create_durable_req_v2 *)context; ksmbd_debug(SMB, ""Request for durable v2 open\n""); dh_info->fp = ksmbd_lookup_fd_cguid(durable_v2_blob->CreateGuid); if (dh_info->fp) { if (!memcmp(conn->ClientGUID, dh_info->fp->client_guid, SMB2_CLIENT_GUID_SIZE)) { if (!(req->hdr.Flags & SMB2_FLAGS_REPLAY_OPERATION)) { err = -ENOEXEC; goto out; } dh_info->fp->conn = conn; dh_info->reconnected = true; goto out; } } if ((lc && (lc->req_state & SMB2_LEASE_HANDLE_CACHING_LE)) || req_op_level == SMB2_OPLOCK_LEVEL_BATCH) { dh_info->CreateGuid = durable_v2_blob->CreateGuid; dh_info->persistent = le32_to_cpu(durable_v2_blob->Flags); dh_info->timeout = le32_to_cpu(durable_v2_blob->Timeout); dh_info->type = dh_idx; } break; } case DURABLE_REQ: if (dh_info->type == DURABLE_RECONN) goto out; if (dh_info->type == DURABLE_RECONN_V2 || dh_info->type == DURABLE_REQ_V2) { err = -EINVAL; goto out; } if ((lc && (lc->req_state & SMB2_LEASE_HANDLE_CACHING_LE)) || req_op_level == SMB2_OPLOCK_LEVEL_BATCH) { ksmbd_debug(SMB, ""Request for durable open\n""); dh_info->type = dh_idx; } } } out: return err; }"
1056----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50298/bad/enetc_pf.c----enetc_pf_remove,"static void enetc_pf_remove(struct pci_dev *pdev) { struct enetc_si *si = pci_get_drvdata(pdev); struct enetc_pf *pf = enetc_si_priv(si); struct enetc_ndev_priv *priv; priv = netdev_priv(si->ndev); if (pf->num_vfs) enetc_sriov_configure(pdev, 0); unregister_netdev(si->ndev); enetc_phylink_destroy(priv); enetc_mdiobus_destroy(pf); enetc_free_msix(priv); enetc_free_si_resources(priv); free_netdev(si->ndev); enetc_psi_destroy(pdev); <S2SV_StartVul> } <S2SV_EndVul>","- }
+ kfree(pf->vf_state);
+ }","static void enetc_pf_remove(struct pci_dev *pdev) { struct enetc_si *si = pci_get_drvdata(pdev); struct enetc_pf *pf = enetc_si_priv(si); struct enetc_ndev_priv *priv; priv = netdev_priv(si->ndev); if (pf->num_vfs) enetc_sriov_configure(pdev, 0); unregister_netdev(si->ndev); enetc_phylink_destroy(priv); enetc_mdiobus_destroy(pf); enetc_free_msix(priv); enetc_free_si_resources(priv); free_netdev(si->ndev); kfree(pf->vf_state); enetc_psi_destroy(pdev); }"
918----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50162/bad/devmap.c----bq_xmit_all,"static void bq_xmit_all(struct xdp_dev_bulk_queue *bq, u32 flags) { struct net_device *dev = bq->dev; unsigned int cnt = bq->count; int sent = 0, err = 0; int to_send = cnt; int i; if (unlikely(!cnt)) return; for (i = 0; i < cnt; i++) { struct xdp_frame *xdpf = bq->q[i]; prefetch(xdpf); } if (bq->xdp_prog) { <S2SV_StartVul> to_send = dev_map_bpf_prog_run(bq->xdp_prog, bq->q, cnt, dev); <S2SV_EndVul> if (!to_send) goto out; } sent = dev->netdev_ops->ndo_xdp_xmit(dev, to_send, bq->q, flags); if (sent < 0) { err = sent; sent = 0; } for (i = sent; unlikely(i < to_send); i++) xdp_return_frame_rx_napi(bq->q[i]); out: bq->count = 0; trace_xdp_devmap_xmit(bq->dev_rx, dev, sent, cnt - sent, err); }","- to_send = dev_map_bpf_prog_run(bq->xdp_prog, bq->q, cnt, dev);
+ to_send = dev_map_bpf_prog_run(bq->xdp_prog, bq->q, cnt, dev, bq->dev_rx);","static void bq_xmit_all(struct xdp_dev_bulk_queue *bq, u32 flags) { struct net_device *dev = bq->dev; unsigned int cnt = bq->count; int sent = 0, err = 0; int to_send = cnt; int i; if (unlikely(!cnt)) return; for (i = 0; i < cnt; i++) { struct xdp_frame *xdpf = bq->q[i]; prefetch(xdpf); } if (bq->xdp_prog) { to_send = dev_map_bpf_prog_run(bq->xdp_prog, bq->q, cnt, dev, bq->dev_rx); if (!to_send) goto out; } sent = dev->netdev_ops->ndo_xdp_xmit(dev, to_send, bq->q, flags); if (sent < 0) { err = sent; sent = 0; } for (i = sent; unlikely(i < to_send); i++) xdp_return_frame_rx_napi(bq->q[i]); out: bq->count = 0; trace_xdp_devmap_xmit(bq->dev_rx, dev, sent, cnt - sent, err); }"
1167----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53140/bad/af_netlink.c----deferred_put_nlk_sk,"static void deferred_put_nlk_sk(struct rcu_head *head) { struct netlink_sock *nlk = container_of(head, struct netlink_sock, rcu); struct sock *sk = &nlk->sk; kfree(nlk->groups); nlk->groups = NULL; if (!refcount_dec_and_test(&sk->sk_refcnt)) <S2SV_StartVul> return; <S2SV_EndVul> <S2SV_StartVul> if (nlk->cb_running && nlk->cb.done) { <S2SV_EndVul> <S2SV_StartVul> INIT_WORK(&nlk->work, netlink_sock_destruct_work); <S2SV_EndVul> <S2SV_StartVul> schedule_work(&nlk->work); <S2SV_EndVul> <S2SV_StartVul> return; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> sk_free(sk); <S2SV_StartVul> } <S2SV_EndVul>","- return;
- if (nlk->cb_running && nlk->cb.done) {
- INIT_WORK(&nlk->work, netlink_sock_destruct_work);
- schedule_work(&nlk->work);
- return;
- }
- }
+ }","static void deferred_put_nlk_sk(struct rcu_head *head) { struct netlink_sock *nlk = container_of(head, struct netlink_sock, rcu); struct sock *sk = &nlk->sk; kfree(nlk->groups); nlk->groups = NULL; if (!refcount_dec_and_test(&sk->sk_refcnt)) return; sk_free(sk); }"
408----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46869/bad/btintel_pcie.c----btintel_pcie_setup_hdev,"static int btintel_pcie_setup_hdev(struct btintel_pcie_data *data) { int err; struct hci_dev *hdev; <S2SV_StartVul> hdev = hci_alloc_dev(); <S2SV_EndVul> if (!hdev) return -ENOMEM; hdev->bus = HCI_PCI; hci_set_drvdata(hdev, data); data->hdev = hdev; SET_HCIDEV_DEV(hdev, &data->pdev->dev); hdev->manufacturer = 2; hdev->open = btintel_pcie_open; hdev->close = btintel_pcie_close; hdev->send = btintel_pcie_send_frame; hdev->setup = btintel_pcie_setup; hdev->shutdown = btintel_shutdown_combined; hdev->hw_error = btintel_hw_error; hdev->set_diag = btintel_set_diag; hdev->set_bdaddr = btintel_set_bdaddr; err = hci_register_dev(hdev); if (err < 0) { BT_ERR(""Failed to register to hdev (%d)"", err); goto exit_error; } return 0; exit_error: hci_free_dev(hdev); return err; }","- hdev = hci_alloc_dev();
+ hdev = hci_alloc_dev_priv(sizeof(struct btintel_data));","static int btintel_pcie_setup_hdev(struct btintel_pcie_data *data) { int err; struct hci_dev *hdev; hdev = hci_alloc_dev_priv(sizeof(struct btintel_data)); if (!hdev) return -ENOMEM; hdev->bus = HCI_PCI; hci_set_drvdata(hdev, data); data->hdev = hdev; SET_HCIDEV_DEV(hdev, &data->pdev->dev); hdev->manufacturer = 2; hdev->open = btintel_pcie_open; hdev->close = btintel_pcie_close; hdev->send = btintel_pcie_send_frame; hdev->setup = btintel_pcie_setup; hdev->shutdown = btintel_shutdown_combined; hdev->hw_error = btintel_hw_error; hdev->set_diag = btintel_set_diag; hdev->set_bdaddr = btintel_set_bdaddr; err = hci_register_dev(hdev); if (err < 0) { BT_ERR(""Failed to register to hdev (%d)"", err); goto exit_error; } return 0; exit_error: hci_free_dev(hdev); return err; }"
724----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49997/bad/lantiq_etop.c----ltq_etop_tx,"ltq_etop_tx(struct sk_buff *skb, struct net_device *dev) { int queue = skb_get_queue_mapping(skb); struct netdev_queue *txq = netdev_get_tx_queue(dev, queue); struct ltq_etop_priv *priv = netdev_priv(dev); struct ltq_etop_chan *ch = &priv->ch[(queue << 1) | 1]; struct ltq_dma_desc *desc = &ch->dma.desc_base[ch->dma.desc]; int len; unsigned long flags; u32 byte_offset; <S2SV_StartVul> len = skb->len < ETH_ZLEN ? ETH_ZLEN : skb->len; <S2SV_EndVul> if ((desc->ctl & (LTQ_DMA_OWN | LTQ_DMA_C)) || ch->skb[ch->dma.desc]) { netdev_err(dev, ""tx ring full\n""); netif_tx_stop_queue(txq); return NETDEV_TX_BUSY; } byte_offset = CPHYSADDR(skb->data) % 16; ch->skb[ch->dma.desc] = skb; netif_trans_update(dev); spin_lock_irqsave(&priv->lock, flags); desc->addr = ((unsigned int) dma_map_single(&priv->pdev->dev, skb->data, len, DMA_TO_DEVICE)) - byte_offset; wmb(); desc->ctl = LTQ_DMA_OWN | LTQ_DMA_SOP | LTQ_DMA_EOP | LTQ_DMA_TX_OFFSET(byte_offset) | (len & LTQ_DMA_SIZE_MASK); ch->dma.desc++; ch->dma.desc %= LTQ_DESC_NUM; spin_unlock_irqrestore(&priv->lock, flags); if (ch->dma.desc_base[ch->dma.desc].ctl & LTQ_DMA_OWN) netif_tx_stop_queue(txq); return NETDEV_TX_OK; }","- len = skb->len < ETH_ZLEN ? ETH_ZLEN : skb->len;
+ if (skb_put_padto(skb, ETH_ZLEN))
+ return NETDEV_TX_OK;
+ len = skb->len;","ltq_etop_tx(struct sk_buff *skb, struct net_device *dev) { int queue = skb_get_queue_mapping(skb); struct netdev_queue *txq = netdev_get_tx_queue(dev, queue); struct ltq_etop_priv *priv = netdev_priv(dev); struct ltq_etop_chan *ch = &priv->ch[(queue << 1) | 1]; struct ltq_dma_desc *desc = &ch->dma.desc_base[ch->dma.desc]; int len; unsigned long flags; u32 byte_offset; if (skb_put_padto(skb, ETH_ZLEN)) return NETDEV_TX_OK; len = skb->len; if ((desc->ctl & (LTQ_DMA_OWN | LTQ_DMA_C)) || ch->skb[ch->dma.desc]) { netdev_err(dev, ""tx ring full\n""); netif_tx_stop_queue(txq); return NETDEV_TX_BUSY; } byte_offset = CPHYSADDR(skb->data) % 16; ch->skb[ch->dma.desc] = skb; netif_trans_update(dev); spin_lock_irqsave(&priv->lock, flags); desc->addr = ((unsigned int) dma_map_single(&priv->pdev->dev, skb->data, len, DMA_TO_DEVICE)) - byte_offset; wmb(); desc->ctl = LTQ_DMA_OWN | LTQ_DMA_SOP | LTQ_DMA_EOP | LTQ_DMA_TX_OFFSET(byte_offset) | (len & LTQ_DMA_SIZE_MASK); ch->dma.desc++; ch->dma.desc %= LTQ_DESC_NUM; spin_unlock_irqrestore(&priv->lock, flags); if (ch->dma.desc_base[ch->dma.desc].ctl & LTQ_DMA_OWN) netif_tx_stop_queue(txq); return NETDEV_TX_OK; }"
625----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49929/bad/tx.c----iwl_mvm_tx_skb_sta,"int iwl_mvm_tx_skb_sta(struct iwl_mvm *mvm, struct sk_buff *skb, struct ieee80211_sta *sta) { <S2SV_StartVul> struct iwl_mvm_sta *mvmsta = iwl_mvm_sta_from_mac80211(sta); <S2SV_EndVul> struct ieee80211_tx_info info; struct sk_buff_head mpdus_skbs; struct ieee80211_vif *vif; unsigned int payload_len; int ret; struct sk_buff *orig_skb = skb; const u8 *addr3; <S2SV_StartVul> if (WARN_ON_ONCE(!mvmsta)) <S2SV_EndVul> <S2SV_StartVul> return -1; <S2SV_EndVul> if (WARN_ON_ONCE(mvmsta->deflink.sta_id == IWL_MVM_INVALID_STA)) <S2SV_StartVul> return -1; <S2SV_EndVul> memcpy(&info, skb->cb, sizeof(info)); if (!skb_is_gso(skb)) return iwl_mvm_tx_mpdu(mvm, skb, &info, sta, NULL); payload_len = skb_tail_pointer(skb) - skb_transport_header(skb) - tcp_hdrlen(skb) + skb->data_len; if (payload_len <= skb_shinfo(skb)->gso_size) return iwl_mvm_tx_mpdu(mvm, skb, &info, sta, NULL); __skb_queue_head_init(&mpdus_skbs); vif = info.control.vif; if (!vif) return -1; ret = iwl_mvm_tx_tso(mvm, skb, &info, sta, &mpdus_skbs); if (ret) return ret; WARN_ON(skb_queue_empty(&mpdus_skbs)); switch (vif->type) { case NL80211_IFTYPE_STATION: addr3 = vif->cfg.ap_addr; break; case NL80211_IFTYPE_AP: addr3 = vif->addr; break; default: addr3 = NULL; break; } while (!skb_queue_empty(&mpdus_skbs)) { struct ieee80211_hdr *hdr; bool amsdu; skb = __skb_dequeue(&mpdus_skbs); hdr = (void *)skb->data; amsdu = ieee80211_is_data_qos(hdr->frame_control) && (*ieee80211_get_qos_ctl(hdr) & IEEE80211_QOS_CTL_A_MSDU_PRESENT); ret = iwl_mvm_tx_mpdu(mvm, skb, &info, sta, amsdu ? addr3 : NULL); if (ret) { __skb_queue_purge(&mpdus_skbs); if (skb == orig_skb) ieee80211_free_txskb(mvm->hw, skb); else kfree_skb(skb); return 0; } } return 0; }","- struct iwl_mvm_sta *mvmsta = iwl_mvm_sta_from_mac80211(sta);
- if (WARN_ON_ONCE(!mvmsta))
- return -1;
- return -1;
+ struct iwl_mvm_sta *mvmsta;
+ if (WARN_ON_ONCE(!sta))
+ return -1;
+ mvmsta = iwl_mvm_sta_from_mac80211(sta);
+ return -1;","int iwl_mvm_tx_skb_sta(struct iwl_mvm *mvm, struct sk_buff *skb, struct ieee80211_sta *sta) { struct iwl_mvm_sta *mvmsta; struct ieee80211_tx_info info; struct sk_buff_head mpdus_skbs; struct ieee80211_vif *vif; unsigned int payload_len; int ret; struct sk_buff *orig_skb = skb; const u8 *addr3; if (WARN_ON_ONCE(!sta)) return -1; mvmsta = iwl_mvm_sta_from_mac80211(sta); if (WARN_ON_ONCE(mvmsta->deflink.sta_id == IWL_MVM_INVALID_STA)) return -1; memcpy(&info, skb->cb, sizeof(info)); if (!skb_is_gso(skb)) return iwl_mvm_tx_mpdu(mvm, skb, &info, sta, NULL); payload_len = skb_tail_pointer(skb) - skb_transport_header(skb) - tcp_hdrlen(skb) + skb->data_len; if (payload_len <= skb_shinfo(skb)->gso_size) return iwl_mvm_tx_mpdu(mvm, skb, &info, sta, NULL); __skb_queue_head_init(&mpdus_skbs); vif = info.control.vif; if (!vif) return -1; ret = iwl_mvm_tx_tso(mvm, skb, &info, sta, &mpdus_skbs); if (ret) return ret; WARN_ON(skb_queue_empty(&mpdus_skbs)); switch (vif->type) { case NL80211_IFTYPE_STATION: addr3 = vif->cfg.ap_addr; break; case NL80211_IFTYPE_AP: addr3 = vif->addr; break; default: addr3 = NULL; break; } while (!skb_queue_empty(&mpdus_skbs)) { struct ieee80211_hdr *hdr; bool amsdu; skb = __skb_dequeue(&mpdus_skbs); hdr = (void *)skb->data; amsdu = ieee80211_is_data_qos(hdr->frame_control) && (*ieee80211_get_qos_ctl(hdr) & IEEE80211_QOS_CTL_A_MSDU_PRESENT); ret = iwl_mvm_tx_mpdu(mvm, skb, &info, sta, amsdu ? addr3 : NULL); if (ret) { __skb_queue_purge(&mpdus_skbs); if (skb == orig_skb) ieee80211_free_txskb(mvm->hw, skb); else kfree_skb(skb); return 0; } } return 0; }"
680----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49979/bad/tcp_offload.c----*tcp4_gso_segment,"static struct sk_buff *tcp4_gso_segment(struct sk_buff *skb, netdev_features_t features) { if (!(skb_shinfo(skb)->gso_type & SKB_GSO_TCPV4)) return ERR_PTR(-EINVAL); if (!pskb_may_pull(skb, sizeof(struct tcphdr))) return ERR_PTR(-EINVAL); <S2SV_StartVul> if (skb_shinfo(skb)->gso_type & SKB_GSO_FRAGLIST) <S2SV_EndVul> <S2SV_StartVul> return __tcp4_gso_segment_list(skb, features); <S2SV_EndVul> if (unlikely(skb->ip_summed != CHECKSUM_PARTIAL)) { const struct iphdr *iph = ip_hdr(skb); struct tcphdr *th = tcp_hdr(skb); th->check = 0; skb->ip_summed = CHECKSUM_PARTIAL; __tcp_v4_send_check(skb, iph->saddr, iph->daddr); } return tcp_gso_segment(skb, features); }","- if (skb_shinfo(skb)->gso_type & SKB_GSO_FRAGLIST)
- return __tcp4_gso_segment_list(skb, features);
+ if (skb_shinfo(skb)->gso_type & SKB_GSO_FRAGLIST) {
+ struct tcphdr *th = tcp_hdr(skb);
+ if (skb_pagelen(skb) - th->doff * 4 == skb_shinfo(skb)->gso_size)
+ return __tcp4_gso_segment_list(skb, features);
+ skb->ip_summed = CHECKSUM_NONE;
+ }","static struct sk_buff *tcp4_gso_segment(struct sk_buff *skb, netdev_features_t features) { if (!(skb_shinfo(skb)->gso_type & SKB_GSO_TCPV4)) return ERR_PTR(-EINVAL); if (!pskb_may_pull(skb, sizeof(struct tcphdr))) return ERR_PTR(-EINVAL); if (skb_shinfo(skb)->gso_type & SKB_GSO_FRAGLIST) { struct tcphdr *th = tcp_hdr(skb); if (skb_pagelen(skb) - th->doff * 4 == skb_shinfo(skb)->gso_size) return __tcp4_gso_segment_list(skb, features); skb->ip_summed = CHECKSUM_NONE; } if (unlikely(skb->ip_summed != CHECKSUM_PARTIAL)) { const struct iphdr *iph = ip_hdr(skb); struct tcphdr *th = tcp_hdr(skb); th->check = 0; skb->ip_summed = CHECKSUM_PARTIAL; __tcp_v4_send_check(skb, iph->saddr, iph->daddr); } return tcp_gso_segment(skb, features); }"
1222----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53232/bad/s390-iommu.c----s390_iommu_attach_device,"static int s390_iommu_attach_device(struct iommu_domain *domain, struct device *dev) { struct s390_domain *s390_domain = to_s390_domain(domain); struct zpci_dev *zdev = to_zpci_dev(dev); unsigned long flags; u8 status; int cc; if (!zdev) return -ENODEV; if (WARN_ON(domain->geometry.aperture_start > zdev->end_dma || domain->geometry.aperture_end < zdev->start_dma)) return -EINVAL; <S2SV_StartVul> if (zdev->s390_domain) <S2SV_EndVul> <S2SV_StartVul> s390_iommu_detach_device(&zdev->s390_domain->domain, dev); <S2SV_EndVul> cc = zpci_register_ioat(zdev, 0, zdev->start_dma, zdev->end_dma, virt_to_phys(s390_domain->dma_table), &status); if (cc && status != ZPCI_PCI_ST_FUNC_NOT_AVAIL) return -EIO; zdev->dma_table = s390_domain->dma_table; <S2SV_StartVul> zdev->s390_domain = s390_domain; <S2SV_EndVul> spin_lock_irqsave(&s390_domain->list_lock, flags); list_add_rcu(&zdev->iommu_list, &s390_domain->devices); spin_unlock_irqrestore(&s390_domain->list_lock, flags); return 0; }","- if (zdev->s390_domain)
- s390_iommu_detach_device(&zdev->s390_domain->domain, dev);
- zdev->s390_domain = s390_domain;
+ blocking_domain_attach_device(&blocking_domain, dev);
+ zdev_s390_domain_update(zdev, domain);","static int s390_iommu_attach_device(struct iommu_domain *domain, struct device *dev) { struct s390_domain *s390_domain = to_s390_domain(domain); struct zpci_dev *zdev = to_zpci_dev(dev); unsigned long flags; u8 status; int cc; if (!zdev) return -ENODEV; if (WARN_ON(domain->geometry.aperture_start > zdev->end_dma || domain->geometry.aperture_end < zdev->start_dma)) return -EINVAL; blocking_domain_attach_device(&blocking_domain, dev); cc = zpci_register_ioat(zdev, 0, zdev->start_dma, zdev->end_dma, virt_to_phys(s390_domain->dma_table), &status); if (cc && status != ZPCI_PCI_ST_FUNC_NOT_AVAIL) return -EIO; zdev->dma_table = s390_domain->dma_table; zdev_s390_domain_update(zdev, domain); spin_lock_irqsave(&s390_domain->list_lock, flags); list_add_rcu(&zdev->iommu_list, &s390_domain->devices); spin_unlock_irqrestore(&s390_domain->list_lock, flags); return 0; }"
12----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-43891/bad/trace_events_inject.c----event_inject_write,"event_inject_write(struct file *filp, const char __user *ubuf, size_t cnt, loff_t *ppos) { struct trace_event_call *call; struct trace_event_file *file; int err = -ENODEV, size; void *entry = NULL; char *buf; if (cnt >= PAGE_SIZE) return -EINVAL; buf = memdup_user_nul(ubuf, cnt); if (IS_ERR(buf)) return PTR_ERR(buf); strim(buf); mutex_lock(&event_mutex); <S2SV_StartVul> file = event_file_data(filp); <S2SV_EndVul> if (file) { call = file->event_call; size = parse_entry(buf, call, &entry); if (size < 0) err = size; else err = trace_inject_entry(file, entry, size); } mutex_unlock(&event_mutex); kfree(entry); kfree(buf); if (err < 0) return err; *ppos += err; return cnt; }","- file = event_file_data(filp);
+ file = event_file_file(filp);","event_inject_write(struct file *filp, const char __user *ubuf, size_t cnt, loff_t *ppos) { struct trace_event_call *call; struct trace_event_file *file; int err = -ENODEV, size; void *entry = NULL; char *buf; if (cnt >= PAGE_SIZE) return -EINVAL; buf = memdup_user_nul(ubuf, cnt); if (IS_ERR(buf)) return PTR_ERR(buf); strim(buf); mutex_lock(&event_mutex); file = event_file_file(filp); if (file) { call = file->event_call; size = parse_entry(buf, call, &entry); if (size < 0) err = size; else err = trace_inject_entry(file, entry, size); } mutex_unlock(&event_mutex); kfree(entry); kfree(buf); if (err < 0) return err; *ppos += err; return cnt; }"
1015----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50252/bad/spectrum_ipip.c----mlxsw_sp_ipip_ol_netdev_change_gre6,"mlxsw_sp_ipip_ol_netdev_change_gre6(struct mlxsw_sp *mlxsw_sp, struct mlxsw_sp_ipip_entry *ipip_entry, struct netlink_ext_ack *extack) { struct mlxsw_sp_ipip_parms new_parms; new_parms = mlxsw_sp_ipip_netdev_parms_init_gre6(ipip_entry->ol_dev); <S2SV_StartVul> return mlxsw_sp_ipip_ol_netdev_change_gre(mlxsw_sp, ipip_entry, <S2SV_EndVul> <S2SV_StartVul> &new_parms, extack); <S2SV_EndVul> }","- return mlxsw_sp_ipip_ol_netdev_change_gre(mlxsw_sp, ipip_entry,
- &new_parms, extack);
+ u32 new_kvdl_index, old_kvdl_index = ipip_entry->dip_kvdl_index;
+ struct in6_addr old_addr6 = ipip_entry->parms.daddr.addr6;
+ int err;
+ err = mlxsw_sp_ipv6_addr_kvdl_index_get(mlxsw_sp,
+ &new_parms.daddr.addr6,
+ &new_kvdl_index);
+ if (err)
+ return err;
+ ipip_entry->dip_kvdl_index = new_kvdl_index;
+ err = mlxsw_sp_ipip_ol_netdev_change_gre(mlxsw_sp, ipip_entry,
+ &new_parms, extack);
+ if (err)
+ goto err_change_gre;
+ mlxsw_sp_ipv6_addr_put(mlxsw_sp, &old_addr6);
+ return 0;
+ err_change_gre:
+ ipip_entry->dip_kvdl_index = old_kvdl_index;
+ mlxsw_sp_ipv6_addr_put(mlxsw_sp, &new_parms.daddr.addr6);
+ return err;","mlxsw_sp_ipip_ol_netdev_change_gre6(struct mlxsw_sp *mlxsw_sp, struct mlxsw_sp_ipip_entry *ipip_entry, struct netlink_ext_ack *extack) { u32 new_kvdl_index, old_kvdl_index = ipip_entry->dip_kvdl_index; struct in6_addr old_addr6 = ipip_entry->parms.daddr.addr6; struct mlxsw_sp_ipip_parms new_parms; int err; new_parms = mlxsw_sp_ipip_netdev_parms_init_gre6(ipip_entry->ol_dev); err = mlxsw_sp_ipv6_addr_kvdl_index_get(mlxsw_sp, &new_parms.daddr.addr6, &new_kvdl_index); if (err) return err; ipip_entry->dip_kvdl_index = new_kvdl_index; err = mlxsw_sp_ipip_ol_netdev_change_gre(mlxsw_sp, ipip_entry, &new_parms, extack); if (err) goto err_change_gre; mlxsw_sp_ipv6_addr_put(mlxsw_sp, &old_addr6); return 0; err_change_gre: ipip_entry->dip_kvdl_index = old_kvdl_index; mlxsw_sp_ipv6_addr_put(mlxsw_sp, &new_parms.daddr.addr6); return err; }"
166----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-45021/bad/memcontrol.c----memcg_write_event_control,"static ssize_t memcg_write_event_control(struct kernfs_open_file *of, char *buf, size_t nbytes, loff_t off) { struct cgroup_subsys_state *css = of_css(of); struct mem_cgroup *memcg = mem_cgroup_from_css(css); struct mem_cgroup_event *event; struct cgroup_subsys_state *cfile_css; unsigned int efd, cfd; struct fd efile; struct fd cfile; struct dentry *cdentry; const char *name; char *endp; int ret; buf = strstrip(buf); efd = simple_strtoul(buf, &endp, 10); if (*endp != ' ') return -EINVAL; buf = endp + 1; cfd = simple_strtoul(buf, &endp, 10); <S2SV_StartVul> if ((*endp != ' ') && (*endp != '\0')) <S2SV_EndVul> return -EINVAL; <S2SV_StartVul> buf = endp + 1; <S2SV_EndVul> event = kzalloc(sizeof(*event), GFP_KERNEL); if (!event) return -ENOMEM; event->memcg = memcg; INIT_LIST_HEAD(&event->list); init_poll_funcptr(&event->pt, memcg_event_ptable_queue_proc); init_waitqueue_func_entry(&event->wait, memcg_event_wake); INIT_WORK(&event->remove, memcg_event_remove); efile = fdget(efd); if (!efile.file) { ret = -EBADF; goto out_kfree; } event->eventfd = eventfd_ctx_fileget(efile.file); if (IS_ERR(event->eventfd)) { ret = PTR_ERR(event->eventfd); goto out_put_efile; } cfile = fdget(cfd); if (!cfile.file) { ret = -EBADF; goto out_put_eventfd; } ret = file_permission(cfile.file, MAY_READ); if (ret < 0) goto out_put_cfile; cdentry = cfile.file->f_path.dentry; if (cdentry->d_sb->s_type != &cgroup_fs_type || !d_is_reg(cdentry)) { ret = -EINVAL; goto out_put_cfile; } name = cdentry->d_name.name; if (!strcmp(name, ""memory.usage_in_bytes"")) { event->register_event = mem_cgroup_usage_register_event; event->unregister_event = mem_cgroup_usage_unregister_event; } else if (!strcmp(name, ""memory.oom_control"")) { event->register_event = mem_cgroup_oom_register_event; event->unregister_event = mem_cgroup_oom_unregister_event; } else if (!strcmp(name, ""memory.pressure_level"")) { event->register_event = vmpressure_register_event; event->unregister_event = vmpressure_unregister_event; } else if (!strcmp(name, ""memory.memsw.usage_in_bytes"")) { event->register_event = memsw_cgroup_usage_register_event; event->unregister_event = memsw_cgroup_usage_unregister_event; } else { ret = -EINVAL; goto out_put_cfile; } cfile_css = css_tryget_online_from_dir(cdentry->d_parent, &memory_cgrp_subsys); ret = -EINVAL; if (IS_ERR(cfile_css)) goto out_put_cfile; if (cfile_css != css) { css_put(cfile_css); goto out_put_cfile; } ret = event->register_event(memcg, event->eventfd, buf); if (ret) goto out_put_css; vfs_poll(efile.file, &event->pt); spin_lock(&memcg->event_list_lock); list_add(&event->list, &memcg->event_list); spin_unlock(&memcg->event_list_lock); fdput(cfile); fdput(efile); return nbytes; out_put_css: css_put(css); out_put_cfile: fdput(cfile); out_put_eventfd: eventfd_ctx_put(event->eventfd); out_put_efile: fdput(efile); out_kfree: kfree(event); return ret; }","- if ((*endp != ' ') && (*endp != '\0'))
- buf = endp + 1;
+ if (*endp == '\0')
+ buf = endp;
+ else if (*endp == ' ')
+ buf = endp + 1;
+ else","static ssize_t memcg_write_event_control(struct kernfs_open_file *of, char *buf, size_t nbytes, loff_t off) { struct cgroup_subsys_state *css = of_css(of); struct mem_cgroup *memcg = mem_cgroup_from_css(css); struct mem_cgroup_event *event; struct cgroup_subsys_state *cfile_css; unsigned int efd, cfd; struct fd efile; struct fd cfile; struct dentry *cdentry; const char *name; char *endp; int ret; buf = strstrip(buf); efd = simple_strtoul(buf, &endp, 10); if (*endp != ' ') return -EINVAL; buf = endp + 1; cfd = simple_strtoul(buf, &endp, 10); if (*endp == '\0') buf = endp; else if (*endp == ' ') buf = endp + 1; else return -EINVAL; event = kzalloc(sizeof(*event), GFP_KERNEL); if (!event) return -ENOMEM; event->memcg = memcg; INIT_LIST_HEAD(&event->list); init_poll_funcptr(&event->pt, memcg_event_ptable_queue_proc); init_waitqueue_func_entry(&event->wait, memcg_event_wake); INIT_WORK(&event->remove, memcg_event_remove); efile = fdget(efd); if (!efile.file) { ret = -EBADF; goto out_kfree; } event->eventfd = eventfd_ctx_fileget(efile.file); if (IS_ERR(event->eventfd)) { ret = PTR_ERR(event->eventfd); goto out_put_efile; } cfile = fdget(cfd); if (!cfile.file) { ret = -EBADF; goto out_put_eventfd; } ret = file_permission(cfile.file, MAY_READ); if (ret < 0) goto out_put_cfile; cdentry = cfile.file->f_path.dentry; if (cdentry->d_sb->s_type != &cgroup_fs_type || !d_is_reg(cdentry)) { ret = -EINVAL; goto out_put_cfile; } name = cdentry->d_name.name; if (!strcmp(name, ""memory.usage_in_bytes"")) { event->register_event = mem_cgroup_usage_register_event; event->unregister_event = mem_cgroup_usage_unregister_event; } else if (!strcmp(name, ""memory.oom_control"")) { event->register_event = mem_cgroup_oom_register_event; event->unregister_event = mem_cgroup_oom_unregister_event; } else if (!strcmp(name, ""memory.pressure_level"")) { event->register_event = vmpressure_register_event; event->unregister_event = vmpressure_unregister_event; } else if (!strcmp(name, ""memory.memsw.usage_in_bytes"")) { event->register_event = memsw_cgroup_usage_register_event; event->unregister_event = memsw_cgroup_usage_unregister_event; } else { ret = -EINVAL; goto out_put_cfile; } cfile_css = css_tryget_online_from_dir(cdentry->d_parent, &memory_cgrp_subsys); ret = -EINVAL; if (IS_ERR(cfile_css)) goto out_put_cfile; if (cfile_css != css) { css_put(cfile_css); goto out_put_cfile; } ret = event->register_event(memcg, event->eventfd, buf); if (ret) goto out_put_css; vfs_poll(efile.file, &event->pt); spin_lock(&memcg->event_list_lock); list_add(&event->list, &memcg->event_list); spin_unlock(&memcg->event_list_lock); fdput(cfile); fdput(efile); return nbytes; out_put_css: css_put(css); out_put_cfile: fdput(cfile); out_put_eventfd: eventfd_ctx_put(event->eventfd); out_put_efile: fdput(efile); out_kfree: kfree(event); return ret; }"
1171----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53145/bad/physmem.c----setup_physmem,"void __init setup_physmem(unsigned long start, unsigned long reserve_end, unsigned long len, unsigned long long highmem) { unsigned long reserve = reserve_end - start; <S2SV_StartVul> long map_size = len - reserve; <S2SV_EndVul> int err; <S2SV_StartVul> if(map_size <= 0) { <S2SV_EndVul> os_warn(""Too few physical memory! Needed=%lu, given=%lu\n"", reserve, len); exit(1); } physmem_fd = create_mem_file(len + highmem); err = os_map_memory((void *) reserve_end, physmem_fd, reserve, map_size, 1, 1, 1); if (err < 0) { <S2SV_StartVul> os_warn(""setup_physmem - mapping %ld bytes of memory at 0x%p "" <S2SV_EndVul> ""failed - errno = %d\n"", map_size, (void *) reserve_end, err); exit(1); } os_seek_file(physmem_fd, __pa(__syscall_stub_start)); os_write_file(physmem_fd, __syscall_stub_start, PAGE_SIZE); os_fsync_file(physmem_fd); memblock_add(__pa(start), len + highmem); memblock_reserve(__pa(start), reserve); min_low_pfn = PFN_UP(__pa(reserve_end)); max_low_pfn = min_low_pfn + (map_size >> PAGE_SHIFT); }","- long map_size = len - reserve;
- if(map_size <= 0) {
- os_warn(""setup_physmem - mapping %ld bytes of memory at 0x%p ""
+ unsigned long map_size = len - reserve;
+ if (len <= reserve) {
+ os_warn(""setup_physmem - mapping %lu bytes of memory at 0x%p ""","void __init setup_physmem(unsigned long start, unsigned long reserve_end, unsigned long len, unsigned long long highmem) { unsigned long reserve = reserve_end - start; unsigned long map_size = len - reserve; int err; if (len <= reserve) { os_warn(""Too few physical memory! Needed=%lu, given=%lu\n"", reserve, len); exit(1); } physmem_fd = create_mem_file(len + highmem); err = os_map_memory((void *) reserve_end, physmem_fd, reserve, map_size, 1, 1, 1); if (err < 0) { os_warn(""setup_physmem - mapping %lu bytes of memory at 0x%p "" ""failed - errno = %d\n"", map_size, (void *) reserve_end, err); exit(1); } os_seek_file(physmem_fd, __pa(__syscall_stub_start)); os_write_file(physmem_fd, __syscall_stub_start, PAGE_SIZE); os_fsync_file(physmem_fd); memblock_add(__pa(start), len + highmem); memblock_reserve(__pa(start), reserve); min_low_pfn = PFN_UP(__pa(reserve_end)); max_low_pfn = min_low_pfn + (map_size >> PAGE_SHIFT); }"
102----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44963/bad/qgroup.c----btrfs_quota_disable,"int btrfs_quota_disable(struct btrfs_fs_info *fs_info) { struct btrfs_root *quota_root = NULL; struct btrfs_trans_handle *trans = NULL; int ret = 0; lockdep_assert_held_write(&fs_info->subvol_sem); lockdep_assert_held(&fs_info->cleaner_mutex); mutex_lock(&fs_info->qgroup_ioctl_lock); if (!fs_info->quota_root) goto out; mutex_unlock(&fs_info->qgroup_ioctl_lock); clear_bit(BTRFS_FS_QUOTA_ENABLED, &fs_info->flags); btrfs_qgroup_wait_for_completion(fs_info, false); ret = flush_reservations(fs_info); if (ret) return ret; trans = btrfs_start_transaction(fs_info->tree_root, 1); mutex_lock(&fs_info->qgroup_ioctl_lock); if (IS_ERR(trans)) { ret = PTR_ERR(trans); trans = NULL; set_bit(BTRFS_FS_QUOTA_ENABLED, &fs_info->flags); goto out; } if (!fs_info->quota_root) goto out; spin_lock(&fs_info->qgroup_lock); quota_root = fs_info->quota_root; fs_info->quota_root = NULL; fs_info->qgroup_flags &= ~BTRFS_QGROUP_STATUS_FLAG_ON; fs_info->qgroup_flags &= ~BTRFS_QGROUP_STATUS_FLAG_SIMPLE_MODE; fs_info->qgroup_drop_subtree_thres = BTRFS_MAX_LEVEL; spin_unlock(&fs_info->qgroup_lock); btrfs_free_qgroup_config(fs_info); ret = btrfs_clean_quota_tree(trans, quota_root); if (ret) { btrfs_abort_transaction(trans, ret); goto out; } ret = btrfs_del_root(trans, &quota_root->root_key); if (ret) { btrfs_abort_transaction(trans, ret); goto out; } spin_lock(&fs_info->trans_lock); list_del(&quota_root->dirty_list); spin_unlock(&fs_info->trans_lock); btrfs_tree_lock(quota_root->node); btrfs_clear_buffer_dirty(trans, quota_root->node); btrfs_tree_unlock(quota_root->node); <S2SV_StartVul> btrfs_free_tree_block(trans, btrfs_root_id(quota_root), <S2SV_EndVul> <S2SV_StartVul> quota_root->node, 0, 1); <S2SV_EndVul> out: btrfs_put_root(quota_root); mutex_unlock(&fs_info->qgroup_ioctl_lock); if (ret && trans) btrfs_end_transaction(trans); else if (trans) ret = btrfs_commit_transaction(trans); return ret; }","- btrfs_free_tree_block(trans, btrfs_root_id(quota_root),
- quota_root->node, 0, 1);
+ ret = btrfs_free_tree_block(trans, btrfs_root_id(quota_root),
+ quota_root->node, 0, 1);
+ if (ret < 0)
+ btrfs_abort_transaction(trans, ret);","int btrfs_quota_disable(struct btrfs_fs_info *fs_info) { struct btrfs_root *quota_root = NULL; struct btrfs_trans_handle *trans = NULL; int ret = 0; lockdep_assert_held_write(&fs_info->subvol_sem); lockdep_assert_held(&fs_info->cleaner_mutex); mutex_lock(&fs_info->qgroup_ioctl_lock); if (!fs_info->quota_root) goto out; mutex_unlock(&fs_info->qgroup_ioctl_lock); clear_bit(BTRFS_FS_QUOTA_ENABLED, &fs_info->flags); btrfs_qgroup_wait_for_completion(fs_info, false); ret = flush_reservations(fs_info); if (ret) return ret; trans = btrfs_start_transaction(fs_info->tree_root, 1); mutex_lock(&fs_info->qgroup_ioctl_lock); if (IS_ERR(trans)) { ret = PTR_ERR(trans); trans = NULL; set_bit(BTRFS_FS_QUOTA_ENABLED, &fs_info->flags); goto out; } if (!fs_info->quota_root) goto out; spin_lock(&fs_info->qgroup_lock); quota_root = fs_info->quota_root; fs_info->quota_root = NULL; fs_info->qgroup_flags &= ~BTRFS_QGROUP_STATUS_FLAG_ON; fs_info->qgroup_flags &= ~BTRFS_QGROUP_STATUS_FLAG_SIMPLE_MODE; fs_info->qgroup_drop_subtree_thres = BTRFS_MAX_LEVEL; spin_unlock(&fs_info->qgroup_lock); btrfs_free_qgroup_config(fs_info); ret = btrfs_clean_quota_tree(trans, quota_root); if (ret) { btrfs_abort_transaction(trans, ret); goto out; } ret = btrfs_del_root(trans, &quota_root->root_key); if (ret) { btrfs_abort_transaction(trans, ret); goto out; } spin_lock(&fs_info->trans_lock); list_del(&quota_root->dirty_list); spin_unlock(&fs_info->trans_lock); btrfs_tree_lock(quota_root->node); btrfs_clear_buffer_dirty(trans, quota_root->node); btrfs_tree_unlock(quota_root->node); ret = btrfs_free_tree_block(trans, btrfs_root_id(quota_root), quota_root->node, 0, 1); if (ret < 0) btrfs_abort_transaction(trans, ret); out: btrfs_put_root(quota_root); mutex_unlock(&fs_info->qgroup_ioctl_lock); if (ret && trans) btrfs_end_transaction(trans); else if (trans) ret = btrfs_commit_transaction(trans); return ret; }"
659----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49959/bad/checkpoint.c----__releases,"__releases(&journal->j_state_lock) { int nblocks, space_left; nblocks = journal->j_max_transaction_buffers; while (jbd2_log_space_left(journal) < nblocks) { write_unlock(&journal->j_state_lock); mutex_lock_io(&journal->j_checkpoint_mutex); write_lock(&journal->j_state_lock); if (journal->j_flags & JBD2_ABORT) { mutex_unlock(&journal->j_checkpoint_mutex); return; } spin_lock(&journal->j_list_lock); space_left = jbd2_log_space_left(journal); if (space_left < nblocks) { int chkpt = journal->j_checkpoint_transactions != NULL; tid_t tid = 0; bool has_transaction = false; if (journal->j_committing_transaction) { tid = journal->j_committing_transaction->t_tid; has_transaction = true; } spin_unlock(&journal->j_list_lock); write_unlock(&journal->j_state_lock); if (chkpt) { jbd2_log_do_checkpoint(journal); <S2SV_StartVul> } else if (jbd2_cleanup_journal_tail(journal) == 0) { <S2SV_EndVul> ; } else if (has_transaction) { mutex_unlock(&journal->j_checkpoint_mutex); jbd2_log_wait_commit(journal, tid); write_lock(&journal->j_state_lock); continue; } else { printk(KERN_ERR ""%s: needed %d blocks and "" ""only had %d space available\n"", __func__, nblocks, space_left); printk(KERN_ERR ""%s: no way to get more "" ""journal space in %s\n"", __func__, journal->j_devname); WARN_ON(1); jbd2_journal_abort(journal, -EIO); } write_lock(&journal->j_state_lock); } else { spin_unlock(&journal->j_list_lock); } mutex_unlock(&journal->j_checkpoint_mutex); } }","- } else if (jbd2_cleanup_journal_tail(journal) == 0) {
+ } else if (jbd2_cleanup_journal_tail(journal) <= 0) {","__releases(&journal->j_state_lock) { int nblocks, space_left; nblocks = journal->j_max_transaction_buffers; while (jbd2_log_space_left(journal) < nblocks) { write_unlock(&journal->j_state_lock); mutex_lock_io(&journal->j_checkpoint_mutex); write_lock(&journal->j_state_lock); if (journal->j_flags & JBD2_ABORT) { mutex_unlock(&journal->j_checkpoint_mutex); return; } spin_lock(&journal->j_list_lock); space_left = jbd2_log_space_left(journal); if (space_left < nblocks) { int chkpt = journal->j_checkpoint_transactions != NULL; tid_t tid = 0; bool has_transaction = false; if (journal->j_committing_transaction) { tid = journal->j_committing_transaction->t_tid; has_transaction = true; } spin_unlock(&journal->j_list_lock); write_unlock(&journal->j_state_lock); if (chkpt) { jbd2_log_do_checkpoint(journal); } else if (jbd2_cleanup_journal_tail(journal) <= 0) { ; } else if (has_transaction) { mutex_unlock(&journal->j_checkpoint_mutex); jbd2_log_wait_commit(journal, tid); write_lock(&journal->j_state_lock); continue; } else { printk(KERN_ERR ""%s: needed %d blocks and "" ""only had %d space available\n"", __func__, nblocks, space_left); printk(KERN_ERR ""%s: no way to get more "" ""journal space in %s\n"", __func__, journal->j_devname); WARN_ON(1); jbd2_journal_abort(journal, -EIO); } write_lock(&journal->j_state_lock); } else { spin_unlock(&journal->j_list_lock); } mutex_unlock(&journal->j_checkpoint_mutex); } }"
1107----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53087/bad/xe_exec.c----xe_exec_ioctl,"int xe_exec_ioctl(struct drm_device *dev, void *data, struct drm_file *file) { struct xe_device *xe = to_xe_device(dev); struct xe_file *xef = to_xe_file(file); struct drm_xe_exec *args = data; struct drm_xe_sync __user *syncs_user = u64_to_user_ptr(args->syncs); u64 __user *addresses_user = u64_to_user_ptr(args->address); struct xe_exec_queue *q; struct xe_sync_entry *syncs = NULL; u64 addresses[XE_HW_ENGINE_MAX_INSTANCE]; struct drm_gpuvm_exec vm_exec = {.extra.fn = xe_exec_fn}; struct drm_exec *exec = &vm_exec.exec; u32 i, num_syncs, num_ufence = 0; struct xe_sched_job *job; struct xe_vm *vm; bool write_locked, skip_retry = false; ktime_t end = 0; int err = 0; struct xe_hw_engine_group *group; enum xe_hw_engine_group_execution_mode mode, previous_mode; if (XE_IOCTL_DBG(xe, args->extensions) || XE_IOCTL_DBG(xe, args->pad[0] || args->pad[1] || args->pad[2]) || XE_IOCTL_DBG(xe, args->reserved[0] || args->reserved[1])) return -EINVAL; q = xe_exec_queue_lookup(xef, args->exec_queue_id); if (XE_IOCTL_DBG(xe, !q)) return -ENOENT; <S2SV_StartVul> if (XE_IOCTL_DBG(xe, q->flags & EXEC_QUEUE_FLAG_VM)) <S2SV_EndVul> <S2SV_StartVul> return -EINVAL; <S2SV_EndVul> if (XE_IOCTL_DBG(xe, args->num_batch_buffer && <S2SV_StartVul> q->width != args->num_batch_buffer)) <S2SV_EndVul> <S2SV_StartVul> return -EINVAL; <S2SV_EndVul> if (XE_IOCTL_DBG(xe, q->ops->reset_status(q))) { err = -ECANCELED; goto err_exec_queue; } if (args->num_syncs) { syncs = kcalloc(args->num_syncs, sizeof(*syncs), GFP_KERNEL); if (!syncs) { err = -ENOMEM; goto err_exec_queue; } } vm = q->vm; for (num_syncs = 0; num_syncs < args->num_syncs; num_syncs++) { err = xe_sync_entry_parse(xe, xef, &syncs[num_syncs], &syncs_user[num_syncs], SYNC_PARSE_FLAG_EXEC | (xe_vm_in_lr_mode(vm) ? SYNC_PARSE_FLAG_LR_MODE : 0)); if (err) goto err_syncs; if (xe_sync_is_ufence(&syncs[num_syncs])) num_ufence++; } if (XE_IOCTL_DBG(xe, num_ufence > 1)) { err = -EINVAL; goto err_syncs; } if (xe_exec_queue_is_parallel(q)) { err = __copy_from_user(addresses, addresses_user, sizeof(u64) * q->width); if (err) { err = -EFAULT; goto err_syncs; } } group = q->hwe->hw_engine_group; mode = xe_hw_engine_group_find_exec_mode(q); if (mode == EXEC_MODE_DMA_FENCE) { err = xe_hw_engine_group_get_mode(group, mode, &previous_mode); if (err) goto err_syncs; } retry: if (!xe_vm_in_lr_mode(vm) && xe_vm_userptr_check_repin(vm)) { err = down_write_killable(&vm->lock); write_locked = true; } else { err = down_read_interruptible(&vm->lock); write_locked = false; } if (err) goto err_syncs; if (write_locked) { err = xe_vm_userptr_pin(vm); downgrade_write(&vm->lock); write_locked = false; if (err) goto err_hw_exec_mode; } if (!args->num_batch_buffer) { err = xe_vm_lock(vm, true); if (err) goto err_unlock_list; if (!xe_vm_in_lr_mode(vm)) { struct dma_fence *fence; fence = xe_sync_in_fence_get(syncs, num_syncs, q, vm); if (IS_ERR(fence)) { err = PTR_ERR(fence); goto err_unlock_list; } for (i = 0; i < num_syncs; i++) xe_sync_entry_signal(&syncs[i], fence); xe_exec_queue_last_fence_set(q, vm, fence); dma_fence_put(fence); } xe_vm_unlock(vm); goto err_unlock_list; } vm_exec.vm = &vm->gpuvm; vm_exec.flags = DRM_EXEC_INTERRUPTIBLE_WAIT; if (xe_vm_in_lr_mode(vm)) { drm_exec_init(exec, vm_exec.flags, 0); } else { err = drm_gpuvm_exec_lock(&vm_exec); if (err) { if (xe_vm_validate_should_retry(exec, err, &end)) err = -EAGAIN; goto err_unlock_list; } } if (xe_vm_is_closed_or_banned(q->vm)) { drm_warn(&xe->drm, ""Trying to schedule after vm is closed or banned\n""); err = -ECANCELED; goto err_exec; } if (xe_exec_queue_is_lr(q) && xe_exec_queue_ring_full(q)) { err = -EWOULDBLOCK; skip_retry = true; goto err_exec; } job = xe_sched_job_create(q, xe_exec_queue_is_parallel(q) ? addresses : &args->address); if (IS_ERR(job)) { err = PTR_ERR(job); goto err_exec; } if (!xe_vm_in_lr_mode(vm)) { err = xe_sched_job_add_deps(job, xe_vm_resv(vm), DMA_RESV_USAGE_KERNEL); if (err) goto err_put_job; } for (i = 0; i < num_syncs && !err; i++) err = xe_sync_entry_add_deps(&syncs[i], job); if (err) goto err_put_job; if (!xe_vm_in_lr_mode(vm)) { err = xe_sched_job_last_fence_add_dep(job, vm); if (err) goto err_put_job; err = down_read_interruptible(&vm->userptr.notifier_lock); if (err) goto err_put_job; err = __xe_vm_userptr_needs_repin(vm); if (err) goto err_repin; } xe_sched_job_arm(job); if (!xe_vm_in_lr_mode(vm)) drm_gpuvm_resv_add_fence(&vm->gpuvm, exec, &job->drm.s_fence->finished, DMA_RESV_USAGE_BOOKKEEP, DMA_RESV_USAGE_BOOKKEEP); for (i = 0; i < num_syncs; i++) { xe_sync_entry_signal(&syncs[i], &job->drm.s_fence->finished); xe_sched_job_init_user_fence(job, &syncs[i]); } if (xe_exec_queue_is_lr(q)) q->ring_ops->emit_job(job); if (!xe_vm_in_lr_mode(vm)) xe_exec_queue_last_fence_set(q, vm, &job->drm.s_fence->finished); xe_sched_job_push(job); xe_vm_reactivate_rebind(vm); if (!err && !xe_vm_in_lr_mode(vm)) { spin_lock(&xe->ttm.lru_lock); ttm_lru_bulk_move_tail(&vm->lru_bulk_move); spin_unlock(&xe->ttm.lru_lock); } if (mode == EXEC_MODE_LR) xe_hw_engine_group_resume_faulting_lr_jobs(group); err_repin: if (!xe_vm_in_lr_mode(vm)) up_read(&vm->userptr.notifier_lock); err_put_job: if (err) xe_sched_job_put(job); err_exec: drm_exec_fini(exec); err_unlock_list: up_read(&vm->lock); if (err == -EAGAIN && !skip_retry) goto retry; err_hw_exec_mode: if (mode == EXEC_MODE_DMA_FENCE) xe_hw_engine_group_put(group); err_syncs: while (num_syncs--) xe_sync_entry_cleanup(&syncs[num_syncs]); kfree(syncs); err_exec_queue: xe_exec_queue_put(q); return err; }","- if (XE_IOCTL_DBG(xe, q->flags & EXEC_QUEUE_FLAG_VM))
- return -EINVAL;
- q->width != args->num_batch_buffer))
- return -EINVAL;
+ if (XE_IOCTL_DBG(xe, q->flags & EXEC_QUEUE_FLAG_VM)) {
+ err = -EINVAL;
+ goto err_exec_queue;
+ }
+ q->width != args->num_batch_buffer)) {
+ err = -EINVAL;
+ goto err_exec_queue;
+ }","int xe_exec_ioctl(struct drm_device *dev, void *data, struct drm_file *file) { struct xe_device *xe = to_xe_device(dev); struct xe_file *xef = to_xe_file(file); struct drm_xe_exec *args = data; struct drm_xe_sync __user *syncs_user = u64_to_user_ptr(args->syncs); u64 __user *addresses_user = u64_to_user_ptr(args->address); struct xe_exec_queue *q; struct xe_sync_entry *syncs = NULL; u64 addresses[XE_HW_ENGINE_MAX_INSTANCE]; struct drm_gpuvm_exec vm_exec = {.extra.fn = xe_exec_fn}; struct drm_exec *exec = &vm_exec.exec; u32 i, num_syncs, num_ufence = 0; struct xe_sched_job *job; struct xe_vm *vm; bool write_locked, skip_retry = false; ktime_t end = 0; int err = 0; struct xe_hw_engine_group *group; enum xe_hw_engine_group_execution_mode mode, previous_mode; if (XE_IOCTL_DBG(xe, args->extensions) || XE_IOCTL_DBG(xe, args->pad[0] || args->pad[1] || args->pad[2]) || XE_IOCTL_DBG(xe, args->reserved[0] || args->reserved[1])) return -EINVAL; q = xe_exec_queue_lookup(xef, args->exec_queue_id); if (XE_IOCTL_DBG(xe, !q)) return -ENOENT; if (XE_IOCTL_DBG(xe, q->flags & EXEC_QUEUE_FLAG_VM)) { err = -EINVAL; goto err_exec_queue; } if (XE_IOCTL_DBG(xe, args->num_batch_buffer && q->width != args->num_batch_buffer)) { err = -EINVAL; goto err_exec_queue; } if (XE_IOCTL_DBG(xe, q->ops->reset_status(q))) { err = -ECANCELED; goto err_exec_queue; } if (args->num_syncs) { syncs = kcalloc(args->num_syncs, sizeof(*syncs), GFP_KERNEL); if (!syncs) { err = -ENOMEM; goto err_exec_queue; } } vm = q->vm; for (num_syncs = 0; num_syncs < args->num_syncs; num_syncs++) { err = xe_sync_entry_parse(xe, xef, &syncs[num_syncs], &syncs_user[num_syncs], SYNC_PARSE_FLAG_EXEC | (xe_vm_in_lr_mode(vm) ? SYNC_PARSE_FLAG_LR_MODE : 0)); if (err) goto err_syncs; if (xe_sync_is_ufence(&syncs[num_syncs])) num_ufence++; } if (XE_IOCTL_DBG(xe, num_ufence > 1)) { err = -EINVAL; goto err_syncs; } if (xe_exec_queue_is_parallel(q)) { err = __copy_from_user(addresses, addresses_user, sizeof(u64) * q->width); if (err) { err = -EFAULT; goto err_syncs; } } group = q->hwe->hw_engine_group; mode = xe_hw_engine_group_find_exec_mode(q); if (mode == EXEC_MODE_DMA_FENCE) { err = xe_hw_engine_group_get_mode(group, mode, &previous_mode); if (err) goto err_syncs; } retry: if (!xe_vm_in_lr_mode(vm) && xe_vm_userptr_check_repin(vm)) { err = down_write_killable(&vm->lock); write_locked = true; } else { err = down_read_interruptible(&vm->lock); write_locked = false; } if (err) goto err_syncs; if (write_locked) { err = xe_vm_userptr_pin(vm); downgrade_write(&vm->lock); write_locked = false; if (err) goto err_hw_exec_mode; } if (!args->num_batch_buffer) { err = xe_vm_lock(vm, true); if (err) goto err_unlock_list; if (!xe_vm_in_lr_mode(vm)) { struct dma_fence *fence; fence = xe_sync_in_fence_get(syncs, num_syncs, q, vm); if (IS_ERR(fence)) { err = PTR_ERR(fence); goto err_unlock_list; } for (i = 0; i < num_syncs; i++) xe_sync_entry_signal(&syncs[i], fence); xe_exec_queue_last_fence_set(q, vm, fence); dma_fence_put(fence); } xe_vm_unlock(vm); goto err_unlock_list; } vm_exec.vm = &vm->gpuvm; vm_exec.flags = DRM_EXEC_INTERRUPTIBLE_WAIT; if (xe_vm_in_lr_mode(vm)) { drm_exec_init(exec, vm_exec.flags, 0); } else { err = drm_gpuvm_exec_lock(&vm_exec); if (err) { if (xe_vm_validate_should_retry(exec, err, &end)) err = -EAGAIN; goto err_unlock_list; } } if (xe_vm_is_closed_or_banned(q->vm)) { drm_warn(&xe->drm, ""Trying to schedule after vm is closed or banned\n""); err = -ECANCELED; goto err_exec; } if (xe_exec_queue_is_lr(q) && xe_exec_queue_ring_full(q)) { err = -EWOULDBLOCK; skip_retry = true; goto err_exec; } job = xe_sched_job_create(q, xe_exec_queue_is_parallel(q) ? addresses : &args->address); if (IS_ERR(job)) { err = PTR_ERR(job); goto err_exec; } if (!xe_vm_in_lr_mode(vm)) { err = xe_sched_job_add_deps(job, xe_vm_resv(vm), DMA_RESV_USAGE_KERNEL); if (err) goto err_put_job; } for (i = 0; i < num_syncs && !err; i++) err = xe_sync_entry_add_deps(&syncs[i], job); if (err) goto err_put_job; if (!xe_vm_in_lr_mode(vm)) { err = xe_sched_job_last_fence_add_dep(job, vm); if (err) goto err_put_job; err = down_read_interruptible(&vm->userptr.notifier_lock); if (err) goto err_put_job; err = __xe_vm_userptr_needs_repin(vm); if (err) goto err_repin; } xe_sched_job_arm(job); if (!xe_vm_in_lr_mode(vm)) drm_gpuvm_resv_add_fence(&vm->gpuvm, exec, &job->drm.s_fence->finished, DMA_RESV_USAGE_BOOKKEEP, DMA_RESV_USAGE_BOOKKEEP); for (i = 0; i < num_syncs; i++) { xe_sync_entry_signal(&syncs[i], &job->drm.s_fence->finished); xe_sched_job_init_user_fence(job, &syncs[i]); } if (xe_exec_queue_is_lr(q)) q->ring_ops->emit_job(job); if (!xe_vm_in_lr_mode(vm)) xe_exec_queue_last_fence_set(q, vm, &job->drm.s_fence->finished); xe_sched_job_push(job); xe_vm_reactivate_rebind(vm); if (!err && !xe_vm_in_lr_mode(vm)) { spin_lock(&xe->ttm.lru_lock); ttm_lru_bulk_move_tail(&vm->lru_bulk_move); spin_unlock(&xe->ttm.lru_lock); } if (mode == EXEC_MODE_LR) xe_hw_engine_group_resume_faulting_lr_jobs(group); err_repin: if (!xe_vm_in_lr_mode(vm)) up_read(&vm->userptr.notifier_lock); err_put_job: if (err) xe_sched_job_put(job); err_exec: drm_exec_fini(exec); err_unlock_list: up_read(&vm->lock); if (err == -EAGAIN && !skip_retry) goto retry; err_hw_exec_mode: if (mode == EXEC_MODE_DMA_FENCE) xe_hw_engine_group_put(group); err_syncs: while (num_syncs--) xe_sync_entry_cleanup(&syncs[num_syncs]); kfree(syncs); err_exec_queue: xe_exec_queue_put(q); return err; }"
632----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49938/bad/hif_usb.c----ath9k_hif_usb_rx_cb,"static void ath9k_hif_usb_rx_cb(struct urb *urb) { struct rx_buf *rx_buf = (struct rx_buf *)urb->context; struct hif_device_usb *hif_dev = rx_buf->hif_dev; struct sk_buff *skb = rx_buf->skb; int ret; if (!skb) return; if (!hif_dev) goto free; switch (urb->status) { case 0: break; case -ENOENT: case -ECONNRESET: case -ENODEV: case -ESHUTDOWN: goto free; default: goto resubmit; } if (likely(urb->actual_length != 0)) { skb_put(skb, urb->actual_length); ath9k_hif_usb_rx_stream(hif_dev, skb); } resubmit: <S2SV_StartVul> skb_reset_tail_pointer(skb); <S2SV_EndVul> <S2SV_StartVul> skb_trim(skb, 0); <S2SV_EndVul> usb_anchor_urb(urb, &hif_dev->rx_submitted); ret = usb_submit_urb(urb, GFP_ATOMIC); if (ret) { usb_unanchor_urb(urb); goto free; } return; free: kfree_skb(skb); kfree(rx_buf); }","- skb_reset_tail_pointer(skb);
- skb_trim(skb, 0);
+ __skb_set_length(skb, 0);","static void ath9k_hif_usb_rx_cb(struct urb *urb) { struct rx_buf *rx_buf = (struct rx_buf *)urb->context; struct hif_device_usb *hif_dev = rx_buf->hif_dev; struct sk_buff *skb = rx_buf->skb; int ret; if (!skb) return; if (!hif_dev) goto free; switch (urb->status) { case 0: break; case -ENOENT: case -ECONNRESET: case -ENODEV: case -ESHUTDOWN: goto free; default: goto resubmit; } if (likely(urb->actual_length != 0)) { skb_put(skb, urb->actual_length); ath9k_hif_usb_rx_stream(hif_dev, skb); } resubmit: __skb_set_length(skb, 0); usb_anchor_urb(urb, &hif_dev->rx_submitted); ret = usb_submit_urb(urb, GFP_ATOMIC); if (ret) { usb_unanchor_urb(urb); goto free; } return; free: kfree_skb(skb); kfree(rx_buf); }"
1524----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2025-21682/bad/bnxt.c----bnxt_set_rx_skb_mode,"int bnxt_set_rx_skb_mode(struct bnxt *bp, bool page_mode) { struct net_device *dev = bp->dev; if (page_mode) { bp->flags &= ~(BNXT_FLAG_AGG_RINGS | BNXT_FLAG_NO_AGG_RINGS); bp->flags |= BNXT_FLAG_RX_PAGE_MODE; if (bp->xdp_prog->aux->xdp_has_frags) dev->max_mtu = min_t(u16, bp->max_mtu, BNXT_MAX_MTU); else dev->max_mtu = min_t(u16, bp->max_mtu, BNXT_MAX_PAGE_MODE_MTU); if (dev->mtu > BNXT_MAX_PAGE_MODE_MTU) { bp->flags |= BNXT_FLAG_JUMBO; bp->rx_skb_func = bnxt_rx_multi_page_skb; } else { bp->flags |= BNXT_FLAG_NO_AGG_RINGS; bp->rx_skb_func = bnxt_rx_page_skb; } bp->rx_dir = DMA_BIDIRECTIONAL; <S2SV_StartVul> netdev_update_features(dev); <S2SV_EndVul> } else { dev->max_mtu = bp->max_mtu; bp->flags &= ~BNXT_FLAG_RX_PAGE_MODE; bp->rx_dir = DMA_FROM_DEVICE; bp->rx_skb_func = bnxt_rx_skb; } <S2SV_StartVul> return 0; <S2SV_EndVul> }","- netdev_update_features(dev);
- return 0;
+ #include <linux/interrupt.h>
+ #include <linux/io.h>
+ #include <linux/irq.h>
+ #include <asm/byteorder.h>
+ #include <asm/page.h>
+ #include <linux/time.h>
+ #include <linux/mdio.h>
+ #include <linux/if.h>
+ #include <linux/if_bridge.h>
+ #include <linux/rtc.h>
+ #include <linux/bpf.h>
+ #include <net/gro.h>
+ #include <net/ip.h>
+ #include <net/tcp.h>
+ #include <net/checksum.h>
+ #include <net/ip6_checksum.h>
+ #include <net/udp_tunnel.h>","#include <linux/module.h> #include <linux/stringify.h> #include <linux/kernel.h> #include <linux/timer.h> #include <linux/errno.h> #include <linux/ioport.h> #include <linux/slab.h> #include <linux/vmalloc.h> #include <linux/interrupt.h> #include <linux/pci.h> #include <linux/netdevice.h> #include <linux/etherdevice.h> #include <linux/skbuff.h> #include <linux/dma-mapping.h> #include <linux/bitops.h> #include <linux/io.h> #include <linux/irq.h> #include <linux/delay.h> #include <asm/byteorder.h> #include <asm/page.h> #include <linux/time.h> #include <linux/mii.h> #include <linux/mdio.h> #include <linux/if.h> #include <linux/if_vlan.h> #include <linux/if_bridge.h> #include <linux/rtc.h> #include <linux/bpf.h> #include <net/gro.h> #include <net/ip.h> #include <net/tcp.h> #include <net/udp.h> #include <net/checksum.h> #include <net/ip6_checksum.h> #include <net/udp_tunnel.h> #include <linux/workqueue.h> #include <linux/prefetch.h> #include <linux/cache.h> #include <linux/log2.h> #include <linux/bitmap.h> #include <linux/cpu_rmap.h> #include <linux/cpumask.h> #include <net/pkt_cls.h> #include <net/page_pool/helpers.h> #include <linux/align.h> #include <net/netdev_queues.h> #include ""bnxt_hsi.h"" #include ""bnxt.h"" #include ""bnxt_hwrm.h"" #include ""bnxt_ulp.h"" #include ""bnxt_sriov.h"" #include ""bnxt_ethtool.h"" #include ""bnxt_dcb.h"" #include ""bnxt_xdp.h"" #include ""bnxt_ptp.h"" #include ""bnxt_vfr.h"" #include ""bnxt_tc.h"" #include ""bnxt_devlink.h"" #include ""bnxt_debugfs.h"" #include ""bnxt_coredump.h"" #include ""bnxt_hwmon.h"" #define BNXT_TX_TIMEOUT (5 * HZ) #define BNXT_DEF_MSG_ENABLE (NETIF_MSG_DRV | NETIF_MSG_HW | \ NETIF_MSG_TX_ERR) MODULE_LICENSE(""GPL""); MODULE_DESCRIPTION(""Broadcom NetXtreme network driver""); #define BNXT_RX_OFFSET (NET_SKB_PAD + NET_IP_ALIGN) #define BNXT_RX_DMA_OFFSET NET_SKB_PAD #define BNXT_RX_COPY_THRESH 256 #define BNXT_TX_PUSH_THRESH 164 static const struct { char *name; } board_info[] = { [BCM57301] = { ""Broadcom BCM57301 NetXtreme-C 10Gb Ethernet"" }, [BCM57302] = { ""Broadcom BCM57302 NetXtreme-C 10Gb/25Gb Ethernet"" }, [BCM57304] = { ""Broadcom BCM57304 NetXtreme-C 10Gb/25Gb/40Gb/50Gb Ethernet"" }, [BCM57417_NPAR] = { ""Broadcom BCM57417 NetXtreme-E Ethernet Partition"" }, [BCM58700] = { ""Broadcom BCM58700 Nitro 1Gb/2.5Gb/10Gb Ethernet"" }, [BCM57311] = { ""Broadcom BCM57311 NetXtreme-C 10Gb Ethernet"" }, [BCM57312] = { ""Broadcom BCM57312 NetXtreme-C 10Gb/25Gb Ethernet"" }, [BCM57402] = { ""Broadcom BCM57402 NetXtreme-E 10Gb Ethernet"" }, [BCM57404] = { ""Broadcom BCM57404 NetXtreme-E 10Gb/25Gb Ethernet"" }, [BCM57406] = { ""Broadcom BCM57406 NetXtreme-E 10GBase-T Ethernet"" }, [BCM57402_NPAR] = { ""Broadcom BCM57402 NetXtreme-E Ethernet Partition"" }, [BCM57407] = { ""Broadcom BCM57407 NetXtreme-E 10GBase-T Ethernet"" }, [BCM57412] = { ""Broadcom BCM57412 NetXtreme-E 10Gb Ethernet"" }, [BCM57414] = { ""Broadcom BCM57414 NetXtreme-E 10Gb/25Gb Ethernet"" }, [BCM57416] = { ""Broadcom BCM57416 NetXtreme-E 10GBase-T Ethernet"" }, [BCM57417] = { ""Broadcom BCM57417 NetXtreme-E 10GBase-T Ethernet"" }, [BCM57412_NPAR] = { ""Broadcom BCM57412 NetXtreme-E Ethernet Partition"" }, [BCM57314] = { ""Broadcom BCM57314 NetXtreme-C 10Gb/25Gb/40Gb/50Gb Ethernet"" }, [BCM57417_SFP] = { ""Broadcom BCM57417 NetXtreme-E 10Gb/25Gb Ethernet"" }, [BCM57416_SFP] = { ""Broadcom BCM57416 NetXtreme-E 10Gb Ethernet"" }, [BCM57404_NPAR] = { ""Broadcom BCM57404 NetXtreme-E Ethernet Partition"" }, [BCM57406_NPAR] = { ""Broadcom BCM57406 NetXtreme-E Ethernet Partition"" }, [BCM57407_SFP] = { ""Broadcom BCM57407 NetXtreme-E 25Gb Ethernet"" }, [BCM57407_NPAR] = { ""Broadcom BCM57407 NetXtreme-E Ethernet Partition"" }, [BCM57414_NPAR] = { ""Broadcom BCM57414 NetXtreme-E Ethernet Partition"" }, [BCM57416_NPAR] = { ""Broadcom BCM57416 NetXtreme-E Ethernet Partition"" }, [BCM57452] = { ""Broadcom BCM57452 NetXtreme-E 10Gb/25Gb/40Gb/50Gb Ethernet"" }, [BCM57454] = { ""Broadcom BCM57454 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb Ethernet"" }, [BCM5745x_NPAR] = { ""Broadcom BCM5745x NetXtreme-E Ethernet Partition"" }, [BCM57508] = { ""Broadcom BCM57508 NetXtreme-E 10Gb/25Gb/50Gb/100Gb/200Gb Ethernet"" }, [BCM57504] = { ""Broadcom BCM57504 NetXtreme-E 10Gb/25Gb/50Gb/100Gb/200Gb Ethernet"" }, [BCM57502] = { ""Broadcom BCM57502 NetXtreme-E 10Gb/25Gb/50Gb Ethernet"" }, [BCM57608] = { ""Broadcom BCM57608 NetXtreme-E 10Gb/25Gb/50Gb/100Gb/200Gb/400Gb Ethernet"" }, [BCM57604] = { ""Broadcom BCM57604 NetXtreme-E 10Gb/25Gb/50Gb/100Gb/200Gb Ethernet"" }, [BCM57602] = { ""Broadcom BCM57602 NetXtreme-E 10Gb/25Gb/50Gb/100Gb Ethernet"" }, [BCM57601] = { ""Broadcom BCM57601 NetXtreme-E 10Gb/25Gb/50Gb/100Gb/200Gb/400Gb Ethernet"" }, [BCM57508_NPAR] = { ""Broadcom BCM57508 NetXtreme-E Ethernet Partition"" }, [BCM57504_NPAR] = { ""Broadcom BCM57504 NetXtreme-E Ethernet Partition"" }, [BCM57502_NPAR] = { ""Broadcom BCM57502 NetXtreme-E Ethernet Partition"" }, [BCM58802] = { ""Broadcom BCM58802 NetXtreme-S 10Gb/25Gb/40Gb/50Gb Ethernet"" }, [BCM58804] = { ""Broadcom BCM58804 NetXtreme-S 10Gb/25Gb/40Gb/50Gb/100Gb Ethernet"" }, [BCM58808] = { ""Broadcom BCM58808 NetXtreme-S 10Gb/25Gb/40Gb/50Gb/100Gb Ethernet"" }, [NETXTREME_E_VF] = { ""Broadcom NetXtreme-E Ethernet Virtual Function"" }, [NETXTREME_C_VF] = { ""Broadcom NetXtreme-C Ethernet Virtual Function"" }, [NETXTREME_S_VF] = { ""Broadcom NetXtreme-S Ethernet Virtual Function"" }, [NETXTREME_C_VF_HV] = { ""Broadcom NetXtreme-C Virtual Function for Hyper-V"" }, [NETXTREME_E_VF_HV] = { ""Broadcom NetXtreme-E Virtual Function for Hyper-V"" }, [NETXTREME_E_P5_VF] = { ""Broadcom BCM5750X NetXtreme-E Ethernet Virtual Function"" }, [NETXTREME_E_P5_VF_HV] = { ""Broadcom BCM5750X NetXtreme-E Virtual Function for Hyper-V"" }, [NETXTREME_E_P7_VF] = { ""Broadcom BCM5760X Virtual Function"" }, };"
380----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46841/bad/extent-tree.c----walk_down_proc,"static noinline int walk_down_proc(struct btrfs_trans_handle *trans, struct btrfs_root *root, struct btrfs_path *path, struct walk_control *wc, int lookup_info) { struct btrfs_fs_info *fs_info = root->fs_info; int level = wc->level; struct extent_buffer *eb = path->nodes[level]; u64 flag = BTRFS_BLOCK_FLAG_FULL_BACKREF; int ret; if (wc->stage == UPDATE_BACKREF && btrfs_header_owner(eb) != root->root_key.objectid) return 1; if (lookup_info && ((wc->stage == DROP_REFERENCE && wc->refs[level] != 1) || (wc->stage == UPDATE_BACKREF && !(wc->flags[level] & flag)))) { ASSERT(path->locks[level]); ret = btrfs_lookup_extent_info(trans, fs_info, eb->start, level, 1, &wc->refs[level], &wc->flags[level]); <S2SV_StartVul> BUG_ON(ret == -ENOMEM); <S2SV_EndVul> if (ret) return ret; if (unlikely(wc->refs[level] == 0)) { btrfs_err(fs_info, ""bytenr %llu has 0 references, expect > 0"", eb->start); return -EUCLEAN; } } if (wc->stage == DROP_REFERENCE) { if (wc->refs[level] > 1) return 1; if (path->locks[level] && !wc->keep_locks) { btrfs_tree_unlock_rw(eb, path->locks[level]); path->locks[level] = 0; } return 0; } if (!(wc->flags[level] & flag)) { ASSERT(path->locks[level]); ret = btrfs_inc_ref(trans, root, eb, 1); BUG_ON(ret); ret = btrfs_dec_ref(trans, root, eb, 0); BUG_ON(ret); ret = btrfs_set_disk_extent_flags(trans, eb, flag, btrfs_header_level(eb)); BUG_ON(ret); wc->flags[level] |= flag; } if (path->locks[level] && level > 0) { btrfs_tree_unlock_rw(eb, path->locks[level]); path->locks[level] = 0; } return 0; }",- BUG_ON(ret == -ENOMEM);,"static noinline int walk_down_proc(struct btrfs_trans_handle *trans, struct btrfs_root *root, struct btrfs_path *path, struct walk_control *wc, int lookup_info) { struct btrfs_fs_info *fs_info = root->fs_info; int level = wc->level; struct extent_buffer *eb = path->nodes[level]; u64 flag = BTRFS_BLOCK_FLAG_FULL_BACKREF; int ret; if (wc->stage == UPDATE_BACKREF && btrfs_header_owner(eb) != root->root_key.objectid) return 1; if (lookup_info && ((wc->stage == DROP_REFERENCE && wc->refs[level] != 1) || (wc->stage == UPDATE_BACKREF && !(wc->flags[level] & flag)))) { ASSERT(path->locks[level]); ret = btrfs_lookup_extent_info(trans, fs_info, eb->start, level, 1, &wc->refs[level], &wc->flags[level]); if (ret) return ret; if (unlikely(wc->refs[level] == 0)) { btrfs_err(fs_info, ""bytenr %llu has 0 references, expect > 0"", eb->start); return -EUCLEAN; } } if (wc->stage == DROP_REFERENCE) { if (wc->refs[level] > 1) return 1; if (path->locks[level] && !wc->keep_locks) { btrfs_tree_unlock_rw(eb, path->locks[level]); path->locks[level] = 0; } return 0; } if (!(wc->flags[level] & flag)) { ASSERT(path->locks[level]); ret = btrfs_inc_ref(trans, root, eb, 1); BUG_ON(ret); ret = btrfs_dec_ref(trans, root, eb, 0); BUG_ON(ret); ret = btrfs_set_disk_extent_flags(trans, eb, flag, btrfs_header_level(eb)); BUG_ON(ret); wc->flags[level] |= flag; } if (path->locks[level] && level > 0) { btrfs_tree_unlock_rw(eb, path->locks[level]); path->locks[level] = 0; } return 0; }"
609----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49921/bad/dcn20_hubp.c----hubp2_is_flip_pending,"bool hubp2_is_flip_pending(struct hubp *hubp) { uint32_t flip_pending = 0; struct dcn20_hubp *hubp2 = TO_DCN20_HUBP(hubp); struct dc_plane_address earliest_inuse_address; if (hubp && hubp->power_gated) return false; REG_GET(DCSURF_FLIP_CONTROL, SURFACE_FLIP_PENDING, &flip_pending); REG_GET(DCSURF_SURFACE_EARLIEST_INUSE, SURFACE_EARLIEST_INUSE_ADDRESS, &earliest_inuse_address.grph.addr.low_part); REG_GET(DCSURF_SURFACE_EARLIEST_INUSE_HIGH, SURFACE_EARLIEST_INUSE_ADDRESS_HIGH, &earliest_inuse_address.grph.addr.high_part); if (flip_pending) return true; <S2SV_StartVul> if (earliest_inuse_address.grph.addr.quad_part != hubp->request_address.grph.addr.quad_part) <S2SV_EndVul> return true; return false; }","- if (earliest_inuse_address.grph.addr.quad_part != hubp->request_address.grph.addr.quad_part)
+ if (hubp &&
+ earliest_inuse_address.grph.addr.quad_part != hubp->request_address.grph.addr.quad_part)","bool hubp2_is_flip_pending(struct hubp *hubp) { uint32_t flip_pending = 0; struct dcn20_hubp *hubp2 = TO_DCN20_HUBP(hubp); struct dc_plane_address earliest_inuse_address; if (hubp && hubp->power_gated) return false; REG_GET(DCSURF_FLIP_CONTROL, SURFACE_FLIP_PENDING, &flip_pending); REG_GET(DCSURF_SURFACE_EARLIEST_INUSE, SURFACE_EARLIEST_INUSE_ADDRESS, &earliest_inuse_address.grph.addr.low_part); REG_GET(DCSURF_SURFACE_EARLIEST_INUSE_HIGH, SURFACE_EARLIEST_INUSE_ADDRESS_HIGH, &earliest_inuse_address.grph.addr.high_part); if (flip_pending) return true; if (hubp && earliest_inuse_address.grph.addr.quad_part != hubp->request_address.grph.addr.quad_part) return true; return false; }"
167----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-45022/bad/vmalloc.c----vm_area_alloc_pages,"vm_area_alloc_pages(gfp_t gfp, int nid, unsigned int order, unsigned int nr_pages, struct page **pages) { unsigned int nr_allocated = 0; gfp_t alloc_gfp = gfp; bool nofail = gfp & __GFP_NOFAIL; struct page *page; int i; if (!order) { gfp_t bulk_gfp = gfp & ~__GFP_NOFAIL; while (nr_allocated < nr_pages) { unsigned int nr, nr_pages_request; nr_pages_request = min(100U, nr_pages - nr_allocated); if (IS_ENABLED(CONFIG_NUMA) && nid == NUMA_NO_NODE) nr = alloc_pages_bulk_array_mempolicy(bulk_gfp, nr_pages_request, pages + nr_allocated); else nr = alloc_pages_bulk_array_node(bulk_gfp, nid, nr_pages_request, pages + nr_allocated); nr_allocated += nr; cond_resched(); if (nr != nr_pages_request) break; } } else if (gfp & __GFP_NOFAIL) { alloc_gfp &= ~__GFP_NOFAIL; } while (nr_allocated < nr_pages) { if (!nofail && fatal_signal_pending(current)) break; if (nid == NUMA_NO_NODE) page = alloc_pages(alloc_gfp, order); else page = alloc_pages_node(nid, alloc_gfp, order); <S2SV_StartVul> if (unlikely(!page)) { <S2SV_EndVul> <S2SV_StartVul> if (!nofail) <S2SV_EndVul> <S2SV_StartVul> break; <S2SV_EndVul> <S2SV_StartVul> alloc_gfp |= __GFP_NOFAIL; <S2SV_EndVul> <S2SV_StartVul> order = 0; <S2SV_EndVul> <S2SV_StartVul> continue; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> if (order) split_page(page, order); for (i = 0; i < (1U << order); i++) pages[nr_allocated + i] = page + i; cond_resched(); nr_allocated += 1U << order; } return nr_allocated; }","- if (unlikely(!page)) {
- if (!nofail)
- break;
- alloc_gfp |= __GFP_NOFAIL;
- order = 0;
- continue;
- }
+ if (unlikely(!page))
+ break;","vm_area_alloc_pages(gfp_t gfp, int nid, unsigned int order, unsigned int nr_pages, struct page **pages) { unsigned int nr_allocated = 0; gfp_t alloc_gfp = gfp; bool nofail = gfp & __GFP_NOFAIL; struct page *page; int i; if (!order) { gfp_t bulk_gfp = gfp & ~__GFP_NOFAIL; while (nr_allocated < nr_pages) { unsigned int nr, nr_pages_request; nr_pages_request = min(100U, nr_pages - nr_allocated); if (IS_ENABLED(CONFIG_NUMA) && nid == NUMA_NO_NODE) nr = alloc_pages_bulk_array_mempolicy(bulk_gfp, nr_pages_request, pages + nr_allocated); else nr = alloc_pages_bulk_array_node(bulk_gfp, nid, nr_pages_request, pages + nr_allocated); nr_allocated += nr; cond_resched(); if (nr != nr_pages_request) break; } } else if (gfp & __GFP_NOFAIL) { alloc_gfp &= ~__GFP_NOFAIL; } while (nr_allocated < nr_pages) { if (!nofail && fatal_signal_pending(current)) break; if (nid == NUMA_NO_NODE) page = alloc_pages(alloc_gfp, order); else page = alloc_pages_node(nid, alloc_gfp, order); if (unlikely(!page)) break; if (order) split_page(page, order); for (i = 0; i < (1U << order); i++) pages[nr_allocated + i] = page + i; cond_resched(); nr_allocated += 1U << order; } return nr_allocated; }"
876----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50132/bad/trace_eprobe.c----__trace_eprobe_create,"static int __trace_eprobe_create(int argc, const char *argv[]) { const char *event = NULL, *group = EPROBE_EVENT_SYSTEM; const char *sys_event = NULL, *sys_name = NULL; struct trace_event_call *event_call; struct trace_eprobe *ep = NULL; char buf1[MAX_EVENT_NAME_LEN]; char buf2[MAX_EVENT_NAME_LEN]; char gbuf[MAX_EVENT_NAME_LEN]; int ret = 0, filter_idx = 0; int i, filter_cnt; if (argc < 2 || argv[0][0] != 'e') return -ECANCELED; trace_probe_log_init(""event_probe"", argc, argv); event = strchr(&argv[0][1], ':'); if (event) { event++; ret = traceprobe_parse_event_name(&event, &group, gbuf, event - argv[0]); if (ret) goto parse_error; } trace_probe_log_set_index(1); sys_event = argv[1]; ret = traceprobe_parse_event_name(&sys_event, &sys_name, buf2, 0); if (ret || !sys_event || !sys_name) { trace_probe_log_err(0, NO_EVENT_INFO); goto parse_error; } if (!event) { strscpy(buf1, sys_event, MAX_EVENT_NAME_LEN); event = buf1; } for (i = 2; i < argc; i++) { if (!strcmp(argv[i], ""if"")) { filter_idx = i + 1; filter_cnt = argc - filter_idx; argc = i; break; } } mutex_lock(&event_mutex); event_call = find_and_get_event(sys_name, sys_event); ep = alloc_event_probe(group, event, event_call, argc - 2); mutex_unlock(&event_mutex); if (IS_ERR(ep)) { ret = PTR_ERR(ep); if (ret == -ENODEV) trace_probe_log_err(0, BAD_ATTACH_EVENT); WARN_ON_ONCE(ret != -ENOMEM && ret != -ENODEV); ep = NULL; goto error; } if (filter_idx) { trace_probe_log_set_index(filter_idx); ret = trace_eprobe_parse_filter(ep, filter_cnt, argv + filter_idx); if (ret) goto parse_error; } else ep->filter_str = NULL; argc -= 2; argv += 2; <S2SV_StartVul> for (i = 0; i < argc && i < MAX_TRACE_ARGS; i++) { <S2SV_EndVul> trace_probe_log_set_index(i + 2); ret = trace_eprobe_tp_update_arg(ep, argv, i); if (ret) goto error; } ret = traceprobe_set_print_fmt(&ep->tp, PROBE_PRINT_EVENT); if (ret < 0) goto error; init_trace_eprobe_call(ep); mutex_lock(&event_mutex); ret = trace_probe_register_event_call(&ep->tp); if (ret) { if (ret == -EEXIST) { trace_probe_log_set_index(0); trace_probe_log_err(0, EVENT_EXIST); } mutex_unlock(&event_mutex); goto error; } ret = dyn_event_add(&ep->devent, &ep->tp.event->call); mutex_unlock(&event_mutex); return ret; parse_error: ret = -EINVAL; error: trace_event_probe_cleanup(ep); return ret; }","- for (i = 0; i < argc && i < MAX_TRACE_ARGS; i++) {
+ }
+ if (argc - 2 > MAX_TRACE_ARGS) {
+ ret = -E2BIG;
+ goto error;
+ }
+ for (i = 0; i < argc; i++) {","static int __trace_eprobe_create(int argc, const char *argv[]) { const char *event = NULL, *group = EPROBE_EVENT_SYSTEM; const char *sys_event = NULL, *sys_name = NULL; struct trace_event_call *event_call; struct trace_eprobe *ep = NULL; char buf1[MAX_EVENT_NAME_LEN]; char buf2[MAX_EVENT_NAME_LEN]; char gbuf[MAX_EVENT_NAME_LEN]; int ret = 0, filter_idx = 0; int i, filter_cnt; if (argc < 2 || argv[0][0] != 'e') return -ECANCELED; trace_probe_log_init(""event_probe"", argc, argv); event = strchr(&argv[0][1], ':'); if (event) { event++; ret = traceprobe_parse_event_name(&event, &group, gbuf, event - argv[0]); if (ret) goto parse_error; } trace_probe_log_set_index(1); sys_event = argv[1]; ret = traceprobe_parse_event_name(&sys_event, &sys_name, buf2, 0); if (ret || !sys_event || !sys_name) { trace_probe_log_err(0, NO_EVENT_INFO); goto parse_error; } if (!event) { strscpy(buf1, sys_event, MAX_EVENT_NAME_LEN); event = buf1; } for (i = 2; i < argc; i++) { if (!strcmp(argv[i], ""if"")) { filter_idx = i + 1; filter_cnt = argc - filter_idx; argc = i; break; } } if (argc - 2 > MAX_TRACE_ARGS) { ret = -E2BIG; goto error; } mutex_lock(&event_mutex); event_call = find_and_get_event(sys_name, sys_event); ep = alloc_event_probe(group, event, event_call, argc - 2); mutex_unlock(&event_mutex); if (IS_ERR(ep)) { ret = PTR_ERR(ep); if (ret == -ENODEV) trace_probe_log_err(0, BAD_ATTACH_EVENT); WARN_ON_ONCE(ret != -ENOMEM && ret != -ENODEV); ep = NULL; goto error; } if (filter_idx) { trace_probe_log_set_index(filter_idx); ret = trace_eprobe_parse_filter(ep, filter_cnt, argv + filter_idx); if (ret) goto parse_error; } else ep->filter_str = NULL; argc -= 2; argv += 2; for (i = 0; i < argc; i++) { trace_probe_log_set_index(i + 2); ret = trace_eprobe_tp_update_arg(ep, argv, i); if (ret) goto error; } ret = traceprobe_set_print_fmt(&ep->tp, PROBE_PRINT_EVENT); if (ret < 0) goto error; init_trace_eprobe_call(ep); mutex_lock(&event_mutex); ret = trace_probe_register_event_call(&ep->tp); if (ret) { if (ret == -EEXIST) { trace_probe_log_set_index(0); trace_probe_log_err(0, EVENT_EXIST); } mutex_unlock(&event_mutex); goto error; } ret = dyn_event_add(&ep->devent, &ep->tp.event->call); mutex_unlock(&event_mutex); return ret; parse_error: ret = -EINVAL; error: trace_event_probe_cleanup(ep); return ret; }"
544----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49864/bad/io_thread.c----rxrpc_encap_rcv,"int rxrpc_encap_rcv(struct sock *udp_sk, struct sk_buff *skb) { struct sk_buff_head *rx_queue; struct rxrpc_local *local = rcu_dereference_sk_user_data(udp_sk); if (unlikely(!local)) { kfree_skb(skb); return 0; } if (skb->tstamp == 0) skb->tstamp = ktime_get_real(); skb->mark = RXRPC_SKB_MARK_PACKET; rxrpc_new_skb(skb, rxrpc_skb_new_encap_rcv); rx_queue = &local->rx_queue; #ifdef CONFIG_AF_RXRPC_INJECT_RX_DELAY if (rxrpc_inject_rx_delay || !skb_queue_empty(&local->rx_delay_queue)) { skb->tstamp = ktime_add_ms(skb->tstamp, rxrpc_inject_rx_delay); rx_queue = &local->rx_delay_queue; } #endif skb_queue_tail(rx_queue, skb); <S2SV_StartVul> rxrpc_wake_up_io_thread(local); <S2SV_EndVul> return 0; }","- rxrpc_wake_up_io_thread(local);
+ struct task_struct *io_thread;
+ kfree_skb(skb);
+ return 0;
+ }
+ io_thread = READ_ONCE(local->io_thread);
+ if (!io_thread) {
+ kfree_skb(skb);
+ return 0;
+ }
+ wake_up_process(io_thread);
+ return 0;
+ }","int rxrpc_encap_rcv(struct sock *udp_sk, struct sk_buff *skb) { struct sk_buff_head *rx_queue; struct rxrpc_local *local = rcu_dereference_sk_user_data(udp_sk); struct task_struct *io_thread; if (unlikely(!local)) { kfree_skb(skb); return 0; } io_thread = READ_ONCE(local->io_thread); if (!io_thread) { kfree_skb(skb); return 0; } if (skb->tstamp == 0) skb->tstamp = ktime_get_real(); skb->mark = RXRPC_SKB_MARK_PACKET; rxrpc_new_skb(skb, rxrpc_skb_new_encap_rcv); rx_queue = &local->rx_queue; #ifdef CONFIG_AF_RXRPC_INJECT_RX_DELAY if (rxrpc_inject_rx_delay || !skb_queue_empty(&local->rx_delay_queue)) { skb->tstamp = ktime_add_ms(skb->tstamp, rxrpc_inject_rx_delay); rx_queue = &local->rx_delay_queue; } #endif skb_queue_tail(rx_queue, skb); wake_up_process(io_thread); return 0; }"
888----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50140/bad/core.c----task_tick_mm_cid,"void task_tick_mm_cid(struct rq *rq, struct task_struct *curr) { struct callback_head *work = &curr->cid_work; unsigned long now = jiffies; if (!curr->mm || (curr->flags & (PF_EXITING | PF_KTHREAD)) || work->next != work) return; if (time_before(now, READ_ONCE(curr->mm->mm_cid_next_scan))) return; <S2SV_StartVul> task_work_add(curr, work, TWA_RESUME); <S2SV_EndVul> }","- task_work_add(curr, work, TWA_RESUME);
+ task_work_add(curr, work, TWA_RESUME | TWAF_NO_ALLOC);","void task_tick_mm_cid(struct rq *rq, struct task_struct *curr) { struct callback_head *work = &curr->cid_work; unsigned long now = jiffies; if (!curr->mm || (curr->flags & (PF_EXITING | PF_KTHREAD)) || work->next != work) return; if (time_before(now, READ_ONCE(curr->mm->mm_cid_next_scan))) return; task_work_add(curr, work, TWA_RESUME | TWAF_NO_ALLOC); }"
1443----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56770/bad/sch_netem.c----netem_enqueue,"static int netem_enqueue(struct sk_buff *skb, struct Qdisc *sch, struct sk_buff **to_free) { struct netem_sched_data *q = qdisc_priv(sch); struct netem_skb_cb *cb; struct sk_buff *skb2 = NULL; struct sk_buff *segs = NULL; unsigned int prev_len = qdisc_pkt_len(skb); int count = 1; skb->prev = NULL; if (q->duplicate && q->duplicate >= get_crandom(&q->dup_cor, &q->prng)) ++count; if (loss_event(q)) { if (q->ecn && INET_ECN_set_ce(skb)) qdisc_qstats_drop(sch); else --count; } if (count == 0) { qdisc_qstats_drop(sch); __qdisc_drop(skb, to_free); return NET_XMIT_SUCCESS | __NET_XMIT_BYPASS; } if (q->latency || q->jitter || q->rate) skb_orphan_partial(skb); if (count > 1) skb2 = skb_clone(skb, GFP_ATOMIC); if (q->corrupt && q->corrupt >= get_crandom(&q->corrupt_cor, &q->prng)) { if (skb_is_gso(skb)) { skb = netem_segment(skb, sch, to_free); if (!skb) goto finish_segs; segs = skb->next; skb_mark_not_on_list(skb); qdisc_skb_cb(skb)->pkt_len = skb->len; } skb = skb_unshare(skb, GFP_ATOMIC); if (unlikely(!skb)) { qdisc_qstats_drop(sch); goto finish_segs; } if (skb->ip_summed == CHECKSUM_PARTIAL && skb_checksum_help(skb)) { qdisc_drop(skb, sch, to_free); skb = NULL; goto finish_segs; } skb->data[get_random_u32_below(skb_headlen(skb))] ^= 1<<get_random_u32_below(8); } <S2SV_StartVul> if (unlikely(sch->q.qlen >= sch->limit)) { <S2SV_EndVul> skb->next = segs; qdisc_drop_all(skb, sch, to_free); if (skb2) __qdisc_drop(skb2, to_free); return NET_XMIT_DROP; } if (skb2) { struct Qdisc *rootq = qdisc_root_bh(sch); u32 dupsave = q->duplicate; q->duplicate = 0; rootq->enqueue(skb2, rootq, to_free); q->duplicate = dupsave; skb2 = NULL; } qdisc_qstats_backlog_inc(sch, skb); cb = netem_skb_cb(skb); if (q->gap == 0 || q->counter < q->gap - 1 || q->reorder < get_crandom(&q->reorder_cor, &q->prng)) { u64 now; s64 delay; delay = tabledist(q->latency, q->jitter, &q->delay_cor, &q->prng, q->delay_dist); now = ktime_get_ns(); if (q->rate) { struct netem_skb_cb *last = NULL; if (sch->q.tail) last = netem_skb_cb(sch->q.tail); if (q->t_root.rb_node) { struct sk_buff *t_skb; struct netem_skb_cb *t_last; t_skb = skb_rb_last(&q->t_root); t_last = netem_skb_cb(t_skb); if (!last || t_last->time_to_send > last->time_to_send) last = t_last; } if (q->t_tail) { struct netem_skb_cb *t_last = netem_skb_cb(q->t_tail); if (!last || t_last->time_to_send > last->time_to_send) last = t_last; } if (last) { delay -= last->time_to_send - now; delay = max_t(s64, 0, delay); now = last->time_to_send; } delay += packet_time_ns(qdisc_pkt_len(skb), q); } cb->time_to_send = now + delay; ++q->counter; tfifo_enqueue(skb, sch); } else { cb->time_to_send = ktime_get_ns(); q->counter = 0; __qdisc_enqueue_head(skb, &sch->q); sch->qstats.requeues++; } finish_segs: if (skb2) __qdisc_drop(skb2, to_free); if (segs) { unsigned int len, last_len; int rc, nb; len = skb ? skb->len : 0; nb = skb ? 1 : 0; while (segs) { skb2 = segs->next; skb_mark_not_on_list(segs); qdisc_skb_cb(segs)->pkt_len = segs->len; last_len = segs->len; rc = qdisc_enqueue(segs, sch, to_free); if (rc != NET_XMIT_SUCCESS) { if (net_xmit_drop_count(rc)) qdisc_qstats_drop(sch); } else { nb++; len += last_len; } segs = skb2; } qdisc_tree_reduce_backlog(sch, -(nb - 1), -(len - prev_len)); } else if (!skb) { return NET_XMIT_DROP; } return NET_XMIT_SUCCESS; }","- if (unlikely(sch->q.qlen >= sch->limit)) {
+ }
+ if (unlikely(q->t_len >= sch->limit)) {","static int netem_enqueue(struct sk_buff *skb, struct Qdisc *sch, struct sk_buff **to_free) { struct netem_sched_data *q = qdisc_priv(sch); struct netem_skb_cb *cb; struct sk_buff *skb2 = NULL; struct sk_buff *segs = NULL; unsigned int prev_len = qdisc_pkt_len(skb); int count = 1; skb->prev = NULL; if (q->duplicate && q->duplicate >= get_crandom(&q->dup_cor, &q->prng)) ++count; if (loss_event(q)) { if (q->ecn && INET_ECN_set_ce(skb)) qdisc_qstats_drop(sch); else --count; } if (count == 0) { qdisc_qstats_drop(sch); __qdisc_drop(skb, to_free); return NET_XMIT_SUCCESS | __NET_XMIT_BYPASS; } if (q->latency || q->jitter || q->rate) skb_orphan_partial(skb); if (count > 1) skb2 = skb_clone(skb, GFP_ATOMIC); if (q->corrupt && q->corrupt >= get_crandom(&q->corrupt_cor, &q->prng)) { if (skb_is_gso(skb)) { skb = netem_segment(skb, sch, to_free); if (!skb) goto finish_segs; segs = skb->next; skb_mark_not_on_list(skb); qdisc_skb_cb(skb)->pkt_len = skb->len; } skb = skb_unshare(skb, GFP_ATOMIC); if (unlikely(!skb)) { qdisc_qstats_drop(sch); goto finish_segs; } if (skb->ip_summed == CHECKSUM_PARTIAL && skb_checksum_help(skb)) { qdisc_drop(skb, sch, to_free); skb = NULL; goto finish_segs; } skb->data[get_random_u32_below(skb_headlen(skb))] ^= 1<<get_random_u32_below(8); } if (unlikely(q->t_len >= sch->limit)) { skb->next = segs; qdisc_drop_all(skb, sch, to_free); if (skb2) __qdisc_drop(skb2, to_free); return NET_XMIT_DROP; } if (skb2) { struct Qdisc *rootq = qdisc_root_bh(sch); u32 dupsave = q->duplicate; q->duplicate = 0; rootq->enqueue(skb2, rootq, to_free); q->duplicate = dupsave; skb2 = NULL; } qdisc_qstats_backlog_inc(sch, skb); cb = netem_skb_cb(skb); if (q->gap == 0 || q->counter < q->gap - 1 || q->reorder < get_crandom(&q->reorder_cor, &q->prng)) { u64 now; s64 delay; delay = tabledist(q->latency, q->jitter, &q->delay_cor, &q->prng, q->delay_dist); now = ktime_get_ns(); if (q->rate) { struct netem_skb_cb *last = NULL; if (sch->q.tail) last = netem_skb_cb(sch->q.tail); if (q->t_root.rb_node) { struct sk_buff *t_skb; struct netem_skb_cb *t_last; t_skb = skb_rb_last(&q->t_root); t_last = netem_skb_cb(t_skb); if (!last || t_last->time_to_send > last->time_to_send) last = t_last; } if (q->t_tail) { struct netem_skb_cb *t_last = netem_skb_cb(q->t_tail); if (!last || t_last->time_to_send > last->time_to_send) last = t_last; } if (last) { delay -= last->time_to_send - now; delay = max_t(s64, 0, delay); now = last->time_to_send; } delay += packet_time_ns(qdisc_pkt_len(skb), q); } cb->time_to_send = now + delay; ++q->counter; tfifo_enqueue(skb, sch); } else { cb->time_to_send = ktime_get_ns(); q->counter = 0; __qdisc_enqueue_head(skb, &sch->q); sch->qstats.requeues++; } finish_segs: if (skb2) __qdisc_drop(skb2, to_free); if (segs) { unsigned int len, last_len; int rc, nb; len = skb ? skb->len : 0; nb = skb ? 1 : 0; while (segs) { skb2 = segs->next; skb_mark_not_on_list(segs); qdisc_skb_cb(segs)->pkt_len = segs->len; last_len = segs->len; rc = qdisc_enqueue(segs, sch, to_free); if (rc != NET_XMIT_SUCCESS) { if (net_xmit_drop_count(rc)) qdisc_qstats_drop(sch); } else { nb++; len += last_len; } segs = skb2; } qdisc_tree_reduce_backlog(sch, -(nb - 1), -(len - prev_len)); } else if (!skb) { return NET_XMIT_DROP; } return NET_XMIT_SUCCESS; }"
1087----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53068/bad/bus.c----__scmi_device_destroy,"static void __scmi_device_destroy(struct scmi_device *scmi_dev) { pr_debug(""(%s) Destroying SCMI device '%s' for protocol 0x%x (%s)\n"", of_node_full_name(scmi_dev->dev.parent->of_node), dev_name(&scmi_dev->dev), scmi_dev->protocol_id, scmi_dev->name); if (scmi_dev->protocol_id == SCMI_PROTOCOL_SYSTEM) atomic_set(&scmi_syspower_registered, 0); <S2SV_StartVul> kfree_const(scmi_dev->name); <S2SV_EndVul> ida_free(&scmi_bus_id, scmi_dev->id); device_unregister(&scmi_dev->dev); }",- kfree_const(scmi_dev->name);,"static void __scmi_device_destroy(struct scmi_device *scmi_dev) { pr_debug(""(%s) Destroying SCMI device '%s' for protocol 0x%x (%s)\n"", of_node_full_name(scmi_dev->dev.parent->of_node), dev_name(&scmi_dev->dev), scmi_dev->protocol_id, scmi_dev->name); if (scmi_dev->protocol_id == SCMI_PROTOCOL_SYSTEM) atomic_set(&scmi_syspower_registered, 0); ida_free(&scmi_bus_id, scmi_dev->id); device_unregister(&scmi_dev->dev); }"
913----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50158/bad/hw_counters.c----bnxt_re_ib_get_hw_stats,"int bnxt_re_ib_get_hw_stats(struct ib_device *ibdev, struct rdma_hw_stats *stats, u32 port, int index) { struct bnxt_re_dev *rdev = to_bnxt_re_dev(ibdev, ibdev); struct bnxt_re_res_cntrs *res_s = &rdev->stats.res; struct bnxt_qplib_roce_stats *err_s = NULL; struct ctx_hw_stats *hw_stats = NULL; int rc = 0; hw_stats = rdev->qplib_ctx.stats.dma; if (!port || !stats) return -EINVAL; stats->value[BNXT_RE_ACTIVE_QP] = atomic_read(&res_s->qp_count); stats->value[BNXT_RE_ACTIVE_RC_QP] = atomic_read(&res_s->rc_qp_count); stats->value[BNXT_RE_ACTIVE_UD_QP] = atomic_read(&res_s->ud_qp_count); stats->value[BNXT_RE_ACTIVE_SRQ] = atomic_read(&res_s->srq_count); stats->value[BNXT_RE_ACTIVE_CQ] = atomic_read(&res_s->cq_count); stats->value[BNXT_RE_ACTIVE_MR] = atomic_read(&res_s->mr_count); stats->value[BNXT_RE_ACTIVE_MW] = atomic_read(&res_s->mw_count); stats->value[BNXT_RE_ACTIVE_PD] = atomic_read(&res_s->pd_count); stats->value[BNXT_RE_ACTIVE_AH] = atomic_read(&res_s->ah_count); stats->value[BNXT_RE_WATERMARK_QP] = res_s->qp_watermark; stats->value[BNXT_RE_WATERMARK_RC_QP] = res_s->rc_qp_watermark; stats->value[BNXT_RE_WATERMARK_UD_QP] = res_s->ud_qp_watermark; stats->value[BNXT_RE_WATERMARK_SRQ] = res_s->srq_watermark; stats->value[BNXT_RE_WATERMARK_CQ] = res_s->cq_watermark; stats->value[BNXT_RE_WATERMARK_MR] = res_s->mr_watermark; stats->value[BNXT_RE_WATERMARK_MW] = res_s->mw_watermark; stats->value[BNXT_RE_WATERMARK_PD] = res_s->pd_watermark; stats->value[BNXT_RE_WATERMARK_AH] = res_s->ah_watermark; stats->value[BNXT_RE_RESIZE_CQ_CNT] = atomic_read(&res_s->resize_count); if (hw_stats) { stats->value[BNXT_RE_RECOVERABLE_ERRORS] = le64_to_cpu(hw_stats->tx_bcast_pkts); stats->value[BNXT_RE_TX_DISCARDS] = le64_to_cpu(hw_stats->tx_discard_pkts); stats->value[BNXT_RE_TX_ERRORS] = le64_to_cpu(hw_stats->tx_error_pkts); stats->value[BNXT_RE_RX_ERRORS] = le64_to_cpu(hw_stats->rx_error_pkts); stats->value[BNXT_RE_RX_DISCARDS] = le64_to_cpu(hw_stats->rx_discard_pkts); stats->value[BNXT_RE_RX_PKTS] = le64_to_cpu(hw_stats->rx_ucast_pkts); stats->value[BNXT_RE_RX_BYTES] = le64_to_cpu(hw_stats->rx_ucast_bytes); stats->value[BNXT_RE_TX_PKTS] = le64_to_cpu(hw_stats->tx_ucast_pkts); stats->value[BNXT_RE_TX_BYTES] = le64_to_cpu(hw_stats->tx_ucast_bytes); } err_s = &rdev->stats.rstat.errs; if (test_bit(BNXT_RE_FLAG_ISSUE_ROCE_STATS, &rdev->flags)) { rc = bnxt_qplib_get_roce_stats(&rdev->rcfw, err_s); if (rc) { clear_bit(BNXT_RE_FLAG_ISSUE_ROCE_STATS, &rdev->flags); goto done; } bnxt_re_copy_err_stats(rdev, stats, err_s); if (_is_ext_stats_supported(rdev->dev_attr.dev_cap_flags) && !rdev->is_virtfn) { rc = bnxt_re_get_ext_stat(rdev, stats); if (rc) { clear_bit(BNXT_RE_FLAG_ISSUE_ROCE_STATS, &rdev->flags); goto done; } } <S2SV_StartVul> if (rdev->pacing.dbr_pacing) <S2SV_EndVul> bnxt_re_copy_db_pacing_stats(rdev, stats); } done: return bnxt_qplib_is_chip_gen_p5_p7(rdev->chip_ctx) ? BNXT_RE_NUM_EXT_COUNTERS : BNXT_RE_NUM_STD_COUNTERS; }","- if (rdev->pacing.dbr_pacing)
+ if (rdev->pacing.dbr_pacing && bnxt_qplib_is_chip_gen_p5_p7(rdev->chip_ctx))","int bnxt_re_ib_get_hw_stats(struct ib_device *ibdev, struct rdma_hw_stats *stats, u32 port, int index) { struct bnxt_re_dev *rdev = to_bnxt_re_dev(ibdev, ibdev); struct bnxt_re_res_cntrs *res_s = &rdev->stats.res; struct bnxt_qplib_roce_stats *err_s = NULL; struct ctx_hw_stats *hw_stats = NULL; int rc = 0; hw_stats = rdev->qplib_ctx.stats.dma; if (!port || !stats) return -EINVAL; stats->value[BNXT_RE_ACTIVE_QP] = atomic_read(&res_s->qp_count); stats->value[BNXT_RE_ACTIVE_RC_QP] = atomic_read(&res_s->rc_qp_count); stats->value[BNXT_RE_ACTIVE_UD_QP] = atomic_read(&res_s->ud_qp_count); stats->value[BNXT_RE_ACTIVE_SRQ] = atomic_read(&res_s->srq_count); stats->value[BNXT_RE_ACTIVE_CQ] = atomic_read(&res_s->cq_count); stats->value[BNXT_RE_ACTIVE_MR] = atomic_read(&res_s->mr_count); stats->value[BNXT_RE_ACTIVE_MW] = atomic_read(&res_s->mw_count); stats->value[BNXT_RE_ACTIVE_PD] = atomic_read(&res_s->pd_count); stats->value[BNXT_RE_ACTIVE_AH] = atomic_read(&res_s->ah_count); stats->value[BNXT_RE_WATERMARK_QP] = res_s->qp_watermark; stats->value[BNXT_RE_WATERMARK_RC_QP] = res_s->rc_qp_watermark; stats->value[BNXT_RE_WATERMARK_UD_QP] = res_s->ud_qp_watermark; stats->value[BNXT_RE_WATERMARK_SRQ] = res_s->srq_watermark; stats->value[BNXT_RE_WATERMARK_CQ] = res_s->cq_watermark; stats->value[BNXT_RE_WATERMARK_MR] = res_s->mr_watermark; stats->value[BNXT_RE_WATERMARK_MW] = res_s->mw_watermark; stats->value[BNXT_RE_WATERMARK_PD] = res_s->pd_watermark; stats->value[BNXT_RE_WATERMARK_AH] = res_s->ah_watermark; stats->value[BNXT_RE_RESIZE_CQ_CNT] = atomic_read(&res_s->resize_count); if (hw_stats) { stats->value[BNXT_RE_RECOVERABLE_ERRORS] = le64_to_cpu(hw_stats->tx_bcast_pkts); stats->value[BNXT_RE_TX_DISCARDS] = le64_to_cpu(hw_stats->tx_discard_pkts); stats->value[BNXT_RE_TX_ERRORS] = le64_to_cpu(hw_stats->tx_error_pkts); stats->value[BNXT_RE_RX_ERRORS] = le64_to_cpu(hw_stats->rx_error_pkts); stats->value[BNXT_RE_RX_DISCARDS] = le64_to_cpu(hw_stats->rx_discard_pkts); stats->value[BNXT_RE_RX_PKTS] = le64_to_cpu(hw_stats->rx_ucast_pkts); stats->value[BNXT_RE_RX_BYTES] = le64_to_cpu(hw_stats->rx_ucast_bytes); stats->value[BNXT_RE_TX_PKTS] = le64_to_cpu(hw_stats->tx_ucast_pkts); stats->value[BNXT_RE_TX_BYTES] = le64_to_cpu(hw_stats->tx_ucast_bytes); } err_s = &rdev->stats.rstat.errs; if (test_bit(BNXT_RE_FLAG_ISSUE_ROCE_STATS, &rdev->flags)) { rc = bnxt_qplib_get_roce_stats(&rdev->rcfw, err_s); if (rc) { clear_bit(BNXT_RE_FLAG_ISSUE_ROCE_STATS, &rdev->flags); goto done; } bnxt_re_copy_err_stats(rdev, stats, err_s); if (_is_ext_stats_supported(rdev->dev_attr.dev_cap_flags) && !rdev->is_virtfn) { rc = bnxt_re_get_ext_stat(rdev, stats); if (rc) { clear_bit(BNXT_RE_FLAG_ISSUE_ROCE_STATS, &rdev->flags); goto done; } } if (rdev->pacing.dbr_pacing && bnxt_qplib_is_chip_gen_p5_p7(rdev->chip_ctx)) bnxt_re_copy_db_pacing_stats(rdev, stats); } done: return bnxt_qplib_is_chip_gen_p5_p7(rdev->chip_ctx) ? BNXT_RE_NUM_EXT_COUNTERS : BNXT_RE_NUM_STD_COUNTERS; }"
722----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49995/bad/bearer.c----bearer_name_validate,"static int bearer_name_validate(const char *name, struct tipc_bearer_names *name_parts) { char name_copy[TIPC_MAX_BEARER_NAME]; char *media_name; char *if_name; u32 media_len; u32 if_len; if (strscpy(name_copy, name, TIPC_MAX_BEARER_NAME) < 0) return 0; media_name = name_copy; if_name = strchr(media_name, ':'); if (if_name == NULL) return 0; *(if_name++) = 0; media_len = if_name - media_name; if_len = strlen(if_name) + 1; if ((media_len <= 1) || (media_len > TIPC_MAX_MEDIA_NAME) || (if_len <= 1) || (if_len > TIPC_MAX_IF_NAME)) return 0; if (name_parts) { <S2SV_StartVul> strcpy(name_parts->media_name, media_name); <S2SV_EndVul> <S2SV_StartVul> strcpy(name_parts->if_name, if_name); <S2SV_EndVul> } return 1; }","- strcpy(name_parts->media_name, media_name);
- strcpy(name_parts->if_name, if_name);
+ if (strscpy(name_parts->media_name, media_name,
+ TIPC_MAX_MEDIA_NAME) < 0)
+ return 0;
+ if (strscpy(name_parts->if_name, if_name,
+ TIPC_MAX_IF_NAME) < 0)
+ return 0;","static int bearer_name_validate(const char *name, struct tipc_bearer_names *name_parts) { char name_copy[TIPC_MAX_BEARER_NAME]; char *media_name; char *if_name; u32 media_len; u32 if_len; if (strscpy(name_copy, name, TIPC_MAX_BEARER_NAME) < 0) return 0; media_name = name_copy; if_name = strchr(media_name, ':'); if (if_name == NULL) return 0; *(if_name++) = 0; media_len = if_name - media_name; if_len = strlen(if_name) + 1; if ((media_len <= 1) || (media_len > TIPC_MAX_MEDIA_NAME) || (if_len <= 1) || (if_len > TIPC_MAX_IF_NAME)) return 0; if (name_parts) { if (strscpy(name_parts->media_name, media_name, TIPC_MAX_MEDIA_NAME) < 0) return 0; if (strscpy(name_parts->if_name, if_name, TIPC_MAX_IF_NAME) < 0) return 0; } return 1; }"
1390----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56722/bad/hns_roce_hw_v2.c----free_mr_modify_rsv_qp,"static int free_mr_modify_rsv_qp(struct hns_roce_dev *hr_dev, struct ib_qp_attr *attr, int sl_num) { struct hns_roce_v2_priv *priv = hr_dev->priv; struct hns_roce_v2_free_mr *free_mr = &priv->free_mr; struct ib_device *ibdev = &hr_dev->ib_dev; struct hns_roce_qp *hr_qp; int loopback; int mask; int ret; hr_qp = to_hr_qp(&free_mr->rsv_qp[sl_num]->ibqp); hr_qp->free_mr_en = 1; hr_qp->ibqp.device = ibdev; hr_qp->ibqp.qp_type = IB_QPT_RC; mask = IB_QP_STATE | IB_QP_PKEY_INDEX | IB_QP_PORT | IB_QP_ACCESS_FLAGS; attr->qp_state = IB_QPS_INIT; attr->port_num = 1; attr->qp_access_flags = IB_ACCESS_REMOTE_WRITE; ret = hr_dev->hw->modify_qp(&hr_qp->ibqp, attr, mask, IB_QPS_INIT, IB_QPS_INIT, NULL); if (ret) { <S2SV_StartVul> ibdev_err(ibdev, ""failed to modify qp to init, ret = %d.\n"", <S2SV_EndVul> <S2SV_StartVul> ret); <S2SV_EndVul> return ret; <S2SV_StartVul> } <S2SV_EndVul> loopback = hr_dev->loop_idc; hr_dev->loop_idc = 1; mask = IB_QP_STATE | IB_QP_AV | IB_QP_PATH_MTU | IB_QP_DEST_QPN | IB_QP_RQ_PSN | IB_QP_MAX_DEST_RD_ATOMIC | IB_QP_MIN_RNR_TIMER; attr->qp_state = IB_QPS_RTR; attr->ah_attr.type = RDMA_AH_ATTR_TYPE_ROCE; attr->path_mtu = IB_MTU_256; attr->dest_qp_num = hr_qp->qpn; attr->rq_psn = HNS_ROCE_FREE_MR_USED_PSN; rdma_ah_set_sl(&attr->ah_attr, (u8)sl_num); ret = hr_dev->hw->modify_qp(&hr_qp->ibqp, attr, mask, IB_QPS_INIT, IB_QPS_RTR, NULL); hr_dev->loop_idc = loopback; if (ret) { ibdev_err(ibdev, ""failed to modify qp to rtr, ret = %d.\n"", ret); return ret; } mask = IB_QP_STATE | IB_QP_SQ_PSN | IB_QP_RETRY_CNT | IB_QP_TIMEOUT | IB_QP_RNR_RETRY | IB_QP_MAX_QP_RD_ATOMIC; attr->qp_state = IB_QPS_RTS; attr->sq_psn = HNS_ROCE_FREE_MR_USED_PSN; attr->retry_cnt = HNS_ROCE_FREE_MR_USED_QP_RETRY_CNT; attr->timeout = HNS_ROCE_FREE_MR_USED_QP_TIMEOUT; ret = hr_dev->hw->modify_qp(&hr_qp->ibqp, attr, mask, IB_QPS_RTR, IB_QPS_RTS, NULL); if (ret) ibdev_err(ibdev, ""failed to modify qp to rts, ret = %d.\n"", ret); return ret; }","- ibdev_err(ibdev, ""failed to modify qp to init, ret = %d.\n"",
- ret);
- }
+ ibdev_err_ratelimited(ibdev, ""failed to modify qp to init, ret = %d.\n"",
+ ret);","static int free_mr_modify_rsv_qp(struct hns_roce_dev *hr_dev, struct ib_qp_attr *attr, int sl_num) { struct hns_roce_v2_priv *priv = hr_dev->priv; struct hns_roce_v2_free_mr *free_mr = &priv->free_mr; struct ib_device *ibdev = &hr_dev->ib_dev; struct hns_roce_qp *hr_qp; int loopback; int mask; int ret; hr_qp = to_hr_qp(&free_mr->rsv_qp[sl_num]->ibqp); hr_qp->free_mr_en = 1; hr_qp->ibqp.device = ibdev; hr_qp->ibqp.qp_type = IB_QPT_RC; mask = IB_QP_STATE | IB_QP_PKEY_INDEX | IB_QP_PORT | IB_QP_ACCESS_FLAGS; attr->qp_state = IB_QPS_INIT; attr->port_num = 1; attr->qp_access_flags = IB_ACCESS_REMOTE_WRITE; ret = hr_dev->hw->modify_qp(&hr_qp->ibqp, attr, mask, IB_QPS_INIT, IB_QPS_INIT, NULL); if (ret) { ibdev_err_ratelimited(ibdev, ""failed to modify qp to init, ret = %d.\n"", ret); return ret; } loopback = hr_dev->loop_idc; hr_dev->loop_idc = 1; mask = IB_QP_STATE | IB_QP_AV | IB_QP_PATH_MTU | IB_QP_DEST_QPN | IB_QP_RQ_PSN | IB_QP_MAX_DEST_RD_ATOMIC | IB_QP_MIN_RNR_TIMER; attr->qp_state = IB_QPS_RTR; attr->ah_attr.type = RDMA_AH_ATTR_TYPE_ROCE; attr->path_mtu = IB_MTU_256; attr->dest_qp_num = hr_qp->qpn; attr->rq_psn = HNS_ROCE_FREE_MR_USED_PSN; rdma_ah_set_sl(&attr->ah_attr, (u8)sl_num); ret = hr_dev->hw->modify_qp(&hr_qp->ibqp, attr, mask, IB_QPS_INIT, IB_QPS_RTR, NULL); hr_dev->loop_idc = loopback; if (ret) { ibdev_err(ibdev, ""failed to modify qp to rtr, ret = %d.\n"", ret); return ret; } mask = IB_QP_STATE | IB_QP_SQ_PSN | IB_QP_RETRY_CNT | IB_QP_TIMEOUT | IB_QP_RNR_RETRY | IB_QP_MAX_QP_RD_ATOMIC; attr->qp_state = IB_QPS_RTS; attr->sq_psn = HNS_ROCE_FREE_MR_USED_PSN; attr->retry_cnt = HNS_ROCE_FREE_MR_USED_QP_RETRY_CNT; attr->timeout = HNS_ROCE_FREE_MR_USED_QP_TIMEOUT; ret = hr_dev->hw->modify_qp(&hr_qp->ibqp, attr, mask, IB_QPS_RTR, IB_QPS_RTS, NULL); if (ret) ibdev_err(ibdev, ""failed to modify qp to rts, ret = %d.\n"", ret); return ret; }"
618----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49923/bad/dcn21_resource.c----dcn21_fast_validate_bw,"bool dcn21_fast_validate_bw(struct dc *dc, struct dc_state *context, display_e2e_pipe_params_st *pipes, int *pipe_cnt_out, int *pipe_split_from, int *vlevel_out, bool fast_validate) { bool out = false; int split[MAX_PIPES] = { 0 }; int pipe_cnt, i, pipe_idx, vlevel; ASSERT(pipes); if (!pipes) return false; dcn20_merge_pipes_for_validate(dc, context); DC_FP_START(); pipe_cnt = dc->res_pool->funcs->populate_dml_pipes(dc, context, pipes, fast_validate); DC_FP_END(); *pipe_cnt_out = pipe_cnt; if (!pipe_cnt) { out = true; goto validate_out; } context->bw_ctx.dml.soc.allow_dram_self_refresh_or_dram_clock_change_in_vblank = dm_allow_self_refresh_and_mclk_switch; vlevel = dml_get_voltage_level(&context->bw_ctx.dml, pipes, pipe_cnt); if (vlevel > context->bw_ctx.dml.soc.num_states) { context->bw_ctx.dml.soc.allow_dram_self_refresh_or_dram_clock_change_in_vblank = dm_allow_self_refresh; vlevel = dml_get_voltage_level(&context->bw_ctx.dml, pipes, pipe_cnt); if (vlevel > context->bw_ctx.dml.soc.num_states) goto validate_fail; } <S2SV_StartVul> vlevel = dcn20_validate_apply_pipe_split_flags(dc, context, vlevel, split, NULL); <S2SV_EndVul> for (i = 0, pipe_idx = 0; i < dc->res_pool->pipe_count; i++) { struct pipe_ctx *pipe = &context->res_ctx.pipe_ctx[i]; struct pipe_ctx *mpo_pipe = pipe->bottom_pipe; struct vba_vars_st *vba = &context->bw_ctx.dml.vba; if (!pipe->stream) continue; if (vba->ODMCombineEnabled[vba->pipe_plane[pipe_idx]] != dm_odm_combine_mode_disabled && pipe->plane_state && mpo_pipe && memcmp(&mpo_pipe->plane_state->clip_rect, &pipe->stream->src, sizeof(struct rect)) != 0) { ASSERT(mpo_pipe->plane_state != pipe->plane_state); goto validate_fail; } pipe_idx++; } for (i = 0; i < MAX_PIPES; i++) pipe_split_from[i] = -1; for (i = 0, pipe_idx = -1; i < dc->res_pool->pipe_count; i++) { struct pipe_ctx *pipe = &context->res_ctx.pipe_ctx[i]; struct pipe_ctx *hsplit_pipe = pipe->bottom_pipe; if (!pipe->stream || pipe_split_from[i] >= 0) continue; pipe_idx++; if (!pipe->top_pipe && !pipe->plane_state && context->bw_ctx.dml.vba.ODMCombineEnabled[pipe_idx]) { hsplit_pipe = dcn20_find_secondary_pipe(dc, &context->res_ctx, dc->res_pool, pipe); ASSERT(hsplit_pipe); if (!dcn20_split_stream_for_odm( dc, &context->res_ctx, pipe, hsplit_pipe)) goto validate_fail; pipe_split_from[hsplit_pipe->pipe_idx] = pipe_idx; dcn20_build_mapped_resource(dc, context, pipe->stream); } if (!pipe->plane_state) continue; if (pipe->top_pipe && pipe->plane_state == pipe->top_pipe->plane_state) continue; if (split[i] == 2) { if (!hsplit_pipe || hsplit_pipe->plane_state != pipe->plane_state) { hsplit_pipe = dcn20_find_secondary_pipe(dc, &context->res_ctx, dc->res_pool, pipe); ASSERT(hsplit_pipe); if (!hsplit_pipe) { DC_FP_START(); dcn20_fpu_adjust_dppclk(&context->bw_ctx.dml.vba, vlevel, context->bw_ctx.dml.vba.maxMpcComb, pipe_idx, true); DC_FP_END(); continue; } if (context->bw_ctx.dml.vba.ODMCombineEnabled[pipe_idx]) { if (!dcn20_split_stream_for_odm( dc, &context->res_ctx, pipe, hsplit_pipe)) goto validate_fail; dcn20_build_mapped_resource(dc, context, pipe->stream); } else { dcn20_split_stream_for_mpc( &context->res_ctx, dc->res_pool, pipe, hsplit_pipe); resource_build_scaling_params(pipe); resource_build_scaling_params(hsplit_pipe); } pipe_split_from[hsplit_pipe->pipe_idx] = pipe_idx; } } else if (hsplit_pipe && hsplit_pipe->plane_state == pipe->plane_state) { ASSERT(0); } } if (!dcn20_validate_dsc(dc, context)) { context->bw_ctx.dml.vba.ValidationStatus[context->bw_ctx.dml.vba.soc.num_states] = DML_FAIL_DSC_VALIDATION_FAILURE; goto validate_fail; } *vlevel_out = vlevel; out = true; goto validate_out; validate_fail: out = false; validate_out: return out; }","- vlevel = dcn20_validate_apply_pipe_split_flags(dc, context, vlevel, split, NULL);
+ bool merge[MAX_PIPES] = { false };
+ vlevel = dcn20_validate_apply_pipe_split_flags(dc, context, vlevel, split, merge);","bool dcn21_fast_validate_bw(struct dc *dc, struct dc_state *context, display_e2e_pipe_params_st *pipes, int *pipe_cnt_out, int *pipe_split_from, int *vlevel_out, bool fast_validate) { bool out = false; int split[MAX_PIPES] = { 0 }; bool merge[MAX_PIPES] = { false }; int pipe_cnt, i, pipe_idx, vlevel; ASSERT(pipes); if (!pipes) return false; dcn20_merge_pipes_for_validate(dc, context); DC_FP_START(); pipe_cnt = dc->res_pool->funcs->populate_dml_pipes(dc, context, pipes, fast_validate); DC_FP_END(); *pipe_cnt_out = pipe_cnt; if (!pipe_cnt) { out = true; goto validate_out; } context->bw_ctx.dml.soc.allow_dram_self_refresh_or_dram_clock_change_in_vblank = dm_allow_self_refresh_and_mclk_switch; vlevel = dml_get_voltage_level(&context->bw_ctx.dml, pipes, pipe_cnt); if (vlevel > context->bw_ctx.dml.soc.num_states) { context->bw_ctx.dml.soc.allow_dram_self_refresh_or_dram_clock_change_in_vblank = dm_allow_self_refresh; vlevel = dml_get_voltage_level(&context->bw_ctx.dml, pipes, pipe_cnt); if (vlevel > context->bw_ctx.dml.soc.num_states) goto validate_fail; } vlevel = dcn20_validate_apply_pipe_split_flags(dc, context, vlevel, split, merge); for (i = 0, pipe_idx = 0; i < dc->res_pool->pipe_count; i++) { struct pipe_ctx *pipe = &context->res_ctx.pipe_ctx[i]; struct pipe_ctx *mpo_pipe = pipe->bottom_pipe; struct vba_vars_st *vba = &context->bw_ctx.dml.vba; if (!pipe->stream) continue; if (vba->ODMCombineEnabled[vba->pipe_plane[pipe_idx]] != dm_odm_combine_mode_disabled && pipe->plane_state && mpo_pipe && memcmp(&mpo_pipe->plane_state->clip_rect, &pipe->stream->src, sizeof(struct rect)) != 0) { ASSERT(mpo_pipe->plane_state != pipe->plane_state); goto validate_fail; } pipe_idx++; } for (i = 0; i < MAX_PIPES; i++) pipe_split_from[i] = -1; for (i = 0, pipe_idx = -1; i < dc->res_pool->pipe_count; i++) { struct pipe_ctx *pipe = &context->res_ctx.pipe_ctx[i]; struct pipe_ctx *hsplit_pipe = pipe->bottom_pipe; if (!pipe->stream || pipe_split_from[i] >= 0) continue; pipe_idx++; if (!pipe->top_pipe && !pipe->plane_state && context->bw_ctx.dml.vba.ODMCombineEnabled[pipe_idx]) { hsplit_pipe = dcn20_find_secondary_pipe(dc, &context->res_ctx, dc->res_pool, pipe); ASSERT(hsplit_pipe); if (!dcn20_split_stream_for_odm( dc, &context->res_ctx, pipe, hsplit_pipe)) goto validate_fail; pipe_split_from[hsplit_pipe->pipe_idx] = pipe_idx; dcn20_build_mapped_resource(dc, context, pipe->stream); } if (!pipe->plane_state) continue; if (pipe->top_pipe && pipe->plane_state == pipe->top_pipe->plane_state) continue; if (split[i] == 2) { if (!hsplit_pipe || hsplit_pipe->plane_state != pipe->plane_state) { hsplit_pipe = dcn20_find_secondary_pipe(dc, &context->res_ctx, dc->res_pool, pipe); ASSERT(hsplit_pipe); if (!hsplit_pipe) { DC_FP_START(); dcn20_fpu_adjust_dppclk(&context->bw_ctx.dml.vba, vlevel, context->bw_ctx.dml.vba.maxMpcComb, pipe_idx, true); DC_FP_END(); continue; } if (context->bw_ctx.dml.vba.ODMCombineEnabled[pipe_idx]) { if (!dcn20_split_stream_for_odm( dc, &context->res_ctx, pipe, hsplit_pipe)) goto validate_fail; dcn20_build_mapped_resource(dc, context, pipe->stream); } else { dcn20_split_stream_for_mpc( &context->res_ctx, dc->res_pool, pipe, hsplit_pipe); resource_build_scaling_params(pipe); resource_build_scaling_params(hsplit_pipe); } pipe_split_from[hsplit_pipe->pipe_idx] = pipe_idx; } } else if (hsplit_pipe && hsplit_pipe->plane_state == pipe->plane_state) { ASSERT(0); } } if (!dcn20_validate_dsc(dc, context)) { context->bw_ctx.dml.vba.ValidationStatus[context->bw_ctx.dml.vba.soc.num_states] = DML_FAIL_DSC_VALIDATION_FAILURE; goto validate_fail; } *vlevel_out = vlevel; out = true; goto validate_out; validate_fail: out = false; validate_out: return out; }"
1380----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56715/bad/ionic_lif.c----ionic_lif_register,"int ionic_lif_register(struct ionic_lif *lif) { int err; ionic_lif_register_phc(lif); INIT_WORK(&lif->ionic->nb_work, ionic_lif_notify_work); lif->ionic->nb.notifier_call = ionic_lif_notify; err = register_netdevice_notifier(&lif->ionic->nb); if (err) lif->ionic->nb.notifier_call = NULL; err = register_netdev(lif->netdev); if (err) { <S2SV_StartVul> dev_err(lif->ionic->dev, ""Cannot register net device, aborting\n""); <S2SV_EndVul> <S2SV_StartVul> ionic_lif_unregister_phc(lif); <S2SV_EndVul> return err; } ionic_link_status_check_request(lif, CAN_SLEEP); lif->registered = true; ionic_lif_set_netdev_info(lif); return 0; }","- dev_err(lif->ionic->dev, ""Cannot register net device, aborting\n"");
- ionic_lif_unregister_phc(lif);
+ dev_err(lif->ionic->dev, ""Cannot register net device: %d, aborting\n"", err);
+ ionic_lif_unregister(lif);","int ionic_lif_register(struct ionic_lif *lif) { int err; ionic_lif_register_phc(lif); INIT_WORK(&lif->ionic->nb_work, ionic_lif_notify_work); lif->ionic->nb.notifier_call = ionic_lif_notify; err = register_netdevice_notifier(&lif->ionic->nb); if (err) lif->ionic->nb.notifier_call = NULL; err = register_netdev(lif->netdev); if (err) { dev_err(lif->ionic->dev, ""Cannot register net device: %d, aborting\n"", err); ionic_lif_unregister(lif); return err; } ionic_link_status_check_request(lif, CAN_SLEEP); lif->registered = true; ionic_lif_set_netdev_info(lif); return 0; }"
1233----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53689/bad/blk-mq.c----blk_mq_realloc_hw_ctxs,"static void blk_mq_realloc_hw_ctxs(struct blk_mq_tag_set *set, struct request_queue *q) { struct blk_mq_hw_ctx *hctx; unsigned long i, j; <S2SV_StartVul> mutex_lock(&q->sysfs_lock); <S2SV_EndVul> for (i = 0; i < set->nr_hw_queues; i++) { int old_node; int node = blk_mq_get_hctx_node(set, i); struct blk_mq_hw_ctx *old_hctx = xa_load(&q->hctx_table, i); if (old_hctx) { old_node = old_hctx->numa_node; blk_mq_exit_hctx(q, set, old_hctx, i); } if (!blk_mq_alloc_and_init_hctx(set, q, i, node)) { if (!old_hctx) break; pr_warn(""Allocate new hctx on node %d fails, fallback to previous one on node %d\n"", node, old_node); hctx = blk_mq_alloc_and_init_hctx(set, q, i, old_node); WARN_ON_ONCE(!hctx); } } if (i != set->nr_hw_queues) { j = q->nr_hw_queues; } else { j = i; q->nr_hw_queues = set->nr_hw_queues; } xa_for_each_start(&q->hctx_table, j, hctx, j) blk_mq_exit_hctx(q, set, hctx, j); <S2SV_StartVul> mutex_unlock(&q->sysfs_lock); <S2SV_EndVul> blk_mq_remove_hw_queues_cpuhp(q); blk_mq_add_hw_queues_cpuhp(q); }","- mutex_lock(&q->sysfs_lock);
- mutex_unlock(&q->sysfs_lock);
+ lockdep_assert_held(&q->sysfs_lock);","static void blk_mq_realloc_hw_ctxs(struct blk_mq_tag_set *set, struct request_queue *q) { struct blk_mq_hw_ctx *hctx; unsigned long i, j; lockdep_assert_held(&q->sysfs_lock); for (i = 0; i < set->nr_hw_queues; i++) { int old_node; int node = blk_mq_get_hctx_node(set, i); struct blk_mq_hw_ctx *old_hctx = xa_load(&q->hctx_table, i); if (old_hctx) { old_node = old_hctx->numa_node; blk_mq_exit_hctx(q, set, old_hctx, i); } if (!blk_mq_alloc_and_init_hctx(set, q, i, node)) { if (!old_hctx) break; pr_warn(""Allocate new hctx on node %d fails, fallback to previous one on node %d\n"", node, old_node); hctx = blk_mq_alloc_and_init_hctx(set, q, i, old_node); WARN_ON_ONCE(!hctx); } } if (i != set->nr_hw_queues) { j = q->nr_hw_queues; } else { j = i; q->nr_hw_queues = set->nr_hw_queues; } xa_for_each_start(&q->hctx_table, j, hctx, j) blk_mq_exit_hctx(q, set, hctx, j); blk_mq_remove_hw_queues_cpuhp(q); blk_mq_add_hw_queues_cpuhp(q); }"
269----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46750/bad/pci.c----pci_slot_trylock,"static int pci_slot_trylock(struct pci_slot *slot) { struct pci_dev *dev; list_for_each_entry(dev, &slot->bus->devices, bus_list) { if (!dev->slot || dev->slot != slot) continue; <S2SV_StartVul> if (!pci_dev_trylock(dev)) <S2SV_EndVul> <S2SV_StartVul> goto unlock; <S2SV_EndVul> if (dev->subordinate) { <S2SV_StartVul> if (!pci_bus_trylock(dev->subordinate)) { <S2SV_EndVul> <S2SV_StartVul> pci_dev_unlock(dev); <S2SV_EndVul> <S2SV_StartVul> goto unlock; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> return 1; unlock: list_for_each_entry_continue_reverse(dev, &slot->bus->devices, bus_list) { if (!dev->slot || dev->slot != slot) continue; if (dev->subordinate) pci_bus_unlock(dev->subordinate); <S2SV_StartVul> pci_dev_unlock(dev); <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> return 0; <S2SV_StartVul> } <S2SV_EndVul>","- if (!pci_dev_trylock(dev))
- goto unlock;
- if (!pci_bus_trylock(dev->subordinate)) {
- pci_dev_unlock(dev);
- goto unlock;
- }
- }
- }
- pci_dev_unlock(dev);
- }
- }
+ pci_dev_unlock(dev);
+ goto unlock;
+ } else if (!pci_dev_trylock(dev))
+ goto unlock;
+ else
+ pci_dev_unlock(dev);
+ return 0;","static int pci_slot_trylock(struct pci_slot *slot) { struct pci_dev *dev; list_for_each_entry(dev, &slot->bus->devices, bus_list) { if (!dev->slot || dev->slot != slot) continue; if (dev->subordinate) { if (!pci_bus_trylock(dev->subordinate)) { pci_dev_unlock(dev); goto unlock; } } else if (!pci_dev_trylock(dev)) goto unlock; } return 1; unlock: list_for_each_entry_continue_reverse(dev, &slot->bus->devices, bus_list) { if (!dev->slot || dev->slot != slot) continue; if (dev->subordinate) pci_bus_unlock(dev->subordinate); else pci_dev_unlock(dev); } return 0; }"
490----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47731/bad/alibaba_uncore_drw_pmu.c----ali_drw_pmu_isr,"static irqreturn_t ali_drw_pmu_isr(int irq_num, void *data) { struct ali_drw_pmu_irq *irq = data; struct ali_drw_pmu *drw_pmu; irqreturn_t ret = IRQ_NONE; rcu_read_lock(); list_for_each_entry_rcu(drw_pmu, &irq->pmus_node, pmus_node) { unsigned long status, clr_status; struct perf_event *event; unsigned int idx; for (idx = 0; idx < ALI_DRW_PMU_COMMON_MAX_COUNTERS; idx++) { event = drw_pmu->events[idx]; if (!event) continue; ali_drw_pmu_disable_counter(event); } status = readl(drw_pmu->cfg_base + ALI_DRW_PMU_OV_INTR_STATUS); status = FIELD_GET(ALI_DRW_PMCOM_CNT_OV_INTR_MASK, status); if (status) { for_each_set_bit(idx, &status, ALI_DRW_PMU_COMMON_MAX_COUNTERS) { event = drw_pmu->events[idx]; if (WARN_ON_ONCE(!event)) continue; ali_drw_pmu_event_update(event); ali_drw_pmu_event_set_period(event); } <S2SV_StartVul> clr_status = FIELD_PREP(ALI_DRW_PMCOM_CNT_OV_INTR_MASK, 1); <S2SV_EndVul> writel(clr_status, drw_pmu->cfg_base + ALI_DRW_PMU_OV_INTR_CLR); } for (idx = 0; idx < ALI_DRW_PMU_COMMON_MAX_COUNTERS; idx++) { event = drw_pmu->events[idx]; if (!event) continue; if (!(event->hw.state & PERF_HES_STOPPED)) ali_drw_pmu_enable_counter(event); } if (status) ret = IRQ_HANDLED; } rcu_read_unlock(); return ret; }","- clr_status = FIELD_PREP(ALI_DRW_PMCOM_CNT_OV_INTR_MASK, 1);
+ clr_status = FIELD_PREP(ALI_DRW_PMCOM_CNT_OV_INTR_MASK, status);","static irqreturn_t ali_drw_pmu_isr(int irq_num, void *data) { struct ali_drw_pmu_irq *irq = data; struct ali_drw_pmu *drw_pmu; irqreturn_t ret = IRQ_NONE; rcu_read_lock(); list_for_each_entry_rcu(drw_pmu, &irq->pmus_node, pmus_node) { unsigned long status, clr_status; struct perf_event *event; unsigned int idx; for (idx = 0; idx < ALI_DRW_PMU_COMMON_MAX_COUNTERS; idx++) { event = drw_pmu->events[idx]; if (!event) continue; ali_drw_pmu_disable_counter(event); } status = readl(drw_pmu->cfg_base + ALI_DRW_PMU_OV_INTR_STATUS); status = FIELD_GET(ALI_DRW_PMCOM_CNT_OV_INTR_MASK, status); if (status) { for_each_set_bit(idx, &status, ALI_DRW_PMU_COMMON_MAX_COUNTERS) { event = drw_pmu->events[idx]; if (WARN_ON_ONCE(!event)) continue; ali_drw_pmu_event_update(event); ali_drw_pmu_event_set_period(event); } clr_status = FIELD_PREP(ALI_DRW_PMCOM_CNT_OV_INTR_MASK, status); writel(clr_status, drw_pmu->cfg_base + ALI_DRW_PMU_OV_INTR_CLR); } for (idx = 0; idx < ALI_DRW_PMU_COMMON_MAX_COUNTERS; idx++) { event = drw_pmu->events[idx]; if (!event) continue; if (!(event->hw.state & PERF_HES_STOPPED)) ali_drw_pmu_enable_counter(event); } if (status) ret = IRQ_HANDLED; } rcu_read_unlock(); return ret; }"
759----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50036/bad/dst.c----*dst_destroy,"struct dst_entry *dst_destroy(struct dst_entry * dst) { struct dst_entry *child = NULL; smp_rmb(); #ifdef CONFIG_XFRM if (dst->xfrm) { struct xfrm_dst *xdst = (struct xfrm_dst *) dst; child = xdst->child; } #endif <S2SV_StartVul> if (!(dst->flags & DST_NOCOUNT)) <S2SV_EndVul> <S2SV_StartVul> dst_entries_add(dst->ops, -1); <S2SV_EndVul> if (dst->ops->destroy) dst->ops->destroy(dst); dev_put(dst->dev); lwtstate_put(dst->lwtstate); if (dst->flags & DST_METADATA) metadata_dst_free((struct metadata_dst *)dst); else kmem_cache_free(dst->ops->kmem_cachep, dst); dst = child; if (dst) dst_release_immediate(dst); return NULL; }","- if (!(dst->flags & DST_NOCOUNT))
- dst_entries_add(dst->ops, -1);
+ }","struct dst_entry *dst_destroy(struct dst_entry * dst) { struct dst_entry *child = NULL; smp_rmb(); #ifdef CONFIG_XFRM if (dst->xfrm) { struct xfrm_dst *xdst = (struct xfrm_dst *) dst; child = xdst->child; } #endif if (dst->ops->destroy) dst->ops->destroy(dst); dev_put(dst->dev); lwtstate_put(dst->lwtstate); if (dst->flags & DST_METADATA) metadata_dst_free((struct metadata_dst *)dst); else kmem_cache_free(dst->ops->kmem_cachep, dst); dst = child; if (dst) dst_release_immediate(dst); return NULL; }"
1264----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56544/bad/udmabuf.c----udmabuf_create,"static long udmabuf_create(struct miscdevice *device, struct udmabuf_create_list *head, struct udmabuf_create_item *list) { pgoff_t pgoff, pgcnt, pglimit, pgbuf = 0; long nr_folios, ret = -EINVAL; struct file *memfd = NULL; struct folio **folios; struct udmabuf *ubuf; u32 i, j, k, flags; loff_t end; ubuf = kzalloc(sizeof(*ubuf), GFP_KERNEL); if (!ubuf) return -ENOMEM; INIT_LIST_HEAD(&ubuf->unpin_list); pglimit = (size_limit_mb * 1024 * 1024) >> PAGE_SHIFT; for (i = 0; i < head->count; i++) { if (!IS_ALIGNED(list[i].offset, PAGE_SIZE)) goto err; if (!IS_ALIGNED(list[i].size, PAGE_SIZE)) goto err; ubuf->pagecount += list[i].size >> PAGE_SHIFT; if (ubuf->pagecount > pglimit) goto err; } if (!ubuf->pagecount) goto err; <S2SV_StartVul> ubuf->folios = kmalloc_array(ubuf->pagecount, sizeof(*ubuf->folios), <S2SV_EndVul> <S2SV_StartVul> GFP_KERNEL); <S2SV_EndVul> if (!ubuf->folios) { ret = -ENOMEM; goto err; } <S2SV_StartVul> ubuf->offsets = kcalloc(ubuf->pagecount, sizeof(*ubuf->offsets), <S2SV_EndVul> <S2SV_StartVul> GFP_KERNEL); <S2SV_EndVul> if (!ubuf->offsets) { ret = -ENOMEM; goto err; } pgbuf = 0; for (i = 0; i < head->count; i++) { memfd = fget(list[i].memfd); ret = check_memfd_seals(memfd); if (ret < 0) goto err; pgcnt = list[i].size >> PAGE_SHIFT; <S2SV_StartVul> folios = kmalloc_array(pgcnt, sizeof(*folios), GFP_KERNEL); <S2SV_EndVul> if (!folios) { ret = -ENOMEM; goto err; } end = list[i].offset + (pgcnt << PAGE_SHIFT) - 1; ret = memfd_pin_folios(memfd, list[i].offset, end, folios, pgcnt, &pgoff); if (ret <= 0) { <S2SV_StartVul> kfree(folios); <S2SV_EndVul> if (!ret) ret = -EINVAL; goto err; } nr_folios = ret; pgoff >>= PAGE_SHIFT; for (j = 0, k = 0; j < pgcnt; j++) { ubuf->folios[pgbuf] = folios[k]; ubuf->offsets[pgbuf] = pgoff << PAGE_SHIFT; if (j == 0 || ubuf->folios[pgbuf-1] != folios[k]) { ret = add_to_unpin_list(&ubuf->unpin_list, folios[k]); if (ret < 0) { kfree(folios); goto err; } } pgbuf++; if (++pgoff == folio_nr_pages(folios[k])) { pgoff = 0; if (++k == nr_folios) break; } } <S2SV_StartVul> kfree(folios); <S2SV_EndVul> fput(memfd); memfd = NULL; } flags = head->flags & UDMABUF_FLAGS_CLOEXEC ? O_CLOEXEC : 0; ret = export_udmabuf(ubuf, device, flags); if (ret < 0) goto err; return ret; err: if (memfd) fput(memfd); unpin_all_folios(&ubuf->unpin_list); <S2SV_StartVul> kfree(ubuf->offsets); <S2SV_EndVul> <S2SV_StartVul> kfree(ubuf->folios); <S2SV_EndVul> kfree(ubuf); return ret; }","- ubuf->folios = kmalloc_array(ubuf->pagecount, sizeof(*ubuf->folios),
- GFP_KERNEL);
- ubuf->offsets = kcalloc(ubuf->pagecount, sizeof(*ubuf->offsets),
- GFP_KERNEL);
- folios = kmalloc_array(pgcnt, sizeof(*folios), GFP_KERNEL);
- kfree(folios);
- kfree(folios);
- kfree(ubuf->offsets);
- kfree(ubuf->folios);
+ ubuf->folios = kvmalloc_array(ubuf->pagecount, sizeof(*ubuf->folios),
+ GFP_KERNEL);
+ ubuf->offsets = kvcalloc(ubuf->pagecount, sizeof(*ubuf->offsets),
+ GFP_KERNEL);
+ folios = kvmalloc_array(pgcnt, sizeof(*folios), GFP_KERNEL);
+ kvfree(folios);
+ kvfree(folios);
+ kvfree(ubuf->offsets);
+ kvfree(ubuf->folios);","static long udmabuf_create(struct miscdevice *device, struct udmabuf_create_list *head, struct udmabuf_create_item *list) { pgoff_t pgoff, pgcnt, pglimit, pgbuf = 0; long nr_folios, ret = -EINVAL; struct file *memfd = NULL; struct folio **folios; struct udmabuf *ubuf; u32 i, j, k, flags; loff_t end; ubuf = kzalloc(sizeof(*ubuf), GFP_KERNEL); if (!ubuf) return -ENOMEM; INIT_LIST_HEAD(&ubuf->unpin_list); pglimit = (size_limit_mb * 1024 * 1024) >> PAGE_SHIFT; for (i = 0; i < head->count; i++) { if (!IS_ALIGNED(list[i].offset, PAGE_SIZE)) goto err; if (!IS_ALIGNED(list[i].size, PAGE_SIZE)) goto err; ubuf->pagecount += list[i].size >> PAGE_SHIFT; if (ubuf->pagecount > pglimit) goto err; } if (!ubuf->pagecount) goto err; ubuf->folios = kvmalloc_array(ubuf->pagecount, sizeof(*ubuf->folios), GFP_KERNEL); if (!ubuf->folios) { ret = -ENOMEM; goto err; } ubuf->offsets = kvcalloc(ubuf->pagecount, sizeof(*ubuf->offsets), GFP_KERNEL); if (!ubuf->offsets) { ret = -ENOMEM; goto err; } pgbuf = 0; for (i = 0; i < head->count; i++) { memfd = fget(list[i].memfd); ret = check_memfd_seals(memfd); if (ret < 0) goto err; pgcnt = list[i].size >> PAGE_SHIFT; folios = kvmalloc_array(pgcnt, sizeof(*folios), GFP_KERNEL); if (!folios) { ret = -ENOMEM; goto err; } end = list[i].offset + (pgcnt << PAGE_SHIFT) - 1; ret = memfd_pin_folios(memfd, list[i].offset, end, folios, pgcnt, &pgoff); if (ret <= 0) { kvfree(folios); if (!ret) ret = -EINVAL; goto err; } nr_folios = ret; pgoff >>= PAGE_SHIFT; for (j = 0, k = 0; j < pgcnt; j++) { ubuf->folios[pgbuf] = folios[k]; ubuf->offsets[pgbuf] = pgoff << PAGE_SHIFT; if (j == 0 || ubuf->folios[pgbuf-1] != folios[k]) { ret = add_to_unpin_list(&ubuf->unpin_list, folios[k]); if (ret < 0) { kfree(folios); goto err; } } pgbuf++; if (++pgoff == folio_nr_pages(folios[k])) { pgoff = 0; if (++k == nr_folios) break; } } kvfree(folios); fput(memfd); memfd = NULL; } flags = head->flags & UDMABUF_FLAGS_CLOEXEC ? O_CLOEXEC : 0; ret = export_udmabuf(ubuf, device, flags); if (ret < 0) goto err; return ret; err: if (memfd) fput(memfd); unpin_all_folios(&ubuf->unpin_list); kvfree(ubuf->offsets); kvfree(ubuf->folios); kfree(ubuf); return ret; }"
424----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47665/bad/dma.c----hci_dma_init,"static int hci_dma_init(struct i3c_hci *hci) { struct hci_rings_data *rings; struct hci_rh_data *rh; u32 regval; unsigned int i, nr_rings, xfers_sz, resps_sz; unsigned int ibi_status_ring_sz, ibi_data_ring_sz; int ret; regval = rhs_reg_read(CONTROL); nr_rings = FIELD_GET(MAX_HEADER_COUNT_CAP, regval); dev_info(&hci->master.dev, ""%d DMA rings available\n"", nr_rings); if (unlikely(nr_rings > 8)) { dev_err(&hci->master.dev, ""number of rings should be <= 8\n""); nr_rings = 8; } if (nr_rings > XFER_RINGS) nr_rings = XFER_RINGS; rings = kzalloc(struct_size(rings, headers, nr_rings), GFP_KERNEL); if (!rings) return -ENOMEM; hci->io_data = rings; rings->total = nr_rings; regval = FIELD_PREP(MAX_HEADER_COUNT, rings->total); rhs_reg_write(CONTROL, regval); for (i = 0; i < rings->total; i++) { u32 offset = rhs_reg_read(RHn_OFFSET(i)); dev_info(&hci->master.dev, ""Ring %d at offset %#x\n"", i, offset); ret = -EINVAL; if (!offset) goto err_out; rh = &rings->headers[i]; rh->regs = hci->base_regs + offset; spin_lock_init(&rh->lock); init_completion(&rh->op_done); rh->xfer_entries = XFER_RING_ENTRIES; regval = rh_reg_read(CR_SETUP); rh->xfer_struct_sz = FIELD_GET(CR_XFER_STRUCT_SIZE, regval); rh->resp_struct_sz = FIELD_GET(CR_RESP_STRUCT_SIZE, regval); DBG(""xfer_struct_sz = %d, resp_struct_sz = %d"", rh->xfer_struct_sz, rh->resp_struct_sz); xfers_sz = rh->xfer_struct_sz * rh->xfer_entries; resps_sz = rh->resp_struct_sz * rh->xfer_entries; rh->xfer = dma_alloc_coherent(&hci->master.dev, xfers_sz, &rh->xfer_dma, GFP_KERNEL); rh->resp = dma_alloc_coherent(&hci->master.dev, resps_sz, &rh->resp_dma, GFP_KERNEL); rh->src_xfers = kmalloc_array(rh->xfer_entries, sizeof(*rh->src_xfers), GFP_KERNEL); ret = -ENOMEM; if (!rh->xfer || !rh->resp || !rh->src_xfers) goto err_out; rh_reg_write(CMD_RING_BASE_LO, lower_32_bits(rh->xfer_dma)); rh_reg_write(CMD_RING_BASE_HI, upper_32_bits(rh->xfer_dma)); rh_reg_write(RESP_RING_BASE_LO, lower_32_bits(rh->resp_dma)); rh_reg_write(RESP_RING_BASE_HI, upper_32_bits(rh->resp_dma)); regval = FIELD_PREP(CR_RING_SIZE, rh->xfer_entries); rh_reg_write(CR_SETUP, regval); rh_reg_write(INTR_STATUS_ENABLE, 0xffffffff); rh_reg_write(INTR_SIGNAL_ENABLE, INTR_IBI_READY | INTR_TRANSFER_COMPLETION | INTR_RING_OP | INTR_TRANSFER_ERR | INTR_WARN_INS_STOP_MODE | INTR_IBI_RING_FULL | INTR_TRANSFER_ABORT); if (i >= IBI_RINGS) goto ring_ready; regval = rh_reg_read(IBI_SETUP); rh->ibi_status_sz = FIELD_GET(IBI_STATUS_STRUCT_SIZE, regval); rh->ibi_status_entries = IBI_STATUS_RING_ENTRIES; rh->ibi_chunks_total = IBI_CHUNK_POOL_SIZE; rh->ibi_chunk_sz = dma_get_cache_alignment(); rh->ibi_chunk_sz *= IBI_CHUNK_CACHELINES; <S2SV_StartVul> BUG_ON(rh->ibi_chunk_sz > 256); <S2SV_EndVul> ibi_status_ring_sz = rh->ibi_status_sz * rh->ibi_status_entries; ibi_data_ring_sz = rh->ibi_chunk_sz * rh->ibi_chunks_total; rh->ibi_status = dma_alloc_coherent(&hci->master.dev, ibi_status_ring_sz, &rh->ibi_status_dma, GFP_KERNEL); rh->ibi_data = kmalloc(ibi_data_ring_sz, GFP_KERNEL); ret = -ENOMEM; if (!rh->ibi_status || !rh->ibi_data) goto err_out; rh->ibi_data_dma = dma_map_single(&hci->master.dev, rh->ibi_data, ibi_data_ring_sz, DMA_FROM_DEVICE); if (dma_mapping_error(&hci->master.dev, rh->ibi_data_dma)) { rh->ibi_data_dma = 0; ret = -ENOMEM; goto err_out; } rh_reg_write(IBI_STATUS_RING_BASE_LO, lower_32_bits(rh->ibi_status_dma)); rh_reg_write(IBI_STATUS_RING_BASE_HI, upper_32_bits(rh->ibi_status_dma)); rh_reg_write(IBI_DATA_RING_BASE_LO, lower_32_bits(rh->ibi_data_dma)); rh_reg_write(IBI_DATA_RING_BASE_HI, upper_32_bits(rh->ibi_data_dma)); regval = FIELD_PREP(IBI_STATUS_RING_SIZE, rh->ibi_status_entries) | FIELD_PREP(IBI_DATA_CHUNK_SIZE, ilog2(rh->ibi_chunk_sz) - 2) | FIELD_PREP(IBI_DATA_CHUNK_COUNT, rh->ibi_chunks_total); rh_reg_write(IBI_SETUP, regval); regval = rh_reg_read(INTR_SIGNAL_ENABLE); regval |= INTR_IBI_READY; rh_reg_write(INTR_SIGNAL_ENABLE, regval); ring_ready: rh_reg_write(RING_CONTROL, RING_CTRL_ENABLE | RING_CTRL_RUN_STOP); } return 0; err_out: hci_dma_cleanup(hci); return ret; }","- BUG_ON(rh->ibi_chunk_sz > 256);
+ if (rh->ibi_chunk_sz > 256) {
+ ret = -EINVAL;
+ goto err_out;
+ }","static int hci_dma_init(struct i3c_hci *hci) { struct hci_rings_data *rings; struct hci_rh_data *rh; u32 regval; unsigned int i, nr_rings, xfers_sz, resps_sz; unsigned int ibi_status_ring_sz, ibi_data_ring_sz; int ret; regval = rhs_reg_read(CONTROL); nr_rings = FIELD_GET(MAX_HEADER_COUNT_CAP, regval); dev_info(&hci->master.dev, ""%d DMA rings available\n"", nr_rings); if (unlikely(nr_rings > 8)) { dev_err(&hci->master.dev, ""number of rings should be <= 8\n""); nr_rings = 8; } if (nr_rings > XFER_RINGS) nr_rings = XFER_RINGS; rings = kzalloc(struct_size(rings, headers, nr_rings), GFP_KERNEL); if (!rings) return -ENOMEM; hci->io_data = rings; rings->total = nr_rings; regval = FIELD_PREP(MAX_HEADER_COUNT, rings->total); rhs_reg_write(CONTROL, regval); for (i = 0; i < rings->total; i++) { u32 offset = rhs_reg_read(RHn_OFFSET(i)); dev_info(&hci->master.dev, ""Ring %d at offset %#x\n"", i, offset); ret = -EINVAL; if (!offset) goto err_out; rh = &rings->headers[i]; rh->regs = hci->base_regs + offset; spin_lock_init(&rh->lock); init_completion(&rh->op_done); rh->xfer_entries = XFER_RING_ENTRIES; regval = rh_reg_read(CR_SETUP); rh->xfer_struct_sz = FIELD_GET(CR_XFER_STRUCT_SIZE, regval); rh->resp_struct_sz = FIELD_GET(CR_RESP_STRUCT_SIZE, regval); DBG(""xfer_struct_sz = %d, resp_struct_sz = %d"", rh->xfer_struct_sz, rh->resp_struct_sz); xfers_sz = rh->xfer_struct_sz * rh->xfer_entries; resps_sz = rh->resp_struct_sz * rh->xfer_entries; rh->xfer = dma_alloc_coherent(&hci->master.dev, xfers_sz, &rh->xfer_dma, GFP_KERNEL); rh->resp = dma_alloc_coherent(&hci->master.dev, resps_sz, &rh->resp_dma, GFP_KERNEL); rh->src_xfers = kmalloc_array(rh->xfer_entries, sizeof(*rh->src_xfers), GFP_KERNEL); ret = -ENOMEM; if (!rh->xfer || !rh->resp || !rh->src_xfers) goto err_out; rh_reg_write(CMD_RING_BASE_LO, lower_32_bits(rh->xfer_dma)); rh_reg_write(CMD_RING_BASE_HI, upper_32_bits(rh->xfer_dma)); rh_reg_write(RESP_RING_BASE_LO, lower_32_bits(rh->resp_dma)); rh_reg_write(RESP_RING_BASE_HI, upper_32_bits(rh->resp_dma)); regval = FIELD_PREP(CR_RING_SIZE, rh->xfer_entries); rh_reg_write(CR_SETUP, regval); rh_reg_write(INTR_STATUS_ENABLE, 0xffffffff); rh_reg_write(INTR_SIGNAL_ENABLE, INTR_IBI_READY | INTR_TRANSFER_COMPLETION | INTR_RING_OP | INTR_TRANSFER_ERR | INTR_WARN_INS_STOP_MODE | INTR_IBI_RING_FULL | INTR_TRANSFER_ABORT); if (i >= IBI_RINGS) goto ring_ready; regval = rh_reg_read(IBI_SETUP); rh->ibi_status_sz = FIELD_GET(IBI_STATUS_STRUCT_SIZE, regval); rh->ibi_status_entries = IBI_STATUS_RING_ENTRIES; rh->ibi_chunks_total = IBI_CHUNK_POOL_SIZE; rh->ibi_chunk_sz = dma_get_cache_alignment(); rh->ibi_chunk_sz *= IBI_CHUNK_CACHELINES; if (rh->ibi_chunk_sz > 256) { ret = -EINVAL; goto err_out; } ibi_status_ring_sz = rh->ibi_status_sz * rh->ibi_status_entries; ibi_data_ring_sz = rh->ibi_chunk_sz * rh->ibi_chunks_total; rh->ibi_status = dma_alloc_coherent(&hci->master.dev, ibi_status_ring_sz, &rh->ibi_status_dma, GFP_KERNEL); rh->ibi_data = kmalloc(ibi_data_ring_sz, GFP_KERNEL); ret = -ENOMEM; if (!rh->ibi_status || !rh->ibi_data) goto err_out; rh->ibi_data_dma = dma_map_single(&hci->master.dev, rh->ibi_data, ibi_data_ring_sz, DMA_FROM_DEVICE); if (dma_mapping_error(&hci->master.dev, rh->ibi_data_dma)) { rh->ibi_data_dma = 0; ret = -ENOMEM; goto err_out; } rh_reg_write(IBI_STATUS_RING_BASE_LO, lower_32_bits(rh->ibi_status_dma)); rh_reg_write(IBI_STATUS_RING_BASE_HI, upper_32_bits(rh->ibi_status_dma)); rh_reg_write(IBI_DATA_RING_BASE_LO, lower_32_bits(rh->ibi_data_dma)); rh_reg_write(IBI_DATA_RING_BASE_HI, upper_32_bits(rh->ibi_data_dma)); regval = FIELD_PREP(IBI_STATUS_RING_SIZE, rh->ibi_status_entries) | FIELD_PREP(IBI_DATA_CHUNK_SIZE, ilog2(rh->ibi_chunk_sz) - 2) | FIELD_PREP(IBI_DATA_CHUNK_COUNT, rh->ibi_chunks_total); rh_reg_write(IBI_SETUP, regval); regval = rh_reg_read(INTR_SIGNAL_ENABLE); regval |= INTR_IBI_READY; rh_reg_write(INTR_SIGNAL_ENABLE, regval); ring_ready: rh_reg_write(RING_CONTROL, RING_CTRL_ENABLE | RING_CTRL_RUN_STOP); } return 0; err_out: hci_dma_cleanup(hci); return ret; }"
302----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46772/bad/dcn315_resource.c----dcn315_populate_dml_pipes_from_context,"static int dcn315_populate_dml_pipes_from_context( struct dc *dc, struct dc_state *context, display_e2e_pipe_params_st *pipes, bool fast_validate) { int i, pipe_cnt, crb_idx, crb_pipes; struct resource_context *res_ctx = &context->res_ctx; struct pipe_ctx *pipe = NULL; const int max_usable_det = context->bw_ctx.dml.ip.config_return_buffer_size_in_kbytes - DCN3_15_MIN_COMPBUF_SIZE_KB; int remaining_det_segs = max_usable_det / DCN3_15_CRB_SEGMENT_SIZE_KB; bool pixel_rate_crb = allow_pixel_rate_crb(dc, context); DC_FP_START(); dcn31x_populate_dml_pipes_from_context(dc, context, pipes, fast_validate); DC_FP_END(); for (i = 0, pipe_cnt = 0, crb_pipes = 0; i < dc->res_pool->pipe_count; i++) { struct dc_crtc_timing *timing; if (!res_ctx->pipe_ctx[i].stream) continue; pipe = &res_ctx->pipe_ctx[i]; timing = &pipe->stream->timing; pipes[pipe_cnt].pipe.src.immediate_flip = true; pipes[pipe_cnt].pipe.src.unbounded_req_mode = false; pipes[pipe_cnt].pipe.dest.vfront_porch = timing->v_front_porch; pipes[pipe_cnt].pipe.src.dcc_rate = 3; pipes[pipe_cnt].dout.dsc_input_bpc = 0; DC_FP_START(); dcn31_zero_pipe_dcc_fraction(pipes, pipe_cnt); if (pixel_rate_crb && !pipe->top_pipe && !pipe->prev_odm_pipe) { int bpp = source_format_to_bpp(pipes[pipe_cnt].pipe.src.source_format); int approx_det_segs_required_for_pstate = dcn_get_approx_det_segs_required_for_pstate( &context->bw_ctx.dml.soc, timing->pix_clk_100hz, bpp, DCN3_15_CRB_SEGMENT_SIZE_KB); if (approx_det_segs_required_for_pstate <= 2 * DCN3_15_MAX_DET_SEGS) { bool split_required = approx_det_segs_required_for_pstate > DCN3_15_MAX_DET_SEGS; split_required = split_required || timing->pix_clk_100hz >= dcn_get_max_non_odm_pix_rate_100hz(&dc->dml.soc); split_required = split_required || (pipe->plane_state && pipe->plane_state->src_rect.width > 5120); if (approx_det_segs_required_for_pstate < 2) approx_det_segs_required_for_pstate = 2; if (split_required) approx_det_segs_required_for_pstate += approx_det_segs_required_for_pstate % 2; pipes[pipe_cnt].pipe.src.det_size_override = approx_det_segs_required_for_pstate; remaining_det_segs -= approx_det_segs_required_for_pstate; } else remaining_det_segs = -1; crb_pipes++; } DC_FP_END(); if (pipes[pipe_cnt].dout.dsc_enable) { switch (timing->display_color_depth) { case COLOR_DEPTH_888: pipes[pipe_cnt].dout.dsc_input_bpc = 8; break; case COLOR_DEPTH_101010: pipes[pipe_cnt].dout.dsc_input_bpc = 10; break; case COLOR_DEPTH_121212: pipes[pipe_cnt].dout.dsc_input_bpc = 12; break; default: ASSERT(0); break; } } pipe_cnt++; } if (pixel_rate_crb) { for (i = 0, pipe_cnt = 0, crb_idx = 0; i < dc->res_pool->pipe_count; i++) { pipe = &res_ctx->pipe_ctx[i]; if (!pipe->stream) continue; if (remaining_det_segs < 0) { pipes[pipe_cnt].pipe.src.det_size_override = 0; pipe_cnt++; continue; } if (!pipe->top_pipe && !pipe->prev_odm_pipe) { bool split_required = pipe->stream->timing.pix_clk_100hz >= dcn_get_max_non_odm_pix_rate_100hz(&dc->dml.soc) || (pipe->plane_state && pipe->plane_state->src_rect.width > 5120); <S2SV_StartVul> if (remaining_det_segs > MIN_RESERVED_DET_SEGS) <S2SV_EndVul> pipes[pipe_cnt].pipe.src.det_size_override += (remaining_det_segs - MIN_RESERVED_DET_SEGS) / crb_pipes + (crb_idx < (remaining_det_segs - MIN_RESERVED_DET_SEGS) % crb_pipes ? 1 : 0); if (pipes[pipe_cnt].pipe.src.det_size_override > 2 * DCN3_15_MAX_DET_SEGS) { remaining_det_segs += pipes[pipe_cnt].pipe.src.det_size_override - 2 * (DCN3_15_MAX_DET_SEGS); pipes[pipe_cnt].pipe.src.det_size_override = 2 * DCN3_15_MAX_DET_SEGS; } if (pipes[pipe_cnt].pipe.src.det_size_override > DCN3_15_MAX_DET_SEGS || split_required) { remaining_det_segs += pipes[pipe_cnt].pipe.src.det_size_override % 2; pipes[pipe_cnt].pipe.src.det_size_override -= pipes[pipe_cnt].pipe.src.det_size_override % 2; } pipes[pipe_cnt].pipe.src.det_size_override *= DCN3_15_CRB_SEGMENT_SIZE_KB; crb_idx++; } pipe_cnt++; } } if (pipe_cnt) context->bw_ctx.dml.ip.det_buffer_size_kbytes = (max_usable_det / DCN3_15_CRB_SEGMENT_SIZE_KB / pipe_cnt) * DCN3_15_CRB_SEGMENT_SIZE_KB; if (context->bw_ctx.dml.ip.det_buffer_size_kbytes > DCN3_15_MAX_DET_SIZE) context->bw_ctx.dml.ip.det_buffer_size_kbytes = DCN3_15_MAX_DET_SIZE; dc->config.enable_4to1MPC = false; if (pipe_cnt == 1 && pipe->plane_state && !dc->debug.disable_z9_mpc) { if (is_dual_plane(pipe->plane_state->format) && pipe->plane_state->src_rect.width <= 1920 && pipe->plane_state->src_rect.height <= 1080) { dc->config.enable_4to1MPC = true; context->bw_ctx.dml.ip.det_buffer_size_kbytes = (max_usable_det / DCN3_15_CRB_SEGMENT_SIZE_KB / 4) * DCN3_15_CRB_SEGMENT_SIZE_KB; } else if (!is_dual_plane(pipe->plane_state->format) && pipe->plane_state->src_rect.width <= 5120 && pipe->stream->timing.pix_clk_100hz < dcn_get_max_non_odm_pix_rate_100hz(&dc->dml.soc)) { context->bw_ctx.dml.ip.det_buffer_size_kbytes = 192; pipes[0].pipe.src.unbounded_req_mode = true; } } return pipe_cnt; }","- if (remaining_det_segs > MIN_RESERVED_DET_SEGS)
+ if (remaining_det_segs > MIN_RESERVED_DET_SEGS && crb_pipes != 0)","static int dcn315_populate_dml_pipes_from_context( struct dc *dc, struct dc_state *context, display_e2e_pipe_params_st *pipes, bool fast_validate) { int i, pipe_cnt, crb_idx, crb_pipes; struct resource_context *res_ctx = &context->res_ctx; struct pipe_ctx *pipe = NULL; const int max_usable_det = context->bw_ctx.dml.ip.config_return_buffer_size_in_kbytes - DCN3_15_MIN_COMPBUF_SIZE_KB; int remaining_det_segs = max_usable_det / DCN3_15_CRB_SEGMENT_SIZE_KB; bool pixel_rate_crb = allow_pixel_rate_crb(dc, context); DC_FP_START(); dcn31x_populate_dml_pipes_from_context(dc, context, pipes, fast_validate); DC_FP_END(); for (i = 0, pipe_cnt = 0, crb_pipes = 0; i < dc->res_pool->pipe_count; i++) { struct dc_crtc_timing *timing; if (!res_ctx->pipe_ctx[i].stream) continue; pipe = &res_ctx->pipe_ctx[i]; timing = &pipe->stream->timing; pipes[pipe_cnt].pipe.src.immediate_flip = true; pipes[pipe_cnt].pipe.src.unbounded_req_mode = false; pipes[pipe_cnt].pipe.dest.vfront_porch = timing->v_front_porch; pipes[pipe_cnt].pipe.src.dcc_rate = 3; pipes[pipe_cnt].dout.dsc_input_bpc = 0; DC_FP_START(); dcn31_zero_pipe_dcc_fraction(pipes, pipe_cnt); if (pixel_rate_crb && !pipe->top_pipe && !pipe->prev_odm_pipe) { int bpp = source_format_to_bpp(pipes[pipe_cnt].pipe.src.source_format); int approx_det_segs_required_for_pstate = dcn_get_approx_det_segs_required_for_pstate( &context->bw_ctx.dml.soc, timing->pix_clk_100hz, bpp, DCN3_15_CRB_SEGMENT_SIZE_KB); if (approx_det_segs_required_for_pstate <= 2 * DCN3_15_MAX_DET_SEGS) { bool split_required = approx_det_segs_required_for_pstate > DCN3_15_MAX_DET_SEGS; split_required = split_required || timing->pix_clk_100hz >= dcn_get_max_non_odm_pix_rate_100hz(&dc->dml.soc); split_required = split_required || (pipe->plane_state && pipe->plane_state->src_rect.width > 5120); if (approx_det_segs_required_for_pstate < 2) approx_det_segs_required_for_pstate = 2; if (split_required) approx_det_segs_required_for_pstate += approx_det_segs_required_for_pstate % 2; pipes[pipe_cnt].pipe.src.det_size_override = approx_det_segs_required_for_pstate; remaining_det_segs -= approx_det_segs_required_for_pstate; } else remaining_det_segs = -1; crb_pipes++; } DC_FP_END(); if (pipes[pipe_cnt].dout.dsc_enable) { switch (timing->display_color_depth) { case COLOR_DEPTH_888: pipes[pipe_cnt].dout.dsc_input_bpc = 8; break; case COLOR_DEPTH_101010: pipes[pipe_cnt].dout.dsc_input_bpc = 10; break; case COLOR_DEPTH_121212: pipes[pipe_cnt].dout.dsc_input_bpc = 12; break; default: ASSERT(0); break; } } pipe_cnt++; } if (pixel_rate_crb) { for (i = 0, pipe_cnt = 0, crb_idx = 0; i < dc->res_pool->pipe_count; i++) { pipe = &res_ctx->pipe_ctx[i]; if (!pipe->stream) continue; if (remaining_det_segs < 0) { pipes[pipe_cnt].pipe.src.det_size_override = 0; pipe_cnt++; continue; } if (!pipe->top_pipe && !pipe->prev_odm_pipe) { bool split_required = pipe->stream->timing.pix_clk_100hz >= dcn_get_max_non_odm_pix_rate_100hz(&dc->dml.soc) || (pipe->plane_state && pipe->plane_state->src_rect.width > 5120); if (remaining_det_segs > MIN_RESERVED_DET_SEGS && crb_pipes != 0) pipes[pipe_cnt].pipe.src.det_size_override += (remaining_det_segs - MIN_RESERVED_DET_SEGS) / crb_pipes + (crb_idx < (remaining_det_segs - MIN_RESERVED_DET_SEGS) % crb_pipes ? 1 : 0); if (pipes[pipe_cnt].pipe.src.det_size_override > 2 * DCN3_15_MAX_DET_SEGS) { remaining_det_segs += pipes[pipe_cnt].pipe.src.det_size_override - 2 * (DCN3_15_MAX_DET_SEGS); pipes[pipe_cnt].pipe.src.det_size_override = 2 * DCN3_15_MAX_DET_SEGS; } if (pipes[pipe_cnt].pipe.src.det_size_override > DCN3_15_MAX_DET_SEGS || split_required) { remaining_det_segs += pipes[pipe_cnt].pipe.src.det_size_override % 2; pipes[pipe_cnt].pipe.src.det_size_override -= pipes[pipe_cnt].pipe.src.det_size_override % 2; } pipes[pipe_cnt].pipe.src.det_size_override *= DCN3_15_CRB_SEGMENT_SIZE_KB; crb_idx++; } pipe_cnt++; } } if (pipe_cnt) context->bw_ctx.dml.ip.det_buffer_size_kbytes = (max_usable_det / DCN3_15_CRB_SEGMENT_SIZE_KB / pipe_cnt) * DCN3_15_CRB_SEGMENT_SIZE_KB; if (context->bw_ctx.dml.ip.det_buffer_size_kbytes > DCN3_15_MAX_DET_SIZE) context->bw_ctx.dml.ip.det_buffer_size_kbytes = DCN3_15_MAX_DET_SIZE; dc->config.enable_4to1MPC = false; if (pipe_cnt == 1 && pipe->plane_state && !dc->debug.disable_z9_mpc) { if (is_dual_plane(pipe->plane_state->format) && pipe->plane_state->src_rect.width <= 1920 && pipe->plane_state->src_rect.height <= 1080) { dc->config.enable_4to1MPC = true; context->bw_ctx.dml.ip.det_buffer_size_kbytes = (max_usable_det / DCN3_15_CRB_SEGMENT_SIZE_KB / 4) * DCN3_15_CRB_SEGMENT_SIZE_KB; } else if (!is_dual_plane(pipe->plane_state->format) && pipe->plane_state->src_rect.width <= 5120 && pipe->stream->timing.pix_clk_100hz < dcn_get_max_non_odm_pix_rate_100hz(&dc->dml.soc)) { context->bw_ctx.dml.ip.det_buffer_size_kbytes = 192; pipes[0].pipe.src.unbounded_req_mode = true; } } return pipe_cnt; }"
857----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50116/bad/page.c----nilfs_forget_buffer,"void nilfs_forget_buffer(struct buffer_head *bh) { struct page *page = bh->b_page; const unsigned long clear_bits = (BIT(BH_Uptodate) | BIT(BH_Dirty) | BIT(BH_Mapped) | BIT(BH_Async_Write) | BIT(BH_NILFS_Volatile) | <S2SV_StartVul> BIT(BH_NILFS_Checked) | BIT(BH_NILFS_Redirected)); <S2SV_EndVul> lock_buffer(bh); set_mask_bits(&bh->b_state, clear_bits, 0); if (nilfs_page_buffers_clean(page)) __nilfs_clear_page_dirty(page); bh->b_blocknr = -1; ClearPageUptodate(page); ClearPageMappedToDisk(page); unlock_buffer(bh); brelse(bh); }","- BIT(BH_NILFS_Checked) | BIT(BH_NILFS_Redirected));
+ BIT(BH_NILFS_Checked) | BIT(BH_NILFS_Redirected) |
+ BIT(BH_Delay));","void nilfs_forget_buffer(struct buffer_head *bh) { struct page *page = bh->b_page; const unsigned long clear_bits = (BIT(BH_Uptodate) | BIT(BH_Dirty) | BIT(BH_Mapped) | BIT(BH_Async_Write) | BIT(BH_NILFS_Volatile) | BIT(BH_NILFS_Checked) | BIT(BH_NILFS_Redirected) | BIT(BH_Delay)); lock_buffer(bh); set_mask_bits(&bh->b_state, clear_bits, 0); if (nilfs_page_buffers_clean(page)) __nilfs_clear_page_dirty(page); bh->b_blocknr = -1; ClearPageUptodate(page); ClearPageMappedToDisk(page); unlock_buffer(bh); brelse(bh); }"
139----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44998/bad/idt77252.c----dequeue_rx,"dequeue_rx(struct idt77252_dev *card, struct rsq_entry *rsqe) { struct atm_vcc *vcc; struct sk_buff *skb; struct rx_pool *rpp; struct vc_map *vc; u32 header, vpi, vci; u32 stat; int i; stat = le32_to_cpu(rsqe->word_4); if (stat & SAR_RSQE_IDLE) { RXPRINTK(""%s: message about inactive connection.\n"", card->name); return; } skb = sb_pool_skb(card, le32_to_cpu(rsqe->word_2)); if (skb == NULL) { printk(""%s: NULL skb in %s, rsqe: %08x %08x %08x %08x\n"", card->name, __func__, le32_to_cpu(rsqe->word_1), le32_to_cpu(rsqe->word_2), le32_to_cpu(rsqe->word_3), le32_to_cpu(rsqe->word_4)); return; } header = le32_to_cpu(rsqe->word_1); vpi = (header >> 16) & 0x00ff; vci = (header >> 0) & 0xffff; RXPRINTK(""%s: SDU for %d.%d received in buffer 0x%p (data 0x%p).\n"", card->name, vpi, vci, skb, skb->data); if ((vpi >= (1 << card->vpibits)) || (vci != (vci & card->vcimask))) { printk(""%s: SDU received for out-of-range vc %u.%u\n"", card->name, vpi, vci); recycle_rx_skb(card, skb); return; } vc = card->vcs[VPCI2VC(card, vpi, vci)]; if (!vc || !test_bit(VCF_RX, &vc->flags)) { printk(""%s: SDU received on non RX vc %u.%u\n"", card->name, vpi, vci); recycle_rx_skb(card, skb); return; } vcc = vc->rx_vcc; dma_sync_single_for_cpu(&card->pcidev->dev, IDT77252_PRV_PADDR(skb), skb_end_pointer(skb) - skb->data, DMA_FROM_DEVICE); if ((vcc->qos.aal == ATM_AAL0) || (vcc->qos.aal == ATM_AAL34)) { struct sk_buff *sb; unsigned char *cell; u32 aal0; cell = skb->data; for (i = (stat & SAR_RSQE_CELLCNT); i; i--) { if ((sb = dev_alloc_skb(64)) == NULL) { printk(""%s: Can't allocate buffers for aal0.\n"", card->name); atomic_add(i, &vcc->stats->rx_drop); break; } if (!atm_charge(vcc, sb->truesize)) { RXPRINTK(""%s: atm_charge() dropped aal0 packets.\n"", card->name); atomic_add(i - 1, &vcc->stats->rx_drop); dev_kfree_skb(sb); break; } aal0 = (vpi << ATM_HDR_VPI_SHIFT) | (vci << ATM_HDR_VCI_SHIFT); aal0 |= (stat & SAR_RSQE_EPDU) ? 0x00000002 : 0; aal0 |= (stat & SAR_RSQE_CLP) ? 0x00000001 : 0; *((u32 *) sb->data) = aal0; skb_put(sb, sizeof(u32)); skb_put_data(sb, cell, ATM_CELL_PAYLOAD); ATM_SKB(sb)->vcc = vcc; __net_timestamp(sb); vcc->push(vcc, sb); atomic_inc(&vcc->stats->rx); cell += ATM_CELL_PAYLOAD; } recycle_rx_skb(card, skb); return; } if (vcc->qos.aal != ATM_AAL5) { printk(""%s: Unexpected AAL type in dequeue_rx(): %d.\n"", card->name, vcc->qos.aal); recycle_rx_skb(card, skb); return; } skb->len = (stat & SAR_RSQE_CELLCNT) * ATM_CELL_PAYLOAD; rpp = &vc->rcv.rx_pool; __skb_queue_tail(&rpp->queue, skb); rpp->len += skb->len; if (stat & SAR_RSQE_EPDU) { unsigned char *l1l2; <S2SV_StartVul> unsigned int len; <S2SV_EndVul> l1l2 = (unsigned char *) ((unsigned long) skb->data + skb->len - 6); len = (l1l2[0] << 8) | l1l2[1]; len = len ? len : 0x10000; RXPRINTK(""%s: PDU has %d bytes.\n"", card->name, len); if ((len + 8 > rpp->len) || (len + (47 + 8) < rpp->len)) { RXPRINTK(""%s: AAL5 PDU size mismatch: %d != %d. "" ""(CDC: %08x)\n"", card->name, len, rpp->len, readl(SAR_REG_CDC)); recycle_rx_pool_skb(card, rpp); atomic_inc(&vcc->stats->rx_err); return; } if (stat & SAR_RSQE_CRC) { RXPRINTK(""%s: AAL5 CRC error.\n"", card->name); recycle_rx_pool_skb(card, rpp); atomic_inc(&vcc->stats->rx_err); return; } if (skb_queue_len(&rpp->queue) > 1) { struct sk_buff *sb; skb = dev_alloc_skb(rpp->len); if (!skb) { RXPRINTK(""%s: Can't alloc RX skb.\n"", card->name); recycle_rx_pool_skb(card, rpp); atomic_inc(&vcc->stats->rx_err); return; } if (!atm_charge(vcc, skb->truesize)) { recycle_rx_pool_skb(card, rpp); dev_kfree_skb(skb); return; } skb_queue_walk(&rpp->queue, sb) skb_put_data(skb, sb->data, sb->len); recycle_rx_pool_skb(card, rpp); skb_trim(skb, len); ATM_SKB(skb)->vcc = vcc; __net_timestamp(skb); vcc->push(vcc, skb); atomic_inc(&vcc->stats->rx); return; } flush_rx_pool(card, rpp); if (!atm_charge(vcc, skb->truesize)) { recycle_rx_skb(card, skb); return; } dma_unmap_single(&card->pcidev->dev, IDT77252_PRV_PADDR(skb), skb_end_pointer(skb) - skb->data, DMA_FROM_DEVICE); sb_pool_remove(card, skb); skb_trim(skb, len); ATM_SKB(skb)->vcc = vcc; __net_timestamp(skb); vcc->push(vcc, skb); atomic_inc(&vcc->stats->rx); <S2SV_StartVul> if (skb->truesize > SAR_FB_SIZE_3) <S2SV_EndVul> add_rx_skb(card, 3, SAR_FB_SIZE_3, 1); <S2SV_StartVul> else if (skb->truesize > SAR_FB_SIZE_2) <S2SV_EndVul> add_rx_skb(card, 2, SAR_FB_SIZE_2, 1); <S2SV_StartVul> else if (skb->truesize > SAR_FB_SIZE_1) <S2SV_EndVul> add_rx_skb(card, 1, SAR_FB_SIZE_1, 1); else add_rx_skb(card, 0, SAR_FB_SIZE_0, 1); return; } }","- unsigned int len;
- if (skb->truesize > SAR_FB_SIZE_3)
- else if (skb->truesize > SAR_FB_SIZE_2)
- else if (skb->truesize > SAR_FB_SIZE_1)
+ unsigned int len, truesize;
+ truesize = skb->truesize;
+ if (truesize > SAR_FB_SIZE_3)
+ else if (truesize > SAR_FB_SIZE_2)
+ else if (truesize > SAR_FB_SIZE_1)","dequeue_rx(struct idt77252_dev *card, struct rsq_entry *rsqe) { struct atm_vcc *vcc; struct sk_buff *skb; struct rx_pool *rpp; struct vc_map *vc; u32 header, vpi, vci; u32 stat; int i; stat = le32_to_cpu(rsqe->word_4); if (stat & SAR_RSQE_IDLE) { RXPRINTK(""%s: message about inactive connection.\n"", card->name); return; } skb = sb_pool_skb(card, le32_to_cpu(rsqe->word_2)); if (skb == NULL) { printk(""%s: NULL skb in %s, rsqe: %08x %08x %08x %08x\n"", card->name, __func__, le32_to_cpu(rsqe->word_1), le32_to_cpu(rsqe->word_2), le32_to_cpu(rsqe->word_3), le32_to_cpu(rsqe->word_4)); return; } header = le32_to_cpu(rsqe->word_1); vpi = (header >> 16) & 0x00ff; vci = (header >> 0) & 0xffff; RXPRINTK(""%s: SDU for %d.%d received in buffer 0x%p (data 0x%p).\n"", card->name, vpi, vci, skb, skb->data); if ((vpi >= (1 << card->vpibits)) || (vci != (vci & card->vcimask))) { printk(""%s: SDU received for out-of-range vc %u.%u\n"", card->name, vpi, vci); recycle_rx_skb(card, skb); return; } vc = card->vcs[VPCI2VC(card, vpi, vci)]; if (!vc || !test_bit(VCF_RX, &vc->flags)) { printk(""%s: SDU received on non RX vc %u.%u\n"", card->name, vpi, vci); recycle_rx_skb(card, skb); return; } vcc = vc->rx_vcc; dma_sync_single_for_cpu(&card->pcidev->dev, IDT77252_PRV_PADDR(skb), skb_end_pointer(skb) - skb->data, DMA_FROM_DEVICE); if ((vcc->qos.aal == ATM_AAL0) || (vcc->qos.aal == ATM_AAL34)) { struct sk_buff *sb; unsigned char *cell; u32 aal0; cell = skb->data; for (i = (stat & SAR_RSQE_CELLCNT); i; i--) { if ((sb = dev_alloc_skb(64)) == NULL) { printk(""%s: Can't allocate buffers for aal0.\n"", card->name); atomic_add(i, &vcc->stats->rx_drop); break; } if (!atm_charge(vcc, sb->truesize)) { RXPRINTK(""%s: atm_charge() dropped aal0 packets.\n"", card->name); atomic_add(i - 1, &vcc->stats->rx_drop); dev_kfree_skb(sb); break; } aal0 = (vpi << ATM_HDR_VPI_SHIFT) | (vci << ATM_HDR_VCI_SHIFT); aal0 |= (stat & SAR_RSQE_EPDU) ? 0x00000002 : 0; aal0 |= (stat & SAR_RSQE_CLP) ? 0x00000001 : 0; *((u32 *) sb->data) = aal0; skb_put(sb, sizeof(u32)); skb_put_data(sb, cell, ATM_CELL_PAYLOAD); ATM_SKB(sb)->vcc = vcc; __net_timestamp(sb); vcc->push(vcc, sb); atomic_inc(&vcc->stats->rx); cell += ATM_CELL_PAYLOAD; } recycle_rx_skb(card, skb); return; } if (vcc->qos.aal != ATM_AAL5) { printk(""%s: Unexpected AAL type in dequeue_rx(): %d.\n"", card->name, vcc->qos.aal); recycle_rx_skb(card, skb); return; } skb->len = (stat & SAR_RSQE_CELLCNT) * ATM_CELL_PAYLOAD; rpp = &vc->rcv.rx_pool; __skb_queue_tail(&rpp->queue, skb); rpp->len += skb->len; if (stat & SAR_RSQE_EPDU) { unsigned int len, truesize; unsigned char *l1l2; l1l2 = (unsigned char *) ((unsigned long) skb->data + skb->len - 6); len = (l1l2[0] << 8) | l1l2[1]; len = len ? len : 0x10000; RXPRINTK(""%s: PDU has %d bytes.\n"", card->name, len); if ((len + 8 > rpp->len) || (len + (47 + 8) < rpp->len)) { RXPRINTK(""%s: AAL5 PDU size mismatch: %d != %d. "" ""(CDC: %08x)\n"", card->name, len, rpp->len, readl(SAR_REG_CDC)); recycle_rx_pool_skb(card, rpp); atomic_inc(&vcc->stats->rx_err); return; } if (stat & SAR_RSQE_CRC) { RXPRINTK(""%s: AAL5 CRC error.\n"", card->name); recycle_rx_pool_skb(card, rpp); atomic_inc(&vcc->stats->rx_err); return; } if (skb_queue_len(&rpp->queue) > 1) { struct sk_buff *sb; skb = dev_alloc_skb(rpp->len); if (!skb) { RXPRINTK(""%s: Can't alloc RX skb.\n"", card->name); recycle_rx_pool_skb(card, rpp); atomic_inc(&vcc->stats->rx_err); return; } if (!atm_charge(vcc, skb->truesize)) { recycle_rx_pool_skb(card, rpp); dev_kfree_skb(skb); return; } skb_queue_walk(&rpp->queue, sb) skb_put_data(skb, sb->data, sb->len); recycle_rx_pool_skb(card, rpp); skb_trim(skb, len); ATM_SKB(skb)->vcc = vcc; __net_timestamp(skb); vcc->push(vcc, skb); atomic_inc(&vcc->stats->rx); return; } flush_rx_pool(card, rpp); if (!atm_charge(vcc, skb->truesize)) { recycle_rx_skb(card, skb); return; } dma_unmap_single(&card->pcidev->dev, IDT77252_PRV_PADDR(skb), skb_end_pointer(skb) - skb->data, DMA_FROM_DEVICE); sb_pool_remove(card, skb); skb_trim(skb, len); ATM_SKB(skb)->vcc = vcc; __net_timestamp(skb); truesize = skb->truesize; vcc->push(vcc, skb); atomic_inc(&vcc->stats->rx); if (truesize > SAR_FB_SIZE_3) add_rx_skb(card, 3, SAR_FB_SIZE_3, 1); else if (truesize > SAR_FB_SIZE_2) add_rx_skb(card, 2, SAR_FB_SIZE_2, 1); else if (truesize > SAR_FB_SIZE_1) add_rx_skb(card, 1, SAR_FB_SIZE_1, 1); else add_rx_skb(card, 0, SAR_FB_SIZE_0, 1); return; } }"
985----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50225/bad/bio.c----btrfs_bio_init,"void btrfs_bio_init(struct btrfs_bio *bbio, struct btrfs_fs_info *fs_info, btrfs_bio_end_io_t end_io, void *private) { memset(bbio, 0, offsetof(struct btrfs_bio, bio)); bbio->fs_info = fs_info; bbio->end_io = end_io; bbio->private = private; atomic_set(&bbio->pending_ios, 1); <S2SV_StartVul> } <S2SV_EndVul>","- }
+ WRITE_ONCE(bbio->status, BLK_STS_OK);
+ }","void btrfs_bio_init(struct btrfs_bio *bbio, struct btrfs_fs_info *fs_info, btrfs_bio_end_io_t end_io, void *private) { memset(bbio, 0, offsetof(struct btrfs_bio, bio)); bbio->fs_info = fs_info; bbio->end_io = end_io; bbio->private = private; atomic_set(&bbio->pending_ios, 1); WRITE_ONCE(bbio->status, BLK_STS_OK); }"
315----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46781/bad/recovery.c----nilfs_salvage_orphan_logs,"int nilfs_salvage_orphan_logs(struct the_nilfs *nilfs, struct super_block *sb, struct nilfs_recovery_info *ri) { struct nilfs_root *root; int err; if (ri->ri_lsegs_start == 0 || ri->ri_lsegs_end == 0) return 0; err = nilfs_attach_checkpoint(sb, ri->ri_cno, true, &root); if (unlikely(err)) { nilfs_err(sb, ""error %d loading the latest checkpoint"", err); return err; } err = nilfs_do_roll_forward(nilfs, sb, root, ri); if (unlikely(err)) goto failed; if (ri->ri_need_recovery == NILFS_RECOVERY_ROLLFORWARD_DONE) { err = nilfs_prepare_segment_for_recovery(nilfs, sb, ri); if (unlikely(err)) { nilfs_err(sb, ""error %d preparing segment for recovery"", err); goto failed; } err = nilfs_attach_log_writer(sb, root); if (unlikely(err)) goto failed; set_nilfs_discontinued(nilfs); err = nilfs_construct_segment(sb); nilfs_detach_log_writer(sb); if (unlikely(err)) { nilfs_err(sb, ""error %d writing segment for recovery"", err); <S2SV_StartVul> goto failed; <S2SV_EndVul> } nilfs_finish_roll_forward(nilfs, ri); } <S2SV_StartVul> failed: <S2SV_EndVul> nilfs_put_root(root); return err; }","- goto failed;
- failed:
+ goto put_root;
+ }
+ }
+ put_root:
+ failed:
+ nilfs_abort_roll_forward(nilfs);
+ goto put_root;
+ }","int nilfs_salvage_orphan_logs(struct the_nilfs *nilfs, struct super_block *sb, struct nilfs_recovery_info *ri) { struct nilfs_root *root; int err; if (ri->ri_lsegs_start == 0 || ri->ri_lsegs_end == 0) return 0; err = nilfs_attach_checkpoint(sb, ri->ri_cno, true, &root); if (unlikely(err)) { nilfs_err(sb, ""error %d loading the latest checkpoint"", err); return err; } err = nilfs_do_roll_forward(nilfs, sb, root, ri); if (unlikely(err)) goto failed; if (ri->ri_need_recovery == NILFS_RECOVERY_ROLLFORWARD_DONE) { err = nilfs_prepare_segment_for_recovery(nilfs, sb, ri); if (unlikely(err)) { nilfs_err(sb, ""error %d preparing segment for recovery"", err); goto failed; } err = nilfs_attach_log_writer(sb, root); if (unlikely(err)) goto failed; set_nilfs_discontinued(nilfs); err = nilfs_construct_segment(sb); nilfs_detach_log_writer(sb); if (unlikely(err)) { nilfs_err(sb, ""error %d writing segment for recovery"", err); goto put_root; } nilfs_finish_roll_forward(nilfs, ri); } put_root: nilfs_put_root(root); return err; failed: nilfs_abort_roll_forward(nilfs); goto put_root; }"
28----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-43902/bad/amdgpu_dm.c----create_eml_sink,"static void create_eml_sink(struct amdgpu_dm_connector *aconnector) { struct dc_sink_init_data init_params = { .link = aconnector->dc_link, .sink_signal = SIGNAL_TYPE_VIRTUAL }; struct edid *edid; if (!aconnector->base.edid_blob_ptr) { DRM_ERROR(""No EDID firmware found on connector: %s ,forcing to OFF!\n"", aconnector->base.name); aconnector->base.force = DRM_FORCE_OFF; aconnector->base.override_edid = false; return; } edid = (struct edid *) aconnector->base.edid_blob_ptr->data; aconnector->edid = edid; aconnector->dc_em_sink = dc_link_add_remote_sink( aconnector->dc_link, (uint8_t *)edid, (edid->extensions + 1) * EDID_LENGTH, &init_params); if (aconnector->base.force == DRM_FORCE_ON) { aconnector->dc_sink = aconnector->dc_link->local_sink ? aconnector->dc_link->local_sink : aconnector->dc_em_sink; <S2SV_StartVul> dc_sink_retain(aconnector->dc_sink); <S2SV_EndVul> } }","- dc_sink_retain(aconnector->dc_sink);
+ if (aconnector->dc_sink)
+ dc_sink_retain(aconnector->dc_sink);","static void create_eml_sink(struct amdgpu_dm_connector *aconnector) { struct dc_sink_init_data init_params = { .link = aconnector->dc_link, .sink_signal = SIGNAL_TYPE_VIRTUAL }; struct edid *edid; if (!aconnector->base.edid_blob_ptr) { DRM_ERROR(""No EDID firmware found on connector: %s ,forcing to OFF!\n"", aconnector->base.name); aconnector->base.force = DRM_FORCE_OFF; aconnector->base.override_edid = false; return; } edid = (struct edid *) aconnector->base.edid_blob_ptr->data; aconnector->edid = edid; aconnector->dc_em_sink = dc_link_add_remote_sink( aconnector->dc_link, (uint8_t *)edid, (edid->extensions + 1) * EDID_LENGTH, &init_params); if (aconnector->base.force == DRM_FORCE_ON) { aconnector->dc_sink = aconnector->dc_link->local_sink ? aconnector->dc_link->local_sink : aconnector->dc_em_sink; if (aconnector->dc_sink) dc_sink_retain(aconnector->dc_sink); } }"
1181----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53166/bad/bfq-iosched.c----bfq_limit_depth,"static void bfq_limit_depth(blk_opf_t opf, struct blk_mq_alloc_data *data) { struct bfq_data *bfqd = data->q->elevator->elevator_data; struct bfq_io_cq *bic = bfq_bic_lookup(data->q); int depth; unsigned limit = data->q->nr_requests; unsigned int act_idx; if (op_is_sync(opf) && !op_is_write(opf)) { depth = 0; } else { depth = bfqd->word_depths[!!bfqd->wr_busy_queues][op_is_sync(opf)]; limit = (limit * depth) >> bfqd->full_depth_shift; } for (act_idx = 0; bic && act_idx < bfqd->num_actuators; act_idx++) { <S2SV_StartVul> struct bfq_queue *bfqq = <S2SV_EndVul> <S2SV_StartVul> bic_to_bfqq(bic, op_is_sync(opf), act_idx); <S2SV_EndVul> <S2SV_StartVul> if (bfqq && bfqq_request_over_limit(bfqq, limit)) { <S2SV_EndVul> depth = 1; break; } } bfq_log(bfqd, ""[%s] wr_busy %d sync %d depth %u"", __func__, bfqd->wr_busy_queues, op_is_sync(opf), depth); if (depth) data->shallow_depth = depth; }","- struct bfq_queue *bfqq =
- bic_to_bfqq(bic, op_is_sync(opf), act_idx);
- if (bfqq && bfqq_request_over_limit(bfqq, limit)) {
+ if (!bic_to_bfqq(bic, op_is_sync(opf), act_idx))
+ continue;
+ if (bfqq_request_over_limit(bfqd, bic, opf, act_idx, limit)) {","static void bfq_limit_depth(blk_opf_t opf, struct blk_mq_alloc_data *data) { struct bfq_data *bfqd = data->q->elevator->elevator_data; struct bfq_io_cq *bic = bfq_bic_lookup(data->q); int depth; unsigned limit = data->q->nr_requests; unsigned int act_idx; if (op_is_sync(opf) && !op_is_write(opf)) { depth = 0; } else { depth = bfqd->word_depths[!!bfqd->wr_busy_queues][op_is_sync(opf)]; limit = (limit * depth) >> bfqd->full_depth_shift; } for (act_idx = 0; bic && act_idx < bfqd->num_actuators; act_idx++) { if (!bic_to_bfqq(bic, op_is_sync(opf), act_idx)) continue; if (bfqq_request_over_limit(bfqd, bic, opf, act_idx, limit)) { depth = 1; break; } } bfq_log(bfqd, ""[%s] wr_busy %d sync %d depth %u"", __func__, bfqd->wr_busy_queues, op_is_sync(opf), depth); if (depth) data->shallow_depth = depth; }"
274----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46757/bad/nct6775-core.c----store_temp_offset,"store_temp_offset(struct device *dev, struct device_attribute *attr, const char *buf, size_t count) { struct nct6775_data *data = dev_get_drvdata(dev); struct sensor_device_attribute *sattr = to_sensor_dev_attr(attr); int nr = sattr->index; long val; int err; err = kstrtol(buf, 10, &val); if (err < 0) return err; <S2SV_StartVul> val = clamp_val(DIV_ROUND_CLOSEST(val, 1000), -128, 127); <S2SV_EndVul> mutex_lock(&data->update_lock); data->temp_offset[nr] = val; err = nct6775_write_value(data, data->REG_TEMP_OFFSET[nr], val); mutex_unlock(&data->update_lock); return err ? : count; }","- val = clamp_val(DIV_ROUND_CLOSEST(val, 1000), -128, 127);
+ val = DIV_ROUND_CLOSEST(clamp_val(val, -128000, 127000), 1000);","store_temp_offset(struct device *dev, struct device_attribute *attr, const char *buf, size_t count) { struct nct6775_data *data = dev_get_drvdata(dev); struct sensor_device_attribute *sattr = to_sensor_dev_attr(attr); int nr = sattr->index; long val; int err; err = kstrtol(buf, 10, &val); if (err < 0) return err; val = DIV_ROUND_CLOSEST(clamp_val(val, -128000, 127000), 1000); mutex_lock(&data->update_lock); data->temp_offset[nr] = val; err = nct6775_write_value(data, data->REG_TEMP_OFFSET[nr], val); mutex_unlock(&data->update_lock); return err ? : count; }"
962----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50204/bad/namespace.c----*copy_mnt_ns,"struct mnt_namespace *copy_mnt_ns(unsigned long flags, struct mnt_namespace *ns, struct user_namespace *user_ns, struct fs_struct *new_fs) { struct mnt_namespace *new_ns; struct vfsmount *rootmnt = NULL, *pwdmnt = NULL; struct mount *p, *q; struct mount *old; struct mount *new; int copy_flags; BUG_ON(!ns); if (likely(!(flags & CLONE_NEWNS))) { get_mnt_ns(ns); return ns; } old = ns->root; new_ns = alloc_mnt_ns(user_ns, false); if (IS_ERR(new_ns)) return new_ns; namespace_lock(); copy_flags = CL_COPY_UNBINDABLE | CL_EXPIRE; if (user_ns != ns->user_ns) copy_flags |= CL_SHARED_TO_SLAVE; new = copy_tree(old, old->mnt.mnt_root, copy_flags); if (IS_ERR(new)) { namespace_unlock(); <S2SV_StartVul> free_mnt_ns(new_ns); <S2SV_EndVul> return ERR_CAST(new); } if (user_ns != ns->user_ns) { lock_mount_hash(); lock_mnt_tree(new); unlock_mount_hash(); } new_ns->root = new; p = old; q = new; while (p) { mnt_add_to_ns(new_ns, q); new_ns->nr_mounts++; if (new_fs) { if (&p->mnt == new_fs->root.mnt) { new_fs->root.mnt = mntget(&q->mnt); rootmnt = &p->mnt; } if (&p->mnt == new_fs->pwd.mnt) { new_fs->pwd.mnt = mntget(&q->mnt); pwdmnt = &p->mnt; } } p = next_mnt(p, old); q = next_mnt(q, new); if (!q) break; while (p->mnt.mnt_root != q->mnt.mnt_root) p = next_mnt(skip_mnt_tree(p), old); } mnt_ns_tree_add(new_ns); namespace_unlock(); if (rootmnt) mntput(rootmnt); if (pwdmnt) mntput(pwdmnt); return new_ns; }","- free_mnt_ns(new_ns);
+ ns_free_inum(&new_ns->ns);
+ dec_mnt_namespaces(new_ns->ucounts);
+ mnt_ns_release(new_ns);","struct mnt_namespace *copy_mnt_ns(unsigned long flags, struct mnt_namespace *ns, struct user_namespace *user_ns, struct fs_struct *new_fs) { struct mnt_namespace *new_ns; struct vfsmount *rootmnt = NULL, *pwdmnt = NULL; struct mount *p, *q; struct mount *old; struct mount *new; int copy_flags; BUG_ON(!ns); if (likely(!(flags & CLONE_NEWNS))) { get_mnt_ns(ns); return ns; } old = ns->root; new_ns = alloc_mnt_ns(user_ns, false); if (IS_ERR(new_ns)) return new_ns; namespace_lock(); copy_flags = CL_COPY_UNBINDABLE | CL_EXPIRE; if (user_ns != ns->user_ns) copy_flags |= CL_SHARED_TO_SLAVE; new = copy_tree(old, old->mnt.mnt_root, copy_flags); if (IS_ERR(new)) { namespace_unlock(); ns_free_inum(&new_ns->ns); dec_mnt_namespaces(new_ns->ucounts); mnt_ns_release(new_ns); return ERR_CAST(new); } if (user_ns != ns->user_ns) { lock_mount_hash(); lock_mnt_tree(new); unlock_mount_hash(); } new_ns->root = new; p = old; q = new; while (p) { mnt_add_to_ns(new_ns, q); new_ns->nr_mounts++; if (new_fs) { if (&p->mnt == new_fs->root.mnt) { new_fs->root.mnt = mntget(&q->mnt); rootmnt = &p->mnt; } if (&p->mnt == new_fs->pwd.mnt) { new_fs->pwd.mnt = mntget(&q->mnt); pwdmnt = &p->mnt; } } p = next_mnt(p, old); q = next_mnt(q, new); if (!q) break; while (p->mnt.mnt_root != q->mnt.mnt_root) p = next_mnt(skip_mnt_tree(p), old); } mnt_ns_tree_add(new_ns); namespace_unlock(); if (rootmnt) mntput(rootmnt); if (pwdmnt) mntput(pwdmnt); return new_ns; }"
1210----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53215/bad/svc_rdma.c----svc_rdma_proc_init,"static int svc_rdma_proc_init(void) { int rc; if (svcrdma_table_header) return 0; rc = percpu_counter_init(&svcrdma_stat_read, 0, GFP_KERNEL); if (rc) <S2SV_StartVul> goto out_err; <S2SV_EndVul> rc = percpu_counter_init(&svcrdma_stat_recv, 0, GFP_KERNEL); if (rc) <S2SV_StartVul> goto out_err; <S2SV_EndVul> rc = percpu_counter_init(&svcrdma_stat_sq_starve, 0, GFP_KERNEL); if (rc) <S2SV_StartVul> goto out_err; <S2SV_EndVul> rc = percpu_counter_init(&svcrdma_stat_write, 0, GFP_KERNEL); if (rc) <S2SV_StartVul> goto out_err; <S2SV_EndVul> svcrdma_table_header = register_sysctl(""sunrpc/svc_rdma"", svcrdma_parm_table); return 0; <S2SV_StartVul> out_err: <S2SV_EndVul> percpu_counter_destroy(&svcrdma_stat_sq_starve); percpu_counter_destroy(&svcrdma_stat_recv); percpu_counter_destroy(&svcrdma_stat_read); return rc; }","- goto out_err;
- goto out_err;
- goto out_err;
- goto out_err;
- out_err:
+ goto err;
+ goto err_read;
+ goto err_recv;
+ goto err_sq;
+ if (!svcrdma_table_header)
+ goto err_write;
+ err_write:
+ rc = -ENOMEM;
+ percpu_counter_destroy(&svcrdma_stat_write);
+ err_sq:
+ err_recv:
+ err_read:
+ err:","static int svc_rdma_proc_init(void) { int rc; if (svcrdma_table_header) return 0; rc = percpu_counter_init(&svcrdma_stat_read, 0, GFP_KERNEL); if (rc) goto err; rc = percpu_counter_init(&svcrdma_stat_recv, 0, GFP_KERNEL); if (rc) goto err_read; rc = percpu_counter_init(&svcrdma_stat_sq_starve, 0, GFP_KERNEL); if (rc) goto err_recv; rc = percpu_counter_init(&svcrdma_stat_write, 0, GFP_KERNEL); if (rc) goto err_sq; svcrdma_table_header = register_sysctl(""sunrpc/svc_rdma"", svcrdma_parm_table); if (!svcrdma_table_header) goto err_write; return 0; err_write: rc = -ENOMEM; percpu_counter_destroy(&svcrdma_stat_write); err_sq: percpu_counter_destroy(&svcrdma_stat_sq_starve); err_recv: percpu_counter_destroy(&svcrdma_stat_recv); err_read: percpu_counter_destroy(&svcrdma_stat_read); err: return rc; }"
631----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49937/bad/nl80211.c----nl80211_start_radar_detection,"static int nl80211_start_radar_detection(struct sk_buff *skb, struct genl_info *info) { struct cfg80211_registered_device *rdev = info->user_ptr[0]; struct net_device *dev = info->user_ptr[1]; struct wireless_dev *wdev = dev->ieee80211_ptr; struct wiphy *wiphy = wdev->wiphy; struct cfg80211_chan_def chandef; enum nl80211_dfs_regions dfs_region; unsigned int cac_time_ms; int err = -EINVAL; flush_delayed_work(&rdev->dfs_update_channels_wk); wiphy_lock(wiphy); dfs_region = reg_get_dfs_region(wiphy); if (dfs_region == NL80211_DFS_UNSET) goto unlock; err = nl80211_parse_chandef(rdev, info, &chandef); if (err) goto unlock; err = cfg80211_chandef_dfs_required(wiphy, &chandef, wdev->iftype); if (err < 0) goto unlock; if (err == 0) { err = -EINVAL; goto unlock; } if (!cfg80211_chandef_dfs_usable(wiphy, &chandef)) { err = -EINVAL; goto unlock; } if (nla_get_flag(info->attrs[NL80211_ATTR_RADAR_BACKGROUND])) { err = cfg80211_start_background_radar_detection(rdev, wdev, &chandef); goto unlock; } if (netif_carrier_ok(dev)) { err = -EBUSY; goto unlock; } if (wdev->cac_started) { err = -EBUSY; goto unlock; } if (wiphy_ext_feature_isset(wiphy, NL80211_EXT_FEATURE_DFS_OFFLOAD)) { err = -EOPNOTSUPP; goto unlock; } if (!rdev->ops->start_radar_detection) { err = -EOPNOTSUPP; goto unlock; } cac_time_ms = cfg80211_chandef_dfs_cac_time(&rdev->wiphy, &chandef); if (WARN_ON(!cac_time_ms)) cac_time_ms = IEEE80211_DFS_MIN_CAC_TIME_MS; err = rdev_start_radar_detection(rdev, dev, &chandef, cac_time_ms); if (!err) { <S2SV_StartVul> wdev->links[0].ap.chandef = chandef; <S2SV_EndVul> wdev->cac_started = true; wdev->cac_start_time = jiffies; wdev->cac_time_ms = cac_time_ms; } unlock: wiphy_unlock(wiphy); return err; }","- wdev->links[0].ap.chandef = chandef;
+ switch (wdev->iftype) {
+ case NL80211_IFTYPE_AP:
+ case NL80211_IFTYPE_P2P_GO:
+ wdev->links[0].ap.chandef = chandef;
+ break;
+ case NL80211_IFTYPE_ADHOC:
+ wdev->u.ibss.chandef = chandef;
+ break;
+ case NL80211_IFTYPE_MESH_POINT:
+ wdev->u.mesh.chandef = chandef;
+ break;
+ default:
+ break;
+ }","static int nl80211_start_radar_detection(struct sk_buff *skb, struct genl_info *info) { struct cfg80211_registered_device *rdev = info->user_ptr[0]; struct net_device *dev = info->user_ptr[1]; struct wireless_dev *wdev = dev->ieee80211_ptr; struct wiphy *wiphy = wdev->wiphy; struct cfg80211_chan_def chandef; enum nl80211_dfs_regions dfs_region; unsigned int cac_time_ms; int err = -EINVAL; flush_delayed_work(&rdev->dfs_update_channels_wk); wiphy_lock(wiphy); dfs_region = reg_get_dfs_region(wiphy); if (dfs_region == NL80211_DFS_UNSET) goto unlock; err = nl80211_parse_chandef(rdev, info, &chandef); if (err) goto unlock; err = cfg80211_chandef_dfs_required(wiphy, &chandef, wdev->iftype); if (err < 0) goto unlock; if (err == 0) { err = -EINVAL; goto unlock; } if (!cfg80211_chandef_dfs_usable(wiphy, &chandef)) { err = -EINVAL; goto unlock; } if (nla_get_flag(info->attrs[NL80211_ATTR_RADAR_BACKGROUND])) { err = cfg80211_start_background_radar_detection(rdev, wdev, &chandef); goto unlock; } if (netif_carrier_ok(dev)) { err = -EBUSY; goto unlock; } if (wdev->cac_started) { err = -EBUSY; goto unlock; } if (wiphy_ext_feature_isset(wiphy, NL80211_EXT_FEATURE_DFS_OFFLOAD)) { err = -EOPNOTSUPP; goto unlock; } if (!rdev->ops->start_radar_detection) { err = -EOPNOTSUPP; goto unlock; } cac_time_ms = cfg80211_chandef_dfs_cac_time(&rdev->wiphy, &chandef); if (WARN_ON(!cac_time_ms)) cac_time_ms = IEEE80211_DFS_MIN_CAC_TIME_MS; err = rdev_start_radar_detection(rdev, dev, &chandef, cac_time_ms); if (!err) { switch (wdev->iftype) { case NL80211_IFTYPE_AP: case NL80211_IFTYPE_P2P_GO: wdev->links[0].ap.chandef = chandef; break; case NL80211_IFTYPE_ADHOC: wdev->u.ibss.chandef = chandef; break; case NL80211_IFTYPE_MESH_POINT: wdev->u.mesh.chandef = chandef; break; default: break; } wdev->cac_started = true; wdev->cac_start_time = jiffies; wdev->cac_time_ms = cac_time_ms; } unlock: wiphy_unlock(wiphy); return err; }"
456----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47699/bad/btree.c----nilfs_btree_root_broken,"static int nilfs_btree_root_broken(const struct nilfs_btree_node *node, struct inode *inode) { int level, flags, nchildren; int ret = 0; level = nilfs_btree_node_get_level(node); flags = nilfs_btree_node_get_flags(node); nchildren = nilfs_btree_node_get_nchildren(node); if (unlikely(level < NILFS_BTREE_LEVEL_NODE_MIN || level >= NILFS_BTREE_LEVEL_MAX || nchildren < 0 || <S2SV_StartVul> nchildren > NILFS_BTREE_ROOT_NCHILDREN_MAX)) { <S2SV_EndVul> nilfs_crit(inode->i_sb, ""bad btree root (ino=%lu): level = %d, flags = 0x%x, nchildren = %d"", inode->i_ino, level, flags, nchildren); ret = 1; } return ret; }","- nchildren > NILFS_BTREE_ROOT_NCHILDREN_MAX)) {
+ nchildren > NILFS_BTREE_ROOT_NCHILDREN_MAX ||
+ (nchildren == 0 && level > NILFS_BTREE_LEVEL_NODE_MIN))) {","static int nilfs_btree_root_broken(const struct nilfs_btree_node *node, struct inode *inode) { int level, flags, nchildren; int ret = 0; level = nilfs_btree_node_get_level(node); flags = nilfs_btree_node_get_flags(node); nchildren = nilfs_btree_node_get_nchildren(node); if (unlikely(level < NILFS_BTREE_LEVEL_NODE_MIN || level >= NILFS_BTREE_LEVEL_MAX || nchildren < 0 || nchildren > NILFS_BTREE_ROOT_NCHILDREN_MAX || (nchildren == 0 && level > NILFS_BTREE_LEVEL_NODE_MIN))) { nilfs_crit(inode->i_sb, ""bad btree root (ino=%lu): level = %d, flags = 0x%x, nchildren = %d"", inode->i_ino, level, flags, nchildren); ret = 1; } return ret; }"
296----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46766/bad/ice_main.c----ice_napi_add,"static void ice_napi_add(struct ice_vsi *vsi) { int v_idx; if (!vsi->netdev) return; <S2SV_StartVul> ice_for_each_q_vector(vsi, v_idx) { <S2SV_EndVul> netif_napi_add(vsi->netdev, &vsi->q_vectors[v_idx]->napi, ice_napi_poll); <S2SV_StartVul> __ice_q_vector_set_napi_queues(vsi->q_vectors[v_idx], false); <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul>","- ice_for_each_q_vector(vsi, v_idx) {
- __ice_q_vector_set_napi_queues(vsi->q_vectors[v_idx], false);
- }
- }
+ ice_for_each_q_vector(vsi, v_idx)
+ }","static void ice_napi_add(struct ice_vsi *vsi) { int v_idx; if (!vsi->netdev) return; ice_for_each_q_vector(vsi, v_idx) netif_napi_add(vsi->netdev, &vsi->q_vectors[v_idx]->napi, ice_napi_poll); }"
885----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50138/bad/ringbuf.c----*bpf_ringbuf_alloc,"static struct bpf_ringbuf *bpf_ringbuf_alloc(size_t data_sz, int numa_node) { struct bpf_ringbuf *rb; rb = bpf_ringbuf_area_alloc(data_sz, numa_node); if (!rb) return NULL; <S2SV_StartVul> spin_lock_init(&rb->spinlock); <S2SV_EndVul> atomic_set(&rb->busy, 0); init_waitqueue_head(&rb->waitq); init_irq_work(&rb->work, bpf_ringbuf_notify); rb->mask = data_sz - 1; rb->consumer_pos = 0; rb->producer_pos = 0; rb->pending_pos = 0; return rb; }","- spin_lock_init(&rb->spinlock);
+ raw_spin_lock_init(&rb->spinlock);","static struct bpf_ringbuf *bpf_ringbuf_alloc(size_t data_sz, int numa_node) { struct bpf_ringbuf *rb; rb = bpf_ringbuf_area_alloc(data_sz, numa_node); if (!rb) return NULL; raw_spin_lock_init(&rb->spinlock); atomic_set(&rb->busy, 0); init_waitqueue_head(&rb->waitq); init_irq_work(&rb->work, bpf_ringbuf_notify); rb->mask = data_sz - 1; rb->consumer_pos = 0; rb->producer_pos = 0; rb->pending_pos = 0; return rb; }"
1243----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-54680/bad/connect.c----generic_ip_connect,"generic_ip_connect(struct TCP_Server_Info *server) { struct sockaddr *saddr; struct socket *socket; int slen, sfamily; __be16 sport; int rc = 0; saddr = (struct sockaddr *) &server->dstaddr; if (server->dstaddr.ss_family == AF_INET6) { struct sockaddr_in6 *ipv6 = (struct sockaddr_in6 *)&server->dstaddr; sport = ipv6->sin6_port; slen = sizeof(struct sockaddr_in6); sfamily = AF_INET6; cifs_dbg(FYI, ""%s: connecting to [%pI6]:%d\n"", __func__, &ipv6->sin6_addr, ntohs(sport)); } else { struct sockaddr_in *ipv4 = (struct sockaddr_in *)&server->dstaddr; sport = ipv4->sin_port; slen = sizeof(struct sockaddr_in); sfamily = AF_INET; cifs_dbg(FYI, ""%s: connecting to %pI4:%d\n"", __func__, &ipv4->sin_addr, ntohs(sport)); } if (server->ssocket) { socket = server->ssocket; } else { struct net *net = cifs_net_ns(server); <S2SV_StartVul> struct sock *sk; <S2SV_EndVul> <S2SV_StartVul> rc = __sock_create(net, sfamily, SOCK_STREAM, <S2SV_EndVul> <S2SV_StartVul> IPPROTO_TCP, &server->ssocket, 1); <S2SV_EndVul> if (rc < 0) { cifs_server_dbg(VFS, ""Error %d creating socket\n"", rc); return rc; } <S2SV_StartVul> sk = server->ssocket->sk; <S2SV_EndVul> <S2SV_StartVul> __netns_tracker_free(net, &sk->ns_tracker, false); <S2SV_EndVul> <S2SV_StartVul> sk->sk_net_refcnt = 1; <S2SV_EndVul> <S2SV_StartVul> get_net_track(net, &sk->ns_tracker, GFP_KERNEL); <S2SV_EndVul> <S2SV_StartVul> sock_inuse_add(net, 1); <S2SV_EndVul> cifs_dbg(FYI, ""Socket created\n""); socket = server->ssocket; socket->sk->sk_allocation = GFP_NOFS; socket->sk->sk_use_task_frag = false; if (sfamily == AF_INET6) cifs_reclassify_socket6(socket); else cifs_reclassify_socket4(socket); } rc = bind_socket(server); <S2SV_StartVul> if (rc < 0) <S2SV_EndVul> return rc; socket->sk->sk_rcvtimeo = 7 * HZ; socket->sk->sk_sndtimeo = 5 * HZ; if (server->noautotune) { if (socket->sk->sk_sndbuf < (200 * 1024)) socket->sk->sk_sndbuf = 200 * 1024; if (socket->sk->sk_rcvbuf < (140 * 1024)) socket->sk->sk_rcvbuf = 140 * 1024; } if (server->tcp_nodelay) tcp_sock_set_nodelay(socket->sk); cifs_dbg(FYI, ""sndbuf %d rcvbuf %d rcvtimeo 0x%lx\n"", socket->sk->sk_sndbuf, socket->sk->sk_rcvbuf, socket->sk->sk_rcvtimeo); rc = kernel_connect(socket, saddr, slen, server->noblockcnt ? O_NONBLOCK : 0); if (server->noblockcnt && rc == -EINPROGRESS) rc = 0; if (rc < 0) { cifs_dbg(FYI, ""Error %d connecting to server\n"", rc); trace_smb3_connect_err(server->hostname, server->conn_id, &server->dstaddr, rc); sock_release(socket); server->ssocket = NULL; return rc; } trace_smb3_connect_done(server->hostname, server->conn_id, &server->dstaddr); if (sport == htons(RFC1001_PORT)) rc = ip_rfc1001_connect(server); return rc; }","- struct sock *sk;
- rc = __sock_create(net, sfamily, SOCK_STREAM,
- IPPROTO_TCP, &server->ssocket, 1);
- sk = server->ssocket->sk;
- __netns_tracker_free(net, &sk->ns_tracker, false);
- sk->sk_net_refcnt = 1;
- get_net_track(net, &sk->ns_tracker, GFP_KERNEL);
- sock_inuse_add(net, 1);
- if (rc < 0)
+ rc = sock_create_kern(net, sfamily, SOCK_STREAM, IPPROTO_TCP, &server->ssocket);
+ if (rc < 0) {
+ }
+ get_net(net);
+ if (rc < 0) {
+ put_net(cifs_net_ns(server));
+ }
+ put_net(cifs_net_ns(server));
+ if (rc < 0)
+ put_net(cifs_net_ns(server));
+ }","generic_ip_connect(struct TCP_Server_Info *server) { struct sockaddr *saddr; struct socket *socket; int slen, sfamily; __be16 sport; int rc = 0; saddr = (struct sockaddr *) &server->dstaddr; if (server->dstaddr.ss_family == AF_INET6) { struct sockaddr_in6 *ipv6 = (struct sockaddr_in6 *)&server->dstaddr; sport = ipv6->sin6_port; slen = sizeof(struct sockaddr_in6); sfamily = AF_INET6; cifs_dbg(FYI, ""%s: connecting to [%pI6]:%d\n"", __func__, &ipv6->sin6_addr, ntohs(sport)); } else { struct sockaddr_in *ipv4 = (struct sockaddr_in *)&server->dstaddr; sport = ipv4->sin_port; slen = sizeof(struct sockaddr_in); sfamily = AF_INET; cifs_dbg(FYI, ""%s: connecting to %pI4:%d\n"", __func__, &ipv4->sin_addr, ntohs(sport)); } if (server->ssocket) { socket = server->ssocket; } else { struct net *net = cifs_net_ns(server); rc = sock_create_kern(net, sfamily, SOCK_STREAM, IPPROTO_TCP, &server->ssocket); if (rc < 0) { cifs_server_dbg(VFS, ""Error %d creating socket\n"", rc); return rc; } get_net(net); cifs_dbg(FYI, ""Socket created\n""); socket = server->ssocket; socket->sk->sk_allocation = GFP_NOFS; socket->sk->sk_use_task_frag = false; if (sfamily == AF_INET6) cifs_reclassify_socket6(socket); else cifs_reclassify_socket4(socket); } rc = bind_socket(server); if (rc < 0) { put_net(cifs_net_ns(server)); return rc; } socket->sk->sk_rcvtimeo = 7 * HZ; socket->sk->sk_sndtimeo = 5 * HZ; if (server->noautotune) { if (socket->sk->sk_sndbuf < (200 * 1024)) socket->sk->sk_sndbuf = 200 * 1024; if (socket->sk->sk_rcvbuf < (140 * 1024)) socket->sk->sk_rcvbuf = 140 * 1024; } if (server->tcp_nodelay) tcp_sock_set_nodelay(socket->sk); cifs_dbg(FYI, ""sndbuf %d rcvbuf %d rcvtimeo 0x%lx\n"", socket->sk->sk_sndbuf, socket->sk->sk_rcvbuf, socket->sk->sk_rcvtimeo); rc = kernel_connect(socket, saddr, slen, server->noblockcnt ? O_NONBLOCK : 0); if (server->noblockcnt && rc == -EINPROGRESS) rc = 0; if (rc < 0) { cifs_dbg(FYI, ""Error %d connecting to server\n"", rc); trace_smb3_connect_err(server->hostname, server->conn_id, &server->dstaddr, rc); put_net(cifs_net_ns(server)); sock_release(socket); server->ssocket = NULL; return rc; } trace_smb3_connect_done(server->hostname, server->conn_id, &server->dstaddr); if (sport == htons(RFC1001_PORT)) rc = ip_rfc1001_connect(server); if (rc < 0) put_net(cifs_net_ns(server)); return rc; }"
846----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50106/bad/nfs4state.c----nfsd4_delegreturn,"nfsd4_delegreturn(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate, union nfsd4_op_u *u) { struct nfsd4_delegreturn *dr = &u->delegreturn; struct nfs4_delegation *dp; stateid_t *stateid = &dr->dr_stateid; struct nfs4_stid *s; __be32 status; struct nfsd_net *nn = net_generic(SVC_NET(rqstp), nfsd_net_id); if ((status = fh_verify(rqstp, &cstate->current_fh, S_IFREG, 0))) return status; <S2SV_StartVul> status = nfsd4_lookup_stateid(cstate, stateid, SC_TYPE_DELEG, 0, &s, nn); <S2SV_EndVul> if (status) goto out; dp = delegstateid(s); status = nfsd4_stid_check_stateid_generation(stateid, &dp->dl_stid, nfsd4_has_session(cstate)); if (status) goto put_stateid; trace_nfsd_deleg_return(stateid); wake_up_var(d_inode(cstate->current_fh.fh_dentry)); destroy_delegation(dp); put_stateid: nfs4_put_stid(&dp->dl_stid); out: return status; }","- status = nfsd4_lookup_stateid(cstate, stateid, SC_TYPE_DELEG, 0, &s, nn);
+ status = nfsd4_lookup_stateid(cstate, stateid, SC_TYPE_DELEG,
+ SC_STATUS_REVOKED | SC_STATUS_FREEABLE,
+ &s, nn);
+ goto out;","nfsd4_delegreturn(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate, union nfsd4_op_u *u) { struct nfsd4_delegreturn *dr = &u->delegreturn; struct nfs4_delegation *dp; stateid_t *stateid = &dr->dr_stateid; struct nfs4_stid *s; __be32 status; struct nfsd_net *nn = net_generic(SVC_NET(rqstp), nfsd_net_id); if ((status = fh_verify(rqstp, &cstate->current_fh, S_IFREG, 0))) return status; status = nfsd4_lookup_stateid(cstate, stateid, SC_TYPE_DELEG, SC_STATUS_REVOKED | SC_STATUS_FREEABLE, &s, nn); if (status) goto out; dp = delegstateid(s); status = nfsd4_stid_check_stateid_generation(stateid, &dp->dl_stid, nfsd4_has_session(cstate)); if (status) goto put_stateid; trace_nfsd_deleg_return(stateid); wake_up_var(d_inode(cstate->current_fh.fh_dentry)); destroy_delegation(dp); put_stateid: nfs4_put_stid(&dp->dl_stid); out: return status; }"
1000----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50236/bad/wmi-tlv.c----ath10k_wmi_tlv_op_cleanup_mgmt_tx_send,"ath10k_wmi_tlv_op_cleanup_mgmt_tx_send(struct ath10k *ar, struct sk_buff *msdu) { struct ath10k_skb_cb *cb = ATH10K_SKB_CB(msdu); struct ath10k_wmi *wmi = &ar->wmi; <S2SV_StartVul> idr_remove(&wmi->mgmt_pending_tx, cb->msdu_id); <S2SV_EndVul> return 0; }","- idr_remove(&wmi->mgmt_pending_tx, cb->msdu_id);
+ struct ath10k_mgmt_tx_pkt_addr *pkt_addr;
+ spin_lock_bh(&ar->data_lock);
+ pkt_addr = idr_remove(&wmi->mgmt_pending_tx, cb->msdu_id);
+ spin_unlock_bh(&ar->data_lock);
+ kfree(pkt_addr);","ath10k_wmi_tlv_op_cleanup_mgmt_tx_send(struct ath10k *ar, struct sk_buff *msdu) { struct ath10k_skb_cb *cb = ATH10K_SKB_CB(msdu); struct ath10k_mgmt_tx_pkt_addr *pkt_addr; struct ath10k_wmi *wmi = &ar->wmi; spin_lock_bh(&ar->data_lock); pkt_addr = idr_remove(&wmi->mgmt_pending_tx, cb->msdu_id); spin_unlock_bh(&ar->data_lock); kfree(pkt_addr); return 0; }"
624----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49929/bad/tx.c----iwl_mvm_tx_mpdu,"static int iwl_mvm_tx_mpdu(struct iwl_mvm *mvm, struct sk_buff *skb, struct ieee80211_tx_info *info, struct ieee80211_sta *sta, const u8 *addr3_override) { struct ieee80211_hdr *hdr = (struct ieee80211_hdr *)skb->data; struct iwl_mvm_sta *mvmsta; struct iwl_device_tx_cmd *dev_cmd; __le16 fc; u16 seq_number = 0; u8 tid = IWL_MAX_TID_COUNT; u16 txq_id; bool is_ampdu = false; int hdrlen; mvmsta = iwl_mvm_sta_from_mac80211(sta); fc = hdr->frame_control; hdrlen = ieee80211_hdrlen(fc); if (IWL_MVM_NON_TRANSMITTING_AP && ieee80211_is_probe_resp(fc)) <S2SV_StartVul> return -1; <S2SV_EndVul> <S2SV_StartVul> if (WARN_ON_ONCE(!mvmsta)) <S2SV_EndVul> <S2SV_StartVul> return -1; <S2SV_EndVul> if (WARN_ON_ONCE(mvmsta->deflink.sta_id == IWL_MVM_INVALID_STA)) <S2SV_StartVul> return -1; <S2SV_EndVul> if (unlikely(ieee80211_is_any_nullfunc(fc)) && sta->deflink.he_cap.has_he) return -1; if (unlikely(ieee80211_is_probe_resp(fc))) iwl_mvm_probe_resp_set_noa(mvm, skb); dev_cmd = iwl_mvm_set_tx_params(mvm, skb, info, hdrlen, sta, mvmsta->deflink.sta_id, addr3_override); if (!dev_cmd) goto drop; info->flags &= ~IEEE80211_TX_STATUS_EOSP; spin_lock(&mvmsta->lock); if (ieee80211_is_data_qos(fc) && !ieee80211_is_qos_nullfunc(fc)) { tid = ieee80211_get_tid(hdr); if (WARN_ONCE(tid >= IWL_MAX_TID_COUNT, ""Invalid TID %d"", tid)) goto drop_unlock_sta; is_ampdu = info->flags & IEEE80211_TX_CTL_AMPDU; if (WARN_ONCE(is_ampdu && mvmsta->tid_data[tid].state != IWL_AGG_ON, ""Invalid internal agg state %d for TID %d"", mvmsta->tid_data[tid].state, tid)) goto drop_unlock_sta; seq_number = mvmsta->tid_data[tid].seq_number; seq_number &= IEEE80211_SCTL_SEQ; if (!iwl_mvm_has_new_tx_api(mvm)) { struct iwl_tx_cmd *tx_cmd = (void *)dev_cmd->payload; hdr->seq_ctrl &= cpu_to_le16(IEEE80211_SCTL_FRAG); hdr->seq_ctrl |= cpu_to_le16(seq_number); tx_cmd->hdr->seq_ctrl = hdr->seq_ctrl; } } else if (ieee80211_is_data(fc) && !ieee80211_is_data_qos(fc) && !ieee80211_is_nullfunc(fc)) { tid = IWL_TID_NON_QOS; } txq_id = mvmsta->tid_data[tid].txq_id; WARN_ON_ONCE(info->flags & IEEE80211_TX_CTL_SEND_AFTER_DTIM); if (WARN_ONCE(txq_id == IWL_MVM_INVALID_QUEUE, ""Invalid TXQ id"")) { iwl_trans_free_tx_cmd(mvm->trans, dev_cmd); spin_unlock(&mvmsta->lock); return -1; } if (!iwl_mvm_has_new_tx_api(mvm)) { mvm->queue_info[txq_id].last_frame_time[tid] = jiffies; if (unlikely(mvm->queue_info[txq_id].status == IWL_MVM_QUEUE_SHARED && iwl_mvm_txq_should_update(mvm, txq_id))) schedule_work(&mvm->add_stream_wk); } IWL_DEBUG_TX(mvm, ""TX to [%d|%d] Q:%d - seq: 0x%x len %d\n"", mvmsta->deflink.sta_id, tid, txq_id, IEEE80211_SEQ_TO_SN(seq_number), skb->len); iwl_mvm_skb_prepare_status(skb, dev_cmd); if (ieee80211_is_data(fc)) iwl_mvm_mei_tx_copy_to_csme(mvm, skb, info->control.hw_key && !iwl_mvm_has_new_tx_api(mvm) ? info->control.hw_key->iv_len : 0); if (iwl_trans_tx(mvm->trans, skb, dev_cmd, txq_id)) goto drop_unlock_sta; if (tid < IWL_MAX_TID_COUNT && !ieee80211_has_morefrags(fc)) mvmsta->tid_data[tid].seq_number = seq_number + 0x10; spin_unlock(&mvmsta->lock); if (iwl_mvm_tx_pkt_queued(mvm, mvmsta, tid == IWL_MAX_TID_COUNT ? 0 : tid)) goto drop; return 0; drop_unlock_sta: iwl_trans_free_tx_cmd(mvm->trans, dev_cmd); spin_unlock(&mvmsta->lock); drop: IWL_DEBUG_TX(mvm, ""TX to [%d|%d] dropped\n"", mvmsta->deflink.sta_id, tid); return -1; }","- return -1;
- if (WARN_ON_ONCE(!mvmsta))
- return -1;
- return -1;
+ if (WARN_ON_ONCE(!sta))
+ return -1;
+ mvmsta = iwl_mvm_sta_from_mac80211(sta);
+ return -1;
+ return -1;","static int iwl_mvm_tx_mpdu(struct iwl_mvm *mvm, struct sk_buff *skb, struct ieee80211_tx_info *info, struct ieee80211_sta *sta, const u8 *addr3_override) { struct ieee80211_hdr *hdr = (struct ieee80211_hdr *)skb->data; struct iwl_mvm_sta *mvmsta; struct iwl_device_tx_cmd *dev_cmd; __le16 fc; u16 seq_number = 0; u8 tid = IWL_MAX_TID_COUNT; u16 txq_id; bool is_ampdu = false; int hdrlen; if (WARN_ON_ONCE(!sta)) return -1; mvmsta = iwl_mvm_sta_from_mac80211(sta); fc = hdr->frame_control; hdrlen = ieee80211_hdrlen(fc); if (IWL_MVM_NON_TRANSMITTING_AP && ieee80211_is_probe_resp(fc)) return -1; if (WARN_ON_ONCE(mvmsta->deflink.sta_id == IWL_MVM_INVALID_STA)) return -1; if (unlikely(ieee80211_is_any_nullfunc(fc)) && sta->deflink.he_cap.has_he) return -1; if (unlikely(ieee80211_is_probe_resp(fc))) iwl_mvm_probe_resp_set_noa(mvm, skb); dev_cmd = iwl_mvm_set_tx_params(mvm, skb, info, hdrlen, sta, mvmsta->deflink.sta_id, addr3_override); if (!dev_cmd) goto drop; info->flags &= ~IEEE80211_TX_STATUS_EOSP; spin_lock(&mvmsta->lock); if (ieee80211_is_data_qos(fc) && !ieee80211_is_qos_nullfunc(fc)) { tid = ieee80211_get_tid(hdr); if (WARN_ONCE(tid >= IWL_MAX_TID_COUNT, ""Invalid TID %d"", tid)) goto drop_unlock_sta; is_ampdu = info->flags & IEEE80211_TX_CTL_AMPDU; if (WARN_ONCE(is_ampdu && mvmsta->tid_data[tid].state != IWL_AGG_ON, ""Invalid internal agg state %d for TID %d"", mvmsta->tid_data[tid].state, tid)) goto drop_unlock_sta; seq_number = mvmsta->tid_data[tid].seq_number; seq_number &= IEEE80211_SCTL_SEQ; if (!iwl_mvm_has_new_tx_api(mvm)) { struct iwl_tx_cmd *tx_cmd = (void *)dev_cmd->payload; hdr->seq_ctrl &= cpu_to_le16(IEEE80211_SCTL_FRAG); hdr->seq_ctrl |= cpu_to_le16(seq_number); tx_cmd->hdr->seq_ctrl = hdr->seq_ctrl; } } else if (ieee80211_is_data(fc) && !ieee80211_is_data_qos(fc) && !ieee80211_is_nullfunc(fc)) { tid = IWL_TID_NON_QOS; } txq_id = mvmsta->tid_data[tid].txq_id; WARN_ON_ONCE(info->flags & IEEE80211_TX_CTL_SEND_AFTER_DTIM); if (WARN_ONCE(txq_id == IWL_MVM_INVALID_QUEUE, ""Invalid TXQ id"")) { iwl_trans_free_tx_cmd(mvm->trans, dev_cmd); spin_unlock(&mvmsta->lock); return -1; } if (!iwl_mvm_has_new_tx_api(mvm)) { mvm->queue_info[txq_id].last_frame_time[tid] = jiffies; if (unlikely(mvm->queue_info[txq_id].status == IWL_MVM_QUEUE_SHARED && iwl_mvm_txq_should_update(mvm, txq_id))) schedule_work(&mvm->add_stream_wk); } IWL_DEBUG_TX(mvm, ""TX to [%d|%d] Q:%d - seq: 0x%x len %d\n"", mvmsta->deflink.sta_id, tid, txq_id, IEEE80211_SEQ_TO_SN(seq_number), skb->len); iwl_mvm_skb_prepare_status(skb, dev_cmd); if (ieee80211_is_data(fc)) iwl_mvm_mei_tx_copy_to_csme(mvm, skb, info->control.hw_key && !iwl_mvm_has_new_tx_api(mvm) ? info->control.hw_key->iv_len : 0); if (iwl_trans_tx(mvm->trans, skb, dev_cmd, txq_id)) goto drop_unlock_sta; if (tid < IWL_MAX_TID_COUNT && !ieee80211_has_morefrags(fc)) mvmsta->tid_data[tid].seq_number = seq_number + 0x10; spin_unlock(&mvmsta->lock); if (iwl_mvm_tx_pkt_queued(mvm, mvmsta, tid == IWL_MAX_TID_COUNT ? 0 : tid)) goto drop; return 0; drop_unlock_sta: iwl_trans_free_tx_cmd(mvm->trans, dev_cmd); spin_unlock(&mvmsta->lock); drop: IWL_DEBUG_TX(mvm, ""TX to [%d|%d] dropped\n"", mvmsta->deflink.sta_id, tid); return -1; }"
593----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49915/bad/dcn32_hwseq.c----dcn32_init_hw,"void dcn32_init_hw(struct dc *dc) { struct abm **abms = dc->res_pool->multiple_abms; struct dce_hwseq *hws = dc->hwseq; struct dc_bios *dcb = dc->ctx->dc_bios; struct resource_pool *res_pool = dc->res_pool; int i; int edp_num; uint32_t backlight = MAX_BACKLIGHT_LEVEL; uint32_t user_level = MAX_BACKLIGHT_LEVEL; <S2SV_StartVul> if (dc->clk_mgr && dc->clk_mgr->funcs->init_clocks) <S2SV_EndVul> dc->clk_mgr->funcs->init_clocks(dc->clk_mgr); if (res_pool->dccg->funcs->dccg_init) res_pool->dccg->funcs->dccg_init(res_pool->dccg); if (!dcb->funcs->is_accelerated_mode(dcb)) { hws->funcs.bios_golden_init(dc); hws->funcs.disable_vga(dc->hwseq); } if (dc->debug.enable_mem_low_power.bits.optc) { REG_SET_2(ODM_MEM_PWR_CTRL3, 0, ODM_MEM_UNASSIGNED_PWR_MODE, 3, ODM_MEM_VBLANK_PWR_MODE, 1); } if (dc->debug.enable_mem_low_power.bits.vga) { REG_UPDATE(MMHUBBUB_MEM_PWR_CNTL, VGA_MEM_PWR_FORCE, 1); } if (dc->ctx->dc_bios->fw_info_valid) { res_pool->ref_clocks.xtalin_clock_inKhz = dc->ctx->dc_bios->fw_info.pll_info.crystal_frequency; if (res_pool->hubbub) { (res_pool->dccg->funcs->get_dccg_ref_freq)(res_pool->dccg, dc->ctx->dc_bios->fw_info.pll_info.crystal_frequency, &res_pool->ref_clocks.dccg_ref_clock_inKhz); (res_pool->hubbub->funcs->get_dchub_ref_freq)(res_pool->hubbub, res_pool->ref_clocks.dccg_ref_clock_inKhz, &res_pool->ref_clocks.dchub_ref_clock_inKhz); } else { res_pool->ref_clocks.dccg_ref_clock_inKhz = res_pool->ref_clocks.xtalin_clock_inKhz; res_pool->ref_clocks.dchub_ref_clock_inKhz = res_pool->ref_clocks.xtalin_clock_inKhz; } } else ASSERT_CRITICAL(false); for (i = 0; i < dc->link_count; i++) { struct dc_link *link = dc->links[i]; link->link_enc->funcs->hw_init(link->link_enc); if (link->link_enc->funcs->is_dig_enabled && link->link_enc->funcs->is_dig_enabled(link->link_enc)) { link->link_status.link_active = true; link->phy_state.symclk_state = SYMCLK_ON_TX_ON; if (link->link_enc->funcs->fec_is_active && link->link_enc->funcs->fec_is_active(link->link_enc)) link->fec_state = dc_link_fec_enabled; } } if (hws->funcs.enable_power_gating_plane) hws->funcs.enable_power_gating_plane(dc->hwseq, true); dc->link_srv->blank_all_dp_displays(dc); if (dcb->funcs->is_accelerated_mode(dcb) || !dc->config.seamless_boot_edp_requested) { if (dc->hwss.enable_accelerated_mode && dc->debug.disable_boot_optimizations) dc->hwss.enable_accelerated_mode(dc, dc->current_state); else hws->funcs.init_pipes(dc, dc->current_state); if (dc->res_pool->hubbub->funcs->allow_self_refresh_control) dc->res_pool->hubbub->funcs->allow_self_refresh_control(dc->res_pool->hubbub, !dc->res_pool->hubbub->ctx->dc->debug.disable_stutter); dcn32_initialize_min_clocks(dc); dc_allow_idle_optimizations(dc, false); dc_allow_idle_optimizations(dc, true); } if (!dc->config.seamless_boot_edp_requested) { struct dc_link *edp_links[MAX_NUM_EDP]; struct dc_link *edp_link; dc_get_edp_links(dc, edp_links, &edp_num); if (edp_num) { for (i = 0; i < edp_num; i++) { edp_link = edp_links[i]; if (edp_link->link_enc->funcs->is_dig_enabled && edp_link->link_enc->funcs->is_dig_enabled(edp_link->link_enc) && dc->hwss.edp_backlight_control && dc->hwss.power_down && dc->hwss.edp_power_control) { dc->hwss.edp_backlight_control(edp_link, false); dc->hwss.power_down(dc); dc->hwss.edp_power_control(edp_link, false); } } } else { for (i = 0; i < dc->link_count; i++) { struct dc_link *link = dc->links[i]; if (link->link_enc->funcs->is_dig_enabled && link->link_enc->funcs->is_dig_enabled(link->link_enc) && dc->hwss.power_down) { dc->hwss.power_down(dc); break; } } } } for (i = 0; i < res_pool->audio_count; i++) { struct audio *audio = res_pool->audios[i]; audio->funcs->hw_init(audio); } for (i = 0; i < dc->link_count; i++) { struct dc_link *link = dc->links[i]; if (link->panel_cntl) { backlight = link->panel_cntl->funcs->hw_init(link->panel_cntl); user_level = link->panel_cntl->stored_backlight_registers.USER_LEVEL; } } for (i = 0; i < dc->res_pool->pipe_count; i++) { if (abms[i] != NULL && abms[i]->funcs != NULL) abms[i]->funcs->abm_init(abms[i], backlight, user_level); } REG_WRITE(DIO_MEM_PWR_CTRL, 0); if (!dc->debug.disable_clock_gate) { REG_WRITE(DCCG_GATE_DISABLE_CNTL, 0); REG_WRITE(DCCG_GATE_DISABLE_CNTL2, 0); REG_UPDATE(DCFCLK_CNTL, DCFCLK_GATE_DIS, 0); } if (!dcb->funcs->is_accelerated_mode(dcb) && dc->res_pool->hubbub->funcs->init_watermarks) dc->res_pool->hubbub->funcs->init_watermarks(dc->res_pool->hubbub); <S2SV_StartVul> if (dc->clk_mgr->funcs->notify_wm_ranges) <S2SV_EndVul> dc->clk_mgr->funcs->notify_wm_ranges(dc->clk_mgr); <S2SV_StartVul> if (dc->clk_mgr->funcs->set_hard_max_memclk && !dc->clk_mgr->dc_mode_softmax_enabled) <S2SV_EndVul> dc->clk_mgr->funcs->set_hard_max_memclk(dc->clk_mgr); if (dc->res_pool->hubbub->funcs->force_pstate_change_control) dc->res_pool->hubbub->funcs->force_pstate_change_control( dc->res_pool->hubbub, false, false); if (dc->res_pool->hubbub->funcs->init_crb) dc->res_pool->hubbub->funcs->init_crb(dc->res_pool->hubbub); if (dc->res_pool->hubbub->funcs->set_request_limit && dc->config.sdpif_request_limit_words_per_umc > 0) dc->res_pool->hubbub->funcs->set_request_limit(dc->res_pool->hubbub, dc->ctx->dc_bios->vram_info.num_chans, dc->config.sdpif_request_limit_words_per_umc); if (dc->ctx->dmub_srv) { dc_dmub_srv_query_caps_cmd(dc->ctx->dmub_srv); dc->caps.dmub_caps.psr = dc->ctx->dmub_srv->dmub->feature_caps.psr; dc->caps.dmub_caps.subvp_psr = dc->ctx->dmub_srv->dmub->feature_caps.subvp_psr_support; dc->caps.dmub_caps.gecc_enable = dc->ctx->dmub_srv->dmub->feature_caps.gecc_enable; dc->caps.dmub_caps.mclk_sw = dc->ctx->dmub_srv->dmub->feature_caps.fw_assisted_mclk_switch_ver; dc->caps.dmub_caps.fams_ver = dc->ctx->dmub_srv->dmub->feature_caps.fw_assisted_mclk_switch_ver; if (dc->caps.dmub_caps.fams_ver == 2) { dc->debug.fams2_config.bits.enable &= true; } else if (dc->ctx->dmub_srv->dmub->fw_version < DMUB_FW_VERSION(7, 0, 35)) { dc->debug.fams2_config.bits.enable = false; if (dc->debug.using_dml2 && dc->res_pool->funcs->update_bw_bounding_box) { dc->res_pool->funcs->update_bw_bounding_box(dc, dc->clk_mgr->bw_params); } dc->debug.force_disable_subvp = true; dc->debug.disable_fpo_optimizations = true; } } }","- if (dc->clk_mgr && dc->clk_mgr->funcs->init_clocks)
- if (dc->clk_mgr->funcs->notify_wm_ranges)
- if (dc->clk_mgr->funcs->set_hard_max_memclk && !dc->clk_mgr->dc_mode_softmax_enabled)
+ if (dc->clk_mgr && dc->clk_mgr->funcs && dc->clk_mgr->funcs->init_clocks)
+ if (dc->clk_mgr && dc->clk_mgr->funcs && dc->clk_mgr->funcs->notify_wm_ranges)
+ if (dc->clk_mgr && dc->clk_mgr->funcs && dc->clk_mgr->funcs->set_hard_max_memclk &&
+ !dc->clk_mgr->dc_mode_softmax_enabled)","void dcn32_init_hw(struct dc *dc) { struct abm **abms = dc->res_pool->multiple_abms; struct dce_hwseq *hws = dc->hwseq; struct dc_bios *dcb = dc->ctx->dc_bios; struct resource_pool *res_pool = dc->res_pool; int i; int edp_num; uint32_t backlight = MAX_BACKLIGHT_LEVEL; uint32_t user_level = MAX_BACKLIGHT_LEVEL; if (dc->clk_mgr && dc->clk_mgr->funcs && dc->clk_mgr->funcs->init_clocks) dc->clk_mgr->funcs->init_clocks(dc->clk_mgr); if (res_pool->dccg->funcs->dccg_init) res_pool->dccg->funcs->dccg_init(res_pool->dccg); if (!dcb->funcs->is_accelerated_mode(dcb)) { hws->funcs.bios_golden_init(dc); hws->funcs.disable_vga(dc->hwseq); } if (dc->debug.enable_mem_low_power.bits.optc) { REG_SET_2(ODM_MEM_PWR_CTRL3, 0, ODM_MEM_UNASSIGNED_PWR_MODE, 3, ODM_MEM_VBLANK_PWR_MODE, 1); } if (dc->debug.enable_mem_low_power.bits.vga) { REG_UPDATE(MMHUBBUB_MEM_PWR_CNTL, VGA_MEM_PWR_FORCE, 1); } if (dc->ctx->dc_bios->fw_info_valid) { res_pool->ref_clocks.xtalin_clock_inKhz = dc->ctx->dc_bios->fw_info.pll_info.crystal_frequency; if (res_pool->hubbub) { (res_pool->dccg->funcs->get_dccg_ref_freq)(res_pool->dccg, dc->ctx->dc_bios->fw_info.pll_info.crystal_frequency, &res_pool->ref_clocks.dccg_ref_clock_inKhz); (res_pool->hubbub->funcs->get_dchub_ref_freq)(res_pool->hubbub, res_pool->ref_clocks.dccg_ref_clock_inKhz, &res_pool->ref_clocks.dchub_ref_clock_inKhz); } else { res_pool->ref_clocks.dccg_ref_clock_inKhz = res_pool->ref_clocks.xtalin_clock_inKhz; res_pool->ref_clocks.dchub_ref_clock_inKhz = res_pool->ref_clocks.xtalin_clock_inKhz; } } else ASSERT_CRITICAL(false); for (i = 0; i < dc->link_count; i++) { struct dc_link *link = dc->links[i]; link->link_enc->funcs->hw_init(link->link_enc); if (link->link_enc->funcs->is_dig_enabled && link->link_enc->funcs->is_dig_enabled(link->link_enc)) { link->link_status.link_active = true; link->phy_state.symclk_state = SYMCLK_ON_TX_ON; if (link->link_enc->funcs->fec_is_active && link->link_enc->funcs->fec_is_active(link->link_enc)) link->fec_state = dc_link_fec_enabled; } } if (hws->funcs.enable_power_gating_plane) hws->funcs.enable_power_gating_plane(dc->hwseq, true); dc->link_srv->blank_all_dp_displays(dc); if (dcb->funcs->is_accelerated_mode(dcb) || !dc->config.seamless_boot_edp_requested) { if (dc->hwss.enable_accelerated_mode && dc->debug.disable_boot_optimizations) dc->hwss.enable_accelerated_mode(dc, dc->current_state); else hws->funcs.init_pipes(dc, dc->current_state); if (dc->res_pool->hubbub->funcs->allow_self_refresh_control) dc->res_pool->hubbub->funcs->allow_self_refresh_control(dc->res_pool->hubbub, !dc->res_pool->hubbub->ctx->dc->debug.disable_stutter); dcn32_initialize_min_clocks(dc); dc_allow_idle_optimizations(dc, false); dc_allow_idle_optimizations(dc, true); } if (!dc->config.seamless_boot_edp_requested) { struct dc_link *edp_links[MAX_NUM_EDP]; struct dc_link *edp_link; dc_get_edp_links(dc, edp_links, &edp_num); if (edp_num) { for (i = 0; i < edp_num; i++) { edp_link = edp_links[i]; if (edp_link->link_enc->funcs->is_dig_enabled && edp_link->link_enc->funcs->is_dig_enabled(edp_link->link_enc) && dc->hwss.edp_backlight_control && dc->hwss.power_down && dc->hwss.edp_power_control) { dc->hwss.edp_backlight_control(edp_link, false); dc->hwss.power_down(dc); dc->hwss.edp_power_control(edp_link, false); } } } else { for (i = 0; i < dc->link_count; i++) { struct dc_link *link = dc->links[i]; if (link->link_enc->funcs->is_dig_enabled && link->link_enc->funcs->is_dig_enabled(link->link_enc) && dc->hwss.power_down) { dc->hwss.power_down(dc); break; } } } } for (i = 0; i < res_pool->audio_count; i++) { struct audio *audio = res_pool->audios[i]; audio->funcs->hw_init(audio); } for (i = 0; i < dc->link_count; i++) { struct dc_link *link = dc->links[i]; if (link->panel_cntl) { backlight = link->panel_cntl->funcs->hw_init(link->panel_cntl); user_level = link->panel_cntl->stored_backlight_registers.USER_LEVEL; } } for (i = 0; i < dc->res_pool->pipe_count; i++) { if (abms[i] != NULL && abms[i]->funcs != NULL) abms[i]->funcs->abm_init(abms[i], backlight, user_level); } REG_WRITE(DIO_MEM_PWR_CTRL, 0); if (!dc->debug.disable_clock_gate) { REG_WRITE(DCCG_GATE_DISABLE_CNTL, 0); REG_WRITE(DCCG_GATE_DISABLE_CNTL2, 0); REG_UPDATE(DCFCLK_CNTL, DCFCLK_GATE_DIS, 0); } if (!dcb->funcs->is_accelerated_mode(dcb) && dc->res_pool->hubbub->funcs->init_watermarks) dc->res_pool->hubbub->funcs->init_watermarks(dc->res_pool->hubbub); if (dc->clk_mgr && dc->clk_mgr->funcs && dc->clk_mgr->funcs->notify_wm_ranges) dc->clk_mgr->funcs->notify_wm_ranges(dc->clk_mgr); if (dc->clk_mgr && dc->clk_mgr->funcs && dc->clk_mgr->funcs->set_hard_max_memclk && !dc->clk_mgr->dc_mode_softmax_enabled) dc->clk_mgr->funcs->set_hard_max_memclk(dc->clk_mgr); if (dc->res_pool->hubbub->funcs->force_pstate_change_control) dc->res_pool->hubbub->funcs->force_pstate_change_control( dc->res_pool->hubbub, false, false); if (dc->res_pool->hubbub->funcs->init_crb) dc->res_pool->hubbub->funcs->init_crb(dc->res_pool->hubbub); if (dc->res_pool->hubbub->funcs->set_request_limit && dc->config.sdpif_request_limit_words_per_umc > 0) dc->res_pool->hubbub->funcs->set_request_limit(dc->res_pool->hubbub, dc->ctx->dc_bios->vram_info.num_chans, dc->config.sdpif_request_limit_words_per_umc); if (dc->ctx->dmub_srv) { dc_dmub_srv_query_caps_cmd(dc->ctx->dmub_srv); dc->caps.dmub_caps.psr = dc->ctx->dmub_srv->dmub->feature_caps.psr; dc->caps.dmub_caps.subvp_psr = dc->ctx->dmub_srv->dmub->feature_caps.subvp_psr_support; dc->caps.dmub_caps.gecc_enable = dc->ctx->dmub_srv->dmub->feature_caps.gecc_enable; dc->caps.dmub_caps.mclk_sw = dc->ctx->dmub_srv->dmub->feature_caps.fw_assisted_mclk_switch_ver; dc->caps.dmub_caps.fams_ver = dc->ctx->dmub_srv->dmub->feature_caps.fw_assisted_mclk_switch_ver; if (dc->caps.dmub_caps.fams_ver == 2) { dc->debug.fams2_config.bits.enable &= true; } else if (dc->ctx->dmub_srv->dmub->fw_version < DMUB_FW_VERSION(7, 0, 35)) { dc->debug.fams2_config.bits.enable = false; if (dc->debug.using_dml2 && dc->res_pool->funcs->update_bw_bounding_box) { dc->res_pool->funcs->update_bw_bounding_box(dc, dc->clk_mgr->bw_params); } dc->debug.force_disable_subvp = true; dc->debug.disable_fpo_optimizations = true; } } }"
914----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50159/bad/driver.c----*scmi_debugfs_common_setup,"static struct scmi_debug_info *scmi_debugfs_common_setup(struct scmi_info *info) { char top_dir[16]; struct dentry *trans, *top_dentry; struct scmi_debug_info *dbg; const char *c_ptr = NULL; dbg = devm_kzalloc(info->dev, sizeof(*dbg), GFP_KERNEL); if (!dbg) return NULL; dbg->name = kstrdup(of_node_full_name(info->dev->of_node), GFP_KERNEL); if (!dbg->name) { devm_kfree(info->dev, dbg); return NULL; } of_property_read_string(info->dev->of_node, ""compatible"", &c_ptr); dbg->type = kstrdup(c_ptr, GFP_KERNEL); if (!dbg->type) { kfree(dbg->name); devm_kfree(info->dev, dbg); return NULL; } snprintf(top_dir, 16, ""%d"", info->id); top_dentry = debugfs_create_dir(top_dir, scmi_top_dentry); trans = debugfs_create_dir(""transport"", top_dentry); dbg->is_atomic = info->desc->atomic_enabled && is_transport_polling_capable(info->desc); debugfs_create_str(""instance_name"", 0400, top_dentry, (char **)&dbg->name); debugfs_create_u32(""atomic_threshold_us"", 0400, top_dentry, &info->atomic_threshold); debugfs_create_str(""type"", 0400, trans, (char **)&dbg->type); debugfs_create_bool(""is_atomic"", 0400, trans, &dbg->is_atomic); debugfs_create_u32(""max_rx_timeout_ms"", 0400, trans, (u32 *)&info->desc->max_rx_timeout_ms); debugfs_create_u32(""max_msg_size"", 0400, trans, (u32 *)&info->desc->max_msg_size); debugfs_create_u32(""tx_max_msg"", 0400, trans, (u32 *)&info->tx_minfo.max_msg); debugfs_create_u32(""rx_max_msg"", 0400, trans, (u32 *)&info->rx_minfo.max_msg); dbg->top_dentry = top_dentry; if (devm_add_action_or_reset(info->dev, <S2SV_StartVul> scmi_debugfs_common_cleanup, dbg)) { <S2SV_EndVul> <S2SV_StartVul> scmi_debugfs_common_cleanup(dbg); <S2SV_EndVul> return NULL; <S2SV_StartVul> } <S2SV_EndVul> return dbg; <S2SV_StartVul> } <S2SV_EndVul>","- scmi_debugfs_common_cleanup, dbg)) {
- scmi_debugfs_common_cleanup(dbg);
- }
- }
+ scmi_debugfs_common_cleanup, dbg))","static struct scmi_debug_info *scmi_debugfs_common_setup(struct scmi_info *info) { char top_dir[16]; struct dentry *trans, *top_dentry; struct scmi_debug_info *dbg; const char *c_ptr = NULL; dbg = devm_kzalloc(info->dev, sizeof(*dbg), GFP_KERNEL); if (!dbg) return NULL; dbg->name = kstrdup(of_node_full_name(info->dev->of_node), GFP_KERNEL); if (!dbg->name) { devm_kfree(info->dev, dbg); return NULL; } of_property_read_string(info->dev->of_node, ""compatible"", &c_ptr); dbg->type = kstrdup(c_ptr, GFP_KERNEL); if (!dbg->type) { kfree(dbg->name); devm_kfree(info->dev, dbg); return NULL; } snprintf(top_dir, 16, ""%d"", info->id); top_dentry = debugfs_create_dir(top_dir, scmi_top_dentry); trans = debugfs_create_dir(""transport"", top_dentry); dbg->is_atomic = info->desc->atomic_enabled && is_transport_polling_capable(info->desc); debugfs_create_str(""instance_name"", 0400, top_dentry, (char **)&dbg->name); debugfs_create_u32(""atomic_threshold_us"", 0400, top_dentry, &info->atomic_threshold); debugfs_create_str(""type"", 0400, trans, (char **)&dbg->type); debugfs_create_bool(""is_atomic"", 0400, trans, &dbg->is_atomic); debugfs_create_u32(""max_rx_timeout_ms"", 0400, trans, (u32 *)&info->desc->max_rx_timeout_ms); debugfs_create_u32(""max_msg_size"", 0400, trans, (u32 *)&info->desc->max_msg_size); debugfs_create_u32(""tx_max_msg"", 0400, trans, (u32 *)&info->tx_minfo.max_msg); debugfs_create_u32(""rx_max_msg"", 0400, trans, (u32 *)&info->rx_minfo.max_msg); dbg->top_dentry = top_dentry; if (devm_add_action_or_reset(info->dev, scmi_debugfs_common_cleanup, dbg)) return NULL; return dbg; }"
819----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50087/bad/tree-log.c----replay_one_name,"static noinline int replay_one_name(struct btrfs_trans_handle *trans, struct btrfs_root *root, struct btrfs_path *path, struct extent_buffer *eb, struct btrfs_dir_item *di, struct btrfs_key *key) { <S2SV_StartVul> struct fscrypt_str name; <S2SV_EndVul> struct btrfs_dir_item *dir_dst_di; struct btrfs_dir_item *index_dst_di; bool dir_dst_matches = false; bool index_dst_matches = false; struct btrfs_key log_key; struct btrfs_key search_key; struct inode *dir; u8 log_flags; bool exists; int ret; bool update_size = true; bool name_added = false; dir = read_one_inode(root, key->objectid); if (!dir) return -EIO; ret = read_alloc_one_name(eb, di + 1, btrfs_dir_name_len(eb, di), &name); if (ret) goto out; log_flags = btrfs_dir_flags(eb, di); btrfs_dir_item_key_to_cpu(eb, di, &log_key); ret = btrfs_lookup_inode(trans, root, path, &log_key, 0); btrfs_release_path(path); if (ret < 0) goto out; exists = (ret == 0); ret = 0; dir_dst_di = btrfs_lookup_dir_item(trans, root, path, key->objectid, &name, 1); if (IS_ERR(dir_dst_di)) { ret = PTR_ERR(dir_dst_di); goto out; } else if (dir_dst_di) { ret = delete_conflicting_dir_entry(trans, BTRFS_I(dir), path, dir_dst_di, &log_key, log_flags, exists); if (ret < 0) goto out; dir_dst_matches = (ret == 1); } btrfs_release_path(path); index_dst_di = btrfs_lookup_dir_index_item(trans, root, path, key->objectid, key->offset, &name, 1); if (IS_ERR(index_dst_di)) { ret = PTR_ERR(index_dst_di); goto out; } else if (index_dst_di) { ret = delete_conflicting_dir_entry(trans, BTRFS_I(dir), path, index_dst_di, &log_key, log_flags, exists); if (ret < 0) goto out; index_dst_matches = (ret == 1); } btrfs_release_path(path); if (dir_dst_matches && index_dst_matches) { ret = 0; update_size = false; goto out; } search_key.objectid = log_key.objectid; search_key.type = BTRFS_INODE_REF_KEY; search_key.offset = key->objectid; ret = backref_in_log(root->log_root, &search_key, 0, &name); if (ret < 0) { goto out; } else if (ret) { ret = 0; update_size = false; goto out; } search_key.objectid = log_key.objectid; search_key.type = BTRFS_INODE_EXTREF_KEY; search_key.offset = key->objectid; ret = backref_in_log(root->log_root, &search_key, key->objectid, &name); if (ret < 0) { goto out; } else if (ret) { ret = 0; update_size = false; goto out; } btrfs_release_path(path); ret = insert_one_name(trans, root, key->objectid, key->offset, &name, &log_key); if (ret && ret != -ENOENT && ret != -EEXIST) goto out; if (!ret) name_added = true; update_size = false; ret = 0; out: if (!ret && update_size) { btrfs_i_size_write(BTRFS_I(dir), dir->i_size + name.len * 2); ret = btrfs_update_inode(trans, BTRFS_I(dir)); } kfree(name.name); iput(dir); if (!ret && name_added) ret = 1; return ret; }","- struct fscrypt_str name;
+ struct fscrypt_str name = { 0 };","static noinline int replay_one_name(struct btrfs_trans_handle *trans, struct btrfs_root *root, struct btrfs_path *path, struct extent_buffer *eb, struct btrfs_dir_item *di, struct btrfs_key *key) { struct fscrypt_str name = { 0 }; struct btrfs_dir_item *dir_dst_di; struct btrfs_dir_item *index_dst_di; bool dir_dst_matches = false; bool index_dst_matches = false; struct btrfs_key log_key; struct btrfs_key search_key; struct inode *dir; u8 log_flags; bool exists; int ret; bool update_size = true; bool name_added = false; dir = read_one_inode(root, key->objectid); if (!dir) return -EIO; ret = read_alloc_one_name(eb, di + 1, btrfs_dir_name_len(eb, di), &name); if (ret) goto out; log_flags = btrfs_dir_flags(eb, di); btrfs_dir_item_key_to_cpu(eb, di, &log_key); ret = btrfs_lookup_inode(trans, root, path, &log_key, 0); btrfs_release_path(path); if (ret < 0) goto out; exists = (ret == 0); ret = 0; dir_dst_di = btrfs_lookup_dir_item(trans, root, path, key->objectid, &name, 1); if (IS_ERR(dir_dst_di)) { ret = PTR_ERR(dir_dst_di); goto out; } else if (dir_dst_di) { ret = delete_conflicting_dir_entry(trans, BTRFS_I(dir), path, dir_dst_di, &log_key, log_flags, exists); if (ret < 0) goto out; dir_dst_matches = (ret == 1); } btrfs_release_path(path); index_dst_di = btrfs_lookup_dir_index_item(trans, root, path, key->objectid, key->offset, &name, 1); if (IS_ERR(index_dst_di)) { ret = PTR_ERR(index_dst_di); goto out; } else if (index_dst_di) { ret = delete_conflicting_dir_entry(trans, BTRFS_I(dir), path, index_dst_di, &log_key, log_flags, exists); if (ret < 0) goto out; index_dst_matches = (ret == 1); } btrfs_release_path(path); if (dir_dst_matches && index_dst_matches) { ret = 0; update_size = false; goto out; } search_key.objectid = log_key.objectid; search_key.type = BTRFS_INODE_REF_KEY; search_key.offset = key->objectid; ret = backref_in_log(root->log_root, &search_key, 0, &name); if (ret < 0) { goto out; } else if (ret) { ret = 0; update_size = false; goto out; } search_key.objectid = log_key.objectid; search_key.type = BTRFS_INODE_EXTREF_KEY; search_key.offset = key->objectid; ret = backref_in_log(root->log_root, &search_key, key->objectid, &name); if (ret < 0) { goto out; } else if (ret) { ret = 0; update_size = false; goto out; } btrfs_release_path(path); ret = insert_one_name(trans, root, key->objectid, key->offset, &name, &log_key); if (ret && ret != -ENOENT && ret != -EEXIST) goto out; if (!ret) name_added = true; update_size = false; ret = 0; out: if (!ret && update_size) { btrfs_i_size_write(BTRFS_I(dir), dir->i_size + name.len * 2); ret = btrfs_update_inode(trans, BTRFS_I(dir)); } kfree(name.name); iput(dir); if (!ret && name_added) ret = 1; return ret; }"
879----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50132/bad/trace_uprobe.c----__trace_uprobe_create,"static int __trace_uprobe_create(int argc, const char **argv) { struct trace_uprobe *tu; const char *event = NULL, *group = UPROBE_EVENT_SYSTEM; char *arg, *filename, *rctr, *rctr_end, *tmp; char buf[MAX_EVENT_NAME_LEN]; char gbuf[MAX_EVENT_NAME_LEN]; enum probe_print_type ptype; struct path path; unsigned long offset, ref_ctr_offset; bool is_return = false; int i, ret; ref_ctr_offset = 0; switch (argv[0][0]) { case 'r': is_return = true; break; case 'p': break; default: return -ECANCELED; } if (argc < 2) return -ECANCELED; if (argv[0][1] == ':') event = &argv[0][2]; if (!strchr(argv[1], '/')) return -ECANCELED; filename = kstrdup(argv[1], GFP_KERNEL); if (!filename) return -ENOMEM; arg = strrchr(filename, ':'); if (!arg || !isdigit(arg[1])) { kfree(filename); return -ECANCELED; } trace_probe_log_init(""trace_uprobe"", argc, argv); trace_probe_log_set_index(1); *arg++ = '\0'; ret = kern_path(filename, LOOKUP_FOLLOW, &path); if (ret) { trace_probe_log_err(0, FILE_NOT_FOUND); kfree(filename); trace_probe_log_clear(); return ret; } if (!d_is_reg(path.dentry)) { trace_probe_log_err(0, NO_REGULAR_FILE); ret = -EINVAL; goto fail_address_parse; } rctr = strchr(arg, '('); if (rctr) { rctr_end = strchr(rctr, ')'); if (!rctr_end) { ret = -EINVAL; rctr_end = rctr + strlen(rctr); trace_probe_log_err(rctr_end - filename, REFCNT_OPEN_BRACE); goto fail_address_parse; } else if (rctr_end[1] != '\0') { ret = -EINVAL; trace_probe_log_err(rctr_end + 1 - filename, BAD_REFCNT_SUFFIX); goto fail_address_parse; } *rctr++ = '\0'; *rctr_end = '\0'; ret = kstrtoul(rctr, 0, &ref_ctr_offset); if (ret) { trace_probe_log_err(rctr - filename, BAD_REFCNT); goto fail_address_parse; } } tmp = strchr(arg, '%'); if (tmp) { if (!strcmp(tmp, ""%return"")) { *tmp = '\0'; is_return = true; } else { trace_probe_log_err(tmp - filename, BAD_ADDR_SUFFIX); ret = -EINVAL; goto fail_address_parse; } } ret = kstrtoul(arg, 0, &offset); if (ret) { trace_probe_log_err(arg - filename, BAD_UPROBE_OFFS); goto fail_address_parse; } trace_probe_log_set_index(0); if (event) { ret = traceprobe_parse_event_name(&event, &group, gbuf, event - argv[0]); if (ret) goto fail_address_parse; } if (!event) { char *tail; char *ptr; tail = kstrdup(kbasename(filename), GFP_KERNEL); if (!tail) { ret = -ENOMEM; goto fail_address_parse; } ptr = strpbrk(tail, "".-_""); if (ptr) *ptr = '\0'; snprintf(buf, MAX_EVENT_NAME_LEN, ""%c_%s_0x%lx"", 'p', tail, offset); event = buf; kfree(tail); } argc -= 2; argv += 2; tu = alloc_trace_uprobe(group, event, argc, is_return); if (IS_ERR(tu)) { ret = PTR_ERR(tu); WARN_ON_ONCE(ret != -ENOMEM); goto fail_address_parse; } tu->offset = offset; tu->ref_ctr_offset = ref_ctr_offset; tu->path = path; tu->filename = filename; <S2SV_StartVul> for (i = 0; i < argc && i < MAX_TRACE_ARGS; i++) { <S2SV_EndVul> struct traceprobe_parse_context ctx = { .flags = (is_return ? TPARG_FL_RETURN : 0) | TPARG_FL_USER, }; trace_probe_log_set_index(i + 2); ret = traceprobe_parse_probe_arg(&tu->tp, i, argv[i], &ctx); traceprobe_finish_parse(&ctx); if (ret) goto error; } ptype = is_ret_probe(tu) ? PROBE_PRINT_RETURN : PROBE_PRINT_NORMAL; ret = traceprobe_set_print_fmt(&tu->tp, ptype); if (ret < 0) goto error; ret = register_trace_uprobe(tu); if (!ret) goto out; error: free_trace_uprobe(tu); out: trace_probe_log_clear(); return ret; fail_address_parse: trace_probe_log_clear(); path_put(&path); kfree(filename); return ret; }","- for (i = 0; i < argc && i < MAX_TRACE_ARGS; i++) {
+ if (argc - 2 > MAX_TRACE_ARGS)
+ return -E2BIG;
+ for (i = 0; i < argc; i++) {","static int __trace_uprobe_create(int argc, const char **argv) { struct trace_uprobe *tu; const char *event = NULL, *group = UPROBE_EVENT_SYSTEM; char *arg, *filename, *rctr, *rctr_end, *tmp; char buf[MAX_EVENT_NAME_LEN]; char gbuf[MAX_EVENT_NAME_LEN]; enum probe_print_type ptype; struct path path; unsigned long offset, ref_ctr_offset; bool is_return = false; int i, ret; ref_ctr_offset = 0; switch (argv[0][0]) { case 'r': is_return = true; break; case 'p': break; default: return -ECANCELED; } if (argc < 2) return -ECANCELED; if (argc - 2 > MAX_TRACE_ARGS) return -E2BIG; if (argv[0][1] == ':') event = &argv[0][2]; if (!strchr(argv[1], '/')) return -ECANCELED; filename = kstrdup(argv[1], GFP_KERNEL); if (!filename) return -ENOMEM; arg = strrchr(filename, ':'); if (!arg || !isdigit(arg[1])) { kfree(filename); return -ECANCELED; } trace_probe_log_init(""trace_uprobe"", argc, argv); trace_probe_log_set_index(1); *arg++ = '\0'; ret = kern_path(filename, LOOKUP_FOLLOW, &path); if (ret) { trace_probe_log_err(0, FILE_NOT_FOUND); kfree(filename); trace_probe_log_clear(); return ret; } if (!d_is_reg(path.dentry)) { trace_probe_log_err(0, NO_REGULAR_FILE); ret = -EINVAL; goto fail_address_parse; } rctr = strchr(arg, '('); if (rctr) { rctr_end = strchr(rctr, ')'); if (!rctr_end) { ret = -EINVAL; rctr_end = rctr + strlen(rctr); trace_probe_log_err(rctr_end - filename, REFCNT_OPEN_BRACE); goto fail_address_parse; } else if (rctr_end[1] != '\0') { ret = -EINVAL; trace_probe_log_err(rctr_end + 1 - filename, BAD_REFCNT_SUFFIX); goto fail_address_parse; } *rctr++ = '\0'; *rctr_end = '\0'; ret = kstrtoul(rctr, 0, &ref_ctr_offset); if (ret) { trace_probe_log_err(rctr - filename, BAD_REFCNT); goto fail_address_parse; } } tmp = strchr(arg, '%'); if (tmp) { if (!strcmp(tmp, ""%return"")) { *tmp = '\0'; is_return = true; } else { trace_probe_log_err(tmp - filename, BAD_ADDR_SUFFIX); ret = -EINVAL; goto fail_address_parse; } } ret = kstrtoul(arg, 0, &offset); if (ret) { trace_probe_log_err(arg - filename, BAD_UPROBE_OFFS); goto fail_address_parse; } trace_probe_log_set_index(0); if (event) { ret = traceprobe_parse_event_name(&event, &group, gbuf, event - argv[0]); if (ret) goto fail_address_parse; } if (!event) { char *tail; char *ptr; tail = kstrdup(kbasename(filename), GFP_KERNEL); if (!tail) { ret = -ENOMEM; goto fail_address_parse; } ptr = strpbrk(tail, "".-_""); if (ptr) *ptr = '\0'; snprintf(buf, MAX_EVENT_NAME_LEN, ""%c_%s_0x%lx"", 'p', tail, offset); event = buf; kfree(tail); } argc -= 2; argv += 2; tu = alloc_trace_uprobe(group, event, argc, is_return); if (IS_ERR(tu)) { ret = PTR_ERR(tu); WARN_ON_ONCE(ret != -ENOMEM); goto fail_address_parse; } tu->offset = offset; tu->ref_ctr_offset = ref_ctr_offset; tu->path = path; tu->filename = filename; for (i = 0; i < argc; i++) { struct traceprobe_parse_context ctx = { .flags = (is_return ? TPARG_FL_RETURN : 0) | TPARG_FL_USER, }; trace_probe_log_set_index(i + 2); ret = traceprobe_parse_probe_arg(&tu->tp, i, argv[i], &ctx); traceprobe_finish_parse(&ctx); if (ret) goto error; } ptype = is_ret_probe(tu) ? PROBE_PRINT_RETURN : PROBE_PRINT_NORMAL; ret = traceprobe_set_print_fmt(&tu->tp, ptype); if (ret < 0) goto error; ret = register_trace_uprobe(tu); if (!ret) goto out; error: free_trace_uprobe(tu); out: trace_probe_log_clear(); return ret; fail_address_parse: trace_probe_log_clear(); path_put(&path); kfree(filename); return ret; }"
330----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46793/bad/bytcht_da7213.c----bytcht_da7213_probe,"static int bytcht_da7213_probe(struct platform_device *pdev) { struct snd_soc_card *card; struct snd_soc_acpi_mach *mach; const char *platform_name; struct acpi_device *adev; bool sof_parent; int dai_index = 0; int ret_val = 0; int i; mach = pdev->dev.platform_data; card = &bytcht_da7213_card; card->dev = &pdev->dev; for (i = 0; i < ARRAY_SIZE(dailink); i++) { <S2SV_StartVul> if (dailink[i].codecs->name && <S2SV_EndVul> !strcmp(dailink[i].codecs->name, ""i2c-DLGS7213:00"")) { dai_index = i; break; } } adev = acpi_dev_get_first_match_dev(mach->id, NULL, -1); if (adev) { snprintf(codec_name, sizeof(codec_name), ""i2c-%s"", acpi_dev_name(adev)); dailink[dai_index].codecs->name = codec_name; } acpi_dev_put(adev); platform_name = mach->mach_params.platform; ret_val = snd_soc_fixup_dai_links_platform_name(card, platform_name); if (ret_val) return ret_val; sof_parent = snd_soc_acpi_sof_parent(&pdev->dev); if (sof_parent) { bytcht_da7213_card.name = SOF_CARD_NAME; bytcht_da7213_card.driver_name = SOF_DRIVER_NAME; } else { bytcht_da7213_card.name = CARD_NAME; bytcht_da7213_card.driver_name = DRIVER_NAME; } if (sof_parent) pdev->dev.driver->pm = &snd_soc_pm_ops; ret_val = devm_snd_soc_register_card(&pdev->dev, card); if (ret_val) { dev_err(&pdev->dev, ""snd_soc_register_card failed %d\n"", ret_val); return ret_val; } platform_set_drvdata(pdev, card); return ret_val; }","- if (dailink[i].codecs->name &&
+ if (dailink[i].num_codecs &&","static int bytcht_da7213_probe(struct platform_device *pdev) { struct snd_soc_card *card; struct snd_soc_acpi_mach *mach; const char *platform_name; struct acpi_device *adev; bool sof_parent; int dai_index = 0; int ret_val = 0; int i; mach = pdev->dev.platform_data; card = &bytcht_da7213_card; card->dev = &pdev->dev; for (i = 0; i < ARRAY_SIZE(dailink); i++) { if (dailink[i].num_codecs && !strcmp(dailink[i].codecs->name, ""i2c-DLGS7213:00"")) { dai_index = i; break; } } adev = acpi_dev_get_first_match_dev(mach->id, NULL, -1); if (adev) { snprintf(codec_name, sizeof(codec_name), ""i2c-%s"", acpi_dev_name(adev)); dailink[dai_index].codecs->name = codec_name; } acpi_dev_put(adev); platform_name = mach->mach_params.platform; ret_val = snd_soc_fixup_dai_links_platform_name(card, platform_name); if (ret_val) return ret_val; sof_parent = snd_soc_acpi_sof_parent(&pdev->dev); if (sof_parent) { bytcht_da7213_card.name = SOF_CARD_NAME; bytcht_da7213_card.driver_name = SOF_DRIVER_NAME; } else { bytcht_da7213_card.name = CARD_NAME; bytcht_da7213_card.driver_name = DRIVER_NAME; } if (sof_parent) pdev->dev.driver->pm = &snd_soc_pm_ops; ret_val = devm_snd_soc_register_card(&pdev->dev, card); if (ret_val) { dev_err(&pdev->dev, ""snd_soc_register_card failed %d\n"", ret_val); return ret_val; } platform_set_drvdata(pdev, card); return ret_val; }"
4----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-43890/bad/tracing_map.c----tracing_map_clear,"void tracing_map_clear(struct tracing_map *map) { unsigned int i; <S2SV_StartVul> atomic_set(&map->next_elt, -1); <S2SV_EndVul> atomic64_set(&map->hits, 0); atomic64_set(&map->drops, 0); tracing_map_array_clear(map->map); for (i = 0; i < map->max_elts; i++) tracing_map_elt_clear(*(TRACING_MAP_ELT(map->elts, i))); }","- atomic_set(&map->next_elt, -1);
+ atomic_set(&map->next_elt, 0);","void tracing_map_clear(struct tracing_map *map) { unsigned int i; atomic_set(&map->next_elt, 0); atomic64_set(&map->hits, 0); atomic64_set(&map->drops, 0); tracing_map_array_clear(map->map); for (i = 0; i < map->max_elts; i++) tracing_map_elt_clear(*(TRACING_MAP_ELT(map->elts, i))); }"
465----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47706/bad/bfq-iosched.c----*bfq_init_rq,"static struct bfq_queue *bfq_init_rq(struct request *rq) { struct request_queue *q = rq->q; struct bio *bio = rq->bio; struct bfq_data *bfqd = q->elevator->elevator_data; struct bfq_io_cq *bic; const int is_sync = rq_is_sync(rq); struct bfq_queue *bfqq; bool new_queue = false; bool bfqq_already_existing = false, split = false; if (unlikely(!rq->elv.icq)) return NULL; if (rq->elv.priv[1]) return rq->elv.priv[1]; bic = icq_to_bic(rq->elv.icq); bfq_check_ioprio_change(bic, bio); bfq_bic_update_cgroup(bic, bio); bfqq = bfq_get_bfqq_handle_split(bfqd, bic, bio, false, is_sync, &new_queue); if (likely(!new_queue)) { if (bfq_bfqq_coop(bfqq) && bfq_bfqq_split_coop(bfqq)) { bfq_log_bfqq(bfqd, bfqq, ""breaking apart bfqq""); if (bfq_bfqq_in_large_burst(bfqq)) bic->saved_in_large_burst = true; bfqq = bfq_split_bfqq(bic, bfqq); split = true; if (!bfqq) bfqq = bfq_get_bfqq_handle_split(bfqd, bic, bio, true, is_sync, NULL); else bfqq_already_existing = true; } } bfqq->allocated++; bfqq->ref++; bfq_log_bfqq(bfqd, bfqq, ""get_request %p: bfqq %p, %d"", rq, bfqq, bfqq->ref); rq->elv.priv[0] = bic; rq->elv.priv[1] = bfqq; <S2SV_StartVul> if (likely(bfqq != &bfqd->oom_bfqq) && bfqq_process_refs(bfqq) == 1) { <S2SV_EndVul> bfqq->bic = bic; if (split) { bfq_bfqq_resume_state(bfqq, bfqd, bic, bfqq_already_existing); } } if (unlikely(bfq_bfqq_just_created(bfqq) && (bfqd->burst_size > 0 || bfq_tot_busy_queues(bfqd) == 0))) bfq_handle_burst(bfqd, bfqq); return bfqq; }","- if (likely(bfqq != &bfqd->oom_bfqq) && bfqq_process_refs(bfqq) == 1) {
+ if (likely(bfqq != &bfqd->oom_bfqq) && !bfqq->new_bfqq &&
+ bfqq_process_refs(bfqq) == 1) {","static struct bfq_queue *bfq_init_rq(struct request *rq) { struct request_queue *q = rq->q; struct bio *bio = rq->bio; struct bfq_data *bfqd = q->elevator->elevator_data; struct bfq_io_cq *bic; const int is_sync = rq_is_sync(rq); struct bfq_queue *bfqq; bool new_queue = false; bool bfqq_already_existing = false, split = false; if (unlikely(!rq->elv.icq)) return NULL; if (rq->elv.priv[1]) return rq->elv.priv[1]; bic = icq_to_bic(rq->elv.icq); bfq_check_ioprio_change(bic, bio); bfq_bic_update_cgroup(bic, bio); bfqq = bfq_get_bfqq_handle_split(bfqd, bic, bio, false, is_sync, &new_queue); if (likely(!new_queue)) { if (bfq_bfqq_coop(bfqq) && bfq_bfqq_split_coop(bfqq)) { bfq_log_bfqq(bfqd, bfqq, ""breaking apart bfqq""); if (bfq_bfqq_in_large_burst(bfqq)) bic->saved_in_large_burst = true; bfqq = bfq_split_bfqq(bic, bfqq); split = true; if (!bfqq) bfqq = bfq_get_bfqq_handle_split(bfqd, bic, bio, true, is_sync, NULL); else bfqq_already_existing = true; } } bfqq->allocated++; bfqq->ref++; bfq_log_bfqq(bfqd, bfqq, ""get_request %p: bfqq %p, %d"", rq, bfqq, bfqq->ref); rq->elv.priv[0] = bic; rq->elv.priv[1] = bfqq; if (likely(bfqq != &bfqd->oom_bfqq) && !bfqq->new_bfqq && bfqq_process_refs(bfqq) == 1) { bfqq->bic = bic; if (split) { bfq_bfqq_resume_state(bfqq, bfqd, bic, bfqq_already_existing); } } if (unlikely(bfq_bfqq_just_created(bfqq) && (bfqd->burst_size > 0 || bfq_tot_busy_queues(bfqd) == 0))) bfq_handle_burst(bfqd, bfqq); return bfqq; }"
1414----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56743/bad/nfslocalio.c----*nfs_open_local_fh,"struct nfsd_file *nfs_open_local_fh(nfs_uuid_t *uuid, struct rpc_clnt *rpc_clnt, const struct cred *cred, const struct nfs_fh *nfs_fh, const fmode_t fmode) { struct net *net; struct nfsd_file *localio; rcu_read_lock(); net = rcu_dereference(uuid->net); if (!net || !nfs_to->nfsd_serv_try_get(net)) { rcu_read_unlock(); return ERR_PTR(-ENXIO); } rcu_read_unlock(); localio = nfs_to->nfsd_open_local_fh(net, uuid->dom, rpc_clnt, cred, nfs_fh, fmode); <S2SV_StartVul> if (IS_ERR(localio)) { <S2SV_EndVul> <S2SV_StartVul> rcu_read_lock(); <S2SV_EndVul> <S2SV_StartVul> nfs_to->nfsd_serv_put(net); <S2SV_EndVul> <S2SV_StartVul> rcu_read_unlock(); <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> return localio; <S2SV_StartVul> } <S2SV_EndVul>","- if (IS_ERR(localio)) {
- rcu_read_lock();
- nfs_to->nfsd_serv_put(net);
- rcu_read_unlock();
- }
- }
+ if (IS_ERR(localio))
+ nfs_to_nfsd_net_put(net);","struct nfsd_file *nfs_open_local_fh(nfs_uuid_t *uuid, struct rpc_clnt *rpc_clnt, const struct cred *cred, const struct nfs_fh *nfs_fh, const fmode_t fmode) { struct net *net; struct nfsd_file *localio; rcu_read_lock(); net = rcu_dereference(uuid->net); if (!net || !nfs_to->nfsd_serv_try_get(net)) { rcu_read_unlock(); return ERR_PTR(-ENXIO); } rcu_read_unlock(); localio = nfs_to->nfsd_open_local_fh(net, uuid->dom, rpc_clnt, cred, nfs_fh, fmode); if (IS_ERR(localio)) nfs_to_nfsd_net_put(net); return localio; }"
1489----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-57914/bad/tcpci.c----tcpci_init,"static int tcpci_init(struct tcpc_dev *tcpc) { struct tcpci *tcpci = tcpc_to_tcpci(tcpc); unsigned long timeout = jiffies + msecs_to_jiffies(2000); unsigned int reg; int ret; while (time_before_eq(jiffies, timeout)) { ret = regmap_read(tcpci->regmap, TCPC_POWER_STATUS, &reg); if (ret < 0) return ret; if (!(reg & TCPC_POWER_STATUS_UNINIT)) break; usleep_range(10000, 20000); } if (time_after(jiffies, timeout)) return -ETIMEDOUT; ret = tcpci_write16(tcpci, TCPC_FAULT_STATUS, TCPC_FAULT_STATUS_ALL_REG_RST_TO_DEFAULT); if (ret < 0) return ret; if (tcpci->data->init) { ret = tcpci->data->init(tcpci, tcpci->data); if (ret < 0) return ret; } ret = tcpci_write16(tcpci, TCPC_ALERT, 0xffff); if (ret < 0) return ret; if (tcpci->controls_vbus) reg = TCPC_POWER_STATUS_VBUS_PRES; else reg = 0; ret = regmap_write(tcpci->regmap, TCPC_POWER_STATUS_MASK, reg); if (ret < 0) return ret; ret = regmap_write(tcpci->regmap, TCPC_COMMAND, TCPC_CMD_ENABLE_VBUS_DETECT); if (ret < 0) return ret; reg = TCPC_ALERT_TX_SUCCESS | TCPC_ALERT_TX_FAILED | TCPC_ALERT_TX_DISCARDED | TCPC_ALERT_RX_STATUS | TCPC_ALERT_RX_HARD_RST | TCPC_ALERT_CC_STATUS; if (tcpci->controls_vbus) reg |= TCPC_ALERT_POWER_STATUS; if (tcpci->data->vbus_vsafe0v) { reg |= TCPC_ALERT_EXTENDED_STATUS; ret = regmap_write(tcpci->regmap, TCPC_EXTENDED_STATUS_MASK, TCPC_EXTENDED_STATUS_VSAFE0V); if (ret < 0) return ret; } tcpci->alert_mask = reg; <S2SV_StartVul> return tcpci_write16(tcpci, TCPC_ALERT_MASK, reg); <S2SV_EndVul> }","- return tcpci_write16(tcpci, TCPC_ALERT_MASK, reg);
+ return 0;","static int tcpci_init(struct tcpc_dev *tcpc) { struct tcpci *tcpci = tcpc_to_tcpci(tcpc); unsigned long timeout = jiffies + msecs_to_jiffies(2000); unsigned int reg; int ret; while (time_before_eq(jiffies, timeout)) { ret = regmap_read(tcpci->regmap, TCPC_POWER_STATUS, &reg); if (ret < 0) return ret; if (!(reg & TCPC_POWER_STATUS_UNINIT)) break; usleep_range(10000, 20000); } if (time_after(jiffies, timeout)) return -ETIMEDOUT; ret = tcpci_write16(tcpci, TCPC_FAULT_STATUS, TCPC_FAULT_STATUS_ALL_REG_RST_TO_DEFAULT); if (ret < 0) return ret; if (tcpci->data->init) { ret = tcpci->data->init(tcpci, tcpci->data); if (ret < 0) return ret; } ret = tcpci_write16(tcpci, TCPC_ALERT, 0xffff); if (ret < 0) return ret; if (tcpci->controls_vbus) reg = TCPC_POWER_STATUS_VBUS_PRES; else reg = 0; ret = regmap_write(tcpci->regmap, TCPC_POWER_STATUS_MASK, reg); if (ret < 0) return ret; ret = regmap_write(tcpci->regmap, TCPC_COMMAND, TCPC_CMD_ENABLE_VBUS_DETECT); if (ret < 0) return ret; reg = TCPC_ALERT_TX_SUCCESS | TCPC_ALERT_TX_FAILED | TCPC_ALERT_TX_DISCARDED | TCPC_ALERT_RX_STATUS | TCPC_ALERT_RX_HARD_RST | TCPC_ALERT_CC_STATUS; if (tcpci->controls_vbus) reg |= TCPC_ALERT_POWER_STATUS; if (tcpci->data->vbus_vsafe0v) { reg |= TCPC_ALERT_EXTENDED_STATUS; ret = regmap_write(tcpci->regmap, TCPC_EXTENDED_STATUS_MASK, TCPC_EXTENDED_STATUS_VSAFE0V); if (ret < 0) return ret; } tcpci->alert_mask = reg; return 0; }"
702----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49988/bad/oplock.c----smb_grant_oplock,"int smb_grant_oplock(struct ksmbd_work *work, int req_op_level, u64 pid, struct ksmbd_file *fp, __u16 tid, struct lease_ctx_info *lctx, int share_ret) { struct ksmbd_session *sess = work->sess; int err = 0; struct oplock_info *opinfo = NULL, *prev_opinfo = NULL; struct ksmbd_inode *ci = fp->f_ci; bool prev_op_has_lease; __le32 prev_op_state = 0; if (S_ISDIR(file_inode(fp->filp)->i_mode)) { if (!lctx || lctx->version != 2 || (lctx->flags != SMB2_LEASE_FLAG_PARENT_LEASE_KEY_SET_LE && !lctx->epoch)) return 0; } opinfo = alloc_opinfo(work, pid, tid); if (!opinfo) return -ENOMEM; if (lctx) { err = alloc_lease(opinfo, lctx); if (err) goto err_out; opinfo->is_lease = 1; } if (!opinfo_count(fp)) goto set_lev; if (fp->attrib_only && fp->cdoption != FILE_OVERWRITE_IF_LE && fp->cdoption != FILE_OVERWRITE_LE && fp->cdoption != FILE_SUPERSEDE_LE) { req_op_level = SMB2_OPLOCK_LEVEL_NONE; goto set_lev; } if (lctx) { struct oplock_info *m_opinfo; m_opinfo = same_client_has_lease(ci, sess->ClientGUID, lctx); if (m_opinfo) { copy_lease(m_opinfo, opinfo); if (atomic_read(&m_opinfo->breaking_cnt)) opinfo->o_lease->flags = SMB2_LEASE_FLAG_BREAK_IN_PROGRESS_LE; goto out; } } prev_opinfo = opinfo_get_list(ci); if (!prev_opinfo || (prev_opinfo->level == SMB2_OPLOCK_LEVEL_NONE && lctx)) { <S2SV_StartVul> opinfo_conn_put(prev_opinfo); <S2SV_EndVul> goto set_lev; <S2SV_StartVul> } <S2SV_EndVul> prev_op_has_lease = prev_opinfo->is_lease; if (prev_op_has_lease) prev_op_state = prev_opinfo->o_lease->state; if (share_ret < 0 && prev_opinfo->level == SMB2_OPLOCK_LEVEL_EXCLUSIVE) { err = share_ret; <S2SV_StartVul> opinfo_conn_put(prev_opinfo); <S2SV_EndVul> goto err_out; <S2SV_StartVul> } <S2SV_EndVul> if (prev_opinfo->level != SMB2_OPLOCK_LEVEL_BATCH && prev_opinfo->level != SMB2_OPLOCK_LEVEL_EXCLUSIVE) { <S2SV_StartVul> opinfo_conn_put(prev_opinfo); <S2SV_EndVul> goto op_break_not_needed; <S2SV_StartVul> } <S2SV_EndVul> list_add(&work->interim_entry, &prev_opinfo->interim_list); err = oplock_break(prev_opinfo, SMB2_OPLOCK_LEVEL_II); <S2SV_StartVul> opinfo_conn_put(prev_opinfo); <S2SV_EndVul> if (err == -ENOENT) goto set_lev; else if (err < 0) goto err_out; op_break_not_needed: if (share_ret < 0) { err = share_ret; goto err_out; } if (req_op_level != SMB2_OPLOCK_LEVEL_NONE) req_op_level = SMB2_OPLOCK_LEVEL_II; if (prev_op_has_lease && !lctx) if (prev_op_state & SMB2_LEASE_HANDLE_CACHING_LE) req_op_level = SMB2_OPLOCK_LEVEL_NONE; if (!prev_op_has_lease && lctx) { req_op_level = SMB2_OPLOCK_LEVEL_II; lctx->req_state = SMB2_LEASE_READ_CACHING_LE; } set_lev: set_oplock_level(opinfo, req_op_level, lctx); out: rcu_assign_pointer(fp->f_opinfo, opinfo); opinfo->o_fp = fp; opinfo_count_inc(fp); opinfo_add(opinfo); if (opinfo->is_lease) { err = add_lease_global_list(opinfo); if (err) goto err_out; } return 0; err_out: free_opinfo(opinfo); return err; }","- opinfo_conn_put(prev_opinfo);
- }
- opinfo_conn_put(prev_opinfo);
- }
- opinfo_conn_put(prev_opinfo);
- }
- opinfo_conn_put(prev_opinfo);
+ opinfo_put(prev_opinfo);
+ opinfo_put(prev_opinfo);
+ opinfo_put(prev_opinfo);
+ opinfo_put(prev_opinfo);","int smb_grant_oplock(struct ksmbd_work *work, int req_op_level, u64 pid, struct ksmbd_file *fp, __u16 tid, struct lease_ctx_info *lctx, int share_ret) { struct ksmbd_session *sess = work->sess; int err = 0; struct oplock_info *opinfo = NULL, *prev_opinfo = NULL; struct ksmbd_inode *ci = fp->f_ci; bool prev_op_has_lease; __le32 prev_op_state = 0; if (S_ISDIR(file_inode(fp->filp)->i_mode)) { if (!lctx || lctx->version != 2 || (lctx->flags != SMB2_LEASE_FLAG_PARENT_LEASE_KEY_SET_LE && !lctx->epoch)) return 0; } opinfo = alloc_opinfo(work, pid, tid); if (!opinfo) return -ENOMEM; if (lctx) { err = alloc_lease(opinfo, lctx); if (err) goto err_out; opinfo->is_lease = 1; } if (!opinfo_count(fp)) goto set_lev; if (fp->attrib_only && fp->cdoption != FILE_OVERWRITE_IF_LE && fp->cdoption != FILE_OVERWRITE_LE && fp->cdoption != FILE_SUPERSEDE_LE) { req_op_level = SMB2_OPLOCK_LEVEL_NONE; goto set_lev; } if (lctx) { struct oplock_info *m_opinfo; m_opinfo = same_client_has_lease(ci, sess->ClientGUID, lctx); if (m_opinfo) { copy_lease(m_opinfo, opinfo); if (atomic_read(&m_opinfo->breaking_cnt)) opinfo->o_lease->flags = SMB2_LEASE_FLAG_BREAK_IN_PROGRESS_LE; goto out; } } prev_opinfo = opinfo_get_list(ci); if (!prev_opinfo || (prev_opinfo->level == SMB2_OPLOCK_LEVEL_NONE && lctx)) { opinfo_put(prev_opinfo); goto set_lev; } prev_op_has_lease = prev_opinfo->is_lease; if (prev_op_has_lease) prev_op_state = prev_opinfo->o_lease->state; if (share_ret < 0 && prev_opinfo->level == SMB2_OPLOCK_LEVEL_EXCLUSIVE) { err = share_ret; opinfo_put(prev_opinfo); goto err_out; } if (prev_opinfo->level != SMB2_OPLOCK_LEVEL_BATCH && prev_opinfo->level != SMB2_OPLOCK_LEVEL_EXCLUSIVE) { opinfo_put(prev_opinfo); goto op_break_not_needed; } list_add(&work->interim_entry, &prev_opinfo->interim_list); err = oplock_break(prev_opinfo, SMB2_OPLOCK_LEVEL_II); opinfo_put(prev_opinfo); if (err == -ENOENT) goto set_lev; else if (err < 0) goto err_out; op_break_not_needed: if (share_ret < 0) { err = share_ret; goto err_out; } if (req_op_level != SMB2_OPLOCK_LEVEL_NONE) req_op_level = SMB2_OPLOCK_LEVEL_II; if (prev_op_has_lease && !lctx) if (prev_op_state & SMB2_LEASE_HANDLE_CACHING_LE) req_op_level = SMB2_OPLOCK_LEVEL_NONE; if (!prev_op_has_lease && lctx) { req_op_level = SMB2_OPLOCK_LEVEL_II; lctx->req_state = SMB2_LEASE_READ_CACHING_LE; } set_lev: set_oplock_level(opinfo, req_op_level, lctx); out: rcu_assign_pointer(fp->f_opinfo, opinfo); opinfo->o_fp = fp; opinfo_count_inc(fp); opinfo_add(opinfo); if (opinfo->is_lease) { err = add_lease_global_list(opinfo); if (err) goto err_out; } return 0; err_out: free_opinfo(opinfo); return err; }"
92----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44961/bad/amdgpu_job.c----amdgpu_job_prepare_job,"amdgpu_job_prepare_job(struct drm_sched_job *sched_job, struct drm_sched_entity *s_entity) { struct amdgpu_ring *ring = to_amdgpu_ring(s_entity->rq->sched); struct amdgpu_job *job = to_amdgpu_job(sched_job); struct dma_fence *fence = NULL; int r; r = drm_sched_entity_error(s_entity); <S2SV_StartVul> if (r && r != -ENODATA) <S2SV_EndVul> goto error; if (!fence && job->gang_submit) fence = amdgpu_device_switch_gang(ring->adev, job->gang_submit); while (!fence && job->vm && !job->vmid) { r = amdgpu_vmid_grab(job->vm, ring, job, &fence); if (r) { DRM_ERROR(""Error getting VM ID (%d)\n"", r); goto error; } } return fence; error: dma_fence_set_error(&job->base.s_fence->finished, r); return NULL; }","- if (r && r != -ENODATA)
+ if (r)","amdgpu_job_prepare_job(struct drm_sched_job *sched_job, struct drm_sched_entity *s_entity) { struct amdgpu_ring *ring = to_amdgpu_ring(s_entity->rq->sched); struct amdgpu_job *job = to_amdgpu_job(sched_job); struct dma_fence *fence = NULL; int r; r = drm_sched_entity_error(s_entity); if (r) goto error; if (!fence && job->gang_submit) fence = amdgpu_device_switch_gang(ring->adev, job->gang_submit); while (!fence && job->vm && !job->vmid) { r = amdgpu_vmid_grab(job->vm, ring, job, &fence); if (r) { DRM_ERROR(""Error getting VM ID (%d)\n"", r); goto error; } } return fence; error: dma_fence_set_error(&job->base.s_fence->finished, r); return NULL; }"
138----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44997/bad/mtk_wed.c----mtk_wed_setup_tc_block,"mtk_wed_setup_tc_block(struct mtk_wed_hw *hw, struct net_device *dev, struct flow_block_offload *f) { struct mtk_wed_flow_block_priv *priv; <S2SV_StartVul> static LIST_HEAD(block_cb_list); <S2SV_EndVul> struct flow_block_cb *block_cb; <S2SV_StartVul> struct mtk_eth *eth = hw->eth; <S2SV_EndVul> flow_setup_cb_t *cb; if (!eth->soc->offload_version) return -EOPNOTSUPP; if (f->binder_type != FLOW_BLOCK_BINDER_TYPE_CLSACT_INGRESS) return -EOPNOTSUPP; cb = mtk_wed_setup_tc_block_cb; f->driver_block_list = &block_cb_list; switch (f->command) { case FLOW_BLOCK_BIND: block_cb = flow_block_cb_lookup(f->block, cb, dev); if (block_cb) { flow_block_cb_incref(block_cb); return 0; } priv = kzalloc(sizeof(*priv), GFP_KERNEL); if (!priv) return -ENOMEM; priv->hw = hw; priv->dev = dev; block_cb = flow_block_cb_alloc(cb, dev, priv, NULL); if (IS_ERR(block_cb)) { kfree(priv); return PTR_ERR(block_cb); } flow_block_cb_incref(block_cb); flow_block_cb_add(block_cb, f); list_add_tail(&block_cb->driver_list, &block_cb_list); return 0; case FLOW_BLOCK_UNBIND: block_cb = flow_block_cb_lookup(f->block, cb, dev); if (!block_cb) return -ENOENT; if (!flow_block_cb_decref(block_cb)) { flow_block_cb_remove(block_cb, f); list_del(&block_cb->driver_list); kfree(block_cb->cb_priv); } return 0; default: return -EOPNOTSUPP; } }","- static LIST_HEAD(block_cb_list);
- struct mtk_eth *eth = hw->eth;
+ static LIST_HEAD(block_cb_list);
+ struct mtk_eth *eth = hw->eth;
+ if (f->binder_type != FLOW_BLOCK_BINDER_TYPE_CLSACT_INGRESS)","mtk_wed_setup_tc_block(struct mtk_wed_hw *hw, struct net_device *dev, struct flow_block_offload *f) { struct mtk_wed_flow_block_priv *priv; static LIST_HEAD(block_cb_list); struct flow_block_cb *block_cb; struct mtk_eth *eth = hw->eth; flow_setup_cb_t *cb; if (!eth->soc->offload_version) return -EOPNOTSUPP; if (f->binder_type != FLOW_BLOCK_BINDER_TYPE_CLSACT_INGRESS) return -EOPNOTSUPP; cb = mtk_wed_setup_tc_block_cb; f->driver_block_list = &block_cb_list; switch (f->command) { case FLOW_BLOCK_BIND: block_cb = flow_block_cb_lookup(f->block, cb, dev); if (block_cb) { flow_block_cb_incref(block_cb); return 0; } priv = kzalloc(sizeof(*priv), GFP_KERNEL); if (!priv) return -ENOMEM; priv->hw = hw; priv->dev = dev; block_cb = flow_block_cb_alloc(cb, dev, priv, NULL); if (IS_ERR(block_cb)) { kfree(priv); return PTR_ERR(block_cb); } flow_block_cb_incref(block_cb); flow_block_cb_add(block_cb, f); list_add_tail(&block_cb->driver_list, &block_cb_list); return 0; case FLOW_BLOCK_UNBIND: block_cb = flow_block_cb_lookup(f->block, cb, dev); if (!block_cb) return -ENOENT; if (!flow_block_cb_decref(block_cb)) { flow_block_cb_remove(block_cb, f); list_del(&block_cb->driver_list); kfree(block_cb->cb_priv); block_cb->cb_priv = NULL; } return 0; default: return -EOPNOTSUPP; } }"
530----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49852/bad/efc_nport.c----efc_nport_vport_del,"efc_nport_vport_del(struct efc *efc, struct efc_domain *domain, u64 wwpn, uint64_t wwnn) { struct efc_nport *nport; struct efc_vport *vport; struct efc_vport *next; unsigned long flags = 0; spin_lock_irqsave(&efc->vport_lock, flags); list_for_each_entry_safe(vport, next, &efc->vport_list, list_entry) { if (vport->wwpn == wwpn && vport->wwnn == wwnn) { list_del(&vport->list_entry); kfree(vport); break; } } spin_unlock_irqrestore(&efc->vport_lock, flags); if (!domain) { return 0; } spin_lock_irqsave(&efc->lock, flags); list_for_each_entry(nport, &domain->nport_list, list_entry) { if (nport->wwpn == wwpn && nport->wwnn == wwnn) { <S2SV_StartVul> kref_put(&nport->ref, nport->release); <S2SV_EndVul> efc_sm_post_event(&nport->sm, EFC_EVT_SHUTDOWN, NULL); break; } } spin_unlock_irqrestore(&efc->lock, flags); return 0; }","- kref_put(&nport->ref, nport->release);
+ kref_put(&nport->ref, nport->release);","efc_nport_vport_del(struct efc *efc, struct efc_domain *domain, u64 wwpn, uint64_t wwnn) { struct efc_nport *nport; struct efc_vport *vport; struct efc_vport *next; unsigned long flags = 0; spin_lock_irqsave(&efc->vport_lock, flags); list_for_each_entry_safe(vport, next, &efc->vport_list, list_entry) { if (vport->wwpn == wwpn && vport->wwnn == wwnn) { list_del(&vport->list_entry); kfree(vport); break; } } spin_unlock_irqrestore(&efc->vport_lock, flags); if (!domain) { return 0; } spin_lock_irqsave(&efc->lock, flags); list_for_each_entry(nport, &domain->nport_list, list_entry) { if (nport->wwpn == wwpn && nport->wwnn == wwnn) { efc_sm_post_event(&nport->sm, EFC_EVT_SHUTDOWN, NULL); kref_put(&nport->ref, nport->release); break; } } spin_unlock_irqrestore(&efc->lock, flags); return 0; }"
1283----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56580/bad/camss.c----camss_configure_pd,"static int camss_configure_pd(struct camss *camss) { const struct camss_resources *res = camss->res; struct device *dev = camss->dev; int vfepd_num; int i; int ret; camss->genpd_num = of_count_phandle_with_args(dev->of_node, ""power-domains"", ""#power-domain-cells""); if (camss->genpd_num < 0) { dev_err(dev, ""Power domains are not defined for camss\n""); return camss->genpd_num; } if (camss->genpd_num == 1) return 0; for (vfepd_num = i = 0; i < camss->res->vfe_num; i++) { if (res->vfe_res[i].vfe.has_pd) vfepd_num++; } if (!(camss->genpd_num > vfepd_num)) return 0; if (camss->res->pd_name) { camss->genpd = dev_pm_domain_attach_by_name(camss->dev, camss->res->pd_name); <S2SV_StartVul> if (IS_ERR(camss->genpd)) { <S2SV_EndVul> <S2SV_StartVul> ret = PTR_ERR(camss->genpd); <S2SV_EndVul> <S2SV_StartVul> goto fail_pm; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> if (!camss->genpd) { camss->genpd = dev_pm_domain_attach_by_id(camss->dev, camss->genpd_num - 1); <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> if (IS_ERR_OR_NULL(camss->genpd)) { <S2SV_EndVul> <S2SV_StartVul> if (!camss->genpd) <S2SV_EndVul> <S2SV_StartVul> ret = -ENODEV; <S2SV_EndVul> <S2SV_StartVul> else <S2SV_EndVul> <S2SV_StartVul> ret = PTR_ERR(camss->genpd); <S2SV_EndVul> <S2SV_StartVul> goto fail_pm; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> camss->genpd_link = device_link_add(camss->dev, camss->genpd, DL_FLAG_STATELESS | DL_FLAG_PM_RUNTIME | DL_FLAG_RPM_ACTIVE); if (!camss->genpd_link) { ret = -EINVAL; goto fail_pm; } return 0; fail_pm: dev_pm_domain_detach(camss->genpd, true); return ret; }","- if (IS_ERR(camss->genpd)) {
- ret = PTR_ERR(camss->genpd);
- goto fail_pm;
- }
- }
- }
- if (IS_ERR_OR_NULL(camss->genpd)) {
- if (!camss->genpd)
- ret = -ENODEV;
- else
- ret = PTR_ERR(camss->genpd);
- goto fail_pm;
- }
+ if (IS_ERR(camss->genpd))
+ return PTR_ERR(camss->genpd);
+ if (IS_ERR(camss->genpd))
+ return PTR_ERR(camss->genpd);
+ if (!camss->genpd)
+ return -ENODEV;","static int camss_configure_pd(struct camss *camss) { const struct camss_resources *res = camss->res; struct device *dev = camss->dev; int vfepd_num; int i; int ret; camss->genpd_num = of_count_phandle_with_args(dev->of_node, ""power-domains"", ""#power-domain-cells""); if (camss->genpd_num < 0) { dev_err(dev, ""Power domains are not defined for camss\n""); return camss->genpd_num; } if (camss->genpd_num == 1) return 0; for (vfepd_num = i = 0; i < camss->res->vfe_num; i++) { if (res->vfe_res[i].vfe.has_pd) vfepd_num++; } if (!(camss->genpd_num > vfepd_num)) return 0; if (camss->res->pd_name) { camss->genpd = dev_pm_domain_attach_by_name(camss->dev, camss->res->pd_name); if (IS_ERR(camss->genpd)) return PTR_ERR(camss->genpd); } if (!camss->genpd) { camss->genpd = dev_pm_domain_attach_by_id(camss->dev, camss->genpd_num - 1); if (IS_ERR(camss->genpd)) return PTR_ERR(camss->genpd); } if (!camss->genpd) return -ENODEV; camss->genpd_link = device_link_add(camss->dev, camss->genpd, DL_FLAG_STATELESS | DL_FLAG_PM_RUNTIME | DL_FLAG_RPM_ACTIVE); if (!camss->genpd_link) { ret = -EINVAL; goto fail_pm; } return 0; fail_pm: dev_pm_domain_detach(camss->genpd, true); return ret; }"
331----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46793/bad/bytcht_es8316.c----snd_byt_cht_es8316_mc_probe,"static int snd_byt_cht_es8316_mc_probe(struct platform_device *pdev) { struct device *dev = &pdev->dev; static const char * const mic_name[] = { ""in1"", ""in2"" }; struct snd_soc_acpi_mach *mach = dev_get_platdata(dev); struct property_entry props[MAX_NO_PROPS] = {}; struct byt_cht_es8316_private *priv; const struct dmi_system_id *dmi_id; struct fwnode_handle *fwnode; bool sof_parent, is_bytcr; const char *platform_name; struct acpi_device *adev; struct device *codec_dev; unsigned int cnt = 0; int dai_index = 0; int i; int ret = 0; priv = devm_kzalloc(dev, sizeof(*priv), GFP_KERNEL); if (!priv) return -ENOMEM; for (i = 0; i < ARRAY_SIZE(byt_cht_es8316_dais); i++) { <S2SV_StartVul> if (byt_cht_es8316_dais[i].codecs->name && <S2SV_EndVul> !strcmp(byt_cht_es8316_dais[i].codecs->name, ""i2c-ESSX8316:00"")) { dai_index = i; break; } } adev = acpi_dev_get_first_match_dev(mach->id, NULL, -1); if (adev) { snprintf(codec_name, sizeof(codec_name), ""i2c-%s"", acpi_dev_name(adev)); byt_cht_es8316_dais[dai_index].codecs->name = codec_name; } else { dev_err(dev, ""Error cannot find '%s' dev\n"", mach->id); return -ENXIO; } codec_dev = acpi_get_first_physical_node(adev); acpi_dev_put(adev); if (!codec_dev) return -EPROBE_DEFER; priv->codec_dev = get_device(codec_dev); byt_cht_es8316_card.dev = dev; platform_name = mach->mach_params.platform; ret = snd_soc_fixup_dai_links_platform_name(&byt_cht_es8316_card, platform_name); if (ret) { put_device(codec_dev); return ret; } es83xx_dsm_dump(priv->codec_dev); is_bytcr = soc_intel_is_byt() && mach->mach_params.acpi_ipc_irq_index == 0; dmi_id = dmi_first_match(byt_cht_es8316_quirk_table); if (dmi_id) { quirk = (unsigned long)dmi_id->driver_data; } else if (!byt_cht_es8316_get_quirks_from_dsm(priv, is_bytcr)) { dev_info(dev, ""Using ACPI DSM info for quirks\n""); } else if (is_bytcr) { quirk = BYT_CHT_ES8316_SSP0 | BYT_CHT_ES8316_INTMIC_IN2_MAP | BYT_CHT_ES8316_MONO_SPEAKER; } else { quirk = BYT_CHT_ES8316_INTMIC_IN1_MAP | BYT_CHT_ES8316_MONO_SPEAKER; } if (quirk_override != -1) { dev_info(dev, ""Overriding quirk 0x%lx => 0x%x\n"", quirk, quirk_override); quirk = quirk_override; } log_quirks(dev); if (quirk & BYT_CHT_ES8316_SSP0) byt_cht_es8316_dais[dai_index].cpus->dai_name = ""ssp0-port""; priv->mclk = devm_clk_get(dev, ""pmc_plt_clk_3""); if (IS_ERR(priv->mclk)) { put_device(codec_dev); return dev_err_probe(dev, PTR_ERR(priv->mclk), ""clk_get pmc_plt_clk_3 failed\n""); } if (quirk & BYT_CHT_ES8316_JD_INVERTED) props[cnt++] = PROPERTY_ENTRY_BOOL(""everest,jack-detect-inverted""); if (cnt) { fwnode = fwnode_create_software_node(props, NULL); if (IS_ERR(fwnode)) { put_device(codec_dev); return PTR_ERR(fwnode); } ret = device_add_software_node(codec_dev, to_software_node(fwnode)); fwnode_handle_put(fwnode); if (ret) { put_device(codec_dev); return ret; } } devm_acpi_dev_add_driver_gpios(codec_dev, byt_cht_es8316_gpios); priv->speaker_en_gpio = gpiod_get_optional(codec_dev, ""speaker-enable"", GPIOD_OUT_LOW | GPIOD_FLAGS_BIT_NONEXCLUSIVE); if (IS_ERR(priv->speaker_en_gpio)) { ret = dev_err_probe(dev, PTR_ERR(priv->speaker_en_gpio), ""get speaker GPIO failed\n""); goto err_put_codec; } snprintf(components_string, sizeof(components_string), ""cfg-spk:%s cfg-mic:%s"", (quirk & BYT_CHT_ES8316_MONO_SPEAKER) ? ""1"" : ""2"", mic_name[BYT_CHT_ES8316_MAP(quirk)]); byt_cht_es8316_card.components = components_string; #if !IS_ENABLED(CONFIG_SND_SOC_INTEL_USER_FRIENDLY_LONG_NAMES) snprintf(long_name, sizeof(long_name), ""bytcht-es8316-%s-spk-%s-mic"", (quirk & BYT_CHT_ES8316_MONO_SPEAKER) ? ""mono"" : ""stereo"", mic_name[BYT_CHT_ES8316_MAP(quirk)]); byt_cht_es8316_card.long_name = long_name; #endif sof_parent = snd_soc_acpi_sof_parent(dev); if (sof_parent) { byt_cht_es8316_card.name = SOF_CARD_NAME; byt_cht_es8316_card.driver_name = SOF_DRIVER_NAME; } else { byt_cht_es8316_card.name = CARD_NAME; byt_cht_es8316_card.driver_name = DRIVER_NAME; } if (sof_parent) dev->driver->pm = &snd_soc_pm_ops; snd_soc_card_set_drvdata(&byt_cht_es8316_card, priv); ret = devm_snd_soc_register_card(dev, &byt_cht_es8316_card); if (ret) { gpiod_put(priv->speaker_en_gpio); dev_err(dev, ""snd_soc_register_card failed: %d\n"", ret); goto err_put_codec; } platform_set_drvdata(pdev, &byt_cht_es8316_card); return 0; err_put_codec: device_remove_software_node(priv->codec_dev); put_device(priv->codec_dev); return ret; }","- if (byt_cht_es8316_dais[i].codecs->name &&
+ if (byt_cht_es8316_dais[i].num_codecs &&","static int snd_byt_cht_es8316_mc_probe(struct platform_device *pdev) { struct device *dev = &pdev->dev; static const char * const mic_name[] = { ""in1"", ""in2"" }; struct snd_soc_acpi_mach *mach = dev_get_platdata(dev); struct property_entry props[MAX_NO_PROPS] = {}; struct byt_cht_es8316_private *priv; const struct dmi_system_id *dmi_id; struct fwnode_handle *fwnode; bool sof_parent, is_bytcr; const char *platform_name; struct acpi_device *adev; struct device *codec_dev; unsigned int cnt = 0; int dai_index = 0; int i; int ret = 0; priv = devm_kzalloc(dev, sizeof(*priv), GFP_KERNEL); if (!priv) return -ENOMEM; for (i = 0; i < ARRAY_SIZE(byt_cht_es8316_dais); i++) { if (byt_cht_es8316_dais[i].num_codecs && !strcmp(byt_cht_es8316_dais[i].codecs->name, ""i2c-ESSX8316:00"")) { dai_index = i; break; } } adev = acpi_dev_get_first_match_dev(mach->id, NULL, -1); if (adev) { snprintf(codec_name, sizeof(codec_name), ""i2c-%s"", acpi_dev_name(adev)); byt_cht_es8316_dais[dai_index].codecs->name = codec_name; } else { dev_err(dev, ""Error cannot find '%s' dev\n"", mach->id); return -ENXIO; } codec_dev = acpi_get_first_physical_node(adev); acpi_dev_put(adev); if (!codec_dev) return -EPROBE_DEFER; priv->codec_dev = get_device(codec_dev); byt_cht_es8316_card.dev = dev; platform_name = mach->mach_params.platform; ret = snd_soc_fixup_dai_links_platform_name(&byt_cht_es8316_card, platform_name); if (ret) { put_device(codec_dev); return ret; } es83xx_dsm_dump(priv->codec_dev); is_bytcr = soc_intel_is_byt() && mach->mach_params.acpi_ipc_irq_index == 0; dmi_id = dmi_first_match(byt_cht_es8316_quirk_table); if (dmi_id) { quirk = (unsigned long)dmi_id->driver_data; } else if (!byt_cht_es8316_get_quirks_from_dsm(priv, is_bytcr)) { dev_info(dev, ""Using ACPI DSM info for quirks\n""); } else if (is_bytcr) { quirk = BYT_CHT_ES8316_SSP0 | BYT_CHT_ES8316_INTMIC_IN2_MAP | BYT_CHT_ES8316_MONO_SPEAKER; } else { quirk = BYT_CHT_ES8316_INTMIC_IN1_MAP | BYT_CHT_ES8316_MONO_SPEAKER; } if (quirk_override != -1) { dev_info(dev, ""Overriding quirk 0x%lx => 0x%x\n"", quirk, quirk_override); quirk = quirk_override; } log_quirks(dev); if (quirk & BYT_CHT_ES8316_SSP0) byt_cht_es8316_dais[dai_index].cpus->dai_name = ""ssp0-port""; priv->mclk = devm_clk_get(dev, ""pmc_plt_clk_3""); if (IS_ERR(priv->mclk)) { put_device(codec_dev); return dev_err_probe(dev, PTR_ERR(priv->mclk), ""clk_get pmc_plt_clk_3 failed\n""); } if (quirk & BYT_CHT_ES8316_JD_INVERTED) props[cnt++] = PROPERTY_ENTRY_BOOL(""everest,jack-detect-inverted""); if (cnt) { fwnode = fwnode_create_software_node(props, NULL); if (IS_ERR(fwnode)) { put_device(codec_dev); return PTR_ERR(fwnode); } ret = device_add_software_node(codec_dev, to_software_node(fwnode)); fwnode_handle_put(fwnode); if (ret) { put_device(codec_dev); return ret; } } devm_acpi_dev_add_driver_gpios(codec_dev, byt_cht_es8316_gpios); priv->speaker_en_gpio = gpiod_get_optional(codec_dev, ""speaker-enable"", GPIOD_OUT_LOW | GPIOD_FLAGS_BIT_NONEXCLUSIVE); if (IS_ERR(priv->speaker_en_gpio)) { ret = dev_err_probe(dev, PTR_ERR(priv->speaker_en_gpio), ""get speaker GPIO failed\n""); goto err_put_codec; } snprintf(components_string, sizeof(components_string), ""cfg-spk:%s cfg-mic:%s"", (quirk & BYT_CHT_ES8316_MONO_SPEAKER) ? ""1"" : ""2"", mic_name[BYT_CHT_ES8316_MAP(quirk)]); byt_cht_es8316_card.components = components_string; #if !IS_ENABLED(CONFIG_SND_SOC_INTEL_USER_FRIENDLY_LONG_NAMES) snprintf(long_name, sizeof(long_name), ""bytcht-es8316-%s-spk-%s-mic"", (quirk & BYT_CHT_ES8316_MONO_SPEAKER) ? ""mono"" : ""stereo"", mic_name[BYT_CHT_ES8316_MAP(quirk)]); byt_cht_es8316_card.long_name = long_name; #endif sof_parent = snd_soc_acpi_sof_parent(dev); if (sof_parent) { byt_cht_es8316_card.name = SOF_CARD_NAME; byt_cht_es8316_card.driver_name = SOF_DRIVER_NAME; } else { byt_cht_es8316_card.name = CARD_NAME; byt_cht_es8316_card.driver_name = DRIVER_NAME; } if (sof_parent) dev->driver->pm = &snd_soc_pm_ops; snd_soc_card_set_drvdata(&byt_cht_es8316_card, priv); ret = devm_snd_soc_register_card(dev, &byt_cht_es8316_card); if (ret) { gpiod_put(priv->speaker_en_gpio); dev_err(dev, ""snd_soc_register_card failed: %d\n"", ret); goto err_put_codec; } platform_set_drvdata(pdev, &byt_cht_es8316_card); return 0; err_put_codec: device_remove_software_node(priv->codec_dev); put_device(priv->codec_dev); return ret; }"
1040----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50281/bad/trusted_dcp.c----do_aead_crypto,"static int do_aead_crypto(u8 *in, u8 *out, size_t len, u8 *key, u8 *nonce, bool do_encrypt) { struct aead_request *aead_req = NULL; struct scatterlist src_sg, dst_sg; struct crypto_aead *aead; int ret; aead = crypto_alloc_aead(""gcm(aes)"", 0, CRYPTO_ALG_ASYNC); if (IS_ERR(aead)) { ret = PTR_ERR(aead); goto out; } ret = crypto_aead_setauthsize(aead, DCP_BLOB_AUTHLEN); if (ret < 0) { pr_err(""Can't set crypto auth tag len: %d\n"", ret); goto free_aead; } aead_req = aead_request_alloc(aead, GFP_KERNEL); if (!aead_req) { ret = -ENOMEM; goto free_aead; } sg_init_one(&src_sg, in, len); if (do_encrypt) { sg_init_one(&dst_sg, out, len + DCP_BLOB_AUTHLEN); } else { sg_init_one(&dst_sg, out, len); } aead_request_set_crypt(aead_req, &src_sg, &dst_sg, len, nonce); <S2SV_StartVul> aead_request_set_callback(aead_req, CRYPTO_TFM_REQ_MAY_SLEEP, NULL, <S2SV_EndVul> <S2SV_StartVul> NULL); <S2SV_EndVul> aead_request_set_ad(aead_req, 0); if (crypto_aead_setkey(aead, key, AES_KEYSIZE_128)) { pr_err(""Can't set crypto AEAD key\n""); ret = -EINVAL; goto free_req; } if (do_encrypt) <S2SV_StartVul> ret = crypto_aead_encrypt(aead_req); <S2SV_EndVul> else <S2SV_StartVul> ret = crypto_aead_decrypt(aead_req); <S2SV_EndVul> free_req: aead_request_free(aead_req); free_aead: crypto_free_aead(aead); out: return ret; }","- aead_request_set_callback(aead_req, CRYPTO_TFM_REQ_MAY_SLEEP, NULL,
- NULL);
- ret = crypto_aead_encrypt(aead_req);
- ret = crypto_aead_decrypt(aead_req);
+ DECLARE_CRYPTO_WAIT(wait);
+ aead_request_set_callback(aead_req, CRYPTO_TFM_REQ_MAY_SLEEP,
+ crypto_req_done, &wait);
+ ret = crypto_wait_req(crypto_aead_encrypt(aead_req), &wait);
+ ret = crypto_wait_req(crypto_aead_decrypt(aead_req), &wait);","static int do_aead_crypto(u8 *in, u8 *out, size_t len, u8 *key, u8 *nonce, bool do_encrypt) { struct aead_request *aead_req = NULL; struct scatterlist src_sg, dst_sg; struct crypto_aead *aead; int ret; DECLARE_CRYPTO_WAIT(wait); aead = crypto_alloc_aead(""gcm(aes)"", 0, CRYPTO_ALG_ASYNC); if (IS_ERR(aead)) { ret = PTR_ERR(aead); goto out; } ret = crypto_aead_setauthsize(aead, DCP_BLOB_AUTHLEN); if (ret < 0) { pr_err(""Can't set crypto auth tag len: %d\n"", ret); goto free_aead; } aead_req = aead_request_alloc(aead, GFP_KERNEL); if (!aead_req) { ret = -ENOMEM; goto free_aead; } sg_init_one(&src_sg, in, len); if (do_encrypt) { sg_init_one(&dst_sg, out, len + DCP_BLOB_AUTHLEN); } else { sg_init_one(&dst_sg, out, len); } aead_request_set_crypt(aead_req, &src_sg, &dst_sg, len, nonce); aead_request_set_callback(aead_req, CRYPTO_TFM_REQ_MAY_SLEEP, crypto_req_done, &wait); aead_request_set_ad(aead_req, 0); if (crypto_aead_setkey(aead, key, AES_KEYSIZE_128)) { pr_err(""Can't set crypto AEAD key\n""); ret = -EINVAL; goto free_req; } if (do_encrypt) ret = crypto_wait_req(crypto_aead_encrypt(aead_req), &wait); else ret = crypto_wait_req(crypto_aead_decrypt(aead_req), &wait); free_req: aead_request_free(aead_req); free_aead: crypto_free_aead(aead); out: return ret; }"
1386----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56722/bad/hns_roce_cq.c----free_cqc,"static void free_cqc(struct hns_roce_dev *hr_dev, struct hns_roce_cq *hr_cq) { struct hns_roce_cq_table *cq_table = &hr_dev->cq_table; struct device *dev = hr_dev->dev; int ret; ret = hns_roce_destroy_hw_ctx(hr_dev, HNS_ROCE_CMD_DESTROY_CQC, <S2SV_StartVul> hr_cq->cqn); <S2SV_EndVul> if (ret) <S2SV_StartVul> dev_err(dev, ""DESTROY_CQ failed (%d) for CQN %06lx\n"", ret, <S2SV_EndVul> <S2SV_StartVul> hr_cq->cqn); <S2SV_EndVul> xa_erase_irq(&cq_table->array, hr_cq->cqn); synchronize_irq(hr_dev->eq_table.eq[hr_cq->vector].irq); if (refcount_dec_and_test(&hr_cq->refcount)) complete(&hr_cq->free); wait_for_completion(&hr_cq->free); hns_roce_table_put(hr_dev, &cq_table->table, hr_cq->cqn); }","- hr_cq->cqn);
- dev_err(dev, ""DESTROY_CQ failed (%d) for CQN %06lx\n"", ret,
- hr_cq->cqn);
+ dev_err_ratelimited(dev, ""DESTROY_CQ failed (%d) for CQN %06lx\n"",
+ ret, hr_cq->cqn);","static void free_cqc(struct hns_roce_dev *hr_dev, struct hns_roce_cq *hr_cq) { struct hns_roce_cq_table *cq_table = &hr_dev->cq_table; struct device *dev = hr_dev->dev; int ret; ret = hns_roce_destroy_hw_ctx(hr_dev, HNS_ROCE_CMD_DESTROY_CQC, hr_cq->cqn); if (ret) dev_err_ratelimited(dev, ""DESTROY_CQ failed (%d) for CQN %06lx\n"", ret, hr_cq->cqn); xa_erase_irq(&cq_table->array, hr_cq->cqn); synchronize_irq(hr_dev->eq_table.eq[hr_cq->vector].irq); if (refcount_dec_and_test(&hr_cq->refcount)) complete(&hr_cq->free); wait_for_completion(&hr_cq->free); hns_roce_table_put(hr_dev, &cq_table->table, hr_cq->cqn); }"
1162----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53138/bad/ktls_tx.c----tx_sync_info_get,"tx_sync_info_get(struct mlx5e_ktls_offload_context_tx *priv_tx, u32 tcp_seq, int datalen, struct tx_sync_info *info) { struct tls_offload_context_tx *tx_ctx = priv_tx->tx_ctx; enum mlx5e_ktls_sync_retval ret = MLX5E_KTLS_SYNC_DONE; struct tls_record_info *record; int remaining, i = 0; unsigned long flags; bool ends_before; spin_lock_irqsave(&tx_ctx->lock, flags); record = tls_get_record(tx_ctx, tcp_seq, &info->rcd_sn); if (unlikely(!record)) { ret = MLX5E_KTLS_SYNC_FAIL; goto out; } ends_before = before(tcp_seq + datalen - 1, tls_record_start_seq(record)); if (unlikely(tls_record_is_start_marker(record))) { ret = ends_before ? MLX5E_KTLS_SYNC_SKIP_NO_DATA : MLX5E_KTLS_SYNC_FAIL; goto out; } else if (ends_before) { ret = MLX5E_KTLS_SYNC_FAIL; goto out; } info->sync_len = tcp_seq - tls_record_start_seq(record); remaining = info->sync_len; while (remaining > 0) { skb_frag_t *frag = &record->frags[i]; <S2SV_StartVul> get_page(skb_frag_page(frag)); <S2SV_EndVul> remaining -= skb_frag_size(frag); info->frags[i++] = *frag; } if (remaining < 0) skb_frag_size_add(&info->frags[i - 1], remaining); info->nr_frags = i; out: spin_unlock_irqrestore(&tx_ctx->lock, flags); return ret; }","- get_page(skb_frag_page(frag));
+ page_ref_inc(skb_frag_page(frag));","tx_sync_info_get(struct mlx5e_ktls_offload_context_tx *priv_tx, u32 tcp_seq, int datalen, struct tx_sync_info *info) { struct tls_offload_context_tx *tx_ctx = priv_tx->tx_ctx; enum mlx5e_ktls_sync_retval ret = MLX5E_KTLS_SYNC_DONE; struct tls_record_info *record; int remaining, i = 0; unsigned long flags; bool ends_before; spin_lock_irqsave(&tx_ctx->lock, flags); record = tls_get_record(tx_ctx, tcp_seq, &info->rcd_sn); if (unlikely(!record)) { ret = MLX5E_KTLS_SYNC_FAIL; goto out; } ends_before = before(tcp_seq + datalen - 1, tls_record_start_seq(record)); if (unlikely(tls_record_is_start_marker(record))) { ret = ends_before ? MLX5E_KTLS_SYNC_SKIP_NO_DATA : MLX5E_KTLS_SYNC_FAIL; goto out; } else if (ends_before) { ret = MLX5E_KTLS_SYNC_FAIL; goto out; } info->sync_len = tcp_seq - tls_record_start_seq(record); remaining = info->sync_len; while (remaining > 0) { skb_frag_t *frag = &record->frags[i]; page_ref_inc(skb_frag_page(frag)); remaining -= skb_frag_size(frag); info->frags[i++] = *frag; } if (remaining < 0) skb_frag_size_add(&info->frags[i - 1], remaining); info->nr_frags = i; out: spin_unlock_irqrestore(&tx_ctx->lock, flags); return ret; }"
588----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49910/bad/dcn401_hwseq.c----dcn401_set_output_transfer_func,"bool dcn401_set_output_transfer_func(struct dc *dc, struct pipe_ctx *pipe_ctx, const struct dc_stream_state *stream) { int mpcc_id = pipe_ctx->plane_res.hubp->inst; struct mpc *mpc = pipe_ctx->stream_res.opp->ctx->dc->res_pool->mpc; const struct pwl_params *params = NULL; bool ret = false; if (resource_is_pipe_type(pipe_ctx, OPP_HEAD)) { ret = dcn32_set_mpc_shaper_3dlut(pipe_ctx, stream); if (ret == false && mpc->funcs->set_output_gamma) { if (stream->out_transfer_func.type == TF_TYPE_HWPWL) params = &stream->out_transfer_func.pwl; else if (pipe_ctx->stream->out_transfer_func.type == TF_TYPE_DISTRIBUTED_POINTS && cm3_helper_translate_curve_to_hw_format( &stream->out_transfer_func, &mpc->blender_params, false)) params = &mpc->blender_params; if (stream->out_transfer_func.type == TF_TYPE_PREDEFINED) BREAK_TO_DEBUGGER(); } } <S2SV_StartVul> mpc->funcs->set_output_gamma(mpc, mpcc_id, params); <S2SV_EndVul> return ret; }","- mpc->funcs->set_output_gamma(mpc, mpcc_id, params);
+ if (mpc->funcs->set_output_gamma)
+ mpc->funcs->set_output_gamma(mpc, mpcc_id, params);","bool dcn401_set_output_transfer_func(struct dc *dc, struct pipe_ctx *pipe_ctx, const struct dc_stream_state *stream) { int mpcc_id = pipe_ctx->plane_res.hubp->inst; struct mpc *mpc = pipe_ctx->stream_res.opp->ctx->dc->res_pool->mpc; const struct pwl_params *params = NULL; bool ret = false; if (resource_is_pipe_type(pipe_ctx, OPP_HEAD)) { ret = dcn32_set_mpc_shaper_3dlut(pipe_ctx, stream); if (ret == false && mpc->funcs->set_output_gamma) { if (stream->out_transfer_func.type == TF_TYPE_HWPWL) params = &stream->out_transfer_func.pwl; else if (pipe_ctx->stream->out_transfer_func.type == TF_TYPE_DISTRIBUTED_POINTS && cm3_helper_translate_curve_to_hw_format( &stream->out_transfer_func, &mpc->blender_params, false)) params = &mpc->blender_params; if (stream->out_transfer_func.type == TF_TYPE_PREDEFINED) BREAK_TO_DEBUGGER(); } } if (mpc->funcs->set_output_gamma) mpc->funcs->set_output_gamma(mpc, mpcc_id, params); return ret; }"
990----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50226/bad/region.c----commit_store,"static ssize_t commit_store(struct device *dev, struct device_attribute *attr, const char *buf, size_t len) { struct cxl_region *cxlr = to_cxl_region(dev); struct cxl_region_params *p = &cxlr->params; bool commit; ssize_t rc; rc = kstrtobool(buf, &commit); if (rc) return rc; rc = down_write_killable(&cxl_region_rwsem); if (rc) return rc; if (commit && p->state >= CXL_CONFIG_COMMIT) goto out; if (!commit && p->state < CXL_CONFIG_COMMIT) goto out; if (commit && p->state < CXL_CONFIG_ACTIVE) { rc = -ENXIO; goto out; } rc = cxl_region_invalidate_memregion(cxlr); if (rc) goto out; if (commit) { rc = cxl_region_decode_commit(cxlr); if (rc == 0) p->state = CXL_CONFIG_COMMIT; } else { p->state = CXL_CONFIG_RESET_PENDING; up_write(&cxl_region_rwsem); device_release_driver(&cxlr->dev); down_write(&cxl_region_rwsem); if (p->state == CXL_CONFIG_RESET_PENDING) { <S2SV_StartVul> rc = cxl_region_decode_reset(cxlr, p->interleave_ways); <S2SV_EndVul> <S2SV_StartVul> if (rc) <S2SV_EndVul> <S2SV_StartVul> p->state = CXL_CONFIG_COMMIT; <S2SV_EndVul> <S2SV_StartVul> else <S2SV_EndVul> <S2SV_StartVul> p->state = CXL_CONFIG_ACTIVE; <S2SV_EndVul> } } out: up_write(&cxl_region_rwsem); if (rc) return rc; return len; }","- rc = cxl_region_decode_reset(cxlr, p->interleave_ways);
- if (rc)
- p->state = CXL_CONFIG_COMMIT;
- else
- p->state = CXL_CONFIG_ACTIVE;
+ cxl_region_decode_reset(cxlr, p->interleave_ways);
+ p->state = CXL_CONFIG_ACTIVE;","static ssize_t commit_store(struct device *dev, struct device_attribute *attr, const char *buf, size_t len) { struct cxl_region *cxlr = to_cxl_region(dev); struct cxl_region_params *p = &cxlr->params; bool commit; ssize_t rc; rc = kstrtobool(buf, &commit); if (rc) return rc; rc = down_write_killable(&cxl_region_rwsem); if (rc) return rc; if (commit && p->state >= CXL_CONFIG_COMMIT) goto out; if (!commit && p->state < CXL_CONFIG_COMMIT) goto out; if (commit && p->state < CXL_CONFIG_ACTIVE) { rc = -ENXIO; goto out; } rc = cxl_region_invalidate_memregion(cxlr); if (rc) goto out; if (commit) { rc = cxl_region_decode_commit(cxlr); if (rc == 0) p->state = CXL_CONFIG_COMMIT; } else { p->state = CXL_CONFIG_RESET_PENDING; up_write(&cxl_region_rwsem); device_release_driver(&cxlr->dev); down_write(&cxl_region_rwsem); if (p->state == CXL_CONFIG_RESET_PENDING) { cxl_region_decode_reset(cxlr, p->interleave_ways); p->state = CXL_CONFIG_ACTIVE; } } out: up_write(&cxl_region_rwsem); if (rc) return rc; return len; }"
146----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-45004/bad/trusted_dcp.c----trusted_dcp_unseal,"static int trusted_dcp_unseal(struct trusted_key_payload *p, char *datablob) { struct dcp_blob_fmt *b = (struct dcp_blob_fmt *)p->blob; int blen, ret; if (b->fmt_version != DCP_BLOB_VERSION) { pr_err(""DCP blob has bad version: %i, expected %i\n"", b->fmt_version, DCP_BLOB_VERSION); ret = -EINVAL; goto out; } p->key_len = le32_to_cpu(b->payload_len); blen = calc_blob_len(p->key_len); if (blen != p->blob_len) { pr_err(""DCP blob has bad length: %i != %i\n"", blen, p->blob_len); ret = -EINVAL; goto out; } <S2SV_StartVul> ret = decrypt_blob_key(b->blob_key); <S2SV_EndVul> if (ret) { pr_err(""Unable to decrypt blob key: %i\n"", ret); goto out; } ret = do_aead_crypto(b->payload, p->key, p->key_len + DCP_BLOB_AUTHLEN, <S2SV_StartVul> b->blob_key, b->nonce, false); <S2SV_EndVul> if (ret) { pr_err(""Unwrap of DCP payload failed: %i\n"", ret); goto out; } ret = 0; out: <S2SV_StartVul> return ret; <S2SV_EndVul> }","- ret = decrypt_blob_key(b->blob_key);
- b->blob_key, b->nonce, false);
- return ret;
+ ret = decrypt_blob_key(b->blob_key, plain_blob_key);
+ goto out;
+ plain_blob_key, b->nonce, false);
+ goto out;
+ ret = 0;
+ out:
+ memzero_explicit(plain_blob_key, sizeof(plain_blob_key));
+ return ret;","static int trusted_dcp_unseal(struct trusted_key_payload *p, char *datablob) { struct dcp_blob_fmt *b = (struct dcp_blob_fmt *)p->blob; int blen, ret; u8 plain_blob_key[AES_KEYSIZE_128]; if (b->fmt_version != DCP_BLOB_VERSION) { pr_err(""DCP blob has bad version: %i, expected %i\n"", b->fmt_version, DCP_BLOB_VERSION); ret = -EINVAL; goto out; } p->key_len = le32_to_cpu(b->payload_len); blen = calc_blob_len(p->key_len); if (blen != p->blob_len) { pr_err(""DCP blob has bad length: %i != %i\n"", blen, p->blob_len); ret = -EINVAL; goto out; } ret = decrypt_blob_key(b->blob_key, plain_blob_key); if (ret) { pr_err(""Unable to decrypt blob key: %i\n"", ret); goto out; } ret = do_aead_crypto(b->payload, p->key, p->key_len + DCP_BLOB_AUTHLEN, plain_blob_key, b->nonce, false); if (ret) { pr_err(""Unwrap of DCP payload failed: %i\n"", ret); goto out; } ret = 0; out: memzero_explicit(plain_blob_key, sizeof(plain_blob_key)); return ret; }"
1340----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56658/bad/net_namespace.c----net_free,"static void net_free(struct net *net) { if (refcount_dec_and_test(&net->passive)) { kfree(rcu_access_pointer(net->gen)); ref_tracker_dir_exit(&net->notrefcnt_tracker); <S2SV_StartVul> kmem_cache_free(net_cachep, net); <S2SV_EndVul> } }","- kmem_cache_free(net_cachep, net);
+ llist_add(&net->defer_free_list, &defer_free_list);
+ }
+ }","static void net_free(struct net *net) { if (refcount_dec_and_test(&net->passive)) { kfree(rcu_access_pointer(net->gen)); ref_tracker_dir_exit(&net->notrefcnt_tracker); llist_add(&net->defer_free_list, &defer_free_list); } }"
827----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50094/bad/efx_channels.c----efx_poll,"static int efx_poll(struct napi_struct *napi, int budget) { struct efx_channel *channel = container_of(napi, struct efx_channel, napi_str); struct efx_nic *efx = channel->efx; #ifdef CONFIG_RFS_ACCEL unsigned int time; #endif int spent; netif_vdbg(efx, intr, efx->net_dev, ""channel %d NAPI poll executing on CPU %d\n"", channel->channel, raw_smp_processor_id()); spent = efx_process_channel(channel, budget); <S2SV_StartVul> xdp_do_flush(); <S2SV_EndVul> if (spent < budget) { if (efx_channel_has_rx_queue(channel) && efx->irq_rx_adaptive && unlikely(++channel->irq_count == 1000)) { efx_update_irq_mod(efx, channel); } #ifdef CONFIG_RFS_ACCEL time = jiffies - channel->rfs_last_expiry; if (channel->rfs_filter_count * time >= 600 * HZ) mod_delayed_work(system_wq, &channel->filter_work, 0); #endif if (napi_complete_done(napi, spent)) efx_nic_eventq_read_ack(channel); } return spent; }","- xdp_do_flush();
+ if (budget)
+ xdp_do_flush();","static int efx_poll(struct napi_struct *napi, int budget) { struct efx_channel *channel = container_of(napi, struct efx_channel, napi_str); struct efx_nic *efx = channel->efx; #ifdef CONFIG_RFS_ACCEL unsigned int time; #endif int spent; netif_vdbg(efx, intr, efx->net_dev, ""channel %d NAPI poll executing on CPU %d\n"", channel->channel, raw_smp_processor_id()); spent = efx_process_channel(channel, budget); if (budget) xdp_do_flush(); if (spent < budget) { if (efx_channel_has_rx_queue(channel) && efx->irq_rx_adaptive && unlikely(++channel->irq_count == 1000)) { efx_update_irq_mod(efx, channel); } #ifdef CONFIG_RFS_ACCEL time = jiffies - channel->rfs_last_expiry; if (channel->rfs_filter_count * time >= 600 * HZ) mod_delayed_work(system_wq, &channel->filter_work, 0); #endif if (napi_complete_done(napi, spent)) efx_nic_eventq_read_ack(channel); } return spent; }"
1350----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56671/bad/gpio-graniterapids.c----gnr_gpio_probe,"static int gnr_gpio_probe(struct platform_device *pdev) { size_t num_backup_pins = IS_ENABLED(CONFIG_PM_SLEEP) ? GNR_NUM_PINS : 0; struct device *dev = &pdev->dev; struct gpio_irq_chip *girq; struct gnr_gpio *priv; void __iomem *regs; int irq, ret; priv = devm_kzalloc(dev, struct_size(priv, pad_backup, num_backup_pins), GFP_KERNEL); if (!priv) return -ENOMEM; raw_spin_lock_init(&priv->lock); regs = devm_platform_ioremap_resource(pdev, 0); if (IS_ERR(regs)) return PTR_ERR(regs); irq = platform_get_irq(pdev, 0); if (irq < 0) return irq; ret = devm_request_irq(dev, irq, gnr_gpio_irq, IRQF_SHARED | IRQF_NO_THREAD, dev_name(dev), priv); if (ret) return dev_err_probe(dev, ret, ""failed to request interrupt\n""); priv->reg_base = regs + readl(regs + GNR_CFG_BAR); gnr_gpio_init_pin_ro_bits(dev, priv->reg_base + GNR_CFG_LOCK_OFFSET, priv->ro_bitmap); priv->gc = gnr_gpio_chip; priv->gc.label = dev_name(dev); priv->gc.parent = dev; priv->gc.ngpio = GNR_NUM_PINS; priv->gc.base = -1; girq = &priv->gc.irq; gpio_irq_chip_set_chip(girq, &gnr_gpio_irq_chip); <S2SV_StartVul> girq->chip->name = dev_name(dev); <S2SV_EndVul> girq->parent_handler = NULL; girq->num_parents = 0; girq->parents = NULL; girq->default_type = IRQ_TYPE_NONE; girq->handler = handle_bad_irq; platform_set_drvdata(pdev, priv); return devm_gpiochip_add_data(dev, &priv->gc, priv); }",- girq->chip->name = dev_name(dev);,"static int gnr_gpio_probe(struct platform_device *pdev) { size_t num_backup_pins = IS_ENABLED(CONFIG_PM_SLEEP) ? GNR_NUM_PINS : 0; struct device *dev = &pdev->dev; struct gpio_irq_chip *girq; struct gnr_gpio *priv; void __iomem *regs; int irq, ret; priv = devm_kzalloc(dev, struct_size(priv, pad_backup, num_backup_pins), GFP_KERNEL); if (!priv) return -ENOMEM; raw_spin_lock_init(&priv->lock); regs = devm_platform_ioremap_resource(pdev, 0); if (IS_ERR(regs)) return PTR_ERR(regs); irq = platform_get_irq(pdev, 0); if (irq < 0) return irq; ret = devm_request_irq(dev, irq, gnr_gpio_irq, IRQF_SHARED | IRQF_NO_THREAD, dev_name(dev), priv); if (ret) return dev_err_probe(dev, ret, ""failed to request interrupt\n""); priv->reg_base = regs + readl(regs + GNR_CFG_BAR); gnr_gpio_init_pin_ro_bits(dev, priv->reg_base + GNR_CFG_LOCK_OFFSET, priv->ro_bitmap); priv->gc = gnr_gpio_chip; priv->gc.label = dev_name(dev); priv->gc.parent = dev; priv->gc.ngpio = GNR_NUM_PINS; priv->gc.base = -1; girq = &priv->gc.irq; gpio_irq_chip_set_chip(girq, &gnr_gpio_irq_chip); girq->parent_handler = NULL; girq->num_parents = 0; girq->parents = NULL; girq->default_type = IRQ_TYPE_NONE; girq->handler = handle_bad_irq; platform_set_drvdata(pdev, priv); return devm_gpiochip_add_data(dev, &priv->gc, priv); }"
260----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46738/bad/vmci_resource.c----vmci_resource_remove,"void vmci_resource_remove(struct vmci_resource *resource) { struct vmci_handle handle = resource->handle; unsigned int idx = vmci_resource_hash(handle); struct vmci_resource *r; spin_lock(&vmci_resource_table.lock); hlist_for_each_entry(r, &vmci_resource_table.entries[idx], node) { <S2SV_StartVul> if (vmci_handle_is_equal(r->handle, resource->handle)) { <S2SV_EndVul> hlist_del_init_rcu(&r->node); break; } } spin_unlock(&vmci_resource_table.lock); synchronize_rcu(); vmci_resource_put(resource); wait_for_completion(&resource->done); }","- if (vmci_handle_is_equal(r->handle, resource->handle)) {
+ if (vmci_handle_is_equal(r->handle, resource->handle) &&
+ resource->type == r->type) {","void vmci_resource_remove(struct vmci_resource *resource) { struct vmci_handle handle = resource->handle; unsigned int idx = vmci_resource_hash(handle); struct vmci_resource *r; spin_lock(&vmci_resource_table.lock); hlist_for_each_entry(r, &vmci_resource_table.entries[idx], node) { if (vmci_handle_is_equal(r->handle, resource->handle) && resource->type == r->type) { hlist_del_init_rcu(&r->node); break; } } spin_unlock(&vmci_resource_table.lock); synchronize_rcu(); vmci_resource_put(resource); wait_for_completion(&resource->done); }"
96----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44963/bad/ctree.c----btrfs_del_leaf,"static noinline int btrfs_del_leaf(struct btrfs_trans_handle *trans, struct btrfs_root *root, struct btrfs_path *path, struct extent_buffer *leaf) { int ret; WARN_ON(btrfs_header_generation(leaf) != trans->transid); ret = btrfs_del_ptr(trans, root, path, 1, path->slots[1]); if (ret < 0) return ret; btrfs_unlock_up_safe(path, 0); root_sub_used_bytes(root); atomic_inc(&leaf->refs); <S2SV_StartVul> btrfs_free_tree_block(trans, btrfs_root_id(root), leaf, 0, 1); <S2SV_EndVul> free_extent_buffer_stale(leaf); <S2SV_StartVul> return 0; <S2SV_EndVul> }","- btrfs_free_tree_block(trans, btrfs_root_id(root), leaf, 0, 1);
- return 0;
+ ret = btrfs_free_tree_block(trans, btrfs_root_id(root), leaf, 0, 1);
+ if (ret < 0)
+ btrfs_abort_transaction(trans, ret);
+ return ret;
+ }","static noinline int btrfs_del_leaf(struct btrfs_trans_handle *trans, struct btrfs_root *root, struct btrfs_path *path, struct extent_buffer *leaf) { int ret; WARN_ON(btrfs_header_generation(leaf) != trans->transid); ret = btrfs_del_ptr(trans, root, path, 1, path->slots[1]); if (ret < 0) return ret; btrfs_unlock_up_safe(path, 0); root_sub_used_bytes(root); atomic_inc(&leaf->refs); ret = btrfs_free_tree_block(trans, btrfs_root_id(root), leaf, 0, 1); free_extent_buffer_stale(leaf); if (ret < 0) btrfs_abort_transaction(trans, ret); return ret; }"
175----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-45026/bad/dasd_eckd.c----*dasd_eckd_build_cp_tpm_track,"static struct dasd_ccw_req *dasd_eckd_build_cp_tpm_track( struct dasd_device *startdev, struct dasd_block *block, struct request *req, sector_t first_rec, sector_t last_rec, sector_t first_trk, sector_t last_trk, unsigned int first_offs, unsigned int last_offs, unsigned int blk_per_trk, unsigned int blksize) { struct dasd_ccw_req *cqr; struct req_iterator iter; struct bio_vec bv; char *dst; unsigned int trkcount, ctidaw; unsigned char cmd; struct dasd_device *basedev; unsigned int tlf; struct itcw *itcw; struct tidaw *last_tidaw = NULL; int itcw_op; size_t itcw_size; u8 tidaw_flags; unsigned int seg_len, part_len, len_to_track_end; unsigned char new_track; sector_t recid, trkid; unsigned int offs; unsigned int count, count_to_trk_end; int ret; basedev = block->base; if (rq_data_dir(req) == READ) { cmd = DASD_ECKD_CCW_READ_TRACK_DATA; itcw_op = ITCW_OP_READ; } else if (rq_data_dir(req) == WRITE) { cmd = DASD_ECKD_CCW_WRITE_TRACK_DATA; itcw_op = ITCW_OP_WRITE; } else return ERR_PTR(-EINVAL); trkcount = last_trk - first_trk + 1; ctidaw = 0; rq_for_each_segment(bv, req, iter) { ++ctidaw; } if (rq_data_dir(req) == WRITE) ctidaw += (last_trk - first_trk); itcw_size = itcw_calc_size(0, ctidaw, 0); cqr = dasd_smalloc_request(DASD_ECKD_MAGIC, 0, itcw_size, startdev, blk_mq_rq_to_pdu(req)); if (IS_ERR(cqr)) return cqr; if (first_trk == last_trk) tlf = last_offs - first_offs + 1; else tlf = last_offs + 1; tlf *= blksize; itcw = itcw_init(cqr->data, itcw_size, itcw_op, 0, ctidaw, 0); if (IS_ERR(itcw)) { ret = -EINVAL; goto out_error; } cqr->cpaddr = itcw_get_tcw(itcw); if (prepare_itcw(itcw, first_trk, last_trk, cmd, basedev, startdev, first_offs + 1, trkcount, blksize, (last_rec - first_rec + 1) * blksize, tlf, blk_per_trk) == -EAGAIN) { ret = -EAGAIN; goto out_error; } len_to_track_end = 0; if (rq_data_dir(req) == WRITE) { new_track = 1; recid = first_rec; rq_for_each_segment(bv, req, iter) { dst = bvec_virt(&bv); seg_len = bv.bv_len; while (seg_len) { if (new_track) { trkid = recid; offs = sector_div(trkid, blk_per_trk); count_to_trk_end = blk_per_trk - offs; count = min((last_rec - recid + 1), (sector_t)count_to_trk_end); len_to_track_end = count * blksize; recid += count; new_track = 0; } part_len = min(seg_len, len_to_track_end); seg_len -= part_len; len_to_track_end -= part_len; if (!len_to_track_end) { new_track = 1; tidaw_flags = TIDAW_FLAGS_INSERT_CBC; } else tidaw_flags = 0; last_tidaw = itcw_add_tidaw(itcw, tidaw_flags, dst, part_len); if (IS_ERR(last_tidaw)) { ret = -EINVAL; goto out_error; } dst += part_len; } } } else { rq_for_each_segment(bv, req, iter) { dst = bvec_virt(&bv); last_tidaw = itcw_add_tidaw(itcw, 0x00, dst, bv.bv_len); if (IS_ERR(last_tidaw)) { ret = -EINVAL; goto out_error; } } } last_tidaw->flags |= TIDAW_FLAGS_LAST; last_tidaw->flags &= ~TIDAW_FLAGS_INSERT_CBC; itcw_finalize(itcw); if (blk_noretry_request(req) || block->base->features & DASD_FEATURE_FAILFAST) set_bit(DASD_CQR_FLAGS_FAILFAST, &cqr->flags); cqr->cpmode = 1; cqr->startdev = startdev; cqr->memdev = startdev; cqr->block = block; cqr->expires = startdev->default_expires * HZ; cqr->lpm = dasd_path_get_ppm(startdev); cqr->retries = startdev->default_retries; cqr->buildclk = get_tod_clock(); cqr->status = DASD_CQR_FILLED; if (dasd_eckd_is_ese(basedev)) { <S2SV_StartVul> set_bit(DASD_CQR_SUPPRESS_FP, &cqr->flags); <S2SV_EndVul> <S2SV_StartVul> set_bit(DASD_CQR_SUPPRESS_IL, &cqr->flags); <S2SV_EndVul> set_bit(DASD_CQR_SUPPRESS_NRF, &cqr->flags); <S2SV_StartVul> } <S2SV_EndVul> return cqr; out_error: dasd_sfree_request(cqr, startdev); return ERR_PTR(ret); }","- set_bit(DASD_CQR_SUPPRESS_FP, &cqr->flags);
- set_bit(DASD_CQR_SUPPRESS_IL, &cqr->flags);
- }
+ set_bit(DASD_CQR_SUPPRESS_IT, &cqr->flags);","static struct dasd_ccw_req *dasd_eckd_build_cp_tpm_track( struct dasd_device *startdev, struct dasd_block *block, struct request *req, sector_t first_rec, sector_t last_rec, sector_t first_trk, sector_t last_trk, unsigned int first_offs, unsigned int last_offs, unsigned int blk_per_trk, unsigned int blksize) { struct dasd_ccw_req *cqr; struct req_iterator iter; struct bio_vec bv; char *dst; unsigned int trkcount, ctidaw; unsigned char cmd; struct dasd_device *basedev; unsigned int tlf; struct itcw *itcw; struct tidaw *last_tidaw = NULL; int itcw_op; size_t itcw_size; u8 tidaw_flags; unsigned int seg_len, part_len, len_to_track_end; unsigned char new_track; sector_t recid, trkid; unsigned int offs; unsigned int count, count_to_trk_end; int ret; basedev = block->base; if (rq_data_dir(req) == READ) { cmd = DASD_ECKD_CCW_READ_TRACK_DATA; itcw_op = ITCW_OP_READ; } else if (rq_data_dir(req) == WRITE) { cmd = DASD_ECKD_CCW_WRITE_TRACK_DATA; itcw_op = ITCW_OP_WRITE; } else return ERR_PTR(-EINVAL); trkcount = last_trk - first_trk + 1; ctidaw = 0; rq_for_each_segment(bv, req, iter) { ++ctidaw; } if (rq_data_dir(req) == WRITE) ctidaw += (last_trk - first_trk); itcw_size = itcw_calc_size(0, ctidaw, 0); cqr = dasd_smalloc_request(DASD_ECKD_MAGIC, 0, itcw_size, startdev, blk_mq_rq_to_pdu(req)); if (IS_ERR(cqr)) return cqr; if (first_trk == last_trk) tlf = last_offs - first_offs + 1; else tlf = last_offs + 1; tlf *= blksize; itcw = itcw_init(cqr->data, itcw_size, itcw_op, 0, ctidaw, 0); if (IS_ERR(itcw)) { ret = -EINVAL; goto out_error; } cqr->cpaddr = itcw_get_tcw(itcw); if (prepare_itcw(itcw, first_trk, last_trk, cmd, basedev, startdev, first_offs + 1, trkcount, blksize, (last_rec - first_rec + 1) * blksize, tlf, blk_per_trk) == -EAGAIN) { ret = -EAGAIN; goto out_error; } len_to_track_end = 0; if (rq_data_dir(req) == WRITE) { new_track = 1; recid = first_rec; rq_for_each_segment(bv, req, iter) { dst = bvec_virt(&bv); seg_len = bv.bv_len; while (seg_len) { if (new_track) { trkid = recid; offs = sector_div(trkid, blk_per_trk); count_to_trk_end = blk_per_trk - offs; count = min((last_rec - recid + 1), (sector_t)count_to_trk_end); len_to_track_end = count * blksize; recid += count; new_track = 0; } part_len = min(seg_len, len_to_track_end); seg_len -= part_len; len_to_track_end -= part_len; if (!len_to_track_end) { new_track = 1; tidaw_flags = TIDAW_FLAGS_INSERT_CBC; } else tidaw_flags = 0; last_tidaw = itcw_add_tidaw(itcw, tidaw_flags, dst, part_len); if (IS_ERR(last_tidaw)) { ret = -EINVAL; goto out_error; } dst += part_len; } } } else { rq_for_each_segment(bv, req, iter) { dst = bvec_virt(&bv); last_tidaw = itcw_add_tidaw(itcw, 0x00, dst, bv.bv_len); if (IS_ERR(last_tidaw)) { ret = -EINVAL; goto out_error; } } } last_tidaw->flags |= TIDAW_FLAGS_LAST; last_tidaw->flags &= ~TIDAW_FLAGS_INSERT_CBC; itcw_finalize(itcw); if (blk_noretry_request(req) || block->base->features & DASD_FEATURE_FAILFAST) set_bit(DASD_CQR_FLAGS_FAILFAST, &cqr->flags); cqr->cpmode = 1; cqr->startdev = startdev; cqr->memdev = startdev; cqr->block = block; cqr->expires = startdev->default_expires * HZ; cqr->lpm = dasd_path_get_ppm(startdev); cqr->retries = startdev->default_retries; cqr->buildclk = get_tod_clock(); cqr->status = DASD_CQR_FILLED; if (dasd_eckd_is_ese(basedev)) { set_bit(DASD_CQR_SUPPRESS_NRF, &cqr->flags); set_bit(DASD_CQR_SUPPRESS_IT, &cqr->flags); } return cqr; out_error: dasd_sfree_request(cqr, startdev); return ERR_PTR(ret); }"
1088----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53070/bad/core.c----dwc3_suspend_common,"static int dwc3_suspend_common(struct dwc3 *dwc, pm_message_t msg) { u32 reg; int i; <S2SV_StartVul> dwc->susphy_state = (dwc3_readl(dwc->regs, DWC3_GUSB2PHYCFG(0)) & <S2SV_EndVul> <S2SV_StartVul> DWC3_GUSB2PHYCFG_SUSPHY) || <S2SV_EndVul> <S2SV_StartVul> (dwc3_readl(dwc->regs, DWC3_GUSB3PIPECTL(0)) & <S2SV_EndVul> <S2SV_StartVul> DWC3_GUSB3PIPECTL_SUSPHY); <S2SV_EndVul> switch (dwc->current_dr_role) { case DWC3_GCTL_PRTCAP_DEVICE: if (pm_runtime_suspended(dwc->dev)) break; dwc3_gadget_suspend(dwc); synchronize_irq(dwc->irq_gadget); dwc3_core_exit(dwc); break; case DWC3_GCTL_PRTCAP_HOST: if (!PMSG_IS_AUTO(msg) && !device_may_wakeup(dwc->dev)) { dwc3_core_exit(dwc); break; } if (dwc->dis_u2_susphy_quirk || dwc->dis_enblslpm_quirk) { for (i = 0; i < dwc->num_usb2_ports; i++) { reg = dwc3_readl(dwc->regs, DWC3_GUSB2PHYCFG(i)); reg |= DWC3_GUSB2PHYCFG_ENBLSLPM | DWC3_GUSB2PHYCFG_SUSPHY; dwc3_writel(dwc->regs, DWC3_GUSB2PHYCFG(i), reg); } usleep_range(5000, 6000); } for (i = 0; i < dwc->num_usb2_ports; i++) phy_pm_runtime_put_sync(dwc->usb2_generic_phy[i]); for (i = 0; i < dwc->num_usb3_ports; i++) phy_pm_runtime_put_sync(dwc->usb3_generic_phy[i]); break; case DWC3_GCTL_PRTCAP_OTG: if (PMSG_IS_AUTO(msg)) break; if (dwc->current_otg_role == DWC3_OTG_ROLE_DEVICE) { dwc3_gadget_suspend(dwc); synchronize_irq(dwc->irq_gadget); } dwc3_otg_exit(dwc); dwc3_core_exit(dwc); break; default: break; <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> if (!PMSG_IS_AUTO(msg)) { <S2SV_EndVul> <S2SV_StartVul> if (!dwc->susphy_state) <S2SV_EndVul> <S2SV_StartVul> dwc3_enable_susphy(dwc, true); <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> return 0; <S2SV_StartVul> } <S2SV_EndVul>","- dwc->susphy_state = (dwc3_readl(dwc->regs, DWC3_GUSB2PHYCFG(0)) &
- DWC3_GUSB2PHYCFG_SUSPHY) ||
- (dwc3_readl(dwc->regs, DWC3_GUSB3PIPECTL(0)) &
- DWC3_GUSB3PIPECTL_SUSPHY);
- }
- if (!PMSG_IS_AUTO(msg)) {
- if (!dwc->susphy_state)
- dwc3_enable_susphy(dwc, true);
- }
- }
+ if (!pm_runtime_suspended(dwc->dev) && !PMSG_IS_AUTO(msg)) {
+ dwc->susphy_state = (dwc3_readl(dwc->regs, DWC3_GUSB2PHYCFG(0)) &
+ DWC3_GUSB2PHYCFG_SUSPHY) ||
+ (dwc3_readl(dwc->regs, DWC3_GUSB3PIPECTL(0)) &
+ DWC3_GUSB3PIPECTL_SUSPHY);
+ if (!dwc->susphy_state)
+ dwc3_enable_susphy(dwc, true);
+ }
+ }
+ }","static int dwc3_suspend_common(struct dwc3 *dwc, pm_message_t msg) { u32 reg; int i; if (!pm_runtime_suspended(dwc->dev) && !PMSG_IS_AUTO(msg)) { dwc->susphy_state = (dwc3_readl(dwc->regs, DWC3_GUSB2PHYCFG(0)) & DWC3_GUSB2PHYCFG_SUSPHY) || (dwc3_readl(dwc->regs, DWC3_GUSB3PIPECTL(0)) & DWC3_GUSB3PIPECTL_SUSPHY); if (!dwc->susphy_state) dwc3_enable_susphy(dwc, true); } switch (dwc->current_dr_role) { case DWC3_GCTL_PRTCAP_DEVICE: if (pm_runtime_suspended(dwc->dev)) break; dwc3_gadget_suspend(dwc); synchronize_irq(dwc->irq_gadget); dwc3_core_exit(dwc); break; case DWC3_GCTL_PRTCAP_HOST: if (!PMSG_IS_AUTO(msg) && !device_may_wakeup(dwc->dev)) { dwc3_core_exit(dwc); break; } if (dwc->dis_u2_susphy_quirk || dwc->dis_enblslpm_quirk) { for (i = 0; i < dwc->num_usb2_ports; i++) { reg = dwc3_readl(dwc->regs, DWC3_GUSB2PHYCFG(i)); reg |= DWC3_GUSB2PHYCFG_ENBLSLPM | DWC3_GUSB2PHYCFG_SUSPHY; dwc3_writel(dwc->regs, DWC3_GUSB2PHYCFG(i), reg); } usleep_range(5000, 6000); } for (i = 0; i < dwc->num_usb2_ports; i++) phy_pm_runtime_put_sync(dwc->usb2_generic_phy[i]); for (i = 0; i < dwc->num_usb3_ports; i++) phy_pm_runtime_put_sync(dwc->usb3_generic_phy[i]); break; case DWC3_GCTL_PRTCAP_OTG: if (PMSG_IS_AUTO(msg)) break; if (dwc->current_otg_role == DWC3_OTG_ROLE_DEVICE) { dwc3_gadget_suspend(dwc); synchronize_irq(dwc->irq_gadget); } dwc3_otg_exit(dwc); dwc3_core_exit(dwc); break; default: break; } return 0; }"
171----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-45025/bad/file.c----copy_fdtable,"static void copy_fdtable(struct fdtable *nfdt, struct fdtable *ofdt) { size_t cpy, set; BUG_ON(nfdt->max_fds < ofdt->max_fds); cpy = ofdt->max_fds * sizeof(struct file *); set = (nfdt->max_fds - ofdt->max_fds) * sizeof(struct file *); memcpy(nfdt->fd, ofdt->fd, cpy); memset((char *)nfdt->fd + cpy, 0, set); <S2SV_StartVul> copy_fd_bitmaps(nfdt, ofdt, ofdt->max_fds); <S2SV_EndVul> }","- copy_fd_bitmaps(nfdt, ofdt, ofdt->max_fds);
+ copy_fd_bitmaps(nfdt, ofdt, fdt_words(ofdt));","static void copy_fdtable(struct fdtable *nfdt, struct fdtable *ofdt) { size_t cpy, set; BUG_ON(nfdt->max_fds < ofdt->max_fds); cpy = ofdt->max_fds * sizeof(struct file *); set = (nfdt->max_fds - ofdt->max_fds) * sizeof(struct file *); memcpy(nfdt->fd, ofdt->fd, cpy); memset((char *)nfdt->fd + cpy, 0, set); copy_fd_bitmaps(nfdt, ofdt, fdt_words(ofdt)); }"
1223----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53235/bad/data.c----*erofs_bread,"void *erofs_bread(struct erofs_buf *buf, erofs_off_t offset, enum erofs_kmap_type type) { pgoff_t index = offset >> PAGE_SHIFT; struct folio *folio = NULL; if (buf->page) { folio = page_folio(buf->page); if (folio_file_page(folio, index) != buf->page) erofs_unmap_metabuf(buf); } if (!folio || !folio_contains(folio, index)) { erofs_put_metabuf(buf); <S2SV_StartVul> folio = read_mapping_folio(buf->mapping, index, NULL); <S2SV_EndVul> if (IS_ERR(folio)) return folio; } buf->page = folio_file_page(folio, index); if (buf->kmap_type == EROFS_NO_KMAP) { if (type == EROFS_KMAP) buf->base = kmap_local_page(buf->page); buf->kmap_type = type; } else if (buf->kmap_type != type) { DBG_BUGON(1); return ERR_PTR(-EFAULT); } if (type == EROFS_NO_KMAP) return NULL; return buf->base + (offset & ~PAGE_MASK); }","- folio = read_mapping_folio(buf->mapping, index, NULL);
+ folio = read_mapping_folio(buf->mapping, index, buf->file);","void *erofs_bread(struct erofs_buf *buf, erofs_off_t offset, enum erofs_kmap_type type) { pgoff_t index = offset >> PAGE_SHIFT; struct folio *folio = NULL; if (buf->page) { folio = page_folio(buf->page); if (folio_file_page(folio, index) != buf->page) erofs_unmap_metabuf(buf); } if (!folio || !folio_contains(folio, index)) { erofs_put_metabuf(buf); folio = read_mapping_folio(buf->mapping, index, buf->file); if (IS_ERR(folio)) return folio; } buf->page = folio_file_page(folio, index); if (buf->kmap_type == EROFS_NO_KMAP) { if (type == EROFS_KMAP) buf->base = kmap_local_page(buf->page); buf->kmap_type = type; } else if (buf->kmap_type != type) { DBG_BUGON(1); return ERR_PTR(-EFAULT); } if (type == EROFS_NO_KMAP) return NULL; return buf->base + (offset & ~PAGE_MASK); }"
1080----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53063/bad/dvbdev.c----dvb_register_device,"int dvb_register_device(struct dvb_adapter *adap, struct dvb_device **pdvbdev, const struct dvb_device *template, void *priv, enum dvb_device_type type, int demux_sink_pads) { struct dvb_device *dvbdev; struct file_operations *dvbdevfops = NULL; struct dvbdevfops_node *node = NULL, *new_node = NULL; struct device *clsdev; int minor; int id, ret; mutex_lock(&dvbdev_register_lock); if ((id = dvbdev_get_free_id (adap, type)) < 0) { mutex_unlock(&dvbdev_register_lock); *pdvbdev = NULL; pr_err(""%s: couldn't find free device id\n"", __func__); return -ENFILE; } *pdvbdev = dvbdev = kzalloc(sizeof(*dvbdev), GFP_KERNEL); if (!dvbdev){ mutex_unlock(&dvbdev_register_lock); return -ENOMEM; } list_for_each_entry(node, &dvbdevfops_list, list_head) { if (node->fops->owner == adap->module && node->type == type && node->template == template) { dvbdevfops = node->fops; break; } } if (dvbdevfops == NULL) { dvbdevfops = kmemdup(template->fops, sizeof(*dvbdevfops), GFP_KERNEL); if (!dvbdevfops) { kfree(dvbdev); *pdvbdev = NULL; mutex_unlock(&dvbdev_register_lock); return -ENOMEM; } new_node = kzalloc(sizeof(struct dvbdevfops_node), GFP_KERNEL); if (!new_node) { kfree(dvbdevfops); kfree(dvbdev); *pdvbdev = NULL; mutex_unlock(&dvbdev_register_lock); return -ENOMEM; } new_node->fops = dvbdevfops; new_node->type = type; new_node->template = template; list_add_tail (&new_node->list_head, &dvbdevfops_list); } memcpy(dvbdev, template, sizeof(struct dvb_device)); kref_init(&dvbdev->ref); dvbdev->type = type; dvbdev->id = id; dvbdev->adapter = adap; dvbdev->priv = priv; dvbdev->fops = dvbdevfops; init_waitqueue_head (&dvbdev->wait_queue); dvbdevfops->owner = adap->module; list_add_tail (&dvbdev->list_head, &adap->device_list); down_write(&minor_rwsem); #ifdef CONFIG_DVB_DYNAMIC_MINORS for (minor = 0; minor < MAX_DVB_MINORS; minor++) if (dvb_minors[minor] == NULL) break; <S2SV_StartVul> if (minor == MAX_DVB_MINORS) { <S2SV_EndVul> if (new_node) { list_del (&new_node->list_head); kfree(dvbdevfops); kfree(new_node); } list_del (&dvbdev->list_head); kfree(dvbdev); *pdvbdev = NULL; up_write(&minor_rwsem); mutex_unlock(&dvbdev_register_lock); return -EINVAL; } #else minor = nums2minor(adap->num, type, id); #endif dvbdev->minor = minor; dvb_minors[minor] = dvb_device_get(dvbdev); up_write(&minor_rwsem); ret = dvb_register_media_device(dvbdev, type, minor, demux_sink_pads); if (ret) { pr_err(""%s: dvb_register_media_device failed to create the mediagraph\n"", __func__); if (new_node) { list_del (&new_node->list_head); kfree(dvbdevfops); kfree(new_node); } dvb_media_device_free(dvbdev); list_del (&dvbdev->list_head); kfree(dvbdev); *pdvbdev = NULL; mutex_unlock(&dvbdev_register_lock); return ret; } clsdev = device_create(dvb_class, adap->device, MKDEV(DVB_MAJOR, minor), dvbdev, ""dvb%d.%s%d"", adap->num, dnames[type], id); if (IS_ERR(clsdev)) { pr_err(""%s: failed to create device dvb%d.%s%d (%ld)\n"", __func__, adap->num, dnames[type], id, PTR_ERR(clsdev)); if (new_node) { list_del (&new_node->list_head); kfree(dvbdevfops); kfree(new_node); } dvb_media_device_free(dvbdev); list_del (&dvbdev->list_head); kfree(dvbdev); *pdvbdev = NULL; mutex_unlock(&dvbdev_register_lock); return PTR_ERR(clsdev); } dprintk(""DVB: register adapter%d/%s%d @ minor: %i (0x%02x)\n"", adap->num, dnames[type], id, minor, minor); mutex_unlock(&dvbdev_register_lock); return 0; }","- if (minor == MAX_DVB_MINORS) {
+ if (minor >= MAX_DVB_MINORS) {
+ if (minor >= MAX_DVB_MINORS) {
+ dvb_media_device_free(dvbdev);
+ list_del(&dvbdev->list_head);
+ kfree(dvbdev);
+ mutex_unlock(&dvbdev_register_lock);
+ return ret;
+ }","int dvb_register_device(struct dvb_adapter *adap, struct dvb_device **pdvbdev, const struct dvb_device *template, void *priv, enum dvb_device_type type, int demux_sink_pads) { struct dvb_device *dvbdev; struct file_operations *dvbdevfops = NULL; struct dvbdevfops_node *node = NULL, *new_node = NULL; struct device *clsdev; int minor; int id, ret; mutex_lock(&dvbdev_register_lock); if ((id = dvbdev_get_free_id (adap, type)) < 0) { mutex_unlock(&dvbdev_register_lock); *pdvbdev = NULL; pr_err(""%s: couldn't find free device id\n"", __func__); return -ENFILE; } *pdvbdev = dvbdev = kzalloc(sizeof(*dvbdev), GFP_KERNEL); if (!dvbdev){ mutex_unlock(&dvbdev_register_lock); return -ENOMEM; } list_for_each_entry(node, &dvbdevfops_list, list_head) { if (node->fops->owner == adap->module && node->type == type && node->template == template) { dvbdevfops = node->fops; break; } } if (dvbdevfops == NULL) { dvbdevfops = kmemdup(template->fops, sizeof(*dvbdevfops), GFP_KERNEL); if (!dvbdevfops) { kfree(dvbdev); *pdvbdev = NULL; mutex_unlock(&dvbdev_register_lock); return -ENOMEM; } new_node = kzalloc(sizeof(struct dvbdevfops_node), GFP_KERNEL); if (!new_node) { kfree(dvbdevfops); kfree(dvbdev); *pdvbdev = NULL; mutex_unlock(&dvbdev_register_lock); return -ENOMEM; } new_node->fops = dvbdevfops; new_node->type = type; new_node->template = template; list_add_tail (&new_node->list_head, &dvbdevfops_list); } memcpy(dvbdev, template, sizeof(struct dvb_device)); kref_init(&dvbdev->ref); dvbdev->type = type; dvbdev->id = id; dvbdev->adapter = adap; dvbdev->priv = priv; dvbdev->fops = dvbdevfops; init_waitqueue_head (&dvbdev->wait_queue); dvbdevfops->owner = adap->module; list_add_tail (&dvbdev->list_head, &adap->device_list); down_write(&minor_rwsem); #ifdef CONFIG_DVB_DYNAMIC_MINORS for (minor = 0; minor < MAX_DVB_MINORS; minor++) if (dvb_minors[minor] == NULL) break; if (minor >= MAX_DVB_MINORS) { if (new_node) { list_del (&new_node->list_head); kfree(dvbdevfops); kfree(new_node); } list_del (&dvbdev->list_head); kfree(dvbdev); *pdvbdev = NULL; up_write(&minor_rwsem); mutex_unlock(&dvbdev_register_lock); return -EINVAL; } #else minor = nums2minor(adap->num, type, id); if (minor >= MAX_DVB_MINORS) { dvb_media_device_free(dvbdev); list_del(&dvbdev->list_head); kfree(dvbdev); *pdvbdev = NULL; mutex_unlock(&dvbdev_register_lock); return ret; } #endif dvbdev->minor = minor; dvb_minors[minor] = dvb_device_get(dvbdev); up_write(&minor_rwsem); ret = dvb_register_media_device(dvbdev, type, minor, demux_sink_pads); if (ret) { pr_err(""%s: dvb_register_media_device failed to create the mediagraph\n"", __func__); if (new_node) { list_del (&new_node->list_head); kfree(dvbdevfops); kfree(new_node); } dvb_media_device_free(dvbdev); list_del (&dvbdev->list_head); kfree(dvbdev); *pdvbdev = NULL; mutex_unlock(&dvbdev_register_lock); return ret; } clsdev = device_create(dvb_class, adap->device, MKDEV(DVB_MAJOR, minor), dvbdev, ""dvb%d.%s%d"", adap->num, dnames[type], id); if (IS_ERR(clsdev)) { pr_err(""%s: failed to create device dvb%d.%s%d (%ld)\n"", __func__, adap->num, dnames[type], id, PTR_ERR(clsdev)); if (new_node) { list_del (&new_node->list_head); kfree(dvbdevfops); kfree(new_node); } dvb_media_device_free(dvbdev); list_del (&dvbdev->list_head); kfree(dvbdev); *pdvbdev = NULL; mutex_unlock(&dvbdev_register_lock); return PTR_ERR(clsdev); } dprintk(""DVB: register adapter%d/%s%d @ minor: %i (0x%02x)\n"", adap->num, dnames[type], id, minor, minor); mutex_unlock(&dvbdev_register_lock); return 0; }"
43----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-43911/bad/agg-tx.c----ieee80211_start_tx_ba_session,"int ieee80211_start_tx_ba_session(struct ieee80211_sta *pubsta, u16 tid, u16 timeout) { struct sta_info *sta = container_of(pubsta, struct sta_info, sta); struct ieee80211_sub_if_data *sdata = sta->sdata; struct ieee80211_local *local = sdata->local; struct tid_ampdu_tx *tid_tx; int ret = 0; trace_api_start_tx_ba_session(pubsta, tid); if (WARN(sta->reserved_tid == tid, ""Requested to start BA session on reserved tid=%d"", tid)) return -EINVAL; if (!pubsta->deflink.ht_cap.ht_supported && <S2SV_StartVul> sta->sdata->vif.bss_conf.chanreq.oper.chan->band != NL80211_BAND_6GHZ) <S2SV_EndVul> return -EINVAL; if (WARN_ON_ONCE(!local->ops->ampdu_action)) return -EINVAL; if ((tid >= IEEE80211_NUM_TIDS) || !ieee80211_hw_check(&local->hw, AMPDU_AGGREGATION) || ieee80211_hw_check(&local->hw, TX_AMPDU_SETUP_IN_HW)) return -EINVAL; if (WARN_ON(tid >= IEEE80211_FIRST_TSPEC_TSID)) return -EINVAL; ht_dbg(sdata, ""Open BA session requested for %pM tid %u\n"", pubsta->addr, tid); if (sdata->vif.type != NL80211_IFTYPE_STATION && sdata->vif.type != NL80211_IFTYPE_MESH_POINT && sdata->vif.type != NL80211_IFTYPE_AP_VLAN && sdata->vif.type != NL80211_IFTYPE_AP && sdata->vif.type != NL80211_IFTYPE_ADHOC) return -EINVAL; if (test_sta_flag(sta, WLAN_STA_BLOCK_BA)) { ht_dbg(sdata, ""BA sessions blocked - Denying BA session request %pM tid %d\n"", sta->sta.addr, tid); return -EINVAL; } if (test_sta_flag(sta, WLAN_STA_MFP) && !test_sta_flag(sta, WLAN_STA_AUTHORIZED)) { ht_dbg(sdata, ""MFP STA not authorized - deny BA session request %pM tid %d\n"", sta->sta.addr, tid); return -EINVAL; } if (sta->sdata->vif.type == NL80211_IFTYPE_ADHOC && !sta->sta.deflink.ht_cap.ht_supported) { ht_dbg(sdata, ""BA request denied - IBSS STA %pM does not advertise HT support\n"", pubsta->addr); return -EINVAL; } spin_lock_bh(&sta->lock); if (sta->ampdu_mlme.addba_req_num[tid] > HT_AGG_MAX_RETRIES) { ret = -EBUSY; goto err_unlock_sta; } if (sta->ampdu_mlme.addba_req_num[tid] > HT_AGG_BURST_RETRIES && time_before(jiffies, sta->ampdu_mlme.last_addba_req_time[tid] + HT_AGG_RETRIES_PERIOD)) { ht_dbg(sdata, ""BA request denied - %d failed requests on %pM tid %u\n"", sta->ampdu_mlme.addba_req_num[tid], sta->sta.addr, tid); ret = -EBUSY; goto err_unlock_sta; } tid_tx = rcu_dereference_protected_tid_tx(sta, tid); if (tid_tx || sta->ampdu_mlme.tid_start_tx[tid]) { ht_dbg(sdata, ""BA request denied - session is not idle on %pM tid %u\n"", sta->sta.addr, tid); ret = -EAGAIN; goto err_unlock_sta; } tid_tx = kzalloc(sizeof(struct tid_ampdu_tx), GFP_ATOMIC); if (!tid_tx) { ret = -ENOMEM; goto err_unlock_sta; } skb_queue_head_init(&tid_tx->pending); __set_bit(HT_AGG_STATE_WANT_START, &tid_tx->state); tid_tx->timeout = timeout; tid_tx->sta = sta; tid_tx->tid = tid; timer_setup(&tid_tx->addba_resp_timer, sta_addba_resp_timer_expired, 0); timer_setup(&tid_tx->session_timer, sta_tx_agg_session_timer_expired, TIMER_DEFERRABLE); sta->ampdu_mlme.dialog_token_allocator++; tid_tx->dialog_token = sta->ampdu_mlme.dialog_token_allocator; sta->ampdu_mlme.tid_start_tx[tid] = tid_tx; wiphy_work_queue(local->hw.wiphy, &sta->ampdu_mlme.work); err_unlock_sta: spin_unlock_bh(&sta->lock); return ret; }","- sta->sdata->vif.bss_conf.chanreq.oper.chan->band != NL80211_BAND_6GHZ)
+ !pubsta->deflink.vht_cap.vht_supported &&
+ !pubsta->deflink.he_cap.has_he &&
+ !pubsta->deflink.eht_cap.has_eht)","int ieee80211_start_tx_ba_session(struct ieee80211_sta *pubsta, u16 tid, u16 timeout) { struct sta_info *sta = container_of(pubsta, struct sta_info, sta); struct ieee80211_sub_if_data *sdata = sta->sdata; struct ieee80211_local *local = sdata->local; struct tid_ampdu_tx *tid_tx; int ret = 0; trace_api_start_tx_ba_session(pubsta, tid); if (WARN(sta->reserved_tid == tid, ""Requested to start BA session on reserved tid=%d"", tid)) return -EINVAL; if (!pubsta->deflink.ht_cap.ht_supported && !pubsta->deflink.vht_cap.vht_supported && !pubsta->deflink.he_cap.has_he && !pubsta->deflink.eht_cap.has_eht) return -EINVAL; if (WARN_ON_ONCE(!local->ops->ampdu_action)) return -EINVAL; if ((tid >= IEEE80211_NUM_TIDS) || !ieee80211_hw_check(&local->hw, AMPDU_AGGREGATION) || ieee80211_hw_check(&local->hw, TX_AMPDU_SETUP_IN_HW)) return -EINVAL; if (WARN_ON(tid >= IEEE80211_FIRST_TSPEC_TSID)) return -EINVAL; ht_dbg(sdata, ""Open BA session requested for %pM tid %u\n"", pubsta->addr, tid); if (sdata->vif.type != NL80211_IFTYPE_STATION && sdata->vif.type != NL80211_IFTYPE_MESH_POINT && sdata->vif.type != NL80211_IFTYPE_AP_VLAN && sdata->vif.type != NL80211_IFTYPE_AP && sdata->vif.type != NL80211_IFTYPE_ADHOC) return -EINVAL; if (test_sta_flag(sta, WLAN_STA_BLOCK_BA)) { ht_dbg(sdata, ""BA sessions blocked - Denying BA session request %pM tid %d\n"", sta->sta.addr, tid); return -EINVAL; } if (test_sta_flag(sta, WLAN_STA_MFP) && !test_sta_flag(sta, WLAN_STA_AUTHORIZED)) { ht_dbg(sdata, ""MFP STA not authorized - deny BA session request %pM tid %d\n"", sta->sta.addr, tid); return -EINVAL; } if (sta->sdata->vif.type == NL80211_IFTYPE_ADHOC && !sta->sta.deflink.ht_cap.ht_supported) { ht_dbg(sdata, ""BA request denied - IBSS STA %pM does not advertise HT support\n"", pubsta->addr); return -EINVAL; } spin_lock_bh(&sta->lock); if (sta->ampdu_mlme.addba_req_num[tid] > HT_AGG_MAX_RETRIES) { ret = -EBUSY; goto err_unlock_sta; } if (sta->ampdu_mlme.addba_req_num[tid] > HT_AGG_BURST_RETRIES && time_before(jiffies, sta->ampdu_mlme.last_addba_req_time[tid] + HT_AGG_RETRIES_PERIOD)) { ht_dbg(sdata, ""BA request denied - %d failed requests on %pM tid %u\n"", sta->ampdu_mlme.addba_req_num[tid], sta->sta.addr, tid); ret = -EBUSY; goto err_unlock_sta; } tid_tx = rcu_dereference_protected_tid_tx(sta, tid); if (tid_tx || sta->ampdu_mlme.tid_start_tx[tid]) { ht_dbg(sdata, ""BA request denied - session is not idle on %pM tid %u\n"", sta->sta.addr, tid); ret = -EAGAIN; goto err_unlock_sta; } tid_tx = kzalloc(sizeof(struct tid_ampdu_tx), GFP_ATOMIC); if (!tid_tx) { ret = -ENOMEM; goto err_unlock_sta; } skb_queue_head_init(&tid_tx->pending); __set_bit(HT_AGG_STATE_WANT_START, &tid_tx->state); tid_tx->timeout = timeout; tid_tx->sta = sta; tid_tx->tid = tid; timer_setup(&tid_tx->addba_resp_timer, sta_addba_resp_timer_expired, 0); timer_setup(&tid_tx->session_timer, sta_tx_agg_session_timer_expired, TIMER_DEFERRABLE); sta->ampdu_mlme.dialog_token_allocator++; tid_tx->dialog_token = sta->ampdu_mlme.dialog_token_allocator; sta->ampdu_mlme.tid_start_tx[tid] = tid_tx; wiphy_work_queue(local->hw.wiphy, &sta->ampdu_mlme.work); err_unlock_sta: spin_unlock_bh(&sta->lock); return ret; }"
1521----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2025-21676/bad/fec_main.c----fec_enet_rx_queue,"fec_enet_rx_queue(struct net_device *ndev, int budget, u16 queue_id) { struct fec_enet_private *fep = netdev_priv(ndev); struct fec_enet_priv_rx_q *rxq; struct bufdesc *bdp; unsigned short status; struct sk_buff *skb; ushort pkt_len; __u8 *data; int pkt_received = 0; struct bufdesc_ex *ebdp = NULL; bool vlan_packet_rcvd = false; u16 vlan_tag; int index = 0; bool need_swap = fep->quirks & FEC_QUIRK_SWAP_FRAME; struct bpf_prog *xdp_prog = READ_ONCE(fep->xdp_prog); u32 ret, xdp_result = FEC_ENET_XDP_PASS; u32 data_start = FEC_ENET_XDP_HEADROOM; int cpu = smp_processor_id(); struct xdp_buff xdp; struct page *page; u32 sub_len = 4; #if !defined(CONFIG_M5272) if (fep->quirks & FEC_QUIRK_HAS_RACC) { data_start += 2; sub_len += 2; } #endif #if defined(CONFIG_COLDFIRE) && !defined(CONFIG_COLDFIRE_COHERENT_DMA) flush_cache_all(); #endif rxq = fep->rx_queue[queue_id]; bdp = rxq->bd.cur; xdp_init_buff(&xdp, PAGE_SIZE, &rxq->xdp_rxq); while (!((status = fec16_to_cpu(bdp->cbd_sc)) & BD_ENET_RX_EMPTY)) { if (pkt_received >= budget) break; pkt_received++; writel(FEC_ENET_RXF_GET(queue_id), fep->hwp + FEC_IEVENT); status ^= BD_ENET_RX_LAST; if (status & (BD_ENET_RX_LG | BD_ENET_RX_SH | BD_ENET_RX_NO | BD_ENET_RX_CR | BD_ENET_RX_OV | BD_ENET_RX_LAST | BD_ENET_RX_CL)) { ndev->stats.rx_errors++; if (status & BD_ENET_RX_OV) { ndev->stats.rx_fifo_errors++; goto rx_processing_done; } if (status & (BD_ENET_RX_LG | BD_ENET_RX_SH | BD_ENET_RX_LAST)) { ndev->stats.rx_length_errors++; if (status & BD_ENET_RX_LAST) netdev_err(ndev, ""rcv is not +last\n""); } if (status & BD_ENET_RX_CR) ndev->stats.rx_crc_errors++; if (status & (BD_ENET_RX_NO | BD_ENET_RX_CL)) ndev->stats.rx_frame_errors++; goto rx_processing_done; } ndev->stats.rx_packets++; pkt_len = fec16_to_cpu(bdp->cbd_datlen); ndev->stats.rx_bytes += pkt_len; index = fec_enet_get_bd_index(bdp, &rxq->bd); page = rxq->rx_skb_info[index].page; dma_sync_single_for_cpu(&fep->pdev->dev, <S2SV_StartVul> fec32_to_cpu(bdp->cbd_bufaddr), <S2SV_EndVul> pkt_len, DMA_FROM_DEVICE); prefetch(page_address(page)); <S2SV_StartVul> fec_enet_update_cbd(rxq, bdp, index); <S2SV_EndVul> if (xdp_prog) { xdp_buff_clear_frags_flag(&xdp); xdp_prepare_buff(&xdp, page_address(page), data_start, pkt_len - sub_len, false); ret = fec_enet_run_xdp(fep, xdp_prog, &xdp, rxq, cpu); xdp_result |= ret; if (ret != FEC_ENET_XDP_PASS) goto rx_processing_done; } skb = build_skb(page_address(page), PAGE_SIZE); if (unlikely(!skb)) { page_pool_recycle_direct(rxq->page_pool, page); ndev->stats.rx_dropped++; netdev_err_once(ndev, ""build_skb failed!\n""); goto rx_processing_done; } skb_reserve(skb, data_start); skb_put(skb, pkt_len - sub_len); skb_mark_for_recycle(skb); if (unlikely(need_swap)) { data = page_address(page) + FEC_ENET_XDP_HEADROOM; swap_buffer(data, pkt_len); } data = skb->data; ebdp = NULL; if (fep->bufdesc_ex) ebdp = (struct bufdesc_ex *)bdp; vlan_packet_rcvd = false; if ((ndev->features & NETIF_F_HW_VLAN_CTAG_RX) && fep->bufdesc_ex && (ebdp->cbd_esc & cpu_to_fec32(BD_ENET_RX_VLAN))) { struct vlan_hdr *vlan_header = (struct vlan_hdr *) (data + ETH_HLEN); vlan_tag = ntohs(vlan_header->h_vlan_TCI); vlan_packet_rcvd = true; memmove(skb->data + VLAN_HLEN, data, ETH_ALEN * 2); skb_pull(skb, VLAN_HLEN); } skb->protocol = eth_type_trans(skb, ndev); if (fep->hwts_rx_en && fep->bufdesc_ex) fec_enet_hwtstamp(fep, fec32_to_cpu(ebdp->ts), skb_hwtstamps(skb)); if (fep->bufdesc_ex && (fep->csum_flags & FLAG_RX_CSUM_ENABLED)) { if (!(ebdp->cbd_esc & cpu_to_fec32(FLAG_RX_CSUM_ERROR))) { skb->ip_summed = CHECKSUM_UNNECESSARY; } else { skb_checksum_none_assert(skb); } } if (vlan_packet_rcvd) __vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), vlan_tag); skb_record_rx_queue(skb, queue_id); napi_gro_receive(&fep->napi, skb); rx_processing_done: status &= ~BD_ENET_RX_STATS; status |= BD_ENET_RX_EMPTY; if (fep->bufdesc_ex) { struct bufdesc_ex *ebdp = (struct bufdesc_ex *)bdp; ebdp->cbd_esc = cpu_to_fec32(BD_ENET_RX_INT); ebdp->cbd_prot = 0; ebdp->cbd_bdu = 0; } wmb(); bdp->cbd_sc = cpu_to_fec16(status); bdp = fec_enet_get_nextdesc(bdp, &rxq->bd); writel(0, rxq->bd.reg_desc_active); } rxq->bd.cur = bdp; if (xdp_result & FEC_ENET_XDP_REDIR) xdp_do_flush(); return pkt_received; }","- fec32_to_cpu(bdp->cbd_bufaddr),
- fec_enet_update_cbd(rxq, bdp, index);
+ __fec32 cbd_bufaddr;
+ cbd_bufaddr = bdp->cbd_bufaddr;
+ if (fec_enet_update_cbd(rxq, bdp, index)) {
+ ndev->stats.rx_dropped++;
+ goto rx_processing_done;
+ }
+ fec32_to_cpu(cbd_bufaddr),","fec_enet_rx_queue(struct net_device *ndev, int budget, u16 queue_id) { struct fec_enet_private *fep = netdev_priv(ndev); struct fec_enet_priv_rx_q *rxq; struct bufdesc *bdp; unsigned short status; struct sk_buff *skb; ushort pkt_len; __u8 *data; int pkt_received = 0; struct bufdesc_ex *ebdp = NULL; bool vlan_packet_rcvd = false; u16 vlan_tag; int index = 0; bool need_swap = fep->quirks & FEC_QUIRK_SWAP_FRAME; struct bpf_prog *xdp_prog = READ_ONCE(fep->xdp_prog); u32 ret, xdp_result = FEC_ENET_XDP_PASS; u32 data_start = FEC_ENET_XDP_HEADROOM; int cpu = smp_processor_id(); struct xdp_buff xdp; struct page *page; __fec32 cbd_bufaddr; u32 sub_len = 4; #if !defined(CONFIG_M5272) if (fep->quirks & FEC_QUIRK_HAS_RACC) { data_start += 2; sub_len += 2; } #endif #if defined(CONFIG_COLDFIRE) && !defined(CONFIG_COLDFIRE_COHERENT_DMA) flush_cache_all(); #endif rxq = fep->rx_queue[queue_id]; bdp = rxq->bd.cur; xdp_init_buff(&xdp, PAGE_SIZE, &rxq->xdp_rxq); while (!((status = fec16_to_cpu(bdp->cbd_sc)) & BD_ENET_RX_EMPTY)) { if (pkt_received >= budget) break; pkt_received++; writel(FEC_ENET_RXF_GET(queue_id), fep->hwp + FEC_IEVENT); status ^= BD_ENET_RX_LAST; if (status & (BD_ENET_RX_LG | BD_ENET_RX_SH | BD_ENET_RX_NO | BD_ENET_RX_CR | BD_ENET_RX_OV | BD_ENET_RX_LAST | BD_ENET_RX_CL)) { ndev->stats.rx_errors++; if (status & BD_ENET_RX_OV) { ndev->stats.rx_fifo_errors++; goto rx_processing_done; } if (status & (BD_ENET_RX_LG | BD_ENET_RX_SH | BD_ENET_RX_LAST)) { ndev->stats.rx_length_errors++; if (status & BD_ENET_RX_LAST) netdev_err(ndev, ""rcv is not +last\n""); } if (status & BD_ENET_RX_CR) ndev->stats.rx_crc_errors++; if (status & (BD_ENET_RX_NO | BD_ENET_RX_CL)) ndev->stats.rx_frame_errors++; goto rx_processing_done; } ndev->stats.rx_packets++; pkt_len = fec16_to_cpu(bdp->cbd_datlen); ndev->stats.rx_bytes += pkt_len; index = fec_enet_get_bd_index(bdp, &rxq->bd); page = rxq->rx_skb_info[index].page; cbd_bufaddr = bdp->cbd_bufaddr; if (fec_enet_update_cbd(rxq, bdp, index)) { ndev->stats.rx_dropped++; goto rx_processing_done; } dma_sync_single_for_cpu(&fep->pdev->dev, fec32_to_cpu(cbd_bufaddr), pkt_len, DMA_FROM_DEVICE); prefetch(page_address(page)); if (xdp_prog) { xdp_buff_clear_frags_flag(&xdp); xdp_prepare_buff(&xdp, page_address(page), data_start, pkt_len - sub_len, false); ret = fec_enet_run_xdp(fep, xdp_prog, &xdp, rxq, cpu); xdp_result |= ret; if (ret != FEC_ENET_XDP_PASS) goto rx_processing_done; } skb = build_skb(page_address(page), PAGE_SIZE); if (unlikely(!skb)) { page_pool_recycle_direct(rxq->page_pool, page); ndev->stats.rx_dropped++; netdev_err_once(ndev, ""build_skb failed!\n""); goto rx_processing_done; } skb_reserve(skb, data_start); skb_put(skb, pkt_len - sub_len); skb_mark_for_recycle(skb); if (unlikely(need_swap)) { data = page_address(page) + FEC_ENET_XDP_HEADROOM; swap_buffer(data, pkt_len); } data = skb->data; ebdp = NULL; if (fep->bufdesc_ex) ebdp = (struct bufdesc_ex *)bdp; vlan_packet_rcvd = false; if ((ndev->features & NETIF_F_HW_VLAN_CTAG_RX) && fep->bufdesc_ex && (ebdp->cbd_esc & cpu_to_fec32(BD_ENET_RX_VLAN))) { struct vlan_hdr *vlan_header = (struct vlan_hdr *) (data + ETH_HLEN); vlan_tag = ntohs(vlan_header->h_vlan_TCI); vlan_packet_rcvd = true; memmove(skb->data + VLAN_HLEN, data, ETH_ALEN * 2); skb_pull(skb, VLAN_HLEN); } skb->protocol = eth_type_trans(skb, ndev); if (fep->hwts_rx_en && fep->bufdesc_ex) fec_enet_hwtstamp(fep, fec32_to_cpu(ebdp->ts), skb_hwtstamps(skb)); if (fep->bufdesc_ex && (fep->csum_flags & FLAG_RX_CSUM_ENABLED)) { if (!(ebdp->cbd_esc & cpu_to_fec32(FLAG_RX_CSUM_ERROR))) { skb->ip_summed = CHECKSUM_UNNECESSARY; } else { skb_checksum_none_assert(skb); } } if (vlan_packet_rcvd) __vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), vlan_tag); skb_record_rx_queue(skb, queue_id); napi_gro_receive(&fep->napi, skb); rx_processing_done: status &= ~BD_ENET_RX_STATS; status |= BD_ENET_RX_EMPTY; if (fep->bufdesc_ex) { struct bufdesc_ex *ebdp = (struct bufdesc_ex *)bdp; ebdp->cbd_esc = cpu_to_fec32(BD_ENET_RX_INT); ebdp->cbd_prot = 0; ebdp->cbd_bdu = 0; } wmb(); bdp->cbd_sc = cpu_to_fec16(status); bdp = fec_enet_get_nextdesc(bdp, &rxq->bd); writel(0, rxq->bd.reg_desc_active); } rxq->bd.cur = bdp; if (xdp_result & FEC_ENET_XDP_REDIR) xdp_do_flush(); return pkt_received; }"
801----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50074/bad/procfs.c----do_hardware_modes,"static int do_hardware_modes(const struct ctl_table *table, int write, void *result, size_t *lenp, loff_t *ppos) { struct parport *port = (struct parport *)table->extra1; char buffer[40]; int len = 0; if (*ppos) { *lenp = 0; return 0; } if (write) return -EACCES; { #define printmode(x) \ do { \ if (port->modes & PARPORT_MODE_##x) \ <S2SV_StartVul> len += snprintf(buffer + len, sizeof(buffer) - len, ""%s%s"", f++ ? "","" : """", #x); \ <S2SV_EndVul> } while (0) int f = 0; printmode(PCSPP); printmode(TRISTATE); printmode(COMPAT); printmode(EPP); printmode(ECP); printmode(DMA); #undef printmode } buffer[len++] = '\n'; if (len > *lenp) len = *lenp; else *lenp = len; *ppos += len; memcpy(result, buffer, len); return 0; }","- len += snprintf(buffer + len, sizeof(buffer) - len, ""%s%s"", f++ ? "","" : """", #x); \
+ len += scnprintf(buffer + len, sizeof(buffer) - len, ""%s%s"", f++ ? "","" : """", #x); \","static int do_hardware_modes(const struct ctl_table *table, int write, void *result, size_t *lenp, loff_t *ppos) { struct parport *port = (struct parport *)table->extra1; char buffer[40]; int len = 0; if (*ppos) { *lenp = 0; return 0; } if (write) return -EACCES; { #define printmode(x) \ do { \ if (port->modes & PARPORT_MODE_##x) \ len += scnprintf(buffer + len, sizeof(buffer) - len, ""%s%s"", f++ ? "","" : """", #x); \ } while (0) int f = 0; printmode(PCSPP); printmode(TRISTATE); printmode(COMPAT); printmode(EPP); printmode(ECP); printmode(DMA); #undef printmode } buffer[len++] = '\n'; if (len > *lenp) len = *lenp; else *lenp = len; *ppos += len; memcpy(result, buffer, len); return 0; }"
450----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47690/bad/namei.c----*f2fs_lookup,"static struct dentry *f2fs_lookup(struct inode *dir, struct dentry *dentry, unsigned int flags) { struct inode *inode = NULL; struct f2fs_dir_entry *de; struct page *page; struct dentry *new; nid_t ino = -1; <S2SV_StartVul> int err = 0; <S2SV_EndVul> <S2SV_StartVul> unsigned int root_ino = F2FS_ROOT_INO(F2FS_I_SB(dir)); <S2SV_EndVul> struct f2fs_filename fname; trace_f2fs_lookup_start(dir, dentry, flags); if (dentry->d_name.len > F2FS_NAME_LEN) { err = -ENAMETOOLONG; goto out; } err = f2fs_prepare_lookup(dir, dentry, &fname); if (err == -ENOENT) goto out_splice; if (err) goto out; de = __f2fs_find_entry(dir, &fname, &page); f2fs_free_filename(&fname); if (!de) { if (IS_ERR(page)) { err = PTR_ERR(page); goto out; } err = -ENOENT; goto out_splice; } ino = le32_to_cpu(de->ino); f2fs_put_page(page, 0); inode = f2fs_iget(dir->i_sb, ino); if (IS_ERR(inode)) { err = PTR_ERR(inode); goto out; <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> if ((dir->i_ino == root_ino) && f2fs_has_inline_dots(dir)) { <S2SV_EndVul> <S2SV_StartVul> err = __recover_dot_dentries(dir, root_ino); <S2SV_EndVul> <S2SV_StartVul> if (err) <S2SV_EndVul> <S2SV_StartVul> goto out_iput; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> if (f2fs_has_inline_dots(inode)) { <S2SV_EndVul> <S2SV_StartVul> err = __recover_dot_dentries(inode, dir->i_ino); <S2SV_EndVul> <S2SV_StartVul> if (err) <S2SV_EndVul> <S2SV_StartVul> goto out_iput; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> if (IS_ENCRYPTED(dir) && (S_ISDIR(inode->i_mode) || S_ISLNK(inode->i_mode)) && !fscrypt_has_permitted_context(dir, inode)) { f2fs_warn(F2FS_I_SB(inode), ""Inconsistent encryption contexts: %lu/%lu"", dir->i_ino, inode->i_ino); err = -EPERM; goto out_iput; } out_splice: #if IS_ENABLED(CONFIG_UNICODE) if (!inode && IS_CASEFOLDED(dir)) { trace_f2fs_lookup_end(dir, dentry, ino, err); return NULL; } #endif new = d_splice_alias(inode, dentry); trace_f2fs_lookup_end(dir, !IS_ERR_OR_NULL(new) ? new : dentry, ino, IS_ERR(new) ? PTR_ERR(new) : err); return new; out_iput: iput(inode); out: trace_f2fs_lookup_end(dir, dentry, ino, err); return ERR_PTR(err); }","- int err = 0;
- unsigned int root_ino = F2FS_ROOT_INO(F2FS_I_SB(dir));
- }
- if ((dir->i_ino == root_ino) && f2fs_has_inline_dots(dir)) {
- err = __recover_dot_dentries(dir, root_ino);
- if (err)
- goto out_iput;
- }
- if (f2fs_has_inline_dots(inode)) {
- err = __recover_dot_dentries(inode, dir->i_ino);
- if (err)
- goto out_iput;
- }","static struct dentry *f2fs_lookup(struct inode *dir, struct dentry *dentry, unsigned int flags) { struct inode *inode = NULL; struct f2fs_dir_entry *de; struct page *page; struct dentry *new; nid_t ino = -1; int err = 0; struct f2fs_filename fname; trace_f2fs_lookup_start(dir, dentry, flags); if (dentry->d_name.len > F2FS_NAME_LEN) { err = -ENAMETOOLONG; goto out; } err = f2fs_prepare_lookup(dir, dentry, &fname); if (err == -ENOENT) goto out_splice; if (err) goto out; de = __f2fs_find_entry(dir, &fname, &page); f2fs_free_filename(&fname); if (!de) { if (IS_ERR(page)) { err = PTR_ERR(page); goto out; } err = -ENOENT; goto out_splice; } ino = le32_to_cpu(de->ino); f2fs_put_page(page, 0); inode = f2fs_iget(dir->i_sb, ino); if (IS_ERR(inode)) { err = PTR_ERR(inode); goto out; } if (IS_ENCRYPTED(dir) && (S_ISDIR(inode->i_mode) || S_ISLNK(inode->i_mode)) && !fscrypt_has_permitted_context(dir, inode)) { f2fs_warn(F2FS_I_SB(inode), ""Inconsistent encryption contexts: %lu/%lu"", dir->i_ino, inode->i_ino); err = -EPERM; goto out_iput; } out_splice: #if IS_ENABLED(CONFIG_UNICODE) if (!inode && IS_CASEFOLDED(dir)) { trace_f2fs_lookup_end(dir, dentry, ino, err); return NULL; } #endif new = d_splice_alias(inode, dentry); trace_f2fs_lookup_end(dir, !IS_ERR_OR_NULL(new) ? new : dentry, ino, IS_ERR(new) ? PTR_ERR(new) : err); return new; out_iput: iput(inode); out: trace_f2fs_lookup_end(dir, dentry, ino, err); return ERR_PTR(err); }"
158----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-45014/bad/startup.c----startup_kernel,"void startup_kernel(void) { unsigned long kernel_size = vmlinux.image_size + vmlinux.bss_size; unsigned long nokaslr_offset_phys, kaslr_large_page_offset; unsigned long amode31_lma = 0; unsigned long max_physmem_end; unsigned long asce_limit; unsigned long safe_addr; psw_t psw; fixup_vmlinux_info(); setup_lpp(); nokaslr_offset_phys = ALIGN(mem_safe_offset(), _SEGMENT_SIZE); safe_addr = PAGE_ALIGN(nokaslr_offset_phys + kernel_size); physmem_reserve(RR_DECOMPRESSOR, 0, safe_addr); if (IS_ENABLED(CONFIG_BLK_DEV_INITRD) && parmarea.initrd_size) physmem_reserve(RR_INITRD, parmarea.initrd_start, parmarea.initrd_size); oldmem_data.start = parmarea.oldmem_base; oldmem_data.size = parmarea.oldmem_size; store_ipl_parmblock(); read_ipl_report(); uv_query_info(); sclp_early_read_info(); setup_boot_command_line(); parse_boot_command_line(); detect_facilities(); cmma_init(); sanitize_prot_virt_host(); max_physmem_end = detect_max_physmem_end(); setup_ident_map_size(max_physmem_end); setup_vmalloc_size(); asce_limit = setup_kernel_memory_layout(kernel_size); physmem_set_usable_limit(ident_map_size); detect_physmem_online_ranges(max_physmem_end); save_ipl_cert_comp_list(); rescue_initrd(safe_addr, ident_map_size); kaslr_large_page_offset = __kaslr_offset & ~_SEGMENT_MASK; if (kaslr_enabled()) { <S2SV_StartVul> unsigned long end = ident_map_size - kaslr_large_page_offset; <S2SV_EndVul> <S2SV_StartVul> __kaslr_offset_phys = randomize_within_range(kernel_size, _SEGMENT_SIZE, 0, end); <S2SV_EndVul> } if (!__kaslr_offset_phys) __kaslr_offset_phys = nokaslr_offset_phys; __kaslr_offset_phys |= kaslr_large_page_offset; kaslr_adjust_vmlinux_info(__kaslr_offset_phys); physmem_reserve(RR_VMLINUX, __kaslr_offset_phys, kernel_size); deploy_kernel((void *)__kaslr_offset_phys); physmem_reserve(RR_DECOMPRESSOR, 0, (unsigned long)_decompressor_end); if (kaslr_enabled()) amode31_lma = randomize_within_range(vmlinux.amode31_size, PAGE_SIZE, 0, SZ_2G); if (!amode31_lma) amode31_lma = __kaslr_offset_phys - vmlinux.amode31_size; physmem_reserve(RR_AMODE31, amode31_lma, vmlinux.amode31_size); clear_bss_section(__kaslr_offset_phys); kaslr_adjust_relocs(__kaslr_offset_phys, __kaslr_offset_phys + vmlinux.image_size, __kaslr_offset, __kaslr_offset_phys); kaslr_adjust_got(__kaslr_offset); setup_vmem(__kaslr_offset, __kaslr_offset + kernel_size, asce_limit); copy_bootdata(); S390_lowcore.vmcore_info = __kaslr_offset_phys ? __kaslr_offset_phys | 0x1UL : 0; psw.addr = __kaslr_offset + vmlinux.entry; psw.mask = PSW_KERNEL_BITS; __load_psw(psw); }","- unsigned long end = ident_map_size - kaslr_large_page_offset;
- __kaslr_offset_phys = randomize_within_range(kernel_size, _SEGMENT_SIZE, 0, end);
+ unsigned long size = kernel_size + kaslr_large_page_offset;
+ __kaslr_offset_phys = randomize_within_range(size, _SEGMENT_SIZE, 0, ident_map_size);","void startup_kernel(void) { unsigned long kernel_size = vmlinux.image_size + vmlinux.bss_size; unsigned long nokaslr_offset_phys, kaslr_large_page_offset; unsigned long amode31_lma = 0; unsigned long max_physmem_end; unsigned long asce_limit; unsigned long safe_addr; psw_t psw; fixup_vmlinux_info(); setup_lpp(); nokaslr_offset_phys = ALIGN(mem_safe_offset(), _SEGMENT_SIZE); safe_addr = PAGE_ALIGN(nokaslr_offset_phys + kernel_size); physmem_reserve(RR_DECOMPRESSOR, 0, safe_addr); if (IS_ENABLED(CONFIG_BLK_DEV_INITRD) && parmarea.initrd_size) physmem_reserve(RR_INITRD, parmarea.initrd_start, parmarea.initrd_size); oldmem_data.start = parmarea.oldmem_base; oldmem_data.size = parmarea.oldmem_size; store_ipl_parmblock(); read_ipl_report(); uv_query_info(); sclp_early_read_info(); setup_boot_command_line(); parse_boot_command_line(); detect_facilities(); cmma_init(); sanitize_prot_virt_host(); max_physmem_end = detect_max_physmem_end(); setup_ident_map_size(max_physmem_end); setup_vmalloc_size(); asce_limit = setup_kernel_memory_layout(kernel_size); physmem_set_usable_limit(ident_map_size); detect_physmem_online_ranges(max_physmem_end); save_ipl_cert_comp_list(); rescue_initrd(safe_addr, ident_map_size); kaslr_large_page_offset = __kaslr_offset & ~_SEGMENT_MASK; if (kaslr_enabled()) { unsigned long size = kernel_size + kaslr_large_page_offset; __kaslr_offset_phys = randomize_within_range(size, _SEGMENT_SIZE, 0, ident_map_size); } if (!__kaslr_offset_phys) __kaslr_offset_phys = nokaslr_offset_phys; __kaslr_offset_phys |= kaslr_large_page_offset; kaslr_adjust_vmlinux_info(__kaslr_offset_phys); physmem_reserve(RR_VMLINUX, __kaslr_offset_phys, kernel_size); deploy_kernel((void *)__kaslr_offset_phys); physmem_reserve(RR_DECOMPRESSOR, 0, (unsigned long)_decompressor_end); if (kaslr_enabled()) amode31_lma = randomize_within_range(vmlinux.amode31_size, PAGE_SIZE, 0, SZ_2G); if (!amode31_lma) amode31_lma = __kaslr_offset_phys - vmlinux.amode31_size; physmem_reserve(RR_AMODE31, amode31_lma, vmlinux.amode31_size); clear_bss_section(__kaslr_offset_phys); kaslr_adjust_relocs(__kaslr_offset_phys, __kaslr_offset_phys + vmlinux.image_size, __kaslr_offset, __kaslr_offset_phys); kaslr_adjust_got(__kaslr_offset); setup_vmem(__kaslr_offset, __kaslr_offset + kernel_size, asce_limit); copy_bootdata(); S390_lowcore.vmcore_info = __kaslr_offset_phys ? __kaslr_offset_phys | 0x1UL : 0; psw.addr = __kaslr_offset + vmlinux.entry; psw.mask = PSW_KERNEL_BITS; __load_psw(psw); }"
926----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50173/bad/panthor_sched.c----tick_ctx_cleanup,"tick_ctx_cleanup(struct panthor_scheduler *sched, struct panthor_sched_tick_ctx *ctx) { struct panthor_group *group, *tmp; u32 i; for (i = 0; i < ARRAY_SIZE(ctx->old_groups); i++) { list_for_each_entry_safe(group, tmp, &ctx->old_groups[i], run_node) { <S2SV_StartVul> drm_WARN_ON(&group->ptdev->base, !ctx->csg_upd_failed_mask && <S2SV_EndVul> group_can_run(group)); if (!group_can_run(group)) { list_del_init(&group->run_node); list_del_init(&group->wait_node); group_queue_work(group, term); } else if (group->csg_id >= 0) { list_del_init(&group->run_node); } else { list_move(&group->run_node, group_is_idle(group) ? &sched->groups.idle[group->priority] : &sched->groups.runnable[group->priority]); } group_put(group); } } for (i = 0; i < ARRAY_SIZE(ctx->groups); i++) { <S2SV_StartVul> drm_WARN_ON(&group->ptdev->base, <S2SV_EndVul> !ctx->csg_upd_failed_mask && !list_empty(&ctx->groups[i])); list_for_each_entry_safe(group, tmp, &ctx->groups[i], run_node) { if (group->csg_id >= 0) { list_del_init(&group->run_node); } else { list_move(&group->run_node, group_is_idle(group) ? &sched->groups.idle[group->priority] : &sched->groups.runnable[group->priority]); } group_put(group); } } }","- drm_WARN_ON(&group->ptdev->base, !ctx->csg_upd_failed_mask &&
- drm_WARN_ON(&group->ptdev->base,
+ struct panthor_device *ptdev = sched->ptdev;
+ drm_WARN_ON(&ptdev->base, !ctx->csg_upd_failed_mask &&
+ drm_WARN_ON(&ptdev->base,","tick_ctx_cleanup(struct panthor_scheduler *sched, struct panthor_sched_tick_ctx *ctx) { struct panthor_device *ptdev = sched->ptdev; struct panthor_group *group, *tmp; u32 i; for (i = 0; i < ARRAY_SIZE(ctx->old_groups); i++) { list_for_each_entry_safe(group, tmp, &ctx->old_groups[i], run_node) { drm_WARN_ON(&ptdev->base, !ctx->csg_upd_failed_mask && group_can_run(group)); if (!group_can_run(group)) { list_del_init(&group->run_node); list_del_init(&group->wait_node); group_queue_work(group, term); } else if (group->csg_id >= 0) { list_del_init(&group->run_node); } else { list_move(&group->run_node, group_is_idle(group) ? &sched->groups.idle[group->priority] : &sched->groups.runnable[group->priority]); } group_put(group); } } for (i = 0; i < ARRAY_SIZE(ctx->groups); i++) { drm_WARN_ON(&ptdev->base, !ctx->csg_upd_failed_mask && !list_empty(&ctx->groups[i])); list_for_each_entry_safe(group, tmp, &ctx->groups[i], run_node) { if (group->csg_id >= 0) { list_del_init(&group->run_node); } else { list_move(&group->run_node, group_is_idle(group) ? &sched->groups.idle[group->priority] : &sched->groups.runnable[group->priority]); } group_put(group); } } }"
1257----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56540/bad/ivpu_jsm_msg.c----ivpu_jsm_dct_disable,"int ivpu_jsm_dct_disable(struct ivpu_device *vdev) { struct vpu_jsm_msg req = { .type = VPU_JSM_MSG_DCT_DISABLE }; struct vpu_jsm_msg resp; <S2SV_StartVul> return ivpu_ipc_send_receive_active(vdev, &req, VPU_JSM_MSG_DCT_DISABLE_DONE, <S2SV_EndVul> <S2SV_StartVul> &resp, VPU_IPC_CHAN_ASYNC_CMD, <S2SV_EndVul> <S2SV_StartVul> vdev->timeout.jsm); <S2SV_EndVul> }","- return ivpu_ipc_send_receive_active(vdev, &req, VPU_JSM_MSG_DCT_DISABLE_DONE,
- &resp, VPU_IPC_CHAN_ASYNC_CMD,
- vdev->timeout.jsm);
+ return ivpu_ipc_send_receive_internal(vdev, &req, VPU_JSM_MSG_DCT_DISABLE_DONE, &resp,
+ VPU_IPC_CHAN_ASYNC_CMD, vdev->timeout.jsm);","int ivpu_jsm_dct_disable(struct ivpu_device *vdev) { struct vpu_jsm_msg req = { .type = VPU_JSM_MSG_DCT_DISABLE }; struct vpu_jsm_msg resp; return ivpu_ipc_send_receive_internal(vdev, &req, VPU_JSM_MSG_DCT_DISABLE_DONE, &resp, VPU_IPC_CHAN_ASYNC_CMD, vdev->timeout.jsm); }"
550----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49870/bad/namei.c----cachefiles_open_file,"static bool cachefiles_open_file(struct cachefiles_object *object, struct dentry *dentry) { struct cachefiles_cache *cache = object->volume->cache; struct file *file; struct path path; int ret; _enter(""%pd"", dentry); if (!cachefiles_mark_inode_in_use(object, d_inode(dentry))) { pr_notice(""cachefiles: Inode already in use: %pd (B=%lx)\n"", dentry, d_inode(dentry)->i_ino); return false; } path.mnt = cache->mnt; path.dentry = dentry; file = kernel_file_open(&path, O_RDWR | O_LARGEFILE | O_DIRECT, cache->cache_cred); if (IS_ERR(file)) { trace_cachefiles_vfs_error(object, d_backing_inode(dentry), PTR_ERR(file), cachefiles_trace_open_error); goto error; } if (unlikely(!file->f_op->read_iter) || unlikely(!file->f_op->write_iter)) { pr_notice(""Cache does not support read_iter and write_iter\n""); goto error_fput; } _debug(""file -> %pd positive"", dentry); ret = cachefiles_ondemand_init_object(object); if (ret < 0) goto error_fput; ret = cachefiles_check_auxdata(object, file); if (ret < 0) goto check_failed; clear_bit(FSCACHE_COOKIE_NO_DATA_TO_READ, &object->cookie->flags); object->file = file; touch_atime(&file->f_path); <S2SV_StartVul> dput(dentry); <S2SV_EndVul> return true; check_failed: fscache_cookie_lookup_negative(object->cookie); cachefiles_unmark_inode_in_use(object, file); fput(file); <S2SV_StartVul> dput(dentry); <S2SV_EndVul> if (ret == -ESTALE) return cachefiles_create_file(object); return false; error_fput: fput(file); error: cachefiles_do_unmark_inode_in_use(object, d_inode(dentry)); <S2SV_StartVul> dput(dentry); <S2SV_EndVul> return false; }","- dput(dentry);
- dput(dentry);
- dput(dentry);","static bool cachefiles_open_file(struct cachefiles_object *object, struct dentry *dentry) { struct cachefiles_cache *cache = object->volume->cache; struct file *file; struct path path; int ret; _enter(""%pd"", dentry); if (!cachefiles_mark_inode_in_use(object, d_inode(dentry))) { pr_notice(""cachefiles: Inode already in use: %pd (B=%lx)\n"", dentry, d_inode(dentry)->i_ino); return false; } path.mnt = cache->mnt; path.dentry = dentry; file = kernel_file_open(&path, O_RDWR | O_LARGEFILE | O_DIRECT, cache->cache_cred); if (IS_ERR(file)) { trace_cachefiles_vfs_error(object, d_backing_inode(dentry), PTR_ERR(file), cachefiles_trace_open_error); goto error; } if (unlikely(!file->f_op->read_iter) || unlikely(!file->f_op->write_iter)) { pr_notice(""Cache does not support read_iter and write_iter\n""); goto error_fput; } _debug(""file -> %pd positive"", dentry); ret = cachefiles_ondemand_init_object(object); if (ret < 0) goto error_fput; ret = cachefiles_check_auxdata(object, file); if (ret < 0) goto check_failed; clear_bit(FSCACHE_COOKIE_NO_DATA_TO_READ, &object->cookie->flags); object->file = file; touch_atime(&file->f_path); return true; check_failed: fscache_cookie_lookup_negative(object->cookie); cachefiles_unmark_inode_in_use(object, file); fput(file); if (ret == -ESTALE) return cachefiles_create_file(object); return false; error_fput: fput(file); error: cachefiles_do_unmark_inode_in_use(object, d_inode(dentry)); return false; }"
169----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-45023/bad/raid1.c----choose_slow_rdev,"static int choose_slow_rdev(struct r1conf *conf, struct r1bio *r1_bio, int *max_sectors) { sector_t this_sector = r1_bio->sector; int bb_disk = -1; int bb_read_len = 0; int disk; for (disk = 0 ; disk < conf->raid_disks * 2 ; disk++) { struct md_rdev *rdev; int len; int read_len; if (r1_bio->bios[disk] == IO_BLOCKED) continue; rdev = conf->mirrors[disk].rdev; if (!rdev || test_bit(Faulty, &rdev->flags) || <S2SV_StartVul> !test_bit(WriteMostly, &rdev->flags)) <S2SV_EndVul> continue; len = r1_bio->sectors; read_len = raid1_check_read_range(rdev, this_sector, &len); if (read_len == r1_bio->sectors) { *max_sectors = read_len; update_read_sectors(conf, disk, this_sector, read_len); return disk; } if (read_len > bb_read_len) { bb_disk = disk; bb_read_len = read_len; } } if (bb_disk != -1) { *max_sectors = bb_read_len; update_read_sectors(conf, bb_disk, this_sector, bb_read_len); } return bb_disk; }","- !test_bit(WriteMostly, &rdev->flags))
+ !test_bit(WriteMostly, &rdev->flags) ||
+ rdev_in_recovery(rdev, r1_bio))","static int choose_slow_rdev(struct r1conf *conf, struct r1bio *r1_bio, int *max_sectors) { sector_t this_sector = r1_bio->sector; int bb_disk = -1; int bb_read_len = 0; int disk; for (disk = 0 ; disk < conf->raid_disks * 2 ; disk++) { struct md_rdev *rdev; int len; int read_len; if (r1_bio->bios[disk] == IO_BLOCKED) continue; rdev = conf->mirrors[disk].rdev; if (!rdev || test_bit(Faulty, &rdev->flags) || !test_bit(WriteMostly, &rdev->flags) || rdev_in_recovery(rdev, r1_bio)) continue; len = r1_bio->sectors; read_len = raid1_check_read_range(rdev, this_sector, &len); if (read_len == r1_bio->sectors) { *max_sectors = read_len; update_read_sectors(conf, disk, this_sector, read_len); return disk; } if (read_len > bb_read_len) { bb_disk = disk; bb_read_len = read_len; } } if (bb_disk != -1) { *max_sectors = bb_read_len; update_read_sectors(conf, bb_disk, this_sector, bb_read_len); } return bb_disk; }"
404----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46865/bad/fou_core.c----*gue_gro_receive,"static struct sk_buff *gue_gro_receive(struct sock *sk, struct list_head *head, struct sk_buff *skb) { const struct net_offload __rcu **offloads; const struct net_offload *ops; struct sk_buff *pp = NULL; struct sk_buff *p; struct guehdr *guehdr; size_t len, optlen, hdrlen, off; void *data; u16 doffset = 0; int flush = 1; struct fou *fou = fou_from_sock(sk); struct gro_remcsum grc; u8 proto; if (!fou) goto out; <S2SV_StartVul> skb_gro_remcsum_init(&grc); <S2SV_EndVul> off = skb_gro_offset(skb); len = off + sizeof(*guehdr); guehdr = skb_gro_header(skb, len, off); if (unlikely(!guehdr)) goto out; switch (guehdr->version) { case 0: break; case 1: switch (((struct iphdr *)guehdr)->version) { case 4: proto = IPPROTO_IPIP; break; case 6: proto = IPPROTO_IPV6; break; default: goto out; } goto next_proto; default: goto out; } optlen = guehdr->hlen << 2; len += optlen; if (skb_gro_header_hard(skb, len)) { guehdr = skb_gro_header_slow(skb, len, off); if (unlikely(!guehdr)) goto out; } if (unlikely(guehdr->control) || guehdr->version != 0 || validate_gue_flags(guehdr, optlen)) goto out; hdrlen = sizeof(*guehdr) + optlen; skb_gro_postpull_rcsum(skb, guehdr, hdrlen); data = &guehdr[1]; if (guehdr->flags & GUE_FLAG_PRIV) { __be32 flags = *(__be32 *)(data + doffset); doffset += GUE_LEN_PRIV; if (flags & GUE_PFLAG_REMCSUM) { guehdr = gue_gro_remcsum(skb, off, guehdr, data + doffset, hdrlen, &grc, !!(fou->flags & FOU_F_REMCSUM_NOPARTIAL)); if (!guehdr) goto out; data = &guehdr[1]; doffset += GUE_PLEN_REMCSUM; } } skb_gro_pull(skb, hdrlen); list_for_each_entry(p, head, list) { const struct guehdr *guehdr2; if (!NAPI_GRO_CB(p)->same_flow) continue; guehdr2 = (struct guehdr *)(p->data + off); if (guehdr->word != guehdr2->word) { NAPI_GRO_CB(p)->same_flow = 0; continue; } if (guehdr->hlen && memcmp(&guehdr[1], &guehdr2[1], guehdr->hlen << 2)) { NAPI_GRO_CB(p)->same_flow = 0; continue; } } proto = guehdr->proto_ctype; next_proto: NAPI_GRO_CB(skb)->encap_mark = 0; NAPI_GRO_CB(skb)->is_fou = 1; offloads = NAPI_GRO_CB(skb)->is_ipv6 ? inet6_offloads : inet_offloads; ops = rcu_dereference(offloads[proto]); if (!ops || !ops->callbacks.gro_receive) goto out; pp = call_gro_receive(ops->callbacks.gro_receive, head, skb); flush = 0; out: skb_gro_flush_final_remcsum(skb, pp, flush, &grc); return pp; }","- skb_gro_remcsum_init(&grc);
+ skb_gro_remcsum_init(&grc);","static struct sk_buff *gue_gro_receive(struct sock *sk, struct list_head *head, struct sk_buff *skb) { const struct net_offload __rcu **offloads; const struct net_offload *ops; struct sk_buff *pp = NULL; struct sk_buff *p; struct guehdr *guehdr; size_t len, optlen, hdrlen, off; void *data; u16 doffset = 0; int flush = 1; struct fou *fou = fou_from_sock(sk); struct gro_remcsum grc; u8 proto; skb_gro_remcsum_init(&grc); if (!fou) goto out; off = skb_gro_offset(skb); len = off + sizeof(*guehdr); guehdr = skb_gro_header(skb, len, off); if (unlikely(!guehdr)) goto out; switch (guehdr->version) { case 0: break; case 1: switch (((struct iphdr *)guehdr)->version) { case 4: proto = IPPROTO_IPIP; break; case 6: proto = IPPROTO_IPV6; break; default: goto out; } goto next_proto; default: goto out; } optlen = guehdr->hlen << 2; len += optlen; if (skb_gro_header_hard(skb, len)) { guehdr = skb_gro_header_slow(skb, len, off); if (unlikely(!guehdr)) goto out; } if (unlikely(guehdr->control) || guehdr->version != 0 || validate_gue_flags(guehdr, optlen)) goto out; hdrlen = sizeof(*guehdr) + optlen; skb_gro_postpull_rcsum(skb, guehdr, hdrlen); data = &guehdr[1]; if (guehdr->flags & GUE_FLAG_PRIV) { __be32 flags = *(__be32 *)(data + doffset); doffset += GUE_LEN_PRIV; if (flags & GUE_PFLAG_REMCSUM) { guehdr = gue_gro_remcsum(skb, off, guehdr, data + doffset, hdrlen, &grc, !!(fou->flags & FOU_F_REMCSUM_NOPARTIAL)); if (!guehdr) goto out; data = &guehdr[1]; doffset += GUE_PLEN_REMCSUM; } } skb_gro_pull(skb, hdrlen); list_for_each_entry(p, head, list) { const struct guehdr *guehdr2; if (!NAPI_GRO_CB(p)->same_flow) continue; guehdr2 = (struct guehdr *)(p->data + off); if (guehdr->word != guehdr2->word) { NAPI_GRO_CB(p)->same_flow = 0; continue; } if (guehdr->hlen && memcmp(&guehdr[1], &guehdr2[1], guehdr->hlen << 2)) { NAPI_GRO_CB(p)->same_flow = 0; continue; } } proto = guehdr->proto_ctype; next_proto: NAPI_GRO_CB(skb)->encap_mark = 0; NAPI_GRO_CB(skb)->is_fou = 1; offloads = NAPI_GRO_CB(skb)->is_ipv6 ? inet6_offloads : inet_offloads; ops = rcu_dereference(offloads[proto]); if (!ops || !ops->callbacks.gro_receive) goto out; pp = call_gro_receive(ops->callbacks.gro_receive, head, skb); flush = 0; out: skb_gro_flush_final_remcsum(skb, pp, flush, &grc); return pp; }"
225----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46703/bad/8250_omap.c----omap8250_resume,static int omap8250_resume(struct device *dev) { struct omap8250_priv *priv = dev_get_drvdata(dev); struct uart_8250_port *up = serial8250_get_port(priv->line); <S2SV_StartVul> struct generic_pm_domain *genpd = pd_to_genpd(dev->pm_domain); <S2SV_EndVul> int err; if (uart_console(&up->port) && console_suspend_enabled) { <S2SV_StartVul> if (console_suspend_enabled) { <S2SV_EndVul> <S2SV_StartVul> err = pm_runtime_force_resume(dev); <S2SV_EndVul> <S2SV_StartVul> if (err) <S2SV_EndVul> <S2SV_StartVul> return err; <S2SV_EndVul> <S2SV_StartVul> } else <S2SV_EndVul> <S2SV_StartVul> genpd->flags = genpd_flags_console; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> serial8250_resume_port(priv->line); pm_runtime_mark_last_busy(dev); pm_runtime_put_autosuspend(dev); return 0; },"- struct generic_pm_domain *genpd = pd_to_genpd(dev->pm_domain);
- if (console_suspend_enabled) {
- err = pm_runtime_force_resume(dev);
- if (err)
- return err;
- } else
- genpd->flags = genpd_flags_console;
- }
+ err = pm_runtime_force_resume(dev);
+ if (err)
+ return err;",static int omap8250_resume(struct device *dev) { struct omap8250_priv *priv = dev_get_drvdata(dev); struct uart_8250_port *up = serial8250_get_port(priv->line); int err; if (uart_console(&up->port) && console_suspend_enabled) { err = pm_runtime_force_resume(dev); if (err) return err; } serial8250_resume_port(priv->line); pm_runtime_mark_last_busy(dev); pm_runtime_put_autosuspend(dev); return 0; }
597----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49919/bad/dcn201_resource.c----*dcn201_acquire_free_pipe_for_layer,"static struct pipe_ctx *dcn201_acquire_free_pipe_for_layer( const struct dc_state *cur_ctx, struct dc_state *new_ctx, const struct resource_pool *pool, const struct pipe_ctx *opp_head_pipe) { struct resource_context *res_ctx = &new_ctx->res_ctx; struct pipe_ctx *head_pipe = resource_get_otg_master_for_stream(res_ctx, opp_head_pipe->stream); struct pipe_ctx *idle_pipe = resource_find_free_secondary_pipe_legacy(res_ctx, pool, head_pipe); <S2SV_StartVul> if (!head_pipe) <S2SV_EndVul> ASSERT(0); if (!idle_pipe) return NULL; idle_pipe->stream = head_pipe->stream; idle_pipe->stream_res.tg = head_pipe->stream_res.tg; idle_pipe->stream_res.opp = head_pipe->stream_res.opp; idle_pipe->plane_res.hubp = pool->hubps[idle_pipe->pipe_idx]; idle_pipe->plane_res.ipp = pool->ipps[idle_pipe->pipe_idx]; idle_pipe->plane_res.dpp = pool->dpps[idle_pipe->pipe_idx]; idle_pipe->plane_res.mpcc_inst = pool->dpps[idle_pipe->pipe_idx]->inst; return idle_pipe; }","- if (!head_pipe)
+ if (!head_pipe) {
+ return NULL;
+ }
+ return NULL;","static struct pipe_ctx *dcn201_acquire_free_pipe_for_layer( const struct dc_state *cur_ctx, struct dc_state *new_ctx, const struct resource_pool *pool, const struct pipe_ctx *opp_head_pipe) { struct resource_context *res_ctx = &new_ctx->res_ctx; struct pipe_ctx *head_pipe = resource_get_otg_master_for_stream(res_ctx, opp_head_pipe->stream); struct pipe_ctx *idle_pipe = resource_find_free_secondary_pipe_legacy(res_ctx, pool, head_pipe); if (!head_pipe) { ASSERT(0); return NULL; } if (!idle_pipe) return NULL; idle_pipe->stream = head_pipe->stream; idle_pipe->stream_res.tg = head_pipe->stream_res.tg; idle_pipe->stream_res.opp = head_pipe->stream_res.opp; idle_pipe->plane_res.hubp = pool->hubps[idle_pipe->pipe_idx]; idle_pipe->plane_res.ipp = pool->ipps[idle_pipe->pipe_idx]; idle_pipe->plane_res.dpp = pool->dpps[idle_pipe->pipe_idx]; idle_pipe->plane_res.mpcc_inst = pool->dpps[idle_pipe->pipe_idx]->inst; return idle_pipe; }"
148----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-45009/bad/pm_netlink.c----mptcp_pm_nl_rm_addr_or_subflow,"static void mptcp_pm_nl_rm_addr_or_subflow(struct mptcp_sock *msk, const struct mptcp_rm_list *rm_list, enum linux_mptcp_mib_field rm_type) { struct mptcp_subflow_context *subflow, *tmp; struct sock *sk = (struct sock *)msk; u8 i; pr_debug(""%s rm_list_nr %d"", rm_type == MPTCP_MIB_RMADDR ? ""address"" : ""subflow"", rm_list->nr); msk_owned_by_me(msk); if (sk->sk_state == TCP_LISTEN) return; if (!rm_list->nr) return; if (list_empty(&msk->conn_list)) return; for (i = 0; i < rm_list->nr; i++) { bool removed = false; list_for_each_entry_safe(subflow, tmp, &msk->conn_list, node) { struct sock *ssk = mptcp_subflow_tcp_sock(subflow); int how = RCV_SHUTDOWN | SEND_SHUTDOWN; u8 id = subflow->local_id; if (rm_type == MPTCP_MIB_RMADDR) id = subflow->remote_id; if (rm_list->ids[i] != id) continue; pr_debug("" -> %s rm_list_ids[%d]=%u local_id=%u remote_id=%u"", rm_type == MPTCP_MIB_RMADDR ? ""address"" : ""subflow"", i, rm_list->ids[i], subflow->local_id, subflow->remote_id); spin_unlock_bh(&msk->pm.lock); mptcp_subflow_shutdown(sk, ssk, how); mptcp_close_ssk(sk, ssk, subflow); spin_lock_bh(&msk->pm.lock); <S2SV_StartVul> removed = true; <S2SV_EndVul> msk->pm.subflows--; if (rm_type == MPTCP_MIB_RMSUBFLOW) __MPTCP_INC_STATS(sock_net(sk), rm_type); } if (rm_type == MPTCP_MIB_RMADDR) __MPTCP_INC_STATS(sock_net(sk), rm_type); if (!removed) continue; <S2SV_StartVul> if (rm_type == MPTCP_MIB_RMADDR) { <S2SV_EndVul> msk->pm.add_addr_accepted--; WRITE_ONCE(msk->pm.accept_addr, true); } else if (rm_type == MPTCP_MIB_RMSUBFLOW) { msk->pm.local_addr_used--; } } }","- removed = true;
- if (rm_type == MPTCP_MIB_RMADDR) {
+ removed |= subflow->request_join;
+ if (rm_type == MPTCP_MIB_RMADDR && rm_list->ids[i] &&
+ msk->pm.add_addr_accepted != 0) {","static void mptcp_pm_nl_rm_addr_or_subflow(struct mptcp_sock *msk, const struct mptcp_rm_list *rm_list, enum linux_mptcp_mib_field rm_type) { struct mptcp_subflow_context *subflow, *tmp; struct sock *sk = (struct sock *)msk; u8 i; pr_debug(""%s rm_list_nr %d"", rm_type == MPTCP_MIB_RMADDR ? ""address"" : ""subflow"", rm_list->nr); msk_owned_by_me(msk); if (sk->sk_state == TCP_LISTEN) return; if (!rm_list->nr) return; if (list_empty(&msk->conn_list)) return; for (i = 0; i < rm_list->nr; i++) { bool removed = false; list_for_each_entry_safe(subflow, tmp, &msk->conn_list, node) { struct sock *ssk = mptcp_subflow_tcp_sock(subflow); int how = RCV_SHUTDOWN | SEND_SHUTDOWN; u8 id = subflow->local_id; if (rm_type == MPTCP_MIB_RMADDR) id = subflow->remote_id; if (rm_list->ids[i] != id) continue; pr_debug("" -> %s rm_list_ids[%d]=%u local_id=%u remote_id=%u"", rm_type == MPTCP_MIB_RMADDR ? ""address"" : ""subflow"", i, rm_list->ids[i], subflow->local_id, subflow->remote_id); spin_unlock_bh(&msk->pm.lock); mptcp_subflow_shutdown(sk, ssk, how); mptcp_close_ssk(sk, ssk, subflow); spin_lock_bh(&msk->pm.lock); removed |= subflow->request_join; msk->pm.subflows--; if (rm_type == MPTCP_MIB_RMSUBFLOW) __MPTCP_INC_STATS(sock_net(sk), rm_type); } if (rm_type == MPTCP_MIB_RMADDR) __MPTCP_INC_STATS(sock_net(sk), rm_type); if (!removed) continue; if (rm_type == MPTCP_MIB_RMADDR && rm_list->ids[i] && msk->pm.add_addr_accepted != 0) { msk->pm.add_addr_accepted--; WRITE_ONCE(msk->pm.accept_addr, true); } else if (rm_type == MPTCP_MIB_RMSUBFLOW) { msk->pm.local_addr_used--; } } }"
719----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49992/bad/ltdc.c----*ltdc_plane_create,"static struct drm_plane *ltdc_plane_create(struct drm_device *ddev, enum drm_plane_type type, int index) { unsigned long possible_crtcs = CRTC_MASK; struct ltdc_device *ldev = ddev->dev_private; struct device *dev = ddev->dev; struct drm_plane *plane; unsigned int i, nb_fmt = 0; u32 *formats; u32 drm_fmt; const u64 *modifiers = ltdc_format_modifiers; u32 lofs = index * LAY_OFS; u32 val; <S2SV_StartVul> int ret; <S2SV_EndVul> formats = devm_kzalloc(dev, (ldev->caps.pix_fmt_nb + ARRAY_SIZE(ltdc_drm_fmt_ycbcr_cp) + ARRAY_SIZE(ltdc_drm_fmt_ycbcr_sp) + ARRAY_SIZE(ltdc_drm_fmt_ycbcr_fp)) * sizeof(*formats), GFP_KERNEL); if (!formats) return NULL; for (i = 0; i < ldev->caps.pix_fmt_nb; i++) { drm_fmt = ldev->caps.pix_fmt_drm[i]; if (ldev->caps.non_alpha_only_l1) if (type != DRM_PLANE_TYPE_PRIMARY && is_xrgb(drm_fmt)) continue; formats[nb_fmt++] = drm_fmt; } if (ldev->caps.ycbcr_input) { regmap_read(ldev->regmap, LTDC_L1C1R + lofs, &val); if (val & LXCR_C1R_YIA) { memcpy(&formats[nb_fmt], ltdc_drm_fmt_ycbcr_cp, ARRAY_SIZE(ltdc_drm_fmt_ycbcr_cp) * sizeof(*formats)); nb_fmt += ARRAY_SIZE(ltdc_drm_fmt_ycbcr_cp); } if (val & LXCR_C1R_YSPA) { memcpy(&formats[nb_fmt], ltdc_drm_fmt_ycbcr_sp, ARRAY_SIZE(ltdc_drm_fmt_ycbcr_sp) * sizeof(*formats)); nb_fmt += ARRAY_SIZE(ltdc_drm_fmt_ycbcr_sp); } if (val & LXCR_C1R_YFPA) { memcpy(&formats[nb_fmt], ltdc_drm_fmt_ycbcr_fp, ARRAY_SIZE(ltdc_drm_fmt_ycbcr_fp) * sizeof(*formats)); nb_fmt += ARRAY_SIZE(ltdc_drm_fmt_ycbcr_fp); } <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> plane = devm_kzalloc(dev, sizeof(*plane), GFP_KERNEL); <S2SV_EndVul> <S2SV_StartVul> if (!plane) <S2SV_EndVul> <S2SV_StartVul> return NULL; <S2SV_EndVul> <S2SV_StartVul> ret = drm_universal_plane_init(ddev, plane, possible_crtcs, <S2SV_EndVul> <S2SV_StartVul> &ltdc_plane_funcs, formats, nb_fmt, <S2SV_EndVul> <S2SV_StartVul> modifiers, type, NULL); <S2SV_EndVul> <S2SV_StartVul> if (ret < 0) <S2SV_EndVul> <S2SV_StartVul> return NULL; <S2SV_EndVul> if (ldev->caps.ycbcr_input) { if (val & (LXCR_C1R_YIA | LXCR_C1R_YSPA | LXCR_C1R_YFPA)) drm_plane_create_color_properties(plane, BIT(DRM_COLOR_YCBCR_BT601) | BIT(DRM_COLOR_YCBCR_BT709), BIT(DRM_COLOR_YCBCR_LIMITED_RANGE) | BIT(DRM_COLOR_YCBCR_FULL_RANGE), DRM_COLOR_YCBCR_BT601, DRM_COLOR_YCBCR_LIMITED_RANGE); } drm_plane_helper_add(plane, &ltdc_plane_helper_funcs); drm_plane_create_alpha_property(plane); DRM_DEBUG_DRIVER(""plane:%d created\n"", plane->base.id); return plane; <S2SV_StartVul> } <S2SV_EndVul>","- int ret;
- }
- plane = devm_kzalloc(dev, sizeof(*plane), GFP_KERNEL);
- if (!plane)
- return NULL;
- ret = drm_universal_plane_init(ddev, plane, possible_crtcs,
- &ltdc_plane_funcs, formats, nb_fmt,
- modifiers, type, NULL);
- if (ret < 0)
- return NULL;
- }
+ plane = drmm_universal_plane_alloc(ddev, struct drm_plane, dev,
+ possible_crtcs, &ltdc_plane_funcs, formats,
+ nb_fmt, modifiers, type, NULL);
+ if (IS_ERR(plane))","static struct drm_plane *ltdc_plane_create(struct drm_device *ddev, enum drm_plane_type type, int index) { unsigned long possible_crtcs = CRTC_MASK; struct ltdc_device *ldev = ddev->dev_private; struct device *dev = ddev->dev; struct drm_plane *plane; unsigned int i, nb_fmt = 0; u32 *formats; u32 drm_fmt; const u64 *modifiers = ltdc_format_modifiers; u32 lofs = index * LAY_OFS; u32 val; formats = devm_kzalloc(dev, (ldev->caps.pix_fmt_nb + ARRAY_SIZE(ltdc_drm_fmt_ycbcr_cp) + ARRAY_SIZE(ltdc_drm_fmt_ycbcr_sp) + ARRAY_SIZE(ltdc_drm_fmt_ycbcr_fp)) * sizeof(*formats), GFP_KERNEL); if (!formats) return NULL; for (i = 0; i < ldev->caps.pix_fmt_nb; i++) { drm_fmt = ldev->caps.pix_fmt_drm[i]; if (ldev->caps.non_alpha_only_l1) if (type != DRM_PLANE_TYPE_PRIMARY && is_xrgb(drm_fmt)) continue; formats[nb_fmt++] = drm_fmt; } if (ldev->caps.ycbcr_input) { regmap_read(ldev->regmap, LTDC_L1C1R + lofs, &val); if (val & LXCR_C1R_YIA) { memcpy(&formats[nb_fmt], ltdc_drm_fmt_ycbcr_cp, ARRAY_SIZE(ltdc_drm_fmt_ycbcr_cp) * sizeof(*formats)); nb_fmt += ARRAY_SIZE(ltdc_drm_fmt_ycbcr_cp); } if (val & LXCR_C1R_YSPA) { memcpy(&formats[nb_fmt], ltdc_drm_fmt_ycbcr_sp, ARRAY_SIZE(ltdc_drm_fmt_ycbcr_sp) * sizeof(*formats)); nb_fmt += ARRAY_SIZE(ltdc_drm_fmt_ycbcr_sp); } if (val & LXCR_C1R_YFPA) { memcpy(&formats[nb_fmt], ltdc_drm_fmt_ycbcr_fp, ARRAY_SIZE(ltdc_drm_fmt_ycbcr_fp) * sizeof(*formats)); nb_fmt += ARRAY_SIZE(ltdc_drm_fmt_ycbcr_fp); } } plane = drmm_universal_plane_alloc(ddev, struct drm_plane, dev, possible_crtcs, &ltdc_plane_funcs, formats, nb_fmt, modifiers, type, NULL); if (IS_ERR(plane)) return NULL; if (ldev->caps.ycbcr_input) { if (val & (LXCR_C1R_YIA | LXCR_C1R_YSPA | LXCR_C1R_YFPA)) drm_plane_create_color_properties(plane, BIT(DRM_COLOR_YCBCR_BT601) | BIT(DRM_COLOR_YCBCR_BT709), BIT(DRM_COLOR_YCBCR_LIMITED_RANGE) | BIT(DRM_COLOR_YCBCR_FULL_RANGE), DRM_COLOR_YCBCR_BT601, DRM_COLOR_YCBCR_LIMITED_RANGE); } drm_plane_helper_add(plane, &ltdc_plane_helper_funcs); drm_plane_create_alpha_property(plane); DRM_DEBUG_DRIVER(""plane:%d created\n"", plane->base.id); return plane; }"
621----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49925/bad/efifb.c----efifb_probe,"static int efifb_probe(struct platform_device *dev) { struct fb_info *info; struct efifb_par *par; int err, orientation; unsigned int size_vmode; unsigned int size_remap; unsigned int size_total; char *option = NULL; efi_memory_desc_t md; if (screen_info.orig_video_isVGA != VIDEO_TYPE_EFI || pci_dev_disabled) return -ENODEV; if (fb_get_options(""efifb"", &option)) return -ENODEV; efifb_setup(option); if (!screen_info.lfb_linelength) return -ENODEV; if (!screen_info.lfb_depth) screen_info.lfb_depth = 32; if (!screen_info.pages) screen_info.pages = 1; if (!fb_base_is_valid()) { printk(KERN_DEBUG ""efifb: invalid framebuffer address\n""); return -ENODEV; } printk(KERN_INFO ""efifb: probing for efifb\n""); if (!screen_info.blue_size) { screen_info.blue_size = 8; screen_info.blue_pos = 0; screen_info.green_size = 8; screen_info.green_pos = 8; screen_info.red_size = 8; screen_info.red_pos = 16; screen_info.rsvd_size = 8; screen_info.rsvd_pos = 24; } efifb_fix.smem_start = screen_info.lfb_base; if (screen_info.capabilities & VIDEO_CAPABILITY_64BIT_BASE) { u64 ext_lfb_base; ext_lfb_base = (u64)(unsigned long)screen_info.ext_lfb_base << 32; efifb_fix.smem_start |= ext_lfb_base; } if (bar_resource && bar_resource->start + bar_offset != efifb_fix.smem_start) { dev_info(&efifb_pci_dev->dev, ""BAR has moved, updating efifb address\n""); efifb_fix.smem_start = bar_resource->start + bar_offset; } efifb_defined.bits_per_pixel = screen_info.lfb_depth; efifb_defined.xres = screen_info.lfb_width; efifb_defined.yres = screen_info.lfb_height; efifb_fix.line_length = screen_info.lfb_linelength; size_vmode = efifb_defined.yres * efifb_fix.line_length; size_total = screen_info.lfb_size; if (size_total < size_vmode) size_total = size_vmode; size_remap = size_vmode * 2; if (size_remap > size_total) size_remap = size_total; if (size_remap % PAGE_SIZE) size_remap += PAGE_SIZE - (size_remap % PAGE_SIZE); efifb_fix.smem_len = size_remap; if (request_mem_region(efifb_fix.smem_start, size_remap, ""efifb"")) { request_mem_succeeded = true; } else { pr_warn(""efifb: cannot reserve video memory at 0x%lx\n"", efifb_fix.smem_start); } info = framebuffer_alloc(sizeof(*par), &dev->dev); if (!info) { err = -ENOMEM; goto err_release_mem; } platform_set_drvdata(dev, info); par = info->par; info->pseudo_palette = par->pseudo_palette; par->base = efifb_fix.smem_start; par->size = size_remap; if (efi_enabled(EFI_MEMMAP) && !efi_mem_desc_lookup(efifb_fix.smem_start, &md)) { if ((efifb_fix.smem_start + efifb_fix.smem_len) > (md.phys_addr + (md.num_pages << EFI_PAGE_SHIFT))) { pr_err(""efifb: video memory @ 0x%lx spans multiple EFI memory regions\n"", efifb_fix.smem_start); err = -EIO; goto err_release_fb; } md.attribute &= EFI_MEMORY_UC | EFI_MEMORY_WC | EFI_MEMORY_WT | EFI_MEMORY_WB; if (md.attribute) { mem_flags |= EFI_MEMORY_WT | EFI_MEMORY_WB; mem_flags &= md.attribute; } } if (mem_flags & EFI_MEMORY_WC) info->screen_base = ioremap_wc(efifb_fix.smem_start, efifb_fix.smem_len); else if (mem_flags & EFI_MEMORY_UC) info->screen_base = ioremap(efifb_fix.smem_start, efifb_fix.smem_len); else if (mem_flags & EFI_MEMORY_WT) info->screen_base = memremap(efifb_fix.smem_start, efifb_fix.smem_len, MEMREMAP_WT); else if (mem_flags & EFI_MEMORY_WB) info->screen_base = memremap(efifb_fix.smem_start, efifb_fix.smem_len, MEMREMAP_WB); if (!info->screen_base) { pr_err(""efifb: abort, cannot remap video memory 0x%x @ 0x%lx\n"", efifb_fix.smem_len, efifb_fix.smem_start); err = -EIO; goto err_release_fb; } efifb_show_boot_graphics(info); pr_info(""efifb: framebuffer at 0x%lx, using %dk, total %dk\n"", efifb_fix.smem_start, size_remap/1024, size_total/1024); pr_info(""efifb: mode is %dx%dx%d, linelength=%d, pages=%d\n"", efifb_defined.xres, efifb_defined.yres, efifb_defined.bits_per_pixel, efifb_fix.line_length, screen_info.pages); efifb_defined.xres_virtual = efifb_defined.xres; efifb_defined.yres_virtual = efifb_fix.smem_len / efifb_fix.line_length; pr_info(""efifb: scrolling: redraw\n""); efifb_defined.yres_virtual = efifb_defined.yres; efifb_defined.pixclock = 10000000 / efifb_defined.xres * 1000 / efifb_defined.yres; efifb_defined.left_margin = (efifb_defined.xres / 8) & 0xf8; efifb_defined.hsync_len = (efifb_defined.xres / 8) & 0xf8; efifb_defined.red.offset = screen_info.red_pos; efifb_defined.red.length = screen_info.red_size; efifb_defined.green.offset = screen_info.green_pos; efifb_defined.green.length = screen_info.green_size; efifb_defined.blue.offset = screen_info.blue_pos; efifb_defined.blue.length = screen_info.blue_size; efifb_defined.transp.offset = screen_info.rsvd_pos; efifb_defined.transp.length = screen_info.rsvd_size; pr_info(""efifb: %s: "" ""size=%d:%d:%d:%d, shift=%d:%d:%d:%d\n"", ""Truecolor"", screen_info.rsvd_size, screen_info.red_size, screen_info.green_size, screen_info.blue_size, screen_info.rsvd_pos, screen_info.red_pos, screen_info.green_pos, screen_info.blue_pos); efifb_fix.ypanstep = 0; efifb_fix.ywrapstep = 0; info->fbops = &efifb_ops; info->var = efifb_defined; info->fix = efifb_fix; orientation = drm_get_panel_orientation_quirk(efifb_defined.xres, efifb_defined.yres); switch (orientation) { default: info->fbcon_rotate_hint = FB_ROTATE_UR; break; case DRM_MODE_PANEL_ORIENTATION_BOTTOM_UP: info->fbcon_rotate_hint = FB_ROTATE_UD; break; case DRM_MODE_PANEL_ORIENTATION_LEFT_UP: info->fbcon_rotate_hint = FB_ROTATE_CCW; break; case DRM_MODE_PANEL_ORIENTATION_RIGHT_UP: info->fbcon_rotate_hint = FB_ROTATE_CW; break; <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> err = sysfs_create_groups(&dev->dev.kobj, efifb_groups); <S2SV_EndVul> <S2SV_StartVul> if (err) { <S2SV_EndVul> <S2SV_StartVul> pr_err(""efifb: cannot add sysfs attrs\n""); <S2SV_EndVul> <S2SV_StartVul> goto err_unmap; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> err = fb_alloc_cmap(&info->cmap, 256, 0); if (err < 0) { pr_err(""efifb: cannot allocate colormap\n""); <S2SV_StartVul> goto err_groups; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> if (efifb_pci_dev) WARN_ON(pm_runtime_get_sync(&efifb_pci_dev->dev) < 0); err = devm_aperture_acquire_for_platform_device(dev, par->base, par->size); if (err) { pr_err(""efifb: cannot acquire aperture\n""); goto err_put_rpm_ref; } err = register_framebuffer(info); if (err < 0) { pr_err(""efifb: cannot register framebuffer\n""); goto err_put_rpm_ref; } fb_info(info, ""%s frame buffer device\n"", info->fix.id); return 0; err_put_rpm_ref: if (efifb_pci_dev) pm_runtime_put(&efifb_pci_dev->dev); fb_dealloc_cmap(&info->cmap); <S2SV_StartVul> err_groups: <S2SV_EndVul> <S2SV_StartVul> sysfs_remove_groups(&dev->dev.kobj, efifb_groups); <S2SV_EndVul> err_unmap: if (mem_flags & (EFI_MEMORY_UC | EFI_MEMORY_WC)) iounmap(info->screen_base); else memunmap(info->screen_base); err_release_fb: framebuffer_release(info); err_release_mem: if (request_mem_succeeded) release_mem_region(efifb_fix.smem_start, size_total); return err; }","- }
- err = sysfs_create_groups(&dev->dev.kobj, efifb_groups);
- if (err) {
- pr_err(""efifb: cannot add sysfs attrs\n"");
- goto err_unmap;
- }
- goto err_groups;
- }
- err_groups:
- sysfs_remove_groups(&dev->dev.kobj, efifb_groups);
+ goto err_unmap;","static int efifb_probe(struct platform_device *dev) { struct fb_info *info; struct efifb_par *par; int err, orientation; unsigned int size_vmode; unsigned int size_remap; unsigned int size_total; char *option = NULL; efi_memory_desc_t md; if (screen_info.orig_video_isVGA != VIDEO_TYPE_EFI || pci_dev_disabled) return -ENODEV; if (fb_get_options(""efifb"", &option)) return -ENODEV; efifb_setup(option); if (!screen_info.lfb_linelength) return -ENODEV; if (!screen_info.lfb_depth) screen_info.lfb_depth = 32; if (!screen_info.pages) screen_info.pages = 1; if (!fb_base_is_valid()) { printk(KERN_DEBUG ""efifb: invalid framebuffer address\n""); return -ENODEV; } printk(KERN_INFO ""efifb: probing for efifb\n""); if (!screen_info.blue_size) { screen_info.blue_size = 8; screen_info.blue_pos = 0; screen_info.green_size = 8; screen_info.green_pos = 8; screen_info.red_size = 8; screen_info.red_pos = 16; screen_info.rsvd_size = 8; screen_info.rsvd_pos = 24; } efifb_fix.smem_start = screen_info.lfb_base; if (screen_info.capabilities & VIDEO_CAPABILITY_64BIT_BASE) { u64 ext_lfb_base; ext_lfb_base = (u64)(unsigned long)screen_info.ext_lfb_base << 32; efifb_fix.smem_start |= ext_lfb_base; } if (bar_resource && bar_resource->start + bar_offset != efifb_fix.smem_start) { dev_info(&efifb_pci_dev->dev, ""BAR has moved, updating efifb address\n""); efifb_fix.smem_start = bar_resource->start + bar_offset; } efifb_defined.bits_per_pixel = screen_info.lfb_depth; efifb_defined.xres = screen_info.lfb_width; efifb_defined.yres = screen_info.lfb_height; efifb_fix.line_length = screen_info.lfb_linelength; size_vmode = efifb_defined.yres * efifb_fix.line_length; size_total = screen_info.lfb_size; if (size_total < size_vmode) size_total = size_vmode; size_remap = size_vmode * 2; if (size_remap > size_total) size_remap = size_total; if (size_remap % PAGE_SIZE) size_remap += PAGE_SIZE - (size_remap % PAGE_SIZE); efifb_fix.smem_len = size_remap; if (request_mem_region(efifb_fix.smem_start, size_remap, ""efifb"")) { request_mem_succeeded = true; } else { pr_warn(""efifb: cannot reserve video memory at 0x%lx\n"", efifb_fix.smem_start); } info = framebuffer_alloc(sizeof(*par), &dev->dev); if (!info) { err = -ENOMEM; goto err_release_mem; } platform_set_drvdata(dev, info); par = info->par; info->pseudo_palette = par->pseudo_palette; par->base = efifb_fix.smem_start; par->size = size_remap; if (efi_enabled(EFI_MEMMAP) && !efi_mem_desc_lookup(efifb_fix.smem_start, &md)) { if ((efifb_fix.smem_start + efifb_fix.smem_len) > (md.phys_addr + (md.num_pages << EFI_PAGE_SHIFT))) { pr_err(""efifb: video memory @ 0x%lx spans multiple EFI memory regions\n"", efifb_fix.smem_start); err = -EIO; goto err_release_fb; } md.attribute &= EFI_MEMORY_UC | EFI_MEMORY_WC | EFI_MEMORY_WT | EFI_MEMORY_WB; if (md.attribute) { mem_flags |= EFI_MEMORY_WT | EFI_MEMORY_WB; mem_flags &= md.attribute; } } if (mem_flags & EFI_MEMORY_WC) info->screen_base = ioremap_wc(efifb_fix.smem_start, efifb_fix.smem_len); else if (mem_flags & EFI_MEMORY_UC) info->screen_base = ioremap(efifb_fix.smem_start, efifb_fix.smem_len); else if (mem_flags & EFI_MEMORY_WT) info->screen_base = memremap(efifb_fix.smem_start, efifb_fix.smem_len, MEMREMAP_WT); else if (mem_flags & EFI_MEMORY_WB) info->screen_base = memremap(efifb_fix.smem_start, efifb_fix.smem_len, MEMREMAP_WB); if (!info->screen_base) { pr_err(""efifb: abort, cannot remap video memory 0x%x @ 0x%lx\n"", efifb_fix.smem_len, efifb_fix.smem_start); err = -EIO; goto err_release_fb; } efifb_show_boot_graphics(info); pr_info(""efifb: framebuffer at 0x%lx, using %dk, total %dk\n"", efifb_fix.smem_start, size_remap/1024, size_total/1024); pr_info(""efifb: mode is %dx%dx%d, linelength=%d, pages=%d\n"", efifb_defined.xres, efifb_defined.yres, efifb_defined.bits_per_pixel, efifb_fix.line_length, screen_info.pages); efifb_defined.xres_virtual = efifb_defined.xres; efifb_defined.yres_virtual = efifb_fix.smem_len / efifb_fix.line_length; pr_info(""efifb: scrolling: redraw\n""); efifb_defined.yres_virtual = efifb_defined.yres; efifb_defined.pixclock = 10000000 / efifb_defined.xres * 1000 / efifb_defined.yres; efifb_defined.left_margin = (efifb_defined.xres / 8) & 0xf8; efifb_defined.hsync_len = (efifb_defined.xres / 8) & 0xf8; efifb_defined.red.offset = screen_info.red_pos; efifb_defined.red.length = screen_info.red_size; efifb_defined.green.offset = screen_info.green_pos; efifb_defined.green.length = screen_info.green_size; efifb_defined.blue.offset = screen_info.blue_pos; efifb_defined.blue.length = screen_info.blue_size; efifb_defined.transp.offset = screen_info.rsvd_pos; efifb_defined.transp.length = screen_info.rsvd_size; pr_info(""efifb: %s: "" ""size=%d:%d:%d:%d, shift=%d:%d:%d:%d\n"", ""Truecolor"", screen_info.rsvd_size, screen_info.red_size, screen_info.green_size, screen_info.blue_size, screen_info.rsvd_pos, screen_info.red_pos, screen_info.green_pos, screen_info.blue_pos); efifb_fix.ypanstep = 0; efifb_fix.ywrapstep = 0; info->fbops = &efifb_ops; info->var = efifb_defined; info->fix = efifb_fix; orientation = drm_get_panel_orientation_quirk(efifb_defined.xres, efifb_defined.yres); switch (orientation) { default: info->fbcon_rotate_hint = FB_ROTATE_UR; break; case DRM_MODE_PANEL_ORIENTATION_BOTTOM_UP: info->fbcon_rotate_hint = FB_ROTATE_UD; break; case DRM_MODE_PANEL_ORIENTATION_LEFT_UP: info->fbcon_rotate_hint = FB_ROTATE_CCW; break; case DRM_MODE_PANEL_ORIENTATION_RIGHT_UP: info->fbcon_rotate_hint = FB_ROTATE_CW; break; } err = fb_alloc_cmap(&info->cmap, 256, 0); if (err < 0) { pr_err(""efifb: cannot allocate colormap\n""); goto err_unmap; } if (efifb_pci_dev) WARN_ON(pm_runtime_get_sync(&efifb_pci_dev->dev) < 0); err = devm_aperture_acquire_for_platform_device(dev, par->base, par->size); if (err) { pr_err(""efifb: cannot acquire aperture\n""); goto err_put_rpm_ref; } err = register_framebuffer(info); if (err < 0) { pr_err(""efifb: cannot register framebuffer\n""); goto err_put_rpm_ref; } fb_info(info, ""%s frame buffer device\n"", info->fix.id); return 0; err_put_rpm_ref: if (efifb_pci_dev) pm_runtime_put(&efifb_pci_dev->dev); fb_dealloc_cmap(&info->cmap); err_unmap: if (mem_flags & (EFI_MEMORY_UC | EFI_MEMORY_WC)) iounmap(info->screen_base); else memunmap(info->screen_base); err_release_fb: framebuffer_release(info); err_release_mem: if (request_mem_succeeded) release_mem_region(efifb_fix.smem_start, size_total); return err; }"
376----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46840/bad/extent-tree.c----walk_down_proc,"static noinline int walk_down_proc(struct btrfs_trans_handle *trans, struct btrfs_root *root, struct btrfs_path *path, struct walk_control *wc, int lookup_info) { struct btrfs_fs_info *fs_info = root->fs_info; int level = wc->level; struct extent_buffer *eb = path->nodes[level]; u64 flag = BTRFS_BLOCK_FLAG_FULL_BACKREF; int ret; if (wc->stage == UPDATE_BACKREF && btrfs_header_owner(eb) != root->root_key.objectid) return 1; if (lookup_info && ((wc->stage == DROP_REFERENCE && wc->refs[level] != 1) || (wc->stage == UPDATE_BACKREF && !(wc->flags[level] & flag)))) { ASSERT(path->locks[level]); ret = btrfs_lookup_extent_info(trans, fs_info, eb->start, level, 1, &wc->refs[level], &wc->flags[level]); BUG_ON(ret == -ENOMEM); if (ret) return ret; <S2SV_StartVul> BUG_ON(wc->refs[level] == 0); <S2SV_EndVul> } if (wc->stage == DROP_REFERENCE) { if (wc->refs[level] > 1) return 1; if (path->locks[level] && !wc->keep_locks) { btrfs_tree_unlock_rw(eb, path->locks[level]); path->locks[level] = 0; } return 0; } if (!(wc->flags[level] & flag)) { ASSERT(path->locks[level]); ret = btrfs_inc_ref(trans, root, eb, 1); BUG_ON(ret); ret = btrfs_dec_ref(trans, root, eb, 0); BUG_ON(ret); ret = btrfs_set_disk_extent_flags(trans, eb, flag, btrfs_header_level(eb), 0); BUG_ON(ret); wc->flags[level] |= flag; } if (path->locks[level] && level > 0) { btrfs_tree_unlock_rw(eb, path->locks[level]); path->locks[level] = 0; } return 0; }","- BUG_ON(wc->refs[level] == 0);
+ if (unlikely(wc->refs[level] == 0)) {
+ btrfs_err(fs_info, ""bytenr %llu has 0 references, expect > 0"",
+ eb->start);
+ return -EUCLEAN;
+ }
+ }","static noinline int walk_down_proc(struct btrfs_trans_handle *trans, struct btrfs_root *root, struct btrfs_path *path, struct walk_control *wc, int lookup_info) { struct btrfs_fs_info *fs_info = root->fs_info; int level = wc->level; struct extent_buffer *eb = path->nodes[level]; u64 flag = BTRFS_BLOCK_FLAG_FULL_BACKREF; int ret; if (wc->stage == UPDATE_BACKREF && btrfs_header_owner(eb) != root->root_key.objectid) return 1; if (lookup_info && ((wc->stage == DROP_REFERENCE && wc->refs[level] != 1) || (wc->stage == UPDATE_BACKREF && !(wc->flags[level] & flag)))) { ASSERT(path->locks[level]); ret = btrfs_lookup_extent_info(trans, fs_info, eb->start, level, 1, &wc->refs[level], &wc->flags[level]); BUG_ON(ret == -ENOMEM); if (ret) return ret; if (unlikely(wc->refs[level] == 0)) { btrfs_err(fs_info, ""bytenr %llu has 0 references, expect > 0"", eb->start); return -EUCLEAN; } } if (wc->stage == DROP_REFERENCE) { if (wc->refs[level] > 1) return 1; if (path->locks[level] && !wc->keep_locks) { btrfs_tree_unlock_rw(eb, path->locks[level]); path->locks[level] = 0; } return 0; } if (!(wc->flags[level] & flag)) { ASSERT(path->locks[level]); ret = btrfs_inc_ref(trans, root, eb, 1); BUG_ON(ret); ret = btrfs_dec_ref(trans, root, eb, 0); BUG_ON(ret); ret = btrfs_set_disk_extent_flags(trans, eb, flag, btrfs_header_level(eb), 0); BUG_ON(ret); wc->flags[level] |= flag; } if (path->locks[level] && level > 0) { btrfs_tree_unlock_rw(eb, path->locks[level]); path->locks[level] = 0; } return 0; }"
810----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50085/bad/pm_netlink.c----mptcp_pm_nl_rm_addr_or_subflow,"static void mptcp_pm_nl_rm_addr_or_subflow(struct mptcp_sock *msk, const struct mptcp_rm_list *rm_list, enum linux_mptcp_mib_field rm_type) { struct mptcp_subflow_context *subflow, *tmp; struct sock *sk = (struct sock *)msk; u8 i; pr_debug(""%s rm_list_nr %d\n"", rm_type == MPTCP_MIB_RMADDR ? ""address"" : ""subflow"", rm_list->nr); msk_owned_by_me(msk); if (sk->sk_state == TCP_LISTEN) return; if (!rm_list->nr) return; if (list_empty(&msk->conn_list)) return; for (i = 0; i < rm_list->nr; i++) { bool removed = false; list_for_each_entry_safe(subflow, tmp, &msk->conn_list, node) { struct sock *ssk = mptcp_subflow_tcp_sock(subflow); int how = RCV_SHUTDOWN | SEND_SHUTDOWN; u8 id = subflow->local_id; if ((1 << inet_sk_state_load(ssk)) & (TCPF_FIN_WAIT1 | TCPF_FIN_WAIT2 | TCPF_CLOSING | TCPF_CLOSE)) continue; if (rm_type == MPTCP_MIB_RMADDR) id = subflow->remote_id; if (rm_list->ids[i] != id) continue; pr_debug("" -> %s rm_list_ids[%d]=%u local_id=%u remote_id=%u\n"", rm_type == MPTCP_MIB_RMADDR ? ""address"" : ""subflow"", i, rm_list->ids[i], subflow->local_id, subflow->remote_id); spin_unlock_bh(&msk->pm.lock); mptcp_subflow_shutdown(sk, ssk, how); mptcp_close_ssk(sk, ssk, subflow); spin_lock_bh(&msk->pm.lock); <S2SV_StartVul> removed |= subflow->request_join; <S2SV_EndVul> msk->pm.subflows--; if (rm_type == MPTCP_MIB_RMSUBFLOW) __MPTCP_INC_STATS(sock_net(sk), rm_type); } if (rm_type == MPTCP_MIB_RMADDR) __MPTCP_INC_STATS(sock_net(sk), rm_type); if (!removed) continue; if (rm_type == MPTCP_MIB_RMADDR && rm_list->ids[i] && msk->pm.add_addr_accepted != 0) { if (--msk->pm.add_addr_accepted < mptcp_pm_get_add_addr_accept_max(msk)) WRITE_ONCE(msk->pm.accept_addr, true); } else if (rm_type == MPTCP_MIB_RMSUBFLOW) { msk->pm.local_addr_used--; } } }","- removed |= subflow->request_join;
+ removed |= subflow->request_join;","static void mptcp_pm_nl_rm_addr_or_subflow(struct mptcp_sock *msk, const struct mptcp_rm_list *rm_list, enum linux_mptcp_mib_field rm_type) { struct mptcp_subflow_context *subflow, *tmp; struct sock *sk = (struct sock *)msk; u8 i; pr_debug(""%s rm_list_nr %d\n"", rm_type == MPTCP_MIB_RMADDR ? ""address"" : ""subflow"", rm_list->nr); msk_owned_by_me(msk); if (sk->sk_state == TCP_LISTEN) return; if (!rm_list->nr) return; if (list_empty(&msk->conn_list)) return; for (i = 0; i < rm_list->nr; i++) { bool removed = false; list_for_each_entry_safe(subflow, tmp, &msk->conn_list, node) { struct sock *ssk = mptcp_subflow_tcp_sock(subflow); int how = RCV_SHUTDOWN | SEND_SHUTDOWN; u8 id = subflow->local_id; if ((1 << inet_sk_state_load(ssk)) & (TCPF_FIN_WAIT1 | TCPF_FIN_WAIT2 | TCPF_CLOSING | TCPF_CLOSE)) continue; if (rm_type == MPTCP_MIB_RMADDR) id = subflow->remote_id; if (rm_list->ids[i] != id) continue; pr_debug("" -> %s rm_list_ids[%d]=%u local_id=%u remote_id=%u\n"", rm_type == MPTCP_MIB_RMADDR ? ""address"" : ""subflow"", i, rm_list->ids[i], subflow->local_id, subflow->remote_id); spin_unlock_bh(&msk->pm.lock); mptcp_subflow_shutdown(sk, ssk, how); removed |= subflow->request_join; mptcp_close_ssk(sk, ssk, subflow); spin_lock_bh(&msk->pm.lock); msk->pm.subflows--; if (rm_type == MPTCP_MIB_RMSUBFLOW) __MPTCP_INC_STATS(sock_net(sk), rm_type); } if (rm_type == MPTCP_MIB_RMADDR) __MPTCP_INC_STATS(sock_net(sk), rm_type); if (!removed) continue; if (rm_type == MPTCP_MIB_RMADDR && rm_list->ids[i] && msk->pm.add_addr_accepted != 0) { if (--msk->pm.add_addr_accepted < mptcp_pm_get_add_addr_accept_max(msk)) WRITE_ONCE(msk->pm.accept_addr, true); } else if (rm_type == MPTCP_MIB_RMSUBFLOW) { msk->pm.local_addr_used--; } } }"
1342----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56660/bad/dr_domain.c----dr_domain_add_vport_cap,"dr_domain_add_vport_cap(struct mlx5dr_domain *dmn, u16 vport) { struct mlx5dr_cmd_caps *caps = &dmn->info.caps; struct mlx5dr_cmd_vport_cap *vport_caps; int ret; vport_caps = kvzalloc(sizeof(*vport_caps), GFP_KERNEL); if (!vport_caps) return NULL; ret = dr_domain_query_vport(dmn, vport, true, vport_caps); if (ret) { kvfree(vport_caps); return NULL; } ret = xa_insert(&caps->vports.vports_caps_xa, vport, vport_caps, GFP_KERNEL); if (ret) { mlx5dr_dbg(dmn, ""Couldn't insert new vport into xarray (%d)\n"", ret); kvfree(vport_caps); <S2SV_StartVul> return ERR_PTR(ret); <S2SV_EndVul> } return vport_caps; }","- return ERR_PTR(ret);
+ if (ret == -EBUSY)
+ return ERR_PTR(-EBUSY);
+ return NULL;","dr_domain_add_vport_cap(struct mlx5dr_domain *dmn, u16 vport) { struct mlx5dr_cmd_caps *caps = &dmn->info.caps; struct mlx5dr_cmd_vport_cap *vport_caps; int ret; vport_caps = kvzalloc(sizeof(*vport_caps), GFP_KERNEL); if (!vport_caps) return NULL; ret = dr_domain_query_vport(dmn, vport, true, vport_caps); if (ret) { kvfree(vport_caps); return NULL; } ret = xa_insert(&caps->vports.vports_caps_xa, vport, vport_caps, GFP_KERNEL); if (ret) { mlx5dr_dbg(dmn, ""Couldn't insert new vport into xarray (%d)\n"", ret); kvfree(vport_caps); if (ret == -EBUSY) return ERR_PTR(-EBUSY); return NULL; } return vport_caps; }"
265----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46749/bad/btnxpuart.c----btnxpuart_close,"static int btnxpuart_close(struct hci_dev *hdev) { struct btnxpuart_dev *nxpdev = hci_get_drvdata(hdev); serdev_device_close(nxpdev->serdev); skb_queue_purge(&nxpdev->txq); <S2SV_StartVul> kfree_skb(nxpdev->rx_skb); <S2SV_EndVul> <S2SV_StartVul> nxpdev->rx_skb = NULL; <S2SV_EndVul> clear_bit(BTNXPUART_SERDEV_OPEN, &nxpdev->tx_state); return 0; }","- kfree_skb(nxpdev->rx_skb);
- nxpdev->rx_skb = NULL;
+ if (!IS_ERR_OR_NULL(nxpdev->rx_skb)) {
+ kfree_skb(nxpdev->rx_skb);
+ nxpdev->rx_skb = NULL;
+ }
+ }","static int btnxpuart_close(struct hci_dev *hdev) { struct btnxpuart_dev *nxpdev = hci_get_drvdata(hdev); serdev_device_close(nxpdev->serdev); skb_queue_purge(&nxpdev->txq); if (!IS_ERR_OR_NULL(nxpdev->rx_skb)) { kfree_skb(nxpdev->rx_skb); nxpdev->rx_skb = NULL; } clear_bit(BTNXPUART_SERDEV_OPEN, &nxpdev->tx_state); return 0; }"
1031----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50271/bad/ucount.c----inc_rlimit_get_ucounts,"long inc_rlimit_get_ucounts(struct ucounts *ucounts, enum rlimit_type type) { struct ucounts *iter; long max = LONG_MAX; long dec, ret = 0; for (iter = ucounts; iter; iter = iter->ns->ucounts) { long new = atomic_long_add_return(1, &iter->rlimit[type]); if (new < 0 || new > max) goto unwind; if (iter == ucounts) ret = new; <S2SV_StartVul> max = get_userns_rlimit_max(iter->ns, type); <S2SV_EndVul> if (new != 1) continue; if (!get_ucounts(iter)) goto dec_unwind; } return ret; dec_unwind: dec = atomic_long_sub_return(1, &iter->rlimit[type]); WARN_ON_ONCE(dec < 0); unwind: do_dec_rlimit_put_ucounts(ucounts, iter, type); return 0; }","- max = get_userns_rlimit_max(iter->ns, type);
+ };","#include <linux/stat.h> #include <linux/sysctl.h> #include <linux/slab.h> #include <linux/cred.h> #include <linux/hash.h> #include <linux/kmemleak.h> #include <linux/user_namespace.h> struct ucounts init_ucounts = { .ns = &init_user_ns, .uid = GLOBAL_ROOT_UID, .count = ATOMIC_INIT(1), };"
646----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49949/bad/dev.c----qdisc_pkt_len_init,"static void qdisc_pkt_len_init(struct sk_buff *skb) { const struct skb_shared_info *shinfo = skb_shinfo(skb); qdisc_skb_cb(skb)->pkt_len = skb->len; if (shinfo->gso_size) { unsigned int hdr_len; u16 gso_segs = shinfo->gso_segs; hdr_len = skb_transport_header(skb) - skb_mac_header(skb); if (likely(shinfo->gso_type & (SKB_GSO_TCPV4 | SKB_GSO_TCPV6))) { const struct tcphdr *th; struct tcphdr _tcphdr; th = skb_header_pointer(skb, skb_transport_offset(skb), sizeof(_tcphdr), &_tcphdr); if (likely(th)) hdr_len += __tcp_hdrlen(th); <S2SV_StartVul> } else { <S2SV_EndVul> struct udphdr _udphdr; if (skb_header_pointer(skb, skb_transport_offset(skb), sizeof(_udphdr), &_udphdr)) hdr_len += sizeof(struct udphdr); } if (shinfo->gso_type & SKB_GSO_DODGY) gso_segs = DIV_ROUND_UP(skb->len - hdr_len, shinfo->gso_size); qdisc_skb_cb(skb)->pkt_len += (gso_segs - 1) * hdr_len; } }","- } else {
+ } else if (shinfo->gso_type & SKB_GSO_UDP_L4) {","static void qdisc_pkt_len_init(struct sk_buff *skb) { const struct skb_shared_info *shinfo = skb_shinfo(skb); qdisc_skb_cb(skb)->pkt_len = skb->len; if (shinfo->gso_size) { unsigned int hdr_len; u16 gso_segs = shinfo->gso_segs; hdr_len = skb_transport_header(skb) - skb_mac_header(skb); if (likely(shinfo->gso_type & (SKB_GSO_TCPV4 | SKB_GSO_TCPV6))) { const struct tcphdr *th; struct tcphdr _tcphdr; th = skb_header_pointer(skb, skb_transport_offset(skb), sizeof(_tcphdr), &_tcphdr); if (likely(th)) hdr_len += __tcp_hdrlen(th); } else if (shinfo->gso_type & SKB_GSO_UDP_L4) { struct udphdr _udphdr; if (skb_header_pointer(skb, skb_transport_offset(skb), sizeof(_udphdr), &_udphdr)) hdr_len += sizeof(struct udphdr); } if (shinfo->gso_type & SKB_GSO_DODGY) gso_segs = DIV_ROUND_UP(skb->len - hdr_len, shinfo->gso_size); qdisc_skb_cb(skb)->pkt_len += (gso_segs - 1) * hdr_len; } }"
343----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46804/bad/hdcp_ddc.c----read,"static enum mod_hdcp_status read(struct mod_hdcp *hdcp, enum mod_hdcp_ddc_message_id msg_id, uint8_t *buf, uint32_t buf_len) { bool success = true; uint32_t cur_size = 0; uint32_t data_offset = 0; <S2SV_StartVul> if (msg_id == MOD_HDCP_MESSAGE_ID_INVALID) { <S2SV_EndVul> return MOD_HDCP_STATUS_DDC_FAILURE; <S2SV_StartVul> } <S2SV_EndVul> if (is_dp_hdcp(hdcp)) { while (buf_len > 0) { cur_size = MIN(buf_len, HDCP_MAX_AUX_TRANSACTION_SIZE); success = hdcp->config.ddc.funcs.read_dpcd(hdcp->config.ddc.handle, hdcp_dpcd_addrs[msg_id] + data_offset, buf + data_offset, cur_size); if (!success) break; buf_len -= cur_size; data_offset += cur_size; <S2SV_StartVul> } <S2SV_EndVul> } else { success = hdcp->config.ddc.funcs.read_i2c( hdcp->config.ddc.handle, HDCP_I2C_ADDR, hdcp_i2c_offsets[msg_id], buf, (uint32_t)buf_len); } return success ? MOD_HDCP_STATUS_SUCCESS : MOD_HDCP_STATUS_DDC_FAILURE; }","- if (msg_id == MOD_HDCP_MESSAGE_ID_INVALID) {
- }
- }
+ if (msg_id == MOD_HDCP_MESSAGE_ID_INVALID ||
+ msg_id >= MOD_HDCP_MESSAGE_ID_MAX)
+ return MOD_HDCP_STATUS_DDC_FAILURE;
+ int num_dpcd_addrs = sizeof(hdcp_dpcd_addrs) /
+ sizeof(hdcp_dpcd_addrs[0]);
+ if (msg_id >= num_dpcd_addrs)
+ return MOD_HDCP_STATUS_DDC_FAILURE;
+ int num_i2c_offsets = sizeof(hdcp_i2c_offsets) /
+ sizeof(hdcp_i2c_offsets[0]);
+ if (msg_id >= num_i2c_offsets)
+ return MOD_HDCP_STATUS_DDC_FAILURE;","static enum mod_hdcp_status read(struct mod_hdcp *hdcp, enum mod_hdcp_ddc_message_id msg_id, uint8_t *buf, uint32_t buf_len) { bool success = true; uint32_t cur_size = 0; uint32_t data_offset = 0; if (msg_id == MOD_HDCP_MESSAGE_ID_INVALID || msg_id >= MOD_HDCP_MESSAGE_ID_MAX) return MOD_HDCP_STATUS_DDC_FAILURE; if (is_dp_hdcp(hdcp)) { int num_dpcd_addrs = sizeof(hdcp_dpcd_addrs) / sizeof(hdcp_dpcd_addrs[0]); if (msg_id >= num_dpcd_addrs) return MOD_HDCP_STATUS_DDC_FAILURE; while (buf_len > 0) { cur_size = MIN(buf_len, HDCP_MAX_AUX_TRANSACTION_SIZE); success = hdcp->config.ddc.funcs.read_dpcd(hdcp->config.ddc.handle, hdcp_dpcd_addrs[msg_id] + data_offset, buf + data_offset, cur_size); if (!success) break; buf_len -= cur_size; data_offset += cur_size; } } else { int num_i2c_offsets = sizeof(hdcp_i2c_offsets) / sizeof(hdcp_i2c_offsets[0]); if (msg_id >= num_i2c_offsets) return MOD_HDCP_STATUS_DDC_FAILURE; success = hdcp->config.ddc.funcs.read_i2c( hdcp->config.ddc.handle, HDCP_I2C_ADDR, hdcp_i2c_offsets[msg_id], buf, (uint32_t)buf_len); } return success ? MOD_HDCP_STATUS_SUCCESS : MOD_HDCP_STATUS_DDC_FAILURE; }"
986----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50226/bad/hdm.c----cxl_decoder_reset,"static int cxl_decoder_reset(struct cxl_decoder *cxld) { struct cxl_port *port = to_cxl_port(cxld->dev.parent); struct cxl_hdm *cxlhdm = dev_get_drvdata(&port->dev); void __iomem *hdm = cxlhdm->regs.hdm_decoder; int id = cxld->id; u32 ctrl; if ((cxld->flags & CXL_DECODER_F_ENABLE) == 0) <S2SV_StartVul> return 0; <S2SV_EndVul> <S2SV_StartVul> if (port->commit_end != id) { <S2SV_EndVul> dev_dbg(&port->dev, ""%s: out of order reset, expected decoder%d.%d\n"", dev_name(&cxld->dev), port->id, port->commit_end); <S2SV_StartVul> return -EBUSY; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> down_read(&cxl_dpa_rwsem); ctrl = readl(hdm + CXL_HDM_DECODER0_CTRL_OFFSET(id)); ctrl &= ~CXL_HDM_DECODER0_CTRL_COMMIT; writel(ctrl, hdm + CXL_HDM_DECODER0_CTRL_OFFSET(id)); writel(0, hdm + CXL_HDM_DECODER0_SIZE_HIGH_OFFSET(id)); writel(0, hdm + CXL_HDM_DECODER0_SIZE_LOW_OFFSET(id)); writel(0, hdm + CXL_HDM_DECODER0_BASE_HIGH_OFFSET(id)); writel(0, hdm + CXL_HDM_DECODER0_BASE_LOW_OFFSET(id)); up_read(&cxl_dpa_rwsem); <S2SV_StartVul> port->commit_end--; <S2SV_EndVul> cxld->flags &= ~CXL_DECODER_F_ENABLE; if (is_endpoint_decoder(&cxld->dev)) { struct cxl_endpoint_decoder *cxled; cxled = to_cxl_endpoint_decoder(&cxld->dev); cxled->state = CXL_DECODER_STATE_MANUAL; <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> return 0; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul>","- return 0;
- if (port->commit_end != id) {
- return -EBUSY;
- }
- port->commit_end--;
- }
- return 0;
- }
+ return 0;","#include <linux/seq_file.h> #include <linux/device.h> #include <linux/delay.h> #include ""cxlmem.h"" #include ""core.h"" DECLARE_RWSEM(cxl_dpa_rwsem); static int add_hdm_decoder(struct cxl_port *port, struct cxl_decoder *cxld, int *target_map) { int rc; rc = cxl_decoder_add_locked(cxld, target_map); if (rc) { put_device(&cxld->dev); dev_err(&port->dev, ""Failed to add decoder\n""); return rc; } rc = cxl_decoder_autoremove(&port->dev, cxld); if (rc) return rc; dev_dbg(&cxld->dev, ""Added to port %s\n"", dev_name(&port->dev)); return 0; }"
1253----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56536/bad/cw1200_spi.c----cw1200_spi_suspend,static int __maybe_unused cw1200_spi_suspend(struct device *dev) { struct hwbus_priv *self = spi_get_drvdata(to_spi_device(dev)); <S2SV_StartVul> if (!cw1200_can_suspend(self->core)) <S2SV_EndVul> return -EAGAIN; return 0; },"- if (!cw1200_can_suspend(self->core))
+ if (self && !cw1200_can_suspend(self->core))",static int __maybe_unused cw1200_spi_suspend(struct device *dev) { struct hwbus_priv *self = spi_get_drvdata(to_spi_device(dev)); if (self && !cw1200_can_suspend(self->core)) return -EAGAIN; return 0; }
8----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-43891/bad/trace_events.c----event_enable_read,"event_enable_read(struct file *filp, char __user *ubuf, size_t cnt, loff_t *ppos) { struct trace_event_file *file; unsigned long flags; char buf[4] = ""0""; mutex_lock(&event_mutex); <S2SV_StartVul> file = event_file_data(filp); <S2SV_EndVul> if (likely(file)) flags = file->flags; mutex_unlock(&event_mutex); <S2SV_StartVul> if (!file || flags & EVENT_FILE_FL_FREED) <S2SV_EndVul> return -ENODEV; if (flags & EVENT_FILE_FL_ENABLED && !(flags & EVENT_FILE_FL_SOFT_DISABLED)) strcpy(buf, ""1""); if (flags & EVENT_FILE_FL_SOFT_DISABLED || flags & EVENT_FILE_FL_SOFT_MODE) strcat(buf, ""*""); strcat(buf, ""\n""); return simple_read_from_buffer(ubuf, cnt, ppos, buf, strlen(buf)); }","- file = event_file_data(filp);
- if (!file || flags & EVENT_FILE_FL_FREED)
+ file = event_file_file(filp);
+ if (!file)","event_enable_read(struct file *filp, char __user *ubuf, size_t cnt, loff_t *ppos) { struct trace_event_file *file; unsigned long flags; char buf[4] = ""0""; mutex_lock(&event_mutex); file = event_file_file(filp); if (likely(file)) flags = file->flags; mutex_unlock(&event_mutex); if (!file) return -ENODEV; if (flags & EVENT_FILE_FL_ENABLED && !(flags & EVENT_FILE_FL_SOFT_DISABLED)) strcpy(buf, ""1""); if (flags & EVENT_FILE_FL_SOFT_DISABLED || flags & EVENT_FILE_FL_SOFT_MODE) strcat(buf, ""*""); strcat(buf, ""\n""); return simple_read_from_buffer(ubuf, cnt, ppos, buf, strlen(buf)); }"
1208----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53210/bad/af_iucv.c----iucv_sock_recvmsg,"static int iucv_sock_recvmsg(struct socket *sock, struct msghdr *msg, size_t len, int flags) { struct sock *sk = sock->sk; struct iucv_sock *iucv = iucv_sk(sk); unsigned int copied, rlen; struct sk_buff *skb, *rskb, *cskb; int err = 0; u32 offset; if ((sk->sk_state == IUCV_DISCONN) && skb_queue_empty(&iucv->backlog_skb_q) && skb_queue_empty(&sk->sk_receive_queue) && list_empty(&iucv->message_q.list)) return 0; if (flags & (MSG_OOB)) return -EOPNOTSUPP; skb = skb_recv_datagram(sk, flags, &err); if (!skb) { if (sk->sk_shutdown & RCV_SHUTDOWN) return 0; return err; } offset = IUCV_SKB_CB(skb)->offset; rlen = skb->len - offset; copied = min_t(unsigned int, rlen, len); if (!rlen) sk->sk_shutdown = sk->sk_shutdown | RCV_SHUTDOWN; cskb = skb; if (skb_copy_datagram_msg(cskb, offset, msg, copied)) { <S2SV_StartVul> if (!(flags & MSG_PEEK)) <S2SV_EndVul> <S2SV_StartVul> skb_queue_head(&sk->sk_receive_queue, skb); <S2SV_EndVul> <S2SV_StartVul> return -EFAULT; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> if (sk->sk_type == SOCK_SEQPACKET) { if (copied < rlen) msg->msg_flags |= MSG_TRUNC; msg->msg_flags |= MSG_EOR; } err = put_cmsg(msg, SOL_IUCV, SCM_IUCV_TRGCLS, sizeof(IUCV_SKB_CB(skb)->class), (void *)&IUCV_SKB_CB(skb)->class); <S2SV_StartVul> if (err) { <S2SV_EndVul> <S2SV_StartVul> if (!(flags & MSG_PEEK)) <S2SV_EndVul> <S2SV_StartVul> skb_queue_head(&sk->sk_receive_queue, skb); <S2SV_EndVul> <S2SV_StartVul> return err; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> if (!(flags & MSG_PEEK)) { if (sk->sk_type == SOCK_STREAM) { if (copied < rlen) { IUCV_SKB_CB(skb)->offset = offset + copied; skb_queue_head(&sk->sk_receive_queue, skb); goto done; } } consume_skb(skb); if (iucv->transport == AF_IUCV_TRANS_HIPER) { atomic_inc(&iucv->msg_recv); if (atomic_read(&iucv->msg_recv) > iucv->msglimit) { WARN_ON(1); iucv_sock_close(sk); return -EFAULT; } } spin_lock_bh(&iucv->message_q.lock); rskb = skb_dequeue(&iucv->backlog_skb_q); while (rskb) { IUCV_SKB_CB(rskb)->offset = 0; if (__sock_queue_rcv_skb(sk, rskb)) { skb_queue_head(&iucv->backlog_skb_q, rskb); break; } rskb = skb_dequeue(&iucv->backlog_skb_q); } if (skb_queue_empty(&iucv->backlog_skb_q)) { if (!list_empty(&iucv->message_q.list)) iucv_process_message_q(sk); if (atomic_read(&iucv->msg_recv) >= iucv->msglimit / 2) { err = iucv_send_ctrl(sk, AF_IUCV_FLAG_WIN); if (err) { sk->sk_state = IUCV_DISCONN; sk->sk_state_change(sk); } } } spin_unlock_bh(&iucv->message_q.lock); } done: if (sk->sk_type == SOCK_SEQPACKET && (flags & MSG_TRUNC)) copied = rlen; return copied; <S2SV_StartVul> } <S2SV_EndVul>","- if (!(flags & MSG_PEEK))
- skb_queue_head(&sk->sk_receive_queue, skb);
- return -EFAULT;
- }
- if (err) {
- if (!(flags & MSG_PEEK))
- skb_queue_head(&sk->sk_receive_queue, skb);
- return err;
- }
- }
+ err = -EFAULT;
+ goto err_out;
+ if (err)
+ goto err_out;
+ if (flags & MSG_PEEK)
+ skb_unref(skb);
+ err_out:
+ if (!(flags & MSG_PEEK))
+ skb_queue_head(&sk->sk_receive_queue, skb);
+ else
+ skb_unref(skb);
+ return err;","static int iucv_sock_recvmsg(struct socket *sock, struct msghdr *msg, size_t len, int flags) { struct sock *sk = sock->sk; struct iucv_sock *iucv = iucv_sk(sk); unsigned int copied, rlen; struct sk_buff *skb, *rskb, *cskb; int err = 0; u32 offset; if ((sk->sk_state == IUCV_DISCONN) && skb_queue_empty(&iucv->backlog_skb_q) && skb_queue_empty(&sk->sk_receive_queue) && list_empty(&iucv->message_q.list)) return 0; if (flags & (MSG_OOB)) return -EOPNOTSUPP; skb = skb_recv_datagram(sk, flags, &err); if (!skb) { if (sk->sk_shutdown & RCV_SHUTDOWN) return 0; return err; } offset = IUCV_SKB_CB(skb)->offset; rlen = skb->len - offset; copied = min_t(unsigned int, rlen, len); if (!rlen) sk->sk_shutdown = sk->sk_shutdown | RCV_SHUTDOWN; cskb = skb; if (skb_copy_datagram_msg(cskb, offset, msg, copied)) { err = -EFAULT; goto err_out; } if (sk->sk_type == SOCK_SEQPACKET) { if (copied < rlen) msg->msg_flags |= MSG_TRUNC; msg->msg_flags |= MSG_EOR; } err = put_cmsg(msg, SOL_IUCV, SCM_IUCV_TRGCLS, sizeof(IUCV_SKB_CB(skb)->class), (void *)&IUCV_SKB_CB(skb)->class); if (err) goto err_out; if (!(flags & MSG_PEEK)) { if (sk->sk_type == SOCK_STREAM) { if (copied < rlen) { IUCV_SKB_CB(skb)->offset = offset + copied; skb_queue_head(&sk->sk_receive_queue, skb); goto done; } } consume_skb(skb); if (iucv->transport == AF_IUCV_TRANS_HIPER) { atomic_inc(&iucv->msg_recv); if (atomic_read(&iucv->msg_recv) > iucv->msglimit) { WARN_ON(1); iucv_sock_close(sk); return -EFAULT; } } spin_lock_bh(&iucv->message_q.lock); rskb = skb_dequeue(&iucv->backlog_skb_q); while (rskb) { IUCV_SKB_CB(rskb)->offset = 0; if (__sock_queue_rcv_skb(sk, rskb)) { skb_queue_head(&iucv->backlog_skb_q, rskb); break; } rskb = skb_dequeue(&iucv->backlog_skb_q); } if (skb_queue_empty(&iucv->backlog_skb_q)) { if (!list_empty(&iucv->message_q.list)) iucv_process_message_q(sk); if (atomic_read(&iucv->msg_recv) >= iucv->msglimit / 2) { err = iucv_send_ctrl(sk, AF_IUCV_FLAG_WIN); if (err) { sk->sk_state = IUCV_DISCONN; sk->sk_state_change(sk); } } } spin_unlock_bh(&iucv->message_q.lock); } done: if (sk->sk_type == SOCK_SEQPACKET && (flags & MSG_TRUNC)) copied = rlen; if (flags & MSG_PEEK) skb_unref(skb); return copied; err_out: if (!(flags & MSG_PEEK)) skb_queue_head(&sk->sk_receive_queue, skb); else skb_unref(skb); return err; }"
313----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46780/bad/sysfs.c----nilfs_dev_volume_name_show,"ssize_t nilfs_dev_volume_name_show(struct nilfs_dev_attr *attr, struct the_nilfs *nilfs, char *buf) { <S2SV_StartVul> struct nilfs_super_block **sbp = nilfs->ns_sbp; <S2SV_EndVul> <S2SV_StartVul> return scnprintf(buf, sizeof(sbp[0]->s_volume_name), ""%s\n"", <S2SV_EndVul> <S2SV_StartVul> sbp[0]->s_volume_name); <S2SV_EndVul> }","- struct nilfs_super_block **sbp = nilfs->ns_sbp;
- return scnprintf(buf, sizeof(sbp[0]->s_volume_name), ""%s\n"",
- sbp[0]->s_volume_name);
+ struct nilfs_super_block *raw_sb;
+ ssize_t len;
+ down_read(&nilfs->ns_sem);
+ raw_sb = nilfs->ns_sbp[0];
+ len = scnprintf(buf, sizeof(raw_sb->s_volume_name), ""%s\n"",
+ raw_sb->s_volume_name);
+ up_read(&nilfs->ns_sem);
+ return len;","ssize_t nilfs_dev_volume_name_show(struct nilfs_dev_attr *attr, struct the_nilfs *nilfs, char *buf) { struct nilfs_super_block *raw_sb; ssize_t len; down_read(&nilfs->ns_sem); raw_sb = nilfs->ns_sbp[0]; len = scnprintf(buf, sizeof(raw_sb->s_volume_name), ""%s\n"", raw_sb->s_volume_name); up_read(&nilfs->ns_sem); return len; }"
612----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49921/bad/dcn35_hwseq.c----dcn35_init_hw,"void dcn35_init_hw(struct dc *dc) { struct abm **abms = dc->res_pool->multiple_abms; struct dce_hwseq *hws = dc->hwseq; struct dc_bios *dcb = dc->ctx->dc_bios; struct resource_pool *res_pool = dc->res_pool; uint32_t backlight = MAX_BACKLIGHT_LEVEL; uint32_t user_level = MAX_BACKLIGHT_LEVEL; int i; if (dc->clk_mgr && dc->clk_mgr->funcs->init_clocks) dc->clk_mgr->funcs->init_clocks(dc->clk_mgr); if (!dcb->funcs->is_accelerated_mode(dcb)) { hws->funcs.bios_golden_init(dc); } if (!dc->debug.disable_clock_gate) { REG_WRITE(DCCG_GATE_DISABLE_CNTL, 0); REG_WRITE(DCCG_GATE_DISABLE_CNTL2, 0); REG_UPDATE_5(DCCG_GATE_DISABLE_CNTL2, PHYASYMCLK_ROOT_GATE_DISABLE, 1, PHYBSYMCLK_ROOT_GATE_DISABLE, 1, PHYCSYMCLK_ROOT_GATE_DISABLE, 1, PHYDSYMCLK_ROOT_GATE_DISABLE, 1, PHYESYMCLK_ROOT_GATE_DISABLE, 1); REG_UPDATE_4(DCCG_GATE_DISABLE_CNTL4, DPIASYMCLK0_GATE_DISABLE, 0, DPIASYMCLK1_GATE_DISABLE, 0, DPIASYMCLK2_GATE_DISABLE, 0, DPIASYMCLK3_GATE_DISABLE, 0); REG_WRITE(DCCG_GATE_DISABLE_CNTL5, 0xFFFFFFFF); REG_UPDATE_4(DCCG_GATE_DISABLE_CNTL5, DTBCLK_P0_GATE_DISABLE, 0, DTBCLK_P1_GATE_DISABLE, 0, DTBCLK_P2_GATE_DISABLE, 0, DTBCLK_P3_GATE_DISABLE, 0); REG_UPDATE_4(DCCG_GATE_DISABLE_CNTL5, DPSTREAMCLK0_GATE_DISABLE, 0, DPSTREAMCLK1_GATE_DISABLE, 0, DPSTREAMCLK2_GATE_DISABLE, 0, DPSTREAMCLK3_GATE_DISABLE, 0); } if (res_pool->dccg->funcs->dccg_init) res_pool->dccg->funcs->dccg_init(res_pool->dccg); if (dc->ctx->dc_bios->fw_info_valid) { res_pool->ref_clocks.xtalin_clock_inKhz = dc->ctx->dc_bios->fw_info.pll_info.crystal_frequency; if (res_pool->hubbub) { (res_pool->dccg->funcs->get_dccg_ref_freq)(res_pool->dccg, dc->ctx->dc_bios->fw_info.pll_info.crystal_frequency, &res_pool->ref_clocks.dccg_ref_clock_inKhz); (res_pool->hubbub->funcs->get_dchub_ref_freq)(res_pool->hubbub, res_pool->ref_clocks.dccg_ref_clock_inKhz, &res_pool->ref_clocks.dchub_ref_clock_inKhz); } else { res_pool->ref_clocks.dccg_ref_clock_inKhz = res_pool->ref_clocks.xtalin_clock_inKhz; res_pool->ref_clocks.dchub_ref_clock_inKhz = res_pool->ref_clocks.xtalin_clock_inKhz; } } else ASSERT_CRITICAL(false); for (i = 0; i < dc->link_count; i++) { struct dc_link *link = dc->links[i]; if (link->ep_type != DISPLAY_ENDPOINT_PHY) continue; link->link_enc->funcs->hw_init(link->link_enc); if (link->link_enc->funcs->is_dig_enabled && link->link_enc->funcs->is_dig_enabled(link->link_enc)) { link->link_status.link_active = true; if (link->link_enc->funcs->fec_is_active && link->link_enc->funcs->fec_is_active(link->link_enc)) link->fec_state = dc_link_fec_enabled; } } dc->link_srv->blank_all_dp_displays(dc); <S2SV_StartVul> if (res_pool->hubbub->funcs->dchubbub_init) <S2SV_EndVul> res_pool->hubbub->funcs->dchubbub_init(dc->res_pool->hubbub); if (dcb->funcs->is_accelerated_mode(dcb) || !dc->config.seamless_boot_edp_requested) { if (!dc->caps.seamless_odm) { for (i = 0; i < dc->res_pool->timing_generator_count; i++) { struct timing_generator *tg = dc->res_pool->timing_generators[i]; uint32_t num_opps, opp_id_src0, opp_id_src1; num_opps = 1; if (tg) { if (tg->funcs->is_tg_enabled(tg) && tg->funcs->get_optc_source) { tg->funcs->get_optc_source(tg, &num_opps, &opp_id_src0, &opp_id_src1); } } if (num_opps > 1) { dc->link_srv->blank_all_edp_displays(dc); break; } } } hws->funcs.init_pipes(dc, dc->current_state); if (dc->res_pool->hubbub->funcs->allow_self_refresh_control) dc->res_pool->hubbub->funcs->allow_self_refresh_control(dc->res_pool->hubbub, !dc->res_pool->hubbub->ctx->dc->debug.disable_stutter); } for (i = 0; i < res_pool->audio_count; i++) { struct audio *audio = res_pool->audios[i]; audio->funcs->hw_init(audio); } for (i = 0; i < dc->link_count; i++) { struct dc_link *link = dc->links[i]; if (link->panel_cntl) { backlight = link->panel_cntl->funcs->hw_init(link->panel_cntl); user_level = link->panel_cntl->stored_backlight_registers.USER_LEVEL; } } if (dc->ctx->dmub_srv) { for (i = 0; i < dc->res_pool->pipe_count; i++) { if (abms[i] != NULL && abms[i]->funcs != NULL) abms[i]->funcs->abm_init(abms[i], backlight, user_level); } } REG_WRITE(DIO_MEM_PWR_CTRL, 0); if (dc->debug.enable_mem_low_power.bits.i2c) REG_UPDATE(DIO_MEM_PWR_CTRL, I2C_LIGHT_SLEEP_FORCE, 0); if (hws->funcs.setup_hpo_hw_control) hws->funcs.setup_hpo_hw_control(hws, false); if (!dc->debug.disable_clock_gate) { REG_WRITE(DCCG_GATE_DISABLE_CNTL, 0); REG_UPDATE_5(DCCG_GATE_DISABLE_CNTL2, SYMCLKA_FE_GATE_DISABLE, 0, SYMCLKB_FE_GATE_DISABLE, 0, SYMCLKC_FE_GATE_DISABLE, 0, SYMCLKD_FE_GATE_DISABLE, 0, SYMCLKE_FE_GATE_DISABLE, 0); REG_UPDATE(DCCG_GATE_DISABLE_CNTL2, HDMICHARCLK0_GATE_DISABLE, 0); REG_UPDATE_5(DCCG_GATE_DISABLE_CNTL2, SYMCLKA_GATE_DISABLE, 0, SYMCLKB_GATE_DISABLE, 0, SYMCLKC_GATE_DISABLE, 0, SYMCLKD_GATE_DISABLE, 0, SYMCLKE_GATE_DISABLE, 0); REG_UPDATE(DCFCLK_CNTL, DCFCLK_GATE_DIS, 0); } if (dc->debug.disable_mem_low_power) { REG_UPDATE(DC_MEM_GLOBAL_PWR_REQ_CNTL, DC_MEM_GLOBAL_PWR_REQ_DIS, 1); } if (!dcb->funcs->is_accelerated_mode(dcb) && dc->res_pool->hubbub->funcs->init_watermarks) dc->res_pool->hubbub->funcs->init_watermarks(dc->res_pool->hubbub); <S2SV_StartVul> if (dc->clk_mgr->funcs->notify_wm_ranges) <S2SV_EndVul> dc->clk_mgr->funcs->notify_wm_ranges(dc->clk_mgr); <S2SV_StartVul> if (dc->clk_mgr->funcs->set_hard_max_memclk && !dc->clk_mgr->dc_mode_softmax_enabled) <S2SV_EndVul> dc->clk_mgr->funcs->set_hard_max_memclk(dc->clk_mgr); if (dc->res_pool->hubbub->funcs->force_pstate_change_control) dc->res_pool->hubbub->funcs->force_pstate_change_control( dc->res_pool->hubbub, false, false); if (dc->res_pool->hubbub->funcs->init_crb) dc->res_pool->hubbub->funcs->init_crb(dc->res_pool->hubbub); if (dc->res_pool->hubbub->funcs->set_request_limit && dc->config.sdpif_request_limit_words_per_umc > 0) dc->res_pool->hubbub->funcs->set_request_limit(dc->res_pool->hubbub, dc->ctx->dc_bios->vram_info.num_chans, dc->config.sdpif_request_limit_words_per_umc); if (dc->ctx->dmub_srv) { dc_dmub_srv_query_caps_cmd(dc->ctx->dmub_srv); dc->caps.dmub_caps.psr = dc->ctx->dmub_srv->dmub->feature_caps.psr; dc->caps.dmub_caps.mclk_sw = dc->ctx->dmub_srv->dmub->feature_caps.fw_assisted_mclk_switch_ver; } if (dc->res_pool->pg_cntl) { if (dc->res_pool->pg_cntl->funcs->init_pg_status) dc->res_pool->pg_cntl->funcs->init_pg_status(dc->res_pool->pg_cntl); } }","- if (res_pool->hubbub->funcs->dchubbub_init)
- if (dc->clk_mgr->funcs->notify_wm_ranges)
- if (dc->clk_mgr->funcs->set_hard_max_memclk && !dc->clk_mgr->dc_mode_softmax_enabled)
+ if (res_pool->hubbub && res_pool->hubbub->funcs->dchubbub_init)
+ if (dc->clk_mgr && dc->clk_mgr->funcs->notify_wm_ranges)
+ if (dc->clk_mgr && dc->clk_mgr->funcs->set_hard_max_memclk && !dc->clk_mgr->dc_mode_softmax_enabled)","void dcn35_init_hw(struct dc *dc) { struct abm **abms = dc->res_pool->multiple_abms; struct dce_hwseq *hws = dc->hwseq; struct dc_bios *dcb = dc->ctx->dc_bios; struct resource_pool *res_pool = dc->res_pool; uint32_t backlight = MAX_BACKLIGHT_LEVEL; uint32_t user_level = MAX_BACKLIGHT_LEVEL; int i; if (dc->clk_mgr && dc->clk_mgr->funcs->init_clocks) dc->clk_mgr->funcs->init_clocks(dc->clk_mgr); if (!dcb->funcs->is_accelerated_mode(dcb)) { hws->funcs.bios_golden_init(dc); } if (!dc->debug.disable_clock_gate) { REG_WRITE(DCCG_GATE_DISABLE_CNTL, 0); REG_WRITE(DCCG_GATE_DISABLE_CNTL2, 0); REG_UPDATE_5(DCCG_GATE_DISABLE_CNTL2, PHYASYMCLK_ROOT_GATE_DISABLE, 1, PHYBSYMCLK_ROOT_GATE_DISABLE, 1, PHYCSYMCLK_ROOT_GATE_DISABLE, 1, PHYDSYMCLK_ROOT_GATE_DISABLE, 1, PHYESYMCLK_ROOT_GATE_DISABLE, 1); REG_UPDATE_4(DCCG_GATE_DISABLE_CNTL4, DPIASYMCLK0_GATE_DISABLE, 0, DPIASYMCLK1_GATE_DISABLE, 0, DPIASYMCLK2_GATE_DISABLE, 0, DPIASYMCLK3_GATE_DISABLE, 0); REG_WRITE(DCCG_GATE_DISABLE_CNTL5, 0xFFFFFFFF); REG_UPDATE_4(DCCG_GATE_DISABLE_CNTL5, DTBCLK_P0_GATE_DISABLE, 0, DTBCLK_P1_GATE_DISABLE, 0, DTBCLK_P2_GATE_DISABLE, 0, DTBCLK_P3_GATE_DISABLE, 0); REG_UPDATE_4(DCCG_GATE_DISABLE_CNTL5, DPSTREAMCLK0_GATE_DISABLE, 0, DPSTREAMCLK1_GATE_DISABLE, 0, DPSTREAMCLK2_GATE_DISABLE, 0, DPSTREAMCLK3_GATE_DISABLE, 0); } if (res_pool->dccg->funcs->dccg_init) res_pool->dccg->funcs->dccg_init(res_pool->dccg); if (dc->ctx->dc_bios->fw_info_valid) { res_pool->ref_clocks.xtalin_clock_inKhz = dc->ctx->dc_bios->fw_info.pll_info.crystal_frequency; if (res_pool->hubbub) { (res_pool->dccg->funcs->get_dccg_ref_freq)(res_pool->dccg, dc->ctx->dc_bios->fw_info.pll_info.crystal_frequency, &res_pool->ref_clocks.dccg_ref_clock_inKhz); (res_pool->hubbub->funcs->get_dchub_ref_freq)(res_pool->hubbub, res_pool->ref_clocks.dccg_ref_clock_inKhz, &res_pool->ref_clocks.dchub_ref_clock_inKhz); } else { res_pool->ref_clocks.dccg_ref_clock_inKhz = res_pool->ref_clocks.xtalin_clock_inKhz; res_pool->ref_clocks.dchub_ref_clock_inKhz = res_pool->ref_clocks.xtalin_clock_inKhz; } } else ASSERT_CRITICAL(false); for (i = 0; i < dc->link_count; i++) { struct dc_link *link = dc->links[i]; if (link->ep_type != DISPLAY_ENDPOINT_PHY) continue; link->link_enc->funcs->hw_init(link->link_enc); if (link->link_enc->funcs->is_dig_enabled && link->link_enc->funcs->is_dig_enabled(link->link_enc)) { link->link_status.link_active = true; if (link->link_enc->funcs->fec_is_active && link->link_enc->funcs->fec_is_active(link->link_enc)) link->fec_state = dc_link_fec_enabled; } } dc->link_srv->blank_all_dp_displays(dc); if (res_pool->hubbub && res_pool->hubbub->funcs->dchubbub_init) res_pool->hubbub->funcs->dchubbub_init(dc->res_pool->hubbub); if (dcb->funcs->is_accelerated_mode(dcb) || !dc->config.seamless_boot_edp_requested) { if (!dc->caps.seamless_odm) { for (i = 0; i < dc->res_pool->timing_generator_count; i++) { struct timing_generator *tg = dc->res_pool->timing_generators[i]; uint32_t num_opps, opp_id_src0, opp_id_src1; num_opps = 1; if (tg) { if (tg->funcs->is_tg_enabled(tg) && tg->funcs->get_optc_source) { tg->funcs->get_optc_source(tg, &num_opps, &opp_id_src0, &opp_id_src1); } } if (num_opps > 1) { dc->link_srv->blank_all_edp_displays(dc); break; } } } hws->funcs.init_pipes(dc, dc->current_state); if (dc->res_pool->hubbub->funcs->allow_self_refresh_control) dc->res_pool->hubbub->funcs->allow_self_refresh_control(dc->res_pool->hubbub, !dc->res_pool->hubbub->ctx->dc->debug.disable_stutter); } for (i = 0; i < res_pool->audio_count; i++) { struct audio *audio = res_pool->audios[i]; audio->funcs->hw_init(audio); } for (i = 0; i < dc->link_count; i++) { struct dc_link *link = dc->links[i]; if (link->panel_cntl) { backlight = link->panel_cntl->funcs->hw_init(link->panel_cntl); user_level = link->panel_cntl->stored_backlight_registers.USER_LEVEL; } } if (dc->ctx->dmub_srv) { for (i = 0; i < dc->res_pool->pipe_count; i++) { if (abms[i] != NULL && abms[i]->funcs != NULL) abms[i]->funcs->abm_init(abms[i], backlight, user_level); } } REG_WRITE(DIO_MEM_PWR_CTRL, 0); if (dc->debug.enable_mem_low_power.bits.i2c) REG_UPDATE(DIO_MEM_PWR_CTRL, I2C_LIGHT_SLEEP_FORCE, 0); if (hws->funcs.setup_hpo_hw_control) hws->funcs.setup_hpo_hw_control(hws, false); if (!dc->debug.disable_clock_gate) { REG_WRITE(DCCG_GATE_DISABLE_CNTL, 0); REG_UPDATE_5(DCCG_GATE_DISABLE_CNTL2, SYMCLKA_FE_GATE_DISABLE, 0, SYMCLKB_FE_GATE_DISABLE, 0, SYMCLKC_FE_GATE_DISABLE, 0, SYMCLKD_FE_GATE_DISABLE, 0, SYMCLKE_FE_GATE_DISABLE, 0); REG_UPDATE(DCCG_GATE_DISABLE_CNTL2, HDMICHARCLK0_GATE_DISABLE, 0); REG_UPDATE_5(DCCG_GATE_DISABLE_CNTL2, SYMCLKA_GATE_DISABLE, 0, SYMCLKB_GATE_DISABLE, 0, SYMCLKC_GATE_DISABLE, 0, SYMCLKD_GATE_DISABLE, 0, SYMCLKE_GATE_DISABLE, 0); REG_UPDATE(DCFCLK_CNTL, DCFCLK_GATE_DIS, 0); } if (dc->debug.disable_mem_low_power) { REG_UPDATE(DC_MEM_GLOBAL_PWR_REQ_CNTL, DC_MEM_GLOBAL_PWR_REQ_DIS, 1); } if (!dcb->funcs->is_accelerated_mode(dcb) && dc->res_pool->hubbub->funcs->init_watermarks) dc->res_pool->hubbub->funcs->init_watermarks(dc->res_pool->hubbub); if (dc->clk_mgr && dc->clk_mgr->funcs->notify_wm_ranges) dc->clk_mgr->funcs->notify_wm_ranges(dc->clk_mgr); if (dc->clk_mgr && dc->clk_mgr->funcs->set_hard_max_memclk && !dc->clk_mgr->dc_mode_softmax_enabled) dc->clk_mgr->funcs->set_hard_max_memclk(dc->clk_mgr); if (dc->res_pool->hubbub->funcs->force_pstate_change_control) dc->res_pool->hubbub->funcs->force_pstate_change_control( dc->res_pool->hubbub, false, false); if (dc->res_pool->hubbub->funcs->init_crb) dc->res_pool->hubbub->funcs->init_crb(dc->res_pool->hubbub); if (dc->res_pool->hubbub->funcs->set_request_limit && dc->config.sdpif_request_limit_words_per_umc > 0) dc->res_pool->hubbub->funcs->set_request_limit(dc->res_pool->hubbub, dc->ctx->dc_bios->vram_info.num_chans, dc->config.sdpif_request_limit_words_per_umc); if (dc->ctx->dmub_srv) { dc_dmub_srv_query_caps_cmd(dc->ctx->dmub_srv); dc->caps.dmub_caps.psr = dc->ctx->dmub_srv->dmub->feature_caps.psr; dc->caps.dmub_caps.mclk_sw = dc->ctx->dmub_srv->dmub->feature_caps.fw_assisted_mclk_switch_ver; } if (dc->res_pool->pg_cntl) { if (dc->res_pool->pg_cntl->funcs->init_pg_status) dc->res_pool->pg_cntl->funcs->init_pg_status(dc->res_pool->pg_cntl); } }"
189----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46678/bad/bond_main.c----bond_ipsec_del_sa_all,"static void bond_ipsec_del_sa_all(struct bonding *bond) { struct net_device *bond_dev = bond->dev; struct net_device *real_dev; struct bond_ipsec *ipsec; struct slave *slave; <S2SV_StartVul> rcu_read_lock(); <S2SV_EndVul> <S2SV_StartVul> slave = rcu_dereference(bond->curr_active_slave); <S2SV_EndVul> <S2SV_StartVul> if (!slave) { <S2SV_EndVul> <S2SV_StartVul> rcu_read_unlock(); <S2SV_EndVul> return; <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> real_dev = slave->dev; <S2SV_EndVul> <S2SV_StartVul> spin_lock_bh(&bond->ipsec_lock); <S2SV_EndVul> list_for_each_entry(ipsec, &bond->ipsec_list, list) { if (!ipsec->xs->xso.real_dev) continue; if (!real_dev->xfrmdev_ops || !real_dev->xfrmdev_ops->xdo_dev_state_delete || netif_is_bond_master(real_dev)) { slave_warn(bond_dev, real_dev, ""%s: no slave xdo_dev_state_delete\n"", __func__); } else { real_dev->xfrmdev_ops->xdo_dev_state_delete(ipsec->xs); if (real_dev->xfrmdev_ops->xdo_dev_state_free) real_dev->xfrmdev_ops->xdo_dev_state_free(ipsec->xs); <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> spin_unlock_bh(&bond->ipsec_lock); <S2SV_EndVul> <S2SV_StartVul> rcu_read_unlock(); <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul>","- rcu_read_lock();
- slave = rcu_dereference(bond->curr_active_slave);
- if (!slave) {
- rcu_read_unlock();
- }
- real_dev = slave->dev;
- spin_lock_bh(&bond->ipsec_lock);
- }
- }
- spin_unlock_bh(&bond->ipsec_lock);
- rcu_read_unlock();
- }
+ slave = rtnl_dereference(bond->curr_active_slave);
+ real_dev = slave ? slave->dev : NULL;
+ if (!real_dev)
+ return;
+ mutex_lock(&bond->ipsec_lock);
+ continue;
+ mutex_unlock(&bond->ipsec_lock);","static void bond_ipsec_del_sa_all(struct bonding *bond) { struct net_device *bond_dev = bond->dev; struct net_device *real_dev; struct bond_ipsec *ipsec; struct slave *slave; slave = rtnl_dereference(bond->curr_active_slave); real_dev = slave ? slave->dev : NULL; if (!real_dev) return; mutex_lock(&bond->ipsec_lock); list_for_each_entry(ipsec, &bond->ipsec_list, list) { if (!ipsec->xs->xso.real_dev) continue; if (!real_dev->xfrmdev_ops || !real_dev->xfrmdev_ops->xdo_dev_state_delete || netif_is_bond_master(real_dev)) { slave_warn(bond_dev, real_dev, ""%s: no slave xdo_dev_state_delete\n"", __func__); } else { real_dev->xfrmdev_ops->xdo_dev_state_delete(ipsec->xs); if (real_dev->xfrmdev_ops->xdo_dev_state_free) real_dev->xfrmdev_ops->xdo_dev_state_free(ipsec->xs); } } mutex_unlock(&bond->ipsec_lock); }"
1010----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50248/bad/record.c----*mi_enum_attr,"struct ATTRIB *mi_enum_attr(struct mft_inode *mi, struct ATTRIB *attr) { const struct MFT_REC *rec = mi->mrec; u32 used = le32_to_cpu(rec->used); u32 t32, off, asize, prev_type; u16 t16; u64 data_size, alloc_size, tot_size; if (!attr) { u32 total = le32_to_cpu(rec->total); off = le16_to_cpu(rec->attr_off); if (used > total) return NULL; if (off >= used || off < MFTRECORD_FIXUP_OFFSET_1 || !IS_ALIGNED(off, 4)) { return NULL; } if (!is_rec_inuse(rec)) return NULL; prev_type = 0; attr = Add2Ptr(rec, off); } else { off = PtrOffset(rec, attr); <S2SV_StartVul> if (off >= used) <S2SV_EndVul> <S2SV_StartVul> return NULL; <S2SV_EndVul> <S2SV_StartVul> asize = le32_to_cpu(attr->size); <S2SV_EndVul> <S2SV_StartVul> if (asize < SIZEOF_RESIDENT) { <S2SV_EndVul> <S2SV_StartVul> return NULL; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> if (off + asize < off) <S2SV_EndVul> <S2SV_StartVul> return NULL; <S2SV_EndVul> prev_type = le32_to_cpu(attr->type); attr = Add2Ptr(attr, asize); off += asize; <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> asize = le32_to_cpu(attr->size); <S2SV_EndVul> if (off + 8 > used) { static_assert(ALIGN(sizeof(enum ATTR_TYPE), 8) == 8); return NULL; } if (attr->type == ATTR_END) { return NULL; } t32 = le32_to_cpu(attr->type); if (!t32 || (t32 & 0xf) || (t32 > 0x100)) return NULL; if (t32 < prev_type) <S2SV_StartVul> return NULL; <S2SV_EndVul> if (off + asize < off || off + asize > used) <S2SV_StartVul> return NULL; <S2SV_EndVul> if (!attr->non_res) { if (asize < SIZEOF_RESIDENT) return NULL; t16 = le16_to_cpu(attr->res.data_off); if (t16 > asize) return NULL; if (le32_to_cpu(attr->res.data_size) > asize - t16) return NULL; t32 = sizeof(short) * attr->name_len; if (t32 && le16_to_cpu(attr->name_off) + t32 > t16) return NULL; return attr; } if (attr->non_res != 1) return NULL; t16 = le16_to_cpu(attr->nres.run_off); if (t16 > asize) return NULL; t32 = sizeof(short) * attr->name_len; if (t32 && le16_to_cpu(attr->name_off) + t32 > t16) return NULL; if (le64_to_cpu(attr->nres.svcn) > le64_to_cpu(attr->nres.evcn) + 1) return NULL; data_size = le64_to_cpu(attr->nres.data_size); if (le64_to_cpu(attr->nres.valid_size) > data_size) return NULL; alloc_size = le64_to_cpu(attr->nres.alloc_size); if (data_size > alloc_size) return NULL; t32 = mi->sbi->cluster_mask; if (alloc_size & t32) return NULL; if (!attr->nres.svcn && is_attr_ext(attr)) { if (asize + 8 < SIZEOF_NONRESIDENT_EX) return NULL; tot_size = le64_to_cpu(attr->nres.total_size); if (tot_size & t32) return NULL; if (tot_size > alloc_size) return NULL; } else { if (asize + 8 < SIZEOF_NONRESIDENT) return NULL; if (attr->nres.c_unit) return NULL; } return attr; }","- if (off >= used)
- return NULL;
- asize = le32_to_cpu(attr->size);
- if (asize < SIZEOF_RESIDENT) {
- return NULL;
- }
- if (off + asize < off)
- return NULL;
- }
- asize = le32_to_cpu(attr->size);
- return NULL;
- return NULL;
+ asize = le32_to_cpu(attr->size);
+ }
+ return NULL;
+ asize = le32_to_cpu(attr->size);
+ if (asize < SIZEOF_RESIDENT) {
+ return NULL;
+ }
+ return NULL;","struct ATTRIB *mi_enum_attr(struct mft_inode *mi, struct ATTRIB *attr) { const struct MFT_REC *rec = mi->mrec; u32 used = le32_to_cpu(rec->used); u32 t32, off, asize, prev_type; u16 t16; u64 data_size, alloc_size, tot_size; if (!attr) { u32 total = le32_to_cpu(rec->total); off = le16_to_cpu(rec->attr_off); if (used > total) return NULL; if (off >= used || off < MFTRECORD_FIXUP_OFFSET_1 || !IS_ALIGNED(off, 4)) { return NULL; } if (!is_rec_inuse(rec)) return NULL; prev_type = 0; attr = Add2Ptr(rec, off); } else { off = PtrOffset(rec, attr); asize = le32_to_cpu(attr->size); prev_type = le32_to_cpu(attr->type); attr = Add2Ptr(attr, asize); off += asize; } if (off + 8 > used) { static_assert(ALIGN(sizeof(enum ATTR_TYPE), 8) == 8); return NULL; } if (attr->type == ATTR_END) { return NULL; } t32 = le32_to_cpu(attr->type); if (!t32 || (t32 & 0xf) || (t32 > 0x100)) return NULL; if (t32 < prev_type) return NULL; asize = le32_to_cpu(attr->size); if (asize < SIZEOF_RESIDENT) { return NULL; } if (off + asize < off || off + asize > used) return NULL; if (!attr->non_res) { if (asize < SIZEOF_RESIDENT) return NULL; t16 = le16_to_cpu(attr->res.data_off); if (t16 > asize) return NULL; if (le32_to_cpu(attr->res.data_size) > asize - t16) return NULL; t32 = sizeof(short) * attr->name_len; if (t32 && le16_to_cpu(attr->name_off) + t32 > t16) return NULL; return attr; } if (attr->non_res != 1) return NULL; t16 = le16_to_cpu(attr->nres.run_off); if (t16 > asize) return NULL; t32 = sizeof(short) * attr->name_len; if (t32 && le16_to_cpu(attr->name_off) + t32 > t16) return NULL; if (le64_to_cpu(attr->nres.svcn) > le64_to_cpu(attr->nres.evcn) + 1) return NULL; data_size = le64_to_cpu(attr->nres.data_size); if (le64_to_cpu(attr->nres.valid_size) > data_size) return NULL; alloc_size = le64_to_cpu(attr->nres.alloc_size); if (data_size > alloc_size) return NULL; t32 = mi->sbi->cluster_mask; if (alloc_size & t32) return NULL; if (!attr->nres.svcn && is_attr_ext(attr)) { if (asize + 8 < SIZEOF_NONRESIDENT_EX) return NULL; tot_size = le64_to_cpu(attr->nres.total_size); if (tot_size & t32) return NULL; if (tot_size > alloc_size) return NULL; } else { if (asize + 8 < SIZEOF_NONRESIDENT) return NULL; if (attr->nres.c_unit) return NULL; } return attr; }"
446----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47685/bad/nf_reject_ipv6.c----nf_reject_ip6_tcphdr_put,"void nf_reject_ip6_tcphdr_put(struct sk_buff *nskb, const struct sk_buff *oldskb, const struct tcphdr *oth, unsigned int otcplen) { struct tcphdr *tcph; <S2SV_StartVul> int needs_ack; <S2SV_EndVul> skb_reset_transport_header(nskb); <S2SV_StartVul> tcph = skb_put(nskb, sizeof(struct tcphdr)); <S2SV_EndVul> tcph->doff = sizeof(struct tcphdr)/4; tcph->source = oth->dest; tcph->dest = oth->source; if (oth->ack) { <S2SV_StartVul> needs_ack = 0; <S2SV_EndVul> tcph->seq = oth->ack_seq; <S2SV_StartVul> tcph->ack_seq = 0; <S2SV_EndVul> } else { <S2SV_StartVul> needs_ack = 1; <S2SV_EndVul> tcph->ack_seq = htonl(ntohl(oth->seq) + oth->syn + oth->fin + otcplen - (oth->doff<<2)); <S2SV_StartVul> tcph->seq = 0; <S2SV_EndVul> } <S2SV_StartVul> ((u_int8_t *)tcph)[13] = 0; <S2SV_EndVul> tcph->rst = 1; <S2SV_StartVul> tcph->ack = needs_ack; <S2SV_EndVul> <S2SV_StartVul> tcph->window = 0; <S2SV_EndVul> <S2SV_StartVul> tcph->urg_ptr = 0; <S2SV_EndVul> <S2SV_StartVul> tcph->check = 0; <S2SV_EndVul> tcph->check = csum_ipv6_magic(&ipv6_hdr(nskb)->saddr, &ipv6_hdr(nskb)->daddr, sizeof(struct tcphdr), IPPROTO_TCP, csum_partial(tcph, sizeof(struct tcphdr), 0)); }","- int needs_ack;
- tcph = skb_put(nskb, sizeof(struct tcphdr));
- needs_ack = 0;
- tcph->ack_seq = 0;
- needs_ack = 1;
- tcph->seq = 0;
- ((u_int8_t *)tcph)[13] = 0;
- tcph->ack = needs_ack;
- tcph->window = 0;
- tcph->urg_ptr = 0;
- tcph->check = 0;
+ tcph = skb_put_zero(nskb, sizeof(struct tcphdr));
+ tcph->ack = 1;","void nf_reject_ip6_tcphdr_put(struct sk_buff *nskb, const struct sk_buff *oldskb, const struct tcphdr *oth, unsigned int otcplen) { struct tcphdr *tcph; skb_reset_transport_header(nskb); tcph = skb_put_zero(nskb, sizeof(struct tcphdr)); tcph->doff = sizeof(struct tcphdr)/4; tcph->source = oth->dest; tcph->dest = oth->source; if (oth->ack) { tcph->seq = oth->ack_seq; } else { tcph->ack_seq = htonl(ntohl(oth->seq) + oth->syn + oth->fin + otcplen - (oth->doff<<2)); tcph->ack = 1; } tcph->rst = 1; tcph->check = csum_ipv6_magic(&ipv6_hdr(nskb)->saddr, &ipv6_hdr(nskb)->daddr, sizeof(struct tcphdr), IPPROTO_TCP, csum_partial(tcph, sizeof(struct tcphdr), 0)); }"
1197----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53201/bad/dcn20_hwseq.c----dcn20_program_pipe,"static void dcn20_program_pipe( struct dc *dc, struct pipe_ctx *pipe_ctx, struct dc_state *context) { struct dce_hwseq *hws = dc->hwseq; if (resource_is_pipe_type(pipe_ctx, OTG_MASTER)) { if (pipe_ctx->update_flags.bits.enable || pipe_ctx->update_flags.bits.odm || pipe_ctx->stream->update_flags.bits.abm_level) hws->funcs.blank_pixel_data(dc, pipe_ctx, !pipe_ctx->plane_state || !pipe_ctx->plane_state->visible); } if (pipe_ctx->update_flags.bits.global_sync && !pipe_ctx->top_pipe && !pipe_ctx->prev_odm_pipe) { pipe_ctx->stream_res.tg->funcs->program_global_sync( pipe_ctx->stream_res.tg, calculate_vready_offset_for_group(pipe_ctx), pipe_ctx->pipe_dlg_param.vstartup_start, pipe_ctx->pipe_dlg_param.vupdate_offset, pipe_ctx->pipe_dlg_param.vupdate_width); if (dc_state_get_pipe_subvp_type(context, pipe_ctx) != SUBVP_PHANTOM) pipe_ctx->stream_res.tg->funcs->wait_for_state(pipe_ctx->stream_res.tg, CRTC_STATE_VACTIVE); pipe_ctx->stream_res.tg->funcs->set_vtg_params( pipe_ctx->stream_res.tg, &pipe_ctx->stream->timing, true); if (hws->funcs.setup_vupdate_interrupt) hws->funcs.setup_vupdate_interrupt(dc, pipe_ctx); } if (pipe_ctx->update_flags.bits.odm) hws->funcs.update_odm(dc, context, pipe_ctx); if (pipe_ctx->update_flags.bits.enable) { if (hws->funcs.enable_plane) hws->funcs.enable_plane(dc, pipe_ctx, context); else dcn20_enable_plane(dc, pipe_ctx, context); if (dc->res_pool->hubbub->funcs->force_wm_propagate_to_pipes) dc->res_pool->hubbub->funcs->force_wm_propagate_to_pipes(dc->res_pool->hubbub); } if (pipe_ctx->update_flags.bits.det_size) { if (dc->res_pool->hubbub->funcs->program_det_size) dc->res_pool->hubbub->funcs->program_det_size( dc->res_pool->hubbub, pipe_ctx->plane_res.hubp->inst, pipe_ctx->det_buffer_size_kb); if (dc->res_pool->hubbub->funcs->program_det_segments) dc->res_pool->hubbub->funcs->program_det_segments( dc->res_pool->hubbub, pipe_ctx->plane_res.hubp->inst, pipe_ctx->hubp_regs.det_size); } <S2SV_StartVul> if (pipe_ctx->update_flags.raw || <S2SV_EndVul> <S2SV_StartVul> (pipe_ctx->plane_state && pipe_ctx->plane_state->update_flags.raw) || <S2SV_EndVul> <S2SV_StartVul> pipe_ctx->stream->update_flags.raw) <S2SV_EndVul> dcn20_update_dchubp_dpp(dc, pipe_ctx, context); if (pipe_ctx->update_flags.bits.enable || (pipe_ctx->plane_state && pipe_ctx->plane_state->update_flags.bits.hdr_mult)) hws->funcs.set_hdr_multiplier(pipe_ctx); if ((pipe_ctx->plane_state && pipe_ctx->plane_state->update_flags.bits.hdr_mult) || pipe_ctx->update_flags.bits.enable) hws->funcs.set_hdr_multiplier(pipe_ctx); if (hws->funcs.populate_mcm_luts) { if (pipe_ctx->plane_state) { hws->funcs.populate_mcm_luts(dc, pipe_ctx, pipe_ctx->plane_state->mcm_luts, pipe_ctx->plane_state->lut_bank_a); pipe_ctx->plane_state->lut_bank_a = !pipe_ctx->plane_state->lut_bank_a; } } if ((pipe_ctx->plane_state && pipe_ctx->plane_state->update_flags.bits.in_transfer_func_change) || (pipe_ctx->plane_state && pipe_ctx->plane_state->update_flags.bits.gamma_change) || (pipe_ctx->plane_state && pipe_ctx->plane_state->update_flags.bits.lut_3d) || pipe_ctx->update_flags.bits.enable) hws->funcs.set_input_transfer_func(dc, pipe_ctx, pipe_ctx->plane_state); if (pipe_ctx->update_flags.bits.enable || pipe_ctx->update_flags.bits.plane_changed || pipe_ctx->stream->update_flags.bits.out_tf || (pipe_ctx->plane_state && pipe_ctx->plane_state->update_flags.bits.output_tf_change)) hws->funcs.set_output_transfer_func(dc, pipe_ctx, pipe_ctx->stream); if (pipe_ctx->update_flags.bits.enable || pipe_ctx->update_flags.bits.opp_changed) { pipe_ctx->stream_res.opp->funcs->opp_set_dyn_expansion( pipe_ctx->stream_res.opp, COLOR_SPACE_YCBCR601, pipe_ctx->stream->timing.display_color_depth, pipe_ctx->stream->signal); pipe_ctx->stream_res.opp->funcs->opp_program_fmt( pipe_ctx->stream_res.opp, &pipe_ctx->stream->bit_depth_params, &pipe_ctx->stream->clamping); } if ((pipe_ctx->plane_state && pipe_ctx->plane_state->visible)) { if (pipe_ctx->stream_res.abm) { dc->hwss.set_pipe(pipe_ctx); pipe_ctx->stream_res.abm->funcs->set_abm_level(pipe_ctx->stream_res.abm, pipe_ctx->stream->abm_level); } } if (pipe_ctx->update_flags.bits.test_pattern_changed) { struct output_pixel_processor *odm_opp = pipe_ctx->stream_res.opp; struct bit_depth_reduction_params params; memset(&params, 0, sizeof(params)); odm_opp->funcs->opp_program_bit_depth_reduction(odm_opp, &params); dc->hwss.set_disp_pattern_generator(dc, pipe_ctx, pipe_ctx->stream_res.test_pattern_params.test_pattern, pipe_ctx->stream_res.test_pattern_params.color_space, pipe_ctx->stream_res.test_pattern_params.color_depth, NULL, pipe_ctx->stream_res.test_pattern_params.width, pipe_ctx->stream_res.test_pattern_params.height, pipe_ctx->stream_res.test_pattern_params.offset); } }","- if (pipe_ctx->update_flags.raw ||
- (pipe_ctx->plane_state && pipe_ctx->plane_state->update_flags.raw) ||
- pipe_ctx->stream->update_flags.raw)
+ if (pipe_ctx->plane_state && (pipe_ctx->update_flags.raw ||
+ pipe_ctx->plane_state->update_flags.raw ||
+ pipe_ctx->stream->update_flags.raw))","static void dcn20_program_pipe( struct dc *dc, struct pipe_ctx *pipe_ctx, struct dc_state *context) { struct dce_hwseq *hws = dc->hwseq; if (resource_is_pipe_type(pipe_ctx, OTG_MASTER)) { if (pipe_ctx->update_flags.bits.enable || pipe_ctx->update_flags.bits.odm || pipe_ctx->stream->update_flags.bits.abm_level) hws->funcs.blank_pixel_data(dc, pipe_ctx, !pipe_ctx->plane_state || !pipe_ctx->plane_state->visible); } if (pipe_ctx->update_flags.bits.global_sync && !pipe_ctx->top_pipe && !pipe_ctx->prev_odm_pipe) { pipe_ctx->stream_res.tg->funcs->program_global_sync( pipe_ctx->stream_res.tg, calculate_vready_offset_for_group(pipe_ctx), pipe_ctx->pipe_dlg_param.vstartup_start, pipe_ctx->pipe_dlg_param.vupdate_offset, pipe_ctx->pipe_dlg_param.vupdate_width); if (dc_state_get_pipe_subvp_type(context, pipe_ctx) != SUBVP_PHANTOM) pipe_ctx->stream_res.tg->funcs->wait_for_state(pipe_ctx->stream_res.tg, CRTC_STATE_VACTIVE); pipe_ctx->stream_res.tg->funcs->set_vtg_params( pipe_ctx->stream_res.tg, &pipe_ctx->stream->timing, true); if (hws->funcs.setup_vupdate_interrupt) hws->funcs.setup_vupdate_interrupt(dc, pipe_ctx); } if (pipe_ctx->update_flags.bits.odm) hws->funcs.update_odm(dc, context, pipe_ctx); if (pipe_ctx->update_flags.bits.enable) { if (hws->funcs.enable_plane) hws->funcs.enable_plane(dc, pipe_ctx, context); else dcn20_enable_plane(dc, pipe_ctx, context); if (dc->res_pool->hubbub->funcs->force_wm_propagate_to_pipes) dc->res_pool->hubbub->funcs->force_wm_propagate_to_pipes(dc->res_pool->hubbub); } if (pipe_ctx->update_flags.bits.det_size) { if (dc->res_pool->hubbub->funcs->program_det_size) dc->res_pool->hubbub->funcs->program_det_size( dc->res_pool->hubbub, pipe_ctx->plane_res.hubp->inst, pipe_ctx->det_buffer_size_kb); if (dc->res_pool->hubbub->funcs->program_det_segments) dc->res_pool->hubbub->funcs->program_det_segments( dc->res_pool->hubbub, pipe_ctx->plane_res.hubp->inst, pipe_ctx->hubp_regs.det_size); } if (pipe_ctx->plane_state && (pipe_ctx->update_flags.raw || pipe_ctx->plane_state->update_flags.raw || pipe_ctx->stream->update_flags.raw)) dcn20_update_dchubp_dpp(dc, pipe_ctx, context); if (pipe_ctx->update_flags.bits.enable || (pipe_ctx->plane_state && pipe_ctx->plane_state->update_flags.bits.hdr_mult)) hws->funcs.set_hdr_multiplier(pipe_ctx); if ((pipe_ctx->plane_state && pipe_ctx->plane_state->update_flags.bits.hdr_mult) || pipe_ctx->update_flags.bits.enable) hws->funcs.set_hdr_multiplier(pipe_ctx); if (hws->funcs.populate_mcm_luts) { if (pipe_ctx->plane_state) { hws->funcs.populate_mcm_luts(dc, pipe_ctx, pipe_ctx->plane_state->mcm_luts, pipe_ctx->plane_state->lut_bank_a); pipe_ctx->plane_state->lut_bank_a = !pipe_ctx->plane_state->lut_bank_a; } } if ((pipe_ctx->plane_state && pipe_ctx->plane_state->update_flags.bits.in_transfer_func_change) || (pipe_ctx->plane_state && pipe_ctx->plane_state->update_flags.bits.gamma_change) || (pipe_ctx->plane_state && pipe_ctx->plane_state->update_flags.bits.lut_3d) || pipe_ctx->update_flags.bits.enable) hws->funcs.set_input_transfer_func(dc, pipe_ctx, pipe_ctx->plane_state); if (pipe_ctx->update_flags.bits.enable || pipe_ctx->update_flags.bits.plane_changed || pipe_ctx->stream->update_flags.bits.out_tf || (pipe_ctx->plane_state && pipe_ctx->plane_state->update_flags.bits.output_tf_change)) hws->funcs.set_output_transfer_func(dc, pipe_ctx, pipe_ctx->stream); if (pipe_ctx->update_flags.bits.enable || pipe_ctx->update_flags.bits.opp_changed) { pipe_ctx->stream_res.opp->funcs->opp_set_dyn_expansion( pipe_ctx->stream_res.opp, COLOR_SPACE_YCBCR601, pipe_ctx->stream->timing.display_color_depth, pipe_ctx->stream->signal); pipe_ctx->stream_res.opp->funcs->opp_program_fmt( pipe_ctx->stream_res.opp, &pipe_ctx->stream->bit_depth_params, &pipe_ctx->stream->clamping); } if ((pipe_ctx->plane_state && pipe_ctx->plane_state->visible)) { if (pipe_ctx->stream_res.abm) { dc->hwss.set_pipe(pipe_ctx); pipe_ctx->stream_res.abm->funcs->set_abm_level(pipe_ctx->stream_res.abm, pipe_ctx->stream->abm_level); } } if (pipe_ctx->update_flags.bits.test_pattern_changed) { struct output_pixel_processor *odm_opp = pipe_ctx->stream_res.opp; struct bit_depth_reduction_params params; memset(&params, 0, sizeof(params)); odm_opp->funcs->opp_program_bit_depth_reduction(odm_opp, &params); dc->hwss.set_disp_pattern_generator(dc, pipe_ctx, pipe_ctx->stream_res.test_pattern_params.test_pattern, pipe_ctx->stream_res.test_pattern_params.color_space, pipe_ctx->stream_res.test_pattern_params.color_depth, NULL, pipe_ctx->stream_res.test_pattern_params.width, pipe_ctx->stream_res.test_pattern_params.height, pipe_ctx->stream_res.test_pattern_params.offset); } }"
961----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50203/bad/bpf_jit_comp.c----prepare_trampoline,"static int prepare_trampoline(struct jit_ctx *ctx, struct bpf_tramp_image *im, struct bpf_tramp_links *tlinks, void *func_addr, int nregs, u32 flags) { int i; int stack_size; int retaddr_off; int regs_off; int retval_off; int args_off; int nregs_off; int ip_off; int run_ctx_off; struct bpf_tramp_links *fentry = &tlinks[BPF_TRAMP_FENTRY]; struct bpf_tramp_links *fexit = &tlinks[BPF_TRAMP_FEXIT]; struct bpf_tramp_links *fmod_ret = &tlinks[BPF_TRAMP_MODIFY_RETURN]; bool save_ret; __le32 **branches = NULL; stack_size = 0; run_ctx_off = stack_size; stack_size += round_up(sizeof(struct bpf_tramp_run_ctx), 8); ip_off = stack_size; if (flags & BPF_TRAMP_F_IP_ARG) stack_size += 8; nregs_off = stack_size; stack_size += 8; args_off = stack_size; stack_size += nregs * 8; retval_off = stack_size; save_ret = flags & (BPF_TRAMP_F_CALL_ORIG | BPF_TRAMP_F_RET_FENTRY_RET); if (save_ret) stack_size += 8; regs_off = stack_size; stack_size += 16; stack_size = round_up(stack_size, 16); retaddr_off = stack_size + 8; emit_bti(A64_BTI_JC, ctx); emit(A64_PUSH(A64_FP, A64_R(9), A64_SP), ctx); emit(A64_MOV(1, A64_FP, A64_SP), ctx); emit(A64_PUSH(A64_FP, A64_LR, A64_SP), ctx); emit(A64_MOV(1, A64_FP, A64_SP), ctx); emit(A64_SUB_I(1, A64_SP, A64_SP, stack_size), ctx); if (flags & BPF_TRAMP_F_IP_ARG) { emit_addr_mov_i64(A64_R(10), (const u64)func_addr, ctx); emit(A64_STR64I(A64_R(10), A64_SP, ip_off), ctx); } emit(A64_MOVZ(1, A64_R(10), nregs, 0), ctx); emit(A64_STR64I(A64_R(10), A64_SP, nregs_off), ctx); save_args(ctx, args_off, nregs); emit(A64_STR64I(A64_R(19), A64_SP, regs_off), ctx); emit(A64_STR64I(A64_R(20), A64_SP, regs_off + 8), ctx); if (flags & BPF_TRAMP_F_CALL_ORIG) { <S2SV_StartVul> emit_a64_mov_i64(A64_R(0), (const u64)im, ctx); <S2SV_EndVul> emit_call((const u64)__bpf_tramp_enter, ctx); } for (i = 0; i < fentry->nr_links; i++) invoke_bpf_prog(ctx, fentry->links[i], args_off, retval_off, run_ctx_off, flags & BPF_TRAMP_F_RET_FENTRY_RET); if (fmod_ret->nr_links) { branches = kcalloc(fmod_ret->nr_links, sizeof(__le32 *), GFP_KERNEL); if (!branches) return -ENOMEM; invoke_bpf_mod_ret(ctx, fmod_ret, args_off, retval_off, run_ctx_off, branches); } if (flags & BPF_TRAMP_F_CALL_ORIG) { restore_args(ctx, args_off, nregs); emit(A64_LDR64I(A64_R(10), A64_SP, retaddr_off), ctx); emit(A64_ADR(A64_LR, AARCH64_INSN_SIZE * 2), ctx); emit(A64_RET(A64_R(10)), ctx); emit(A64_STR64I(A64_R(0), A64_SP, retval_off), ctx); im->ip_after_call = ctx->ro_image + ctx->idx; emit(A64_NOP, ctx); } for (i = 0; i < fmod_ret->nr_links && ctx->image != NULL; i++) { int offset = &ctx->image[ctx->idx] - branches[i]; *branches[i] = cpu_to_le32(A64_CBNZ(1, A64_R(10), offset)); } for (i = 0; i < fexit->nr_links; i++) invoke_bpf_prog(ctx, fexit->links[i], args_off, retval_off, run_ctx_off, false); if (flags & BPF_TRAMP_F_CALL_ORIG) { im->ip_epilogue = ctx->ro_image + ctx->idx; <S2SV_StartVul> emit_a64_mov_i64(A64_R(0), (const u64)im, ctx); <S2SV_EndVul> emit_call((const u64)__bpf_tramp_exit, ctx); } if (flags & BPF_TRAMP_F_RESTORE_REGS) restore_args(ctx, args_off, nregs); emit(A64_LDR64I(A64_R(19), A64_SP, regs_off), ctx); emit(A64_LDR64I(A64_R(20), A64_SP, regs_off + 8), ctx); if (save_ret) emit(A64_LDR64I(A64_R(0), A64_SP, retval_off), ctx); emit(A64_MOV(1, A64_SP, A64_FP), ctx); emit(A64_POP(A64_FP, A64_LR, A64_SP), ctx); emit(A64_POP(A64_FP, A64_R(9), A64_SP), ctx); if (flags & BPF_TRAMP_F_SKIP_FRAME) { emit(A64_MOV(1, A64_LR, A64_R(9)), ctx); emit(A64_RET(A64_R(9)), ctx); } else { emit(A64_MOV(1, A64_R(10), A64_LR), ctx); emit(A64_MOV(1, A64_LR, A64_R(9)), ctx); emit(A64_RET(A64_R(10)), ctx); } kfree(branches); return ctx->idx; }","- emit_a64_mov_i64(A64_R(0), (const u64)im, ctx);
- emit_a64_mov_i64(A64_R(0), (const u64)im, ctx);
+ if (!ctx->image)
+ ctx->idx += 4;
+ else
+ emit_a64_mov_i64(A64_R(0), (const u64)im, ctx);
+ if (!ctx->image)
+ ctx->idx += 4;
+ else
+ emit_a64_mov_i64(A64_R(0), (const u64)im, ctx);","static int prepare_trampoline(struct jit_ctx *ctx, struct bpf_tramp_image *im, struct bpf_tramp_links *tlinks, void *func_addr, int nregs, u32 flags) { int i; int stack_size; int retaddr_off; int regs_off; int retval_off; int args_off; int nregs_off; int ip_off; int run_ctx_off; struct bpf_tramp_links *fentry = &tlinks[BPF_TRAMP_FENTRY]; struct bpf_tramp_links *fexit = &tlinks[BPF_TRAMP_FEXIT]; struct bpf_tramp_links *fmod_ret = &tlinks[BPF_TRAMP_MODIFY_RETURN]; bool save_ret; __le32 **branches = NULL; stack_size = 0; run_ctx_off = stack_size; stack_size += round_up(sizeof(struct bpf_tramp_run_ctx), 8); ip_off = stack_size; if (flags & BPF_TRAMP_F_IP_ARG) stack_size += 8; nregs_off = stack_size; stack_size += 8; args_off = stack_size; stack_size += nregs * 8; retval_off = stack_size; save_ret = flags & (BPF_TRAMP_F_CALL_ORIG | BPF_TRAMP_F_RET_FENTRY_RET); if (save_ret) stack_size += 8; regs_off = stack_size; stack_size += 16; stack_size = round_up(stack_size, 16); retaddr_off = stack_size + 8; emit_bti(A64_BTI_JC, ctx); emit(A64_PUSH(A64_FP, A64_R(9), A64_SP), ctx); emit(A64_MOV(1, A64_FP, A64_SP), ctx); emit(A64_PUSH(A64_FP, A64_LR, A64_SP), ctx); emit(A64_MOV(1, A64_FP, A64_SP), ctx); emit(A64_SUB_I(1, A64_SP, A64_SP, stack_size), ctx); if (flags & BPF_TRAMP_F_IP_ARG) { emit_addr_mov_i64(A64_R(10), (const u64)func_addr, ctx); emit(A64_STR64I(A64_R(10), A64_SP, ip_off), ctx); } emit(A64_MOVZ(1, A64_R(10), nregs, 0), ctx); emit(A64_STR64I(A64_R(10), A64_SP, nregs_off), ctx); save_args(ctx, args_off, nregs); emit(A64_STR64I(A64_R(19), A64_SP, regs_off), ctx); emit(A64_STR64I(A64_R(20), A64_SP, regs_off + 8), ctx); if (flags & BPF_TRAMP_F_CALL_ORIG) { if (!ctx->image) ctx->idx += 4; else emit_a64_mov_i64(A64_R(0), (const u64)im, ctx); emit_call((const u64)__bpf_tramp_enter, ctx); } for (i = 0; i < fentry->nr_links; i++) invoke_bpf_prog(ctx, fentry->links[i], args_off, retval_off, run_ctx_off, flags & BPF_TRAMP_F_RET_FENTRY_RET); if (fmod_ret->nr_links) { branches = kcalloc(fmod_ret->nr_links, sizeof(__le32 *), GFP_KERNEL); if (!branches) return -ENOMEM; invoke_bpf_mod_ret(ctx, fmod_ret, args_off, retval_off, run_ctx_off, branches); } if (flags & BPF_TRAMP_F_CALL_ORIG) { restore_args(ctx, args_off, nregs); emit(A64_LDR64I(A64_R(10), A64_SP, retaddr_off), ctx); emit(A64_ADR(A64_LR, AARCH64_INSN_SIZE * 2), ctx); emit(A64_RET(A64_R(10)), ctx); emit(A64_STR64I(A64_R(0), A64_SP, retval_off), ctx); im->ip_after_call = ctx->ro_image + ctx->idx; emit(A64_NOP, ctx); } for (i = 0; i < fmod_ret->nr_links && ctx->image != NULL; i++) { int offset = &ctx->image[ctx->idx] - branches[i]; *branches[i] = cpu_to_le32(A64_CBNZ(1, A64_R(10), offset)); } for (i = 0; i < fexit->nr_links; i++) invoke_bpf_prog(ctx, fexit->links[i], args_off, retval_off, run_ctx_off, false); if (flags & BPF_TRAMP_F_CALL_ORIG) { im->ip_epilogue = ctx->ro_image + ctx->idx; if (!ctx->image) ctx->idx += 4; else emit_a64_mov_i64(A64_R(0), (const u64)im, ctx); emit_call((const u64)__bpf_tramp_exit, ctx); } if (flags & BPF_TRAMP_F_RESTORE_REGS) restore_args(ctx, args_off, nregs); emit(A64_LDR64I(A64_R(19), A64_SP, regs_off), ctx); emit(A64_LDR64I(A64_R(20), A64_SP, regs_off + 8), ctx); if (save_ret) emit(A64_LDR64I(A64_R(0), A64_SP, retval_off), ctx); emit(A64_MOV(1, A64_SP, A64_FP), ctx); emit(A64_POP(A64_FP, A64_LR, A64_SP), ctx); emit(A64_POP(A64_FP, A64_R(9), A64_SP), ctx); if (flags & BPF_TRAMP_F_SKIP_FRAME) { emit(A64_MOV(1, A64_LR, A64_R(9)), ctx); emit(A64_RET(A64_R(9)), ctx); } else { emit(A64_MOV(1, A64_R(10), A64_LR), ctx); emit(A64_MOV(1, A64_LR, A64_R(9)), ctx); emit(A64_RET(A64_R(10)), ctx); } kfree(branches); return ctx->idx; }"
943----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50190/bad/ice_ddp.c----ice_find_seg_in_pkg,"ice_find_seg_in_pkg(struct ice_hw *hw, u32 seg_type, struct ice_pkg_hdr *pkg_hdr) { u32 i; ice_debug(hw, ICE_DBG_PKG, ""Package format version: %d.%d.%d.%d\n"", pkg_hdr->pkg_format_ver.major, pkg_hdr->pkg_format_ver.minor, pkg_hdr->pkg_format_ver.update, pkg_hdr->pkg_format_ver.draft); for (i = 0; i < le32_to_cpu(pkg_hdr->seg_count); i++) { <S2SV_StartVul> struct ice_generic_seg_hdr *seg; <S2SV_EndVul> <S2SV_StartVul> seg = (struct ice_generic_seg_hdr <S2SV_EndVul> *)((u8 *)pkg_hdr + <S2SV_StartVul> le32_to_cpu(pkg_hdr->seg_offset[i])); <S2SV_EndVul> if (le32_to_cpu(seg->seg_type) == seg_type) return seg; } return NULL; }","- struct ice_generic_seg_hdr *seg;
- seg = (struct ice_generic_seg_hdr
- le32_to_cpu(pkg_hdr->seg_offset[i]));
+ const struct ice_generic_seg_hdr *seg;
+ seg = (void *)pkg_hdr + le32_to_cpu(pkg_hdr->seg_offset[i]);","ice_find_seg_in_pkg(struct ice_hw *hw, u32 seg_type, const struct ice_pkg_hdr *pkg_hdr) { u32 i; ice_debug(hw, ICE_DBG_PKG, ""Package format version: %d.%d.%d.%d\n"", pkg_hdr->pkg_format_ver.major, pkg_hdr->pkg_format_ver.minor, pkg_hdr->pkg_format_ver.update, pkg_hdr->pkg_format_ver.draft); for (i = 0; i < le32_to_cpu(pkg_hdr->seg_count); i++) { const struct ice_generic_seg_hdr *seg; seg = (void *)pkg_hdr + le32_to_cpu(pkg_hdr->seg_offset[i]); if (le32_to_cpu(seg->seg_type) == seg_type) return seg; } return NULL; }"
1441----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56766/bad/pmecc.c----atmel_pmecc_create_user,"atmel_pmecc_create_user(struct atmel_pmecc *pmecc, struct atmel_pmecc_user_req *req) { struct atmel_pmecc_user *user; const struct atmel_pmecc_gf_tables *gf_tables; int strength, size, ret; ret = atmel_pmecc_prepare_user_req(pmecc, req); if (ret) return ERR_PTR(ret); size = sizeof(*user); size = ALIGN(size, sizeof(u16)); size += ((2 * req->ecc.strength) + 1) * sizeof(u16) * (2 + req->ecc.strength + 2); size += (req->ecc.strength + 1) * sizeof(u16); size = ALIGN(size, sizeof(s32)); size += (req->ecc.strength + 1) * sizeof(s32) * 3; user = devm_kzalloc(pmecc->dev, size, GFP_KERNEL); if (!user) return ERR_PTR(-ENOMEM); user->pmecc = pmecc; user->partial_syn = (s16 *)PTR_ALIGN(user + 1, sizeof(u16)); user->si = user->partial_syn + ((2 * req->ecc.strength) + 1); user->lmu = user->si + ((2 * req->ecc.strength) + 1); user->smu = user->lmu + (req->ecc.strength + 1); user->mu = (s32 *)PTR_ALIGN(user->smu + (((2 * req->ecc.strength) + 1) * (req->ecc.strength + 2)), sizeof(s32)); user->dmu = user->mu + req->ecc.strength + 1; user->delta = user->dmu + req->ecc.strength + 1; gf_tables = atmel_pmecc_get_gf_tables(req); <S2SV_StartVul> if (IS_ERR(gf_tables)) { <S2SV_EndVul> <S2SV_StartVul> kfree(user); <S2SV_EndVul> return ERR_CAST(gf_tables); <S2SV_StartVul> } <S2SV_EndVul> user->gf_tables = gf_tables; user->eccbytes = req->ecc.bytes / req->ecc.nsectors; for (strength = 0; strength < pmecc->caps->nstrengths; strength++) { if (pmecc->caps->strengths[strength] == req->ecc.strength) break; } user->cache.cfg = PMECC_CFG_BCH_STRENGTH(strength) | PMECC_CFG_NSECTORS(req->ecc.nsectors); if (req->ecc.sectorsize == 1024) user->cache.cfg |= PMECC_CFG_SECTOR1024; user->cache.sarea = req->oobsize - 1; user->cache.saddr = req->ecc.ooboffset; user->cache.eaddr = req->ecc.ooboffset + req->ecc.bytes - 1; return user; }","- if (IS_ERR(gf_tables)) {
- kfree(user);
- }
+ if (IS_ERR(gf_tables))","atmel_pmecc_create_user(struct atmel_pmecc *pmecc, struct atmel_pmecc_user_req *req) { struct atmel_pmecc_user *user; const struct atmel_pmecc_gf_tables *gf_tables; int strength, size, ret; ret = atmel_pmecc_prepare_user_req(pmecc, req); if (ret) return ERR_PTR(ret); size = sizeof(*user); size = ALIGN(size, sizeof(u16)); size += ((2 * req->ecc.strength) + 1) * sizeof(u16) * (2 + req->ecc.strength + 2); size += (req->ecc.strength + 1) * sizeof(u16); size = ALIGN(size, sizeof(s32)); size += (req->ecc.strength + 1) * sizeof(s32) * 3; user = devm_kzalloc(pmecc->dev, size, GFP_KERNEL); if (!user) return ERR_PTR(-ENOMEM); user->pmecc = pmecc; user->partial_syn = (s16 *)PTR_ALIGN(user + 1, sizeof(u16)); user->si = user->partial_syn + ((2 * req->ecc.strength) + 1); user->lmu = user->si + ((2 * req->ecc.strength) + 1); user->smu = user->lmu + (req->ecc.strength + 1); user->mu = (s32 *)PTR_ALIGN(user->smu + (((2 * req->ecc.strength) + 1) * (req->ecc.strength + 2)), sizeof(s32)); user->dmu = user->mu + req->ecc.strength + 1; user->delta = user->dmu + req->ecc.strength + 1; gf_tables = atmel_pmecc_get_gf_tables(req); if (IS_ERR(gf_tables)) return ERR_CAST(gf_tables); user->gf_tables = gf_tables; user->eccbytes = req->ecc.bytes / req->ecc.nsectors; for (strength = 0; strength < pmecc->caps->nstrengths; strength++) { if (pmecc->caps->strengths[strength] == req->ecc.strength) break; } user->cache.cfg = PMECC_CFG_BCH_STRENGTH(strength) | PMECC_CFG_NSECTORS(req->ecc.nsectors); if (req->ecc.sectorsize == 1024) user->cache.cfg |= PMECC_CFG_SECTOR1024; user->cache.sarea = req->oobsize - 1; user->cache.saddr = req->ecc.ooboffset; user->cache.eaddr = req->ecc.ooboffset + req->ecc.bytes - 1; return user; }"
1129----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53100/bad/tcp.c----nvme_tcp_get_address,"static int nvme_tcp_get_address(struct nvme_ctrl *ctrl, char *buf, int size) { struct nvme_tcp_queue *queue = &to_tcp_ctrl(ctrl)->queues[0]; struct sockaddr_storage src_addr; int ret, len; len = nvmf_get_address(ctrl, buf, size); mutex_lock(&queue->queue_lock); <S2SV_StartVul> if (!test_bit(NVME_TCP_Q_LIVE, &queue->flags)) <S2SV_EndVul> <S2SV_StartVul> goto done; <S2SV_EndVul> ret = kernel_getsockname(queue->sock, (struct sockaddr *)&src_addr); if (ret > 0) { if (len > 0) len--; len += scnprintf(buf + len, size - len, ""%ssrc_addr=%pISc\n"", (len) ? "","" : """", &src_addr); } <S2SV_StartVul> done: <S2SV_EndVul> mutex_unlock(&queue->queue_lock); return len; }","- if (!test_bit(NVME_TCP_Q_LIVE, &queue->flags))
- goto done;
- done:
+ if (!test_bit(NVME_TCP_Q_LIVE, &queue->flags))
+ return len;
+ return len;","static int nvme_tcp_get_address(struct nvme_ctrl *ctrl, char *buf, int size) { struct nvme_tcp_queue *queue = &to_tcp_ctrl(ctrl)->queues[0]; struct sockaddr_storage src_addr; int ret, len; len = nvmf_get_address(ctrl, buf, size); if (!test_bit(NVME_TCP_Q_LIVE, &queue->flags)) return len; mutex_lock(&queue->queue_lock); ret = kernel_getsockname(queue->sock, (struct sockaddr *)&src_addr); if (ret > 0) { if (len > 0) len--; len += scnprintf(buf + len, size - len, ""%ssrc_addr=%pISc\n"", (len) ? "","" : """", &src_addr); } mutex_unlock(&queue->queue_lock); return len; }"
1472----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-57887/bad/adv7533.c----adv7533_parse_dt,"int adv7533_parse_dt(struct device_node *np, struct adv7511 *adv) { u32 num_lanes; of_property_read_u32(np, ""adi,dsi-lanes"", &num_lanes); if (num_lanes < 1 || num_lanes > 4) return -EINVAL; adv->num_dsi_lanes = num_lanes; adv->host_node = of_graph_get_remote_node(np, 0, 0); if (!adv->host_node) return -ENODEV; <S2SV_StartVul> of_node_put(adv->host_node); <S2SV_EndVul> adv->use_timing_gen = !of_property_read_bool(np, ""adi,disable-timing-generator""); adv->rgb = true; adv->embedded_sync = false; return 0; }",- of_node_put(adv->host_node);,"int adv7533_parse_dt(struct device_node *np, struct adv7511 *adv) { u32 num_lanes; of_property_read_u32(np, ""adi,dsi-lanes"", &num_lanes); if (num_lanes < 1 || num_lanes > 4) return -EINVAL; adv->num_dsi_lanes = num_lanes; adv->host_node = of_graph_get_remote_node(np, 0, 0); if (!adv->host_node) return -ENODEV; adv->use_timing_gen = !of_property_read_bool(np, ""adi,disable-timing-generator""); adv->rgb = true; adv->embedded_sync = false; return 0; }"
1391----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56722/bad/hns_roce_hw_v2.c----hns_roce_v2_destroy_qp,"int hns_roce_v2_destroy_qp(struct ib_qp *ibqp, struct ib_udata *udata) { struct hns_roce_dev *hr_dev = to_hr_dev(ibqp->device); struct hns_roce_qp *hr_qp = to_hr_qp(ibqp); unsigned long flags; int ret; spin_lock_irqsave(&hr_qp->flush_lock, flags); set_bit(HNS_ROCE_STOP_FLUSH_FLAG, &hr_qp->flush_flag); spin_unlock_irqrestore(&hr_qp->flush_lock, flags); flush_work(&hr_qp->flush_work.work); ret = hns_roce_v2_destroy_qp_common(hr_dev, hr_qp, udata); <S2SV_StartVul> if (ret) <S2SV_EndVul> <S2SV_StartVul> ibdev_err(&hr_dev->ib_dev, <S2SV_EndVul> <S2SV_StartVul> ""failed to destroy QP, QPN = 0x%06lx, ret = %d.\n"", <S2SV_EndVul> <S2SV_StartVul> hr_qp->qpn, ret); <S2SV_EndVul> hns_roce_qp_destroy(hr_dev, hr_qp, udata); return 0; }","- if (ret)
- ibdev_err(&hr_dev->ib_dev,
- ""failed to destroy QP, QPN = 0x%06lx, ret = %d.\n"",
- hr_qp->qpn, ret);
+ if (ret)
+ ibdev_err_ratelimited(&hr_dev->ib_dev,
+ ""failed to destroy QP, QPN = 0x%06lx, ret = %d.\n"",","int hns_roce_v2_destroy_qp(struct ib_qp *ibqp, struct ib_udata *udata) { struct hns_roce_dev *hr_dev = to_hr_dev(ibqp->device); struct hns_roce_qp *hr_qp = to_hr_qp(ibqp); unsigned long flags; int ret; spin_lock_irqsave(&hr_qp->flush_lock, flags); set_bit(HNS_ROCE_STOP_FLUSH_FLAG, &hr_qp->flush_flag); spin_unlock_irqrestore(&hr_qp->flush_lock, flags); flush_work(&hr_qp->flush_work.work); ret = hns_roce_v2_destroy_qp_common(hr_dev, hr_qp, udata); if (ret) ibdev_err_ratelimited(&hr_dev->ib_dev, ""failed to destroy QP, QPN = 0x%06lx, ret = %d.\n"", hr_qp->qpn, ret); hns_roce_qp_destroy(hr_dev, hr_qp, udata); return 0; }"
701----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49988/bad/oplock.c----smb_send_parent_lease_break_noti,"void smb_send_parent_lease_break_noti(struct ksmbd_file *fp, struct lease_ctx_info *lctx) { struct oplock_info *opinfo; struct ksmbd_inode *p_ci = NULL; if (lctx->version != 2) return; p_ci = ksmbd_inode_lookup_lock(fp->filp->f_path.dentry->d_parent); if (!p_ci) return; down_read(&p_ci->m_lock); list_for_each_entry(opinfo, &p_ci->m_op_list, op_entry) { if (opinfo->conn == NULL || !opinfo->is_lease) continue; if (opinfo->o_lease->state != SMB2_OPLOCK_LEVEL_NONE && (!(lctx->flags & SMB2_LEASE_FLAG_PARENT_LEASE_KEY_SET_LE) || !compare_guid_key(opinfo, fp->conn->ClientGUID, lctx->parent_lease_key))) { if (!atomic_inc_not_zero(&opinfo->refcount)) continue; <S2SV_StartVul> atomic_inc(&opinfo->conn->r_count); <S2SV_EndVul> <S2SV_StartVul> if (ksmbd_conn_releasing(opinfo->conn)) { <S2SV_EndVul> <S2SV_StartVul> atomic_dec(&opinfo->conn->r_count); <S2SV_EndVul> continue; <S2SV_StartVul> } <S2SV_EndVul> oplock_break(opinfo, SMB2_OPLOCK_LEVEL_NONE); <S2SV_StartVul> opinfo_conn_put(opinfo); <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> up_read(&p_ci->m_lock); ksmbd_inode_put(p_ci); }","- atomic_inc(&opinfo->conn->r_count);
- if (ksmbd_conn_releasing(opinfo->conn)) {
- atomic_dec(&opinfo->conn->r_count);
- }
- opinfo_conn_put(opinfo);
- }
- }
+ if (ksmbd_conn_releasing(opinfo->conn))
+ opinfo_put(opinfo);","void smb_send_parent_lease_break_noti(struct ksmbd_file *fp, struct lease_ctx_info *lctx) { struct oplock_info *opinfo; struct ksmbd_inode *p_ci = NULL; if (lctx->version != 2) return; p_ci = ksmbd_inode_lookup_lock(fp->filp->f_path.dentry->d_parent); if (!p_ci) return; down_read(&p_ci->m_lock); list_for_each_entry(opinfo, &p_ci->m_op_list, op_entry) { if (opinfo->conn == NULL || !opinfo->is_lease) continue; if (opinfo->o_lease->state != SMB2_OPLOCK_LEVEL_NONE && (!(lctx->flags & SMB2_LEASE_FLAG_PARENT_LEASE_KEY_SET_LE) || !compare_guid_key(opinfo, fp->conn->ClientGUID, lctx->parent_lease_key))) { if (!atomic_inc_not_zero(&opinfo->refcount)) continue; if (ksmbd_conn_releasing(opinfo->conn)) continue; oplock_break(opinfo, SMB2_OPLOCK_LEVEL_NONE); opinfo_put(opinfo); } } up_read(&p_ci->m_lock); ksmbd_inode_put(p_ci); }"
730----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50005/bad/scan.c----mac802154_scan_worker,"void mac802154_scan_worker(struct work_struct *work) { struct ieee802154_local *local = container_of(work, struct ieee802154_local, scan_work.work); struct cfg802154_scan_request *scan_req; struct ieee802154_sub_if_data *sdata; unsigned int scan_duration = 0; struct wpan_phy *wpan_phy; u8 scan_req_duration; u8 page, channel; int ret; drv_stop(local); synchronize_net(); mac802154_flush_queued_beacons(local); rcu_read_lock(); scan_req = rcu_dereference(local->scan_req); if (unlikely(!scan_req)) { rcu_read_unlock(); return; } sdata = IEEE802154_WPAN_DEV_TO_SUB_IF(scan_req->wpan_dev); if (local->suspended || !ieee802154_sdata_running(sdata)) { rcu_read_unlock(); queue_delayed_work(local->mac_wq, &local->scan_work, msecs_to_jiffies(1000)); return; } wpan_phy = scan_req->wpan_phy; scan_req_duration = scan_req->duration; page = local->scan_page; channel = local->scan_channel; do { ret = mac802154_scan_find_next_chan(local, scan_req, page, &channel); if (ret) { rcu_read_unlock(); goto end_scan; } } while (!ieee802154_chan_is_valid(scan_req->wpan_phy, page, channel)); rcu_read_unlock(); rtnl_lock(); ret = drv_set_channel(local, page, channel); rtnl_unlock(); if (ret) { dev_err(&sdata->dev->dev, ""Channel change failure during scan, aborting (%d)\n"", ret); goto end_scan; } local->scan_page = page; local->scan_channel = channel; rtnl_lock(); ret = drv_start(local, IEEE802154_FILTERING_3_SCAN, &local->addr_filt); rtnl_unlock(); if (ret) { dev_err(&sdata->dev->dev, ""Restarting failure after channel change, aborting (%d)\n"", ret); goto end_scan; } <S2SV_StartVul> if (scan_req->type == NL802154_SCAN_ACTIVE) { <S2SV_EndVul> ret = mac802154_transmit_beacon_req(local, sdata); if (ret) dev_err(&sdata->dev->dev, ""Error when transmitting beacon request (%d)\n"", ret); } ieee802154_configure_durations(wpan_phy, page, channel); scan_duration = mac802154_scan_get_channel_time(scan_req_duration, wpan_phy->symbol_duration); dev_dbg(&sdata->dev->dev, ""Scan page %u channel %u for %ums\n"", page, channel, jiffies_to_msecs(scan_duration)); queue_delayed_work(local->mac_wq, &local->scan_work, scan_duration); return; end_scan: rtnl_lock(); mac802154_scan_cleanup_locked(local, sdata, false); rtnl_unlock(); }","- if (scan_req->type == NL802154_SCAN_ACTIVE) {
+ enum nl802154_scan_types scan_req_type;
+ scan_req_type = scan_req->type;
+ if (scan_req_type == NL802154_SCAN_ACTIVE) {","void mac802154_scan_worker(struct work_struct *work) { struct ieee802154_local *local = container_of(work, struct ieee802154_local, scan_work.work); struct cfg802154_scan_request *scan_req; enum nl802154_scan_types scan_req_type; struct ieee802154_sub_if_data *sdata; unsigned int scan_duration = 0; struct wpan_phy *wpan_phy; u8 scan_req_duration; u8 page, channel; int ret; drv_stop(local); synchronize_net(); mac802154_flush_queued_beacons(local); rcu_read_lock(); scan_req = rcu_dereference(local->scan_req); if (unlikely(!scan_req)) { rcu_read_unlock(); return; } sdata = IEEE802154_WPAN_DEV_TO_SUB_IF(scan_req->wpan_dev); if (local->suspended || !ieee802154_sdata_running(sdata)) { rcu_read_unlock(); queue_delayed_work(local->mac_wq, &local->scan_work, msecs_to_jiffies(1000)); return; } wpan_phy = scan_req->wpan_phy; scan_req_type = scan_req->type; scan_req_duration = scan_req->duration; page = local->scan_page; channel = local->scan_channel; do { ret = mac802154_scan_find_next_chan(local, scan_req, page, &channel); if (ret) { rcu_read_unlock(); goto end_scan; } } while (!ieee802154_chan_is_valid(scan_req->wpan_phy, page, channel)); rcu_read_unlock(); rtnl_lock(); ret = drv_set_channel(local, page, channel); rtnl_unlock(); if (ret) { dev_err(&sdata->dev->dev, ""Channel change failure during scan, aborting (%d)\n"", ret); goto end_scan; } local->scan_page = page; local->scan_channel = channel; rtnl_lock(); ret = drv_start(local, IEEE802154_FILTERING_3_SCAN, &local->addr_filt); rtnl_unlock(); if (ret) { dev_err(&sdata->dev->dev, ""Restarting failure after channel change, aborting (%d)\n"", ret); goto end_scan; } if (scan_req_type == NL802154_SCAN_ACTIVE) { ret = mac802154_transmit_beacon_req(local, sdata); if (ret) dev_err(&sdata->dev->dev, ""Error when transmitting beacon request (%d)\n"", ret); } ieee802154_configure_durations(wpan_phy, page, channel); scan_duration = mac802154_scan_get_channel_time(scan_req_duration, wpan_phy->symbol_duration); dev_dbg(&sdata->dev->dev, ""Scan page %u channel %u for %ums\n"", page, channel, jiffies_to_msecs(scan_duration)); queue_delayed_work(local->mac_wq, &local->scan_work, scan_duration); return; end_scan: rtnl_lock(); mac802154_scan_cleanup_locked(local, sdata, false); rtnl_unlock(); }"
1260----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56542/bad/amdgpu_dm.c----dm_sw_fini,"static int dm_sw_fini(void *handle) { struct amdgpu_device *adev = (struct amdgpu_device *)handle; struct dal_allocation *da; list_for_each_entry(da, &adev->dm.da_list, list) { if (adev->dm.bb_from_dmub == (void *) da->cpu_ptr) { amdgpu_bo_free_kernel(&da->bo, &da->gpu_addr, &da->cpu_ptr); list_del(&da->list); kfree(da); break; } } <S2SV_StartVul> adev->dm.bb_from_dmub = NULL; <S2SV_EndVul> kfree(adev->dm.dmub_fb_info); adev->dm.dmub_fb_info = NULL; if (adev->dm.dmub_srv) { dmub_srv_destroy(adev->dm.dmub_srv); kfree(adev->dm.dmub_srv); adev->dm.dmub_srv = NULL; } amdgpu_ucode_release(&adev->dm.dmub_fw); amdgpu_ucode_release(&adev->dm.fw_dmcu); return 0; }","- adev->dm.bb_from_dmub = NULL;
+ list_del(&da->list);
+ kfree(da);
+ adev->dm.bb_from_dmub = NULL;
+ break;
+ }
+ }","static int dm_sw_fini(void *handle) { struct amdgpu_device *adev = (struct amdgpu_device *)handle; struct dal_allocation *da; list_for_each_entry(da, &adev->dm.da_list, list) { if (adev->dm.bb_from_dmub == (void *) da->cpu_ptr) { amdgpu_bo_free_kernel(&da->bo, &da->gpu_addr, &da->cpu_ptr); list_del(&da->list); kfree(da); adev->dm.bb_from_dmub = NULL; break; } } kfree(adev->dm.dmub_fb_info); adev->dm.dmub_fb_info = NULL; if (adev->dm.dmub_srv) { dmub_srv_destroy(adev->dm.dmub_srv); kfree(adev->dm.dmub_srv); adev->dm.dmub_srv = NULL; } amdgpu_ucode_release(&adev->dm.dmub_fw); amdgpu_ucode_release(&adev->dm.fw_dmcu); return 0; }"
418----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47660/bad/fsnotify.c----__fsnotify_parent,"int __fsnotify_parent(struct dentry *dentry, __u32 mask, const void *data, int data_type) { const struct path *path = fsnotify_data_path(data, data_type); struct mount *mnt = path ? real_mount(path->mnt) : NULL; struct inode *inode = d_inode(dentry); struct dentry *parent; bool parent_watched = dentry->d_flags & DCACHE_FSNOTIFY_PARENT_WATCHED; bool parent_needed, parent_interested; __u32 p_mask; struct inode *p_inode = NULL; struct name_snapshot name; struct qstr *file_name = NULL; int ret = 0; if (!inode->i_fsnotify_marks && !inode->i_sb->s_fsnotify_marks && (!mnt || !mnt->mnt_fsnotify_marks) && !parent_watched) return 0; parent = NULL; parent_needed = fsnotify_event_needs_parent(inode, mnt, mask); if (!parent_watched && !parent_needed) goto notify; parent = dget_parent(dentry); p_inode = parent->d_inode; p_mask = fsnotify_inode_watches_children(p_inode); if (unlikely(parent_watched && !p_mask)) <S2SV_StartVul> __fsnotify_update_child_dentry_flags(p_inode); <S2SV_EndVul> parent_interested = mask & p_mask & ALL_FSNOTIFY_EVENTS; if (parent_needed || parent_interested) { WARN_ON_ONCE(inode != fsnotify_data_inode(data, data_type)); take_dentry_name_snapshot(&name, dentry); file_name = &name.name; if (parent_interested) mask |= FS_EVENT_ON_CHILD; } notify: ret = fsnotify(mask, data, data_type, p_inode, file_name, inode, 0); if (file_name) release_dentry_name_snapshot(&name); dput(parent); return ret; }","- __fsnotify_update_child_dentry_flags(p_inode);
+ fsnotify_clear_child_dentry_flag(p_inode, dentry);","int __fsnotify_parent(struct dentry *dentry, __u32 mask, const void *data, int data_type) { const struct path *path = fsnotify_data_path(data, data_type); struct mount *mnt = path ? real_mount(path->mnt) : NULL; struct inode *inode = d_inode(dentry); struct dentry *parent; bool parent_watched = dentry->d_flags & DCACHE_FSNOTIFY_PARENT_WATCHED; bool parent_needed, parent_interested; __u32 p_mask; struct inode *p_inode = NULL; struct name_snapshot name; struct qstr *file_name = NULL; int ret = 0; if (!inode->i_fsnotify_marks && !inode->i_sb->s_fsnotify_marks && (!mnt || !mnt->mnt_fsnotify_marks) && !parent_watched) return 0; parent = NULL; parent_needed = fsnotify_event_needs_parent(inode, mnt, mask); if (!parent_watched && !parent_needed) goto notify; parent = dget_parent(dentry); p_inode = parent->d_inode; p_mask = fsnotify_inode_watches_children(p_inode); if (unlikely(parent_watched && !p_mask)) fsnotify_clear_child_dentry_flag(p_inode, dentry); parent_interested = mask & p_mask & ALL_FSNOTIFY_EVENTS; if (parent_needed || parent_interested) { WARN_ON_ONCE(inode != fsnotify_data_inode(data, data_type)); take_dentry_name_snapshot(&name, dentry); file_name = &name.name; if (parent_interested) mask |= FS_EVENT_ON_CHILD; } notify: ret = fsnotify(mask, data, data_type, p_inode, file_name, inode, 0); if (file_name) release_dentry_name_snapshot(&name); dput(parent); return ret; }"
1082----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53065/bad/slab_common.c----*kmem_buckets_create,"kmem_buckets *kmem_buckets_create(const char *name, slab_flags_t flags, unsigned int useroffset, unsigned int usersize, void (*ctor)(void *)) { kmem_buckets *b; <S2SV_StartVul> int idx; <S2SV_EndVul> if (!IS_ENABLED(CONFIG_SLAB_BUCKETS)) return ZERO_SIZE_PTR; if (WARN_ON(!kmem_buckets_cache)) return NULL; b = kmem_cache_alloc(kmem_buckets_cache, GFP_KERNEL|__GFP_ZERO); if (WARN_ON(!b)) return NULL; flags |= SLAB_NO_MERGE; for (idx = 0; idx < ARRAY_SIZE(kmalloc_caches[KMALLOC_NORMAL]); idx++) { char *short_size, *cache_name; unsigned int cache_useroffset, cache_usersize; <S2SV_StartVul> unsigned int size; <S2SV_EndVul> if (!kmalloc_caches[KMALLOC_NORMAL][idx]) continue; size = kmalloc_caches[KMALLOC_NORMAL][idx]->object_size; if (!size) continue; short_size = strchr(kmalloc_caches[KMALLOC_NORMAL][idx]->name, '-'); if (WARN_ON(!short_size)) <S2SV_StartVul> goto fail; <S2SV_EndVul> <S2SV_StartVul> cache_name = kasprintf(GFP_KERNEL, ""%s-%s"", name, short_size + 1); <S2SV_EndVul> <S2SV_StartVul> if (WARN_ON(!cache_name)) <S2SV_EndVul> <S2SV_StartVul> goto fail; <S2SV_EndVul> if (useroffset >= size) { cache_useroffset = 0; cache_usersize = 0; } else { cache_useroffset = useroffset; cache_usersize = min(size - cache_useroffset, usersize); } <S2SV_StartVul> (*b)[idx] = kmem_cache_create_usercopy(cache_name, size, <S2SV_EndVul> 0, flags, cache_useroffset, cache_usersize, ctor); <S2SV_StartVul> kfree(cache_name); <S2SV_EndVul> <S2SV_StartVul> if (WARN_ON(!(*b)[idx])) <S2SV_EndVul> <S2SV_StartVul> goto fail; <S2SV_EndVul> } return b; fail: <S2SV_StartVul> for (idx = 0; idx < ARRAY_SIZE(kmalloc_caches[KMALLOC_NORMAL]); idx++) <S2SV_EndVul> kmem_cache_destroy((*b)[idx]); kmem_cache_free(kmem_buckets_cache, b); return NULL; }","- int idx;
- unsigned int size;
- goto fail;
- cache_name = kasprintf(GFP_KERNEL, ""%s-%s"", name, short_size + 1);
- if (WARN_ON(!cache_name))
- goto fail;
- (*b)[idx] = kmem_cache_create_usercopy(cache_name, size,
- kfree(cache_name);
- if (WARN_ON(!(*b)[idx]))
- goto fail;
- for (idx = 0; idx < ARRAY_SIZE(kmalloc_caches[KMALLOC_NORMAL]); idx++)
+ unsigned long mask = 0;
+ unsigned int idx;
+ BUILD_BUG_ON(ARRAY_SIZE(kmalloc_caches[KMALLOC_NORMAL]) > BITS_PER_LONG);
+ unsigned int size, aligned_idx;
+ goto fail;
+ }
+ aligned_idx = __kmalloc_index(size, false);
+ if (!(*b)[aligned_idx]) {
+ cache_name = kasprintf(GFP_KERNEL, ""%s-%s"", name, short_size + 1);
+ if (WARN_ON(!cache_name))
+ goto fail;
+ (*b)[aligned_idx] = kmem_cache_create_usercopy(cache_name, size,
+ kfree(cache_name);
+ if (WARN_ON(!(*b)[aligned_idx]))
+ goto fail;
+ set_bit(aligned_idx, &mask);
+ }
+ if (idx != aligned_idx)
+ (*b)[idx] = (*b)[aligned_idx];
+ }
+ for_each_set_bit(idx, &mask, ARRAY_SIZE(kmalloc_caches[KMALLOC_NORMAL]))","kmem_buckets *kmem_buckets_create(const char *name, slab_flags_t flags, unsigned int useroffset, unsigned int usersize, void (*ctor)(void *)) { unsigned long mask = 0; unsigned int idx; kmem_buckets *b; BUILD_BUG_ON(ARRAY_SIZE(kmalloc_caches[KMALLOC_NORMAL]) > BITS_PER_LONG); if (!IS_ENABLED(CONFIG_SLAB_BUCKETS)) return ZERO_SIZE_PTR; if (WARN_ON(!kmem_buckets_cache)) return NULL; b = kmem_cache_alloc(kmem_buckets_cache, GFP_KERNEL|__GFP_ZERO); if (WARN_ON(!b)) return NULL; flags |= SLAB_NO_MERGE; for (idx = 0; idx < ARRAY_SIZE(kmalloc_caches[KMALLOC_NORMAL]); idx++) { char *short_size, *cache_name; unsigned int cache_useroffset, cache_usersize; unsigned int size, aligned_idx; if (!kmalloc_caches[KMALLOC_NORMAL][idx]) continue; size = kmalloc_caches[KMALLOC_NORMAL][idx]->object_size; if (!size) continue; short_size = strchr(kmalloc_caches[KMALLOC_NORMAL][idx]->name, '-'); if (WARN_ON(!short_size)) goto fail; if (useroffset >= size) { cache_useroffset = 0; cache_usersize = 0; } else { cache_useroffset = useroffset; cache_usersize = min(size - cache_useroffset, usersize); } aligned_idx = __kmalloc_index(size, false); if (!(*b)[aligned_idx]) { cache_name = kasprintf(GFP_KERNEL, ""%s-%s"", name, short_size + 1); if (WARN_ON(!cache_name)) goto fail; (*b)[aligned_idx] = kmem_cache_create_usercopy(cache_name, size, 0, flags, cache_useroffset, cache_usersize, ctor); kfree(cache_name); if (WARN_ON(!(*b)[aligned_idx])) goto fail; set_bit(aligned_idx, &mask); } if (idx != aligned_idx) (*b)[idx] = (*b)[aligned_idx]; } return b; fail: for_each_set_bit(idx, &mask, ARRAY_SIZE(kmalloc_caches[KMALLOC_NORMAL])) kmem_cache_destroy((*b)[idx]); kmem_cache_free(kmem_buckets_cache, b); return NULL; }"
696----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49988/bad/connection.c----ksmbd_conn_free,void ksmbd_conn_free(struct ksmbd_conn *conn) { down_write(&conn_list_lock); list_del(&conn->conns_list); up_write(&conn_list_lock); xa_destroy(&conn->sessions); kvfree(conn->request_buf); kfree(conn->preauth_info); <S2SV_StartVul> kfree(conn); <S2SV_EndVul> },"- kfree(conn);
+ if (atomic_dec_and_test(&conn->refcnt))
+ kfree(conn);",void ksmbd_conn_free(struct ksmbd_conn *conn) { down_write(&conn_list_lock); list_del(&conn->conns_list); up_write(&conn_list_lock); xa_destroy(&conn->sessions); kvfree(conn->request_buf); kfree(conn->preauth_info); if (atomic_dec_and_test(&conn->refcnt)) kfree(conn); }
623----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49927/bad/io_apic.c----mp_irqdomain_alloc,"int mp_irqdomain_alloc(struct irq_domain *domain, unsigned int virq, unsigned int nr_irqs, void *arg) { struct irq_alloc_info *info = arg; struct mp_chip_data *data; struct irq_data *irq_data; int ret, ioapic, pin; unsigned long flags; if (!info || nr_irqs > 1) return -EINVAL; irq_data = irq_domain_get_irq_data(domain, virq); if (!irq_data) return -EINVAL; ioapic = mp_irqdomain_ioapic_idx(domain); pin = info->ioapic.pin; if (irq_find_mapping(domain, (irq_hw_number_t)pin) > 0) return -EEXIST; data = kzalloc(sizeof(*data), GFP_KERNEL); if (!data) return -ENOMEM; ret = irq_domain_alloc_irqs_parent(domain, virq, nr_irqs, info); <S2SV_StartVul> if (ret < 0) { <S2SV_EndVul> <S2SV_StartVul> kfree(data); <S2SV_EndVul> <S2SV_StartVul> return ret; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> INIT_LIST_HEAD(&data->irq_2_pin); irq_data->hwirq = info->ioapic.pin; irq_data->chip = (domain->parent == x86_vector_domain) ? &ioapic_chip : &ioapic_ir_chip; irq_data->chip_data = data; mp_irqdomain_get_attr(mp_pin_to_gsi(ioapic, pin), data, info); <S2SV_StartVul> add_pin_to_irq_node(data, ioapic_alloc_attr_node(info), ioapic, pin); <S2SV_EndVul> mp_preconfigure_entry(data); mp_register_handler(virq, data->is_level); local_irq_save(flags); if (virq < nr_legacy_irqs()) legacy_pic->mask(virq); local_irq_restore(flags); apic_printk(APIC_VERBOSE, KERN_DEBUG ""IOAPIC[%d]: Preconfigured routing entry (%d-%d -> IRQ %d Level:%i ActiveLow:%i)\n"", ioapic, mpc_ioapic_id(ioapic), pin, virq, data->is_level, data->active_low); <S2SV_StartVul> return 0; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul>","- if (ret < 0) {
- kfree(data);
- return ret;
- }
- add_pin_to_irq_node(data, ioapic_alloc_attr_node(info), ioapic, pin);
- return 0;
- }
+ if (ret < 0)
+ goto free_data;
+ if (!add_pin_to_irq_node(data, ioapic_alloc_attr_node(info), ioapic, pin)) {
+ ret = -ENOMEM;
+ goto free_irqs;
+ }
+ free_irqs:
+ irq_domain_free_irqs_parent(domain, virq, nr_irqs);
+ free_data:
+ kfree(data);
+ return ret;
+ }","int mp_irqdomain_alloc(struct irq_domain *domain, unsigned int virq, unsigned int nr_irqs, void *arg) { struct irq_alloc_info *info = arg; struct mp_chip_data *data; struct irq_data *irq_data; int ret, ioapic, pin; unsigned long flags; if (!info || nr_irqs > 1) return -EINVAL; irq_data = irq_domain_get_irq_data(domain, virq); if (!irq_data) return -EINVAL; ioapic = mp_irqdomain_ioapic_idx(domain); pin = info->ioapic.pin; if (irq_find_mapping(domain, (irq_hw_number_t)pin) > 0) return -EEXIST; data = kzalloc(sizeof(*data), GFP_KERNEL); if (!data) return -ENOMEM; ret = irq_domain_alloc_irqs_parent(domain, virq, nr_irqs, info); if (ret < 0) goto free_data; INIT_LIST_HEAD(&data->irq_2_pin); irq_data->hwirq = info->ioapic.pin; irq_data->chip = (domain->parent == x86_vector_domain) ? &ioapic_chip : &ioapic_ir_chip; irq_data->chip_data = data; mp_irqdomain_get_attr(mp_pin_to_gsi(ioapic, pin), data, info); if (!add_pin_to_irq_node(data, ioapic_alloc_attr_node(info), ioapic, pin)) { ret = -ENOMEM; goto free_irqs; } mp_preconfigure_entry(data); mp_register_handler(virq, data->is_level); local_irq_save(flags); if (virq < nr_legacy_irqs()) legacy_pic->mask(virq); local_irq_restore(flags); apic_printk(APIC_VERBOSE, KERN_DEBUG ""IOAPIC[%d]: Preconfigured routing entry (%d-%d -> IRQ %d Level:%i ActiveLow:%i)\n"", ioapic, mpc_ioapic_id(ioapic), pin, virq, data->is_level, data->active_low); return 0; free_irqs: irq_domain_free_irqs_parent(domain, virq, nr_irqs); free_data: kfree(data); return ret; }"
334----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46793/bad/bytcr_wm5102.c----snd_byt_wm5102_mc_probe,"static int snd_byt_wm5102_mc_probe(struct platform_device *pdev) { static const char * const out_map_name[] = { ""spk"", ""hpout2"" }; static const char * const intmic_map_name[] = { ""in3l"", ""in1l"" }; static const char * const hsmic_map_name[] = { ""in1l"", ""in2l"" }; char codec_name[SND_ACPI_I2C_ID_LEN]; struct device *dev = &pdev->dev; struct byt_wm5102_private *priv; struct snd_soc_acpi_mach *mach; const char *platform_name; struct acpi_device *adev; struct device *codec_dev; int dai_index = 0; bool sof_parent; int i, ret; priv = devm_kzalloc(dev, sizeof(*priv), GFP_KERNEL); if (!priv) return -ENOMEM; priv->mclk = devm_clk_get(dev, ""pmc_plt_clk_3""); if (IS_ERR(priv->mclk)) return dev_err_probe(dev, PTR_ERR(priv->mclk), ""getting pmc_plt_clk_3\n""); mach = dev->platform_data; adev = acpi_dev_get_first_match_dev(mach->id, NULL, -1); if (adev) { snprintf(codec_name, sizeof(codec_name), ""spi-%s"", acpi_dev_name(adev)); acpi_dev_put(adev); } else { strscpy(codec_name, ""spi1.0"", sizeof(codec_name)); } codec_dev = bus_find_device_by_name(&spi_bus_type, NULL, codec_name); if (!codec_dev) return -EPROBE_DEFER; priv->spkvdd_en_gpio = gpiod_get(codec_dev, ""wlf,spkvdd-ena"", GPIOD_OUT_LOW); put_device(codec_dev); if (IS_ERR(priv->spkvdd_en_gpio)) { ret = PTR_ERR(priv->spkvdd_en_gpio); if (ret == -ENOENT) ret = -EPROBE_DEFER; return dev_err_probe(dev, ret, ""getting spkvdd-GPIO\n""); } if (soc_intel_is_cht()) { quirk = BYT_WM5102_SSP2 | BYT_WM5102_MCLK_19_2MHZ | BYT_WM5102_INTMIC_IN1L_HSMIC_IN2L | BYT_WM5102_SPK_HPOUT2_MAP; } if (quirk_override != -1) { dev_info_once(dev, ""Overriding quirk 0x%lx => 0x%x\n"", quirk, quirk_override); quirk = quirk_override; } log_quirks(dev); snprintf(byt_wm5102_components, sizeof(byt_wm5102_components), ""cfg-spk:%s cfg-intmic:%s cfg-hsmic:%s"", out_map_name[FIELD_GET(BYT_WM5102_OUT_MAP, quirk)], intmic_map_name[FIELD_GET(BYT_WM5102_IN_MAP, quirk)], hsmic_map_name[FIELD_GET(BYT_WM5102_IN_MAP, quirk)]); byt_wm5102_card.components = byt_wm5102_components; for (i = 0; i < ARRAY_SIZE(byt_wm5102_dais); i++) { <S2SV_StartVul> if (byt_wm5102_dais[i].codecs->name && <S2SV_EndVul> !strcmp(byt_wm5102_dais[i].codecs->name, ""wm5102-codec"")) { dai_index = i; break; } } byt_wm5102_card.dev = dev; platform_name = mach->mach_params.platform; ret = snd_soc_fixup_dai_links_platform_name(&byt_wm5102_card, platform_name); if (ret) goto out_put_gpio; if (quirk & BYT_WM5102_SSP2) byt_wm5102_dais[dai_index].cpus->dai_name = ""ssp2-port""; sof_parent = snd_soc_acpi_sof_parent(dev); if (sof_parent) { byt_wm5102_card.name = SOF_CARD_NAME; byt_wm5102_card.driver_name = SOF_DRIVER_NAME; dev->driver->pm = &snd_soc_pm_ops; } else { byt_wm5102_card.name = CARD_NAME; byt_wm5102_card.driver_name = DRIVER_NAME; } snd_soc_card_set_drvdata(&byt_wm5102_card, priv); ret = devm_snd_soc_register_card(dev, &byt_wm5102_card); if (ret) { dev_err_probe(dev, ret, ""registering card\n""); goto out_put_gpio; } platform_set_drvdata(pdev, &byt_wm5102_card); return 0; out_put_gpio: gpiod_put(priv->spkvdd_en_gpio); return ret; }","- if (byt_wm5102_dais[i].codecs->name &&
+ if (byt_wm5102_dais[i].num_codecs &&","static int snd_byt_wm5102_mc_probe(struct platform_device *pdev) { static const char * const out_map_name[] = { ""spk"", ""hpout2"" }; static const char * const intmic_map_name[] = { ""in3l"", ""in1l"" }; static const char * const hsmic_map_name[] = { ""in1l"", ""in2l"" }; char codec_name[SND_ACPI_I2C_ID_LEN]; struct device *dev = &pdev->dev; struct byt_wm5102_private *priv; struct snd_soc_acpi_mach *mach; const char *platform_name; struct acpi_device *adev; struct device *codec_dev; int dai_index = 0; bool sof_parent; int i, ret; priv = devm_kzalloc(dev, sizeof(*priv), GFP_KERNEL); if (!priv) return -ENOMEM; priv->mclk = devm_clk_get(dev, ""pmc_plt_clk_3""); if (IS_ERR(priv->mclk)) return dev_err_probe(dev, PTR_ERR(priv->mclk), ""getting pmc_plt_clk_3\n""); mach = dev->platform_data; adev = acpi_dev_get_first_match_dev(mach->id, NULL, -1); if (adev) { snprintf(codec_name, sizeof(codec_name), ""spi-%s"", acpi_dev_name(adev)); acpi_dev_put(adev); } else { strscpy(codec_name, ""spi1.0"", sizeof(codec_name)); } codec_dev = bus_find_device_by_name(&spi_bus_type, NULL, codec_name); if (!codec_dev) return -EPROBE_DEFER; priv->spkvdd_en_gpio = gpiod_get(codec_dev, ""wlf,spkvdd-ena"", GPIOD_OUT_LOW); put_device(codec_dev); if (IS_ERR(priv->spkvdd_en_gpio)) { ret = PTR_ERR(priv->spkvdd_en_gpio); if (ret == -ENOENT) ret = -EPROBE_DEFER; return dev_err_probe(dev, ret, ""getting spkvdd-GPIO\n""); } if (soc_intel_is_cht()) { quirk = BYT_WM5102_SSP2 | BYT_WM5102_MCLK_19_2MHZ | BYT_WM5102_INTMIC_IN1L_HSMIC_IN2L | BYT_WM5102_SPK_HPOUT2_MAP; } if (quirk_override != -1) { dev_info_once(dev, ""Overriding quirk 0x%lx => 0x%x\n"", quirk, quirk_override); quirk = quirk_override; } log_quirks(dev); snprintf(byt_wm5102_components, sizeof(byt_wm5102_components), ""cfg-spk:%s cfg-intmic:%s cfg-hsmic:%s"", out_map_name[FIELD_GET(BYT_WM5102_OUT_MAP, quirk)], intmic_map_name[FIELD_GET(BYT_WM5102_IN_MAP, quirk)], hsmic_map_name[FIELD_GET(BYT_WM5102_IN_MAP, quirk)]); byt_wm5102_card.components = byt_wm5102_components; for (i = 0; i < ARRAY_SIZE(byt_wm5102_dais); i++) { if (byt_wm5102_dais[i].num_codecs && !strcmp(byt_wm5102_dais[i].codecs->name, ""wm5102-codec"")) { dai_index = i; break; } } byt_wm5102_card.dev = dev; platform_name = mach->mach_params.platform; ret = snd_soc_fixup_dai_links_platform_name(&byt_wm5102_card, platform_name); if (ret) goto out_put_gpio; if (quirk & BYT_WM5102_SSP2) byt_wm5102_dais[dai_index].cpus->dai_name = ""ssp2-port""; sof_parent = snd_soc_acpi_sof_parent(dev); if (sof_parent) { byt_wm5102_card.name = SOF_CARD_NAME; byt_wm5102_card.driver_name = SOF_DRIVER_NAME; dev->driver->pm = &snd_soc_pm_ops; } else { byt_wm5102_card.name = CARD_NAME; byt_wm5102_card.driver_name = DRIVER_NAME; } snd_soc_card_set_drvdata(&byt_wm5102_card, priv); ret = devm_snd_soc_register_card(dev, &byt_wm5102_card); if (ret) { dev_err_probe(dev, ret, ""registering card\n""); goto out_put_gpio; } platform_set_drvdata(pdev, &byt_wm5102_card); return 0; out_put_gpio: gpiod_put(priv->spkvdd_en_gpio); return ret; }"
1469----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-57881/bad/page_alloc.c----split_large_buddy,"static void split_large_buddy(struct zone *zone, struct page *page, unsigned long pfn, int order, fpi_t fpi) { unsigned long end = pfn + (1 << order); VM_WARN_ON_ONCE(!IS_ALIGNED(pfn, 1 << order)); VM_WARN_ON_ONCE(PageBuddy(page)); if (order > pageblock_order) order = pageblock_order; <S2SV_StartVul> while (pfn != end) { <S2SV_EndVul> int mt = get_pfnblock_migratetype(page, pfn); __free_one_page(page, pfn, zone, order, mt, fpi); pfn += 1 << order; page = pfn_to_page(pfn); <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul>","- while (pfn != end) {
- }
- }
+ do {
+ if (pfn == end)
+ break;
+ } while (1);","static void split_large_buddy(struct zone *zone, struct page *page, unsigned long pfn, int order, fpi_t fpi) { unsigned long end = pfn + (1 << order); VM_WARN_ON_ONCE(!IS_ALIGNED(pfn, 1 << order)); VM_WARN_ON_ONCE(PageBuddy(page)); if (order > pageblock_order) order = pageblock_order; do { int mt = get_pfnblock_migratetype(page, pfn); __free_one_page(page, pfn, zone, order, mt, fpi); pfn += 1 << order; if (pfn == end) break; page = pfn_to_page(pfn); } while (1); }"
150----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-45010/bad/pm_netlink.c----mptcp_nl_remove_subflow_and_signal_addr,"static int mptcp_nl_remove_subflow_and_signal_addr(struct net *net, const struct mptcp_pm_addr_entry *entry) { const struct mptcp_addr_info *addr = &entry->addr; struct mptcp_rm_list list = { .nr = 0 }; long s_slot = 0, s_num = 0; struct mptcp_sock *msk; pr_debug(""remove_id=%d"", addr->id); list.ids[list.nr++] = addr->id; while ((msk = mptcp_token_iter_next(net, &s_slot, &s_num)) != NULL) { struct sock *sk = (struct sock *)msk; bool remove_subflow; if (mptcp_pm_is_userspace(msk)) goto next; if (list_empty(&msk->conn_list)) { mptcp_pm_remove_anno_addr(msk, addr, false); goto next; } lock_sock(sk); remove_subflow = lookup_subflow_by_saddr(&msk->conn_list, addr); mptcp_pm_remove_anno_addr(msk, addr, remove_subflow && !(entry->flags & MPTCP_PM_ADDR_FLAG_IMPLICIT)); if (remove_subflow) { spin_lock_bh(&msk->pm.lock); mptcp_pm_nl_rm_subflow_received(msk, &list); spin_unlock_bh(&msk->pm.lock); <S2SV_StartVul> } else if (entry->flags & MPTCP_PM_ADDR_FLAG_SUBFLOW) { <S2SV_EndVul> spin_lock_bh(&msk->pm.lock); <S2SV_StartVul> if (!__test_and_set_bit(entry->addr.id, msk->pm.id_avail_bitmap)) <S2SV_EndVul> <S2SV_StartVul> msk->pm.local_addr_used--; <S2SV_EndVul> spin_unlock_bh(&msk->pm.lock); } release_sock(sk); next: sock_put(sk); cond_resched(); } return 0; }","- } else if (entry->flags & MPTCP_PM_ADDR_FLAG_SUBFLOW) {
- if (!__test_and_set_bit(entry->addr.id, msk->pm.id_avail_bitmap))
- msk->pm.local_addr_used--;
+ }
+ if (entry->flags & MPTCP_PM_ADDR_FLAG_SUBFLOW) {
+ __mark_subflow_endp_available(msk, list.ids[0]);
+ }","static int mptcp_nl_remove_subflow_and_signal_addr(struct net *net, const struct mptcp_pm_addr_entry *entry) { const struct mptcp_addr_info *addr = &entry->addr; struct mptcp_rm_list list = { .nr = 0 }; long s_slot = 0, s_num = 0; struct mptcp_sock *msk; pr_debug(""remove_id=%d"", addr->id); list.ids[list.nr++] = addr->id; while ((msk = mptcp_token_iter_next(net, &s_slot, &s_num)) != NULL) { struct sock *sk = (struct sock *)msk; bool remove_subflow; if (mptcp_pm_is_userspace(msk)) goto next; if (list_empty(&msk->conn_list)) { mptcp_pm_remove_anno_addr(msk, addr, false); goto next; } lock_sock(sk); remove_subflow = lookup_subflow_by_saddr(&msk->conn_list, addr); mptcp_pm_remove_anno_addr(msk, addr, remove_subflow && !(entry->flags & MPTCP_PM_ADDR_FLAG_IMPLICIT)); if (remove_subflow) { spin_lock_bh(&msk->pm.lock); mptcp_pm_nl_rm_subflow_received(msk, &list); spin_unlock_bh(&msk->pm.lock); } if (entry->flags & MPTCP_PM_ADDR_FLAG_SUBFLOW) { spin_lock_bh(&msk->pm.lock); __mark_subflow_endp_available(msk, list.ids[0]); spin_unlock_bh(&msk->pm.lock); } release_sock(sk); next: sock_put(sk); cond_resched(); } return 0; }"
244----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46715/bad/inkern.c----iio_channel_read_avail,"static int iio_channel_read_avail(struct iio_channel *chan, const int **vals, int *type, int *length, enum iio_chan_info_enum info) { if (!iio_channel_has_available(chan->channel, info)) return -EINVAL; <S2SV_StartVul> return chan->indio_dev->info->read_avail(chan->indio_dev, chan->channel, <S2SV_EndVul> <S2SV_StartVul> vals, type, length, info); <S2SV_EndVul> }","- return chan->indio_dev->info->read_avail(chan->indio_dev, chan->channel,
- vals, type, length, info);
+ const struct iio_info *iio_info = chan->indio_dev->info;
+ return -EINVAL;
+ if (iio_info->read_avail)
+ return iio_info->read_avail(chan->indio_dev, chan->channel,
+ vals, type, length, info);
+ return -EINVAL;","static int iio_channel_read_avail(struct iio_channel *chan, const int **vals, int *type, int *length, enum iio_chan_info_enum info) { const struct iio_info *iio_info = chan->indio_dev->info; if (!iio_channel_has_available(chan->channel, info)) return -EINVAL; if (iio_info->read_avail) return iio_info->read_avail(chan->indio_dev, chan->channel, vals, type, length, info); return -EINVAL; }"
1202----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53207/bad/mgmt.c----set_default_phy_complete,"static void set_default_phy_complete(struct hci_dev *hdev, void *data, int err) { struct mgmt_pending_cmd *cmd = data; struct sk_buff *skb = cmd->skb; u8 status = mgmt_status(err); <S2SV_StartVul> if (cmd != pending_find(MGMT_OP_SET_PHY_CONFIGURATION, hdev)) <S2SV_EndVul> return; if (!status) { if (!skb) status = MGMT_STATUS_FAILED; else if (IS_ERR(skb)) status = mgmt_status(PTR_ERR(skb)); else status = mgmt_status(skb->data[0]); } bt_dev_dbg(hdev, ""status %d"", status); if (status) { mgmt_cmd_status(cmd->sk, hdev->id, MGMT_OP_SET_PHY_CONFIGURATION, status); } else { mgmt_cmd_complete(cmd->sk, hdev->id, MGMT_OP_SET_PHY_CONFIGURATION, 0, NULL, 0); mgmt_phy_configuration_changed(hdev, cmd->sk); } if (skb && !IS_ERR(skb)) kfree_skb(skb); mgmt_pending_remove(cmd); }","- if (cmd != pending_find(MGMT_OP_SET_PHY_CONFIGURATION, hdev))
+ if (err == -ECANCELED ||
+ cmd != pending_find(MGMT_OP_SET_PHY_CONFIGURATION, hdev))
+ return;","static void set_default_phy_complete(struct hci_dev *hdev, void *data, int err) { struct mgmt_pending_cmd *cmd = data; struct sk_buff *skb = cmd->skb; u8 status = mgmt_status(err); if (err == -ECANCELED || cmd != pending_find(MGMT_OP_SET_PHY_CONFIGURATION, hdev)) return; if (!status) { if (!skb) status = MGMT_STATUS_FAILED; else if (IS_ERR(skb)) status = mgmt_status(PTR_ERR(skb)); else status = mgmt_status(skb->data[0]); } bt_dev_dbg(hdev, ""status %d"", status); if (status) { mgmt_cmd_status(cmd->sk, hdev->id, MGMT_OP_SET_PHY_CONFIGURATION, status); } else { mgmt_cmd_complete(cmd->sk, hdev->id, MGMT_OP_SET_PHY_CONFIGURATION, 0, NULL, 0); mgmt_phy_configuration_changed(hdev, cmd->sk); } if (skb && !IS_ERR(skb)) kfree_skb(skb); mgmt_pending_remove(cmd); }"
558----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49876/bad/xe_device.c----*xe_device_create,"struct xe_device *xe_device_create(struct pci_dev *pdev, const struct pci_device_id *ent) { struct xe_device *xe; int err; xe_display_driver_set_hooks(&driver); err = drm_aperture_remove_conflicting_pci_framebuffers(pdev, &driver); if (err) return ERR_PTR(err); xe = devm_drm_dev_alloc(&pdev->dev, &driver, struct xe_device, drm); if (IS_ERR(xe)) return xe; err = ttm_device_init(&xe->ttm, &xe_ttm_funcs, xe->drm.dev, xe->drm.anon_inode->i_mapping, xe->drm.vma_offset_manager, false, false); if (WARN_ON(err)) goto err; err = drmm_add_action_or_reset(&xe->drm, xe_device_destroy, NULL); if (err) goto err; xe->info.devid = pdev->device; xe->info.revid = pdev->revision; xe->info.force_execlist = xe_modparam.force_execlist; spin_lock_init(&xe->irq.lock); spin_lock_init(&xe->clients.lock); init_waitqueue_head(&xe->ufence_wq); err = drmm_mutex_init(&xe->drm, &xe->usm.lock); if (err) goto err; xa_init_flags(&xe->usm.asid_to_vm, XA_FLAGS_ALLOC); if (IS_ENABLED(CONFIG_DRM_XE_DEBUG)) { u32 asid; BUILD_BUG_ON(XE_MAX_ASID < 2); err = xa_alloc_cyclic(&xe->usm.asid_to_vm, &asid, NULL, XA_LIMIT(XE_MAX_ASID - 2, XE_MAX_ASID - 1), &xe->usm.next_asid, GFP_KERNEL); drm_WARN_ON(&xe->drm, err); if (err >= 0) xa_erase(&xe->usm.asid_to_vm, asid); } spin_lock_init(&xe->pinned.lock); INIT_LIST_HEAD(&xe->pinned.kernel_bo_present); INIT_LIST_HEAD(&xe->pinned.external_vram); INIT_LIST_HEAD(&xe->pinned.evicted); xe->preempt_fence_wq = alloc_ordered_workqueue(""xe-preempt-fence-wq"", 0); xe->ordered_wq = alloc_ordered_workqueue(""xe-ordered-wq"", 0); xe->unordered_wq = alloc_workqueue(""xe-unordered-wq"", 0, 0); if (!xe->ordered_wq || !xe->unordered_wq || <S2SV_StartVul> !xe->preempt_fence_wq) { <S2SV_EndVul> drm_err(&xe->drm, ""Failed to allocate xe workqueues\n""); err = -ENOMEM; goto err; } err = xe_display_create(xe); if (WARN_ON(err)) goto err; return xe; err: return ERR_PTR(err); }","- !xe->preempt_fence_wq) {
+ xe->destroy_wq = alloc_workqueue(""xe-destroy-wq"", 0, 0);
+ !xe->preempt_fence_wq || !xe->destroy_wq) {","struct xe_device *xe_device_create(struct pci_dev *pdev, const struct pci_device_id *ent) { struct xe_device *xe; int err; xe_display_driver_set_hooks(&driver); err = drm_aperture_remove_conflicting_pci_framebuffers(pdev, &driver); if (err) return ERR_PTR(err); xe = devm_drm_dev_alloc(&pdev->dev, &driver, struct xe_device, drm); if (IS_ERR(xe)) return xe; err = ttm_device_init(&xe->ttm, &xe_ttm_funcs, xe->drm.dev, xe->drm.anon_inode->i_mapping, xe->drm.vma_offset_manager, false, false); if (WARN_ON(err)) goto err; err = drmm_add_action_or_reset(&xe->drm, xe_device_destroy, NULL); if (err) goto err; xe->info.devid = pdev->device; xe->info.revid = pdev->revision; xe->info.force_execlist = xe_modparam.force_execlist; spin_lock_init(&xe->irq.lock); spin_lock_init(&xe->clients.lock); init_waitqueue_head(&xe->ufence_wq); err = drmm_mutex_init(&xe->drm, &xe->usm.lock); if (err) goto err; xa_init_flags(&xe->usm.asid_to_vm, XA_FLAGS_ALLOC); if (IS_ENABLED(CONFIG_DRM_XE_DEBUG)) { u32 asid; BUILD_BUG_ON(XE_MAX_ASID < 2); err = xa_alloc_cyclic(&xe->usm.asid_to_vm, &asid, NULL, XA_LIMIT(XE_MAX_ASID - 2, XE_MAX_ASID - 1), &xe->usm.next_asid, GFP_KERNEL); drm_WARN_ON(&xe->drm, err); if (err >= 0) xa_erase(&xe->usm.asid_to_vm, asid); } spin_lock_init(&xe->pinned.lock); INIT_LIST_HEAD(&xe->pinned.kernel_bo_present); INIT_LIST_HEAD(&xe->pinned.external_vram); INIT_LIST_HEAD(&xe->pinned.evicted); xe->preempt_fence_wq = alloc_ordered_workqueue(""xe-preempt-fence-wq"", 0); xe->ordered_wq = alloc_ordered_workqueue(""xe-ordered-wq"", 0); xe->unordered_wq = alloc_workqueue(""xe-unordered-wq"", 0, 0); xe->destroy_wq = alloc_workqueue(""xe-destroy-wq"", 0, 0); if (!xe->ordered_wq || !xe->unordered_wq || !xe->preempt_fence_wq || !xe->destroy_wq) { drm_err(&xe->drm, ""Failed to allocate xe workqueues\n""); err = -ENOMEM; goto err; } err = xe_display_create(xe); if (WARN_ON(err)) goto err; return xe; err: return ERR_PTR(err); }"
186----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46677/bad/gtp.c----*gtp_encap_enable_socket,"static struct sock *gtp_encap_enable_socket(int fd, int type, struct gtp_dev *gtp) { struct udp_tunnel_sock_cfg tuncfg = {NULL}; struct socket *sock; struct sock *sk; int err; pr_debug(""enable gtp on %d, %d\n"", fd, type); sock = sockfd_lookup(fd, &err); if (!sock) { pr_debug(""gtp socket fd=%d not found\n"", fd); <S2SV_StartVul> return NULL; <S2SV_EndVul> } sk = sock->sk; if (sk->sk_protocol != IPPROTO_UDP || sk->sk_type != SOCK_DGRAM || (sk->sk_family != AF_INET && sk->sk_family != AF_INET6)) { pr_debug(""socket fd=%d not UDP\n"", fd); sk = ERR_PTR(-EINVAL); goto out_sock; } lock_sock(sk); if (sk->sk_user_data) { sk = ERR_PTR(-EBUSY); goto out_rel_sock; } sock_hold(sk); tuncfg.sk_user_data = gtp; tuncfg.encap_type = type; tuncfg.encap_rcv = gtp_encap_recv; tuncfg.encap_destroy = gtp_encap_destroy; setup_udp_tunnel_sock(sock_net(sock->sk), sock, &tuncfg); out_rel_sock: release_sock(sock->sk); out_sock: sockfd_put(sock); return sk; }","- return NULL;
+ return ERR_PTR(err);","static struct sock *gtp_encap_enable_socket(int fd, int type, struct gtp_dev *gtp) { struct udp_tunnel_sock_cfg tuncfg = {NULL}; struct socket *sock; struct sock *sk; int err; pr_debug(""enable gtp on %d, %d\n"", fd, type); sock = sockfd_lookup(fd, &err); if (!sock) { pr_debug(""gtp socket fd=%d not found\n"", fd); return ERR_PTR(err); } sk = sock->sk; if (sk->sk_protocol != IPPROTO_UDP || sk->sk_type != SOCK_DGRAM || (sk->sk_family != AF_INET && sk->sk_family != AF_INET6)) { pr_debug(""socket fd=%d not UDP\n"", fd); sk = ERR_PTR(-EINVAL); goto out_sock; } lock_sock(sk); if (sk->sk_user_data) { sk = ERR_PTR(-EBUSY); goto out_rel_sock; } sock_hold(sk); tuncfg.sk_user_data = gtp; tuncfg.encap_type = type; tuncfg.encap_rcv = gtp_encap_recv; tuncfg.encap_destroy = gtp_encap_destroy; setup_udp_tunnel_sock(sock_net(sock->sk), sock, &tuncfg); out_rel_sock: release_sock(sock->sk); out_sock: sockfd_put(sock); return sk; }"
259----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46737/bad/tcp.c----nvmet_tcp_install_queue,"static u16 nvmet_tcp_install_queue(struct nvmet_sq *sq) { struct nvmet_tcp_queue *queue = container_of(sq, struct nvmet_tcp_queue, nvme_sq); if (sq->qid == 0) { flush_scheduled_work(); } queue->nr_cmds = sq->size * 2; <S2SV_StartVul> if (nvmet_tcp_alloc_cmds(queue)) <S2SV_EndVul> return NVME_SC_INTERNAL; return 0; }","- if (nvmet_tcp_alloc_cmds(queue))
+ if (nvmet_tcp_alloc_cmds(queue)) {
+ queue->nr_cmds = 0;
+ }
+ }","static u16 nvmet_tcp_install_queue(struct nvmet_sq *sq) { struct nvmet_tcp_queue *queue = container_of(sq, struct nvmet_tcp_queue, nvme_sq); if (sq->qid == 0) { flush_scheduled_work(); } queue->nr_cmds = sq->size * 2; if (nvmet_tcp_alloc_cmds(queue)) { queue->nr_cmds = 0; return NVME_SC_INTERNAL; } return 0; }"
1086----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53068/bad/bus.c----__scmi_device_create,"__scmi_device_create(struct device_node *np, struct device *parent, int protocol, const char *name) { int id, retval; struct scmi_device *scmi_dev; scmi_dev = scmi_child_dev_find(parent, protocol, name); if (scmi_dev) return scmi_dev; if (protocol == SCMI_PROTOCOL_SYSTEM && atomic_cmpxchg(&scmi_syspower_registered, 0, 1)) { dev_warn(parent, ""SCMI SystemPower protocol device must be unique !\n""); return NULL; } scmi_dev = kzalloc(sizeof(*scmi_dev), GFP_KERNEL); if (!scmi_dev) return NULL; scmi_dev->name = kstrdup_const(name ?: ""unknown"", GFP_KERNEL); if (!scmi_dev->name) { kfree(scmi_dev); return NULL; } id = ida_alloc_min(&scmi_bus_id, 1, GFP_KERNEL); if (id < 0) { kfree_const(scmi_dev->name); kfree(scmi_dev); return NULL; } scmi_dev->id = id; scmi_dev->protocol_id = protocol; scmi_dev->dev.parent = parent; device_set_node(&scmi_dev->dev, of_fwnode_handle(np)); scmi_dev->dev.bus = &scmi_bus_type; scmi_dev->dev.release = scmi_device_release; dev_set_name(&scmi_dev->dev, ""scmi_dev.%d"", id); retval = device_register(&scmi_dev->dev); if (retval) goto put_dev; pr_debug(""(%s) Created SCMI device '%s' for protocol 0x%x (%s)\n"", of_node_full_name(parent->of_node), dev_name(&scmi_dev->dev), protocol, name); return scmi_dev; put_dev: <S2SV_StartVul> kfree_const(scmi_dev->name); <S2SV_EndVul> put_device(&scmi_dev->dev); ida_free(&scmi_bus_id, id); return NULL; }",- kfree_const(scmi_dev->name);,"__scmi_device_create(struct device_node *np, struct device *parent, int protocol, const char *name) { int id, retval; struct scmi_device *scmi_dev; scmi_dev = scmi_child_dev_find(parent, protocol, name); if (scmi_dev) return scmi_dev; if (protocol == SCMI_PROTOCOL_SYSTEM && atomic_cmpxchg(&scmi_syspower_registered, 0, 1)) { dev_warn(parent, ""SCMI SystemPower protocol device must be unique !\n""); return NULL; } scmi_dev = kzalloc(sizeof(*scmi_dev), GFP_KERNEL); if (!scmi_dev) return NULL; scmi_dev->name = kstrdup_const(name ?: ""unknown"", GFP_KERNEL); if (!scmi_dev->name) { kfree(scmi_dev); return NULL; } id = ida_alloc_min(&scmi_bus_id, 1, GFP_KERNEL); if (id < 0) { kfree_const(scmi_dev->name); kfree(scmi_dev); return NULL; } scmi_dev->id = id; scmi_dev->protocol_id = protocol; scmi_dev->dev.parent = parent; device_set_node(&scmi_dev->dev, of_fwnode_handle(np)); scmi_dev->dev.bus = &scmi_bus_type; scmi_dev->dev.release = scmi_device_release; dev_set_name(&scmi_dev->dev, ""scmi_dev.%d"", id); retval = device_register(&scmi_dev->dev); if (retval) goto put_dev; pr_debug(""(%s) Created SCMI device '%s' for protocol 0x%x (%s)\n"", of_node_full_name(parent->of_node), dev_name(&scmi_dev->dev), protocol, name); return scmi_dev; put_dev: put_device(&scmi_dev->dev); ida_free(&scmi_bus_id, id); return NULL; }"
349----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46810/bad/tc358767.c----tc_irq_handler,"static irqreturn_t tc_irq_handler(int irq, void *arg) { struct tc_data *tc = arg; u32 val; int r; r = regmap_read(tc->regmap, INTSTS_G, &val); if (r) return IRQ_NONE; if (!val) return IRQ_NONE; if (val & INT_SYSERR) { u32 stat = 0; regmap_read(tc->regmap, SYSSTAT, &stat); dev_err(tc->dev, ""syserr %x\n"", stat); } <S2SV_StartVul> if (tc->hpd_pin >= 0 && tc->bridge.dev) { <S2SV_EndVul> bool h = val & INT_GPIO_H(tc->hpd_pin); bool lc = val & INT_GPIO_LC(tc->hpd_pin); dev_dbg(tc->dev, ""GPIO%d: %s %s\n"", tc->hpd_pin, h ? ""H"" : """", lc ? ""LC"" : """"); if (h || lc) drm_kms_helper_hotplug_event(tc->bridge.dev); } regmap_write(tc->regmap, INTSTS_G, val); return IRQ_HANDLED; }","- if (tc->hpd_pin >= 0 && tc->bridge.dev) {
+ if (tc->hpd_pin >= 0 && tc->bridge.dev && tc->aux.drm_dev) {","static irqreturn_t tc_irq_handler(int irq, void *arg) { struct tc_data *tc = arg; u32 val; int r; r = regmap_read(tc->regmap, INTSTS_G, &val); if (r) return IRQ_NONE; if (!val) return IRQ_NONE; if (val & INT_SYSERR) { u32 stat = 0; regmap_read(tc->regmap, SYSSTAT, &stat); dev_err(tc->dev, ""syserr %x\n"", stat); } if (tc->hpd_pin >= 0 && tc->bridge.dev && tc->aux.drm_dev) { bool h = val & INT_GPIO_H(tc->hpd_pin); bool lc = val & INT_GPIO_LC(tc->hpd_pin); dev_dbg(tc->dev, ""GPIO%d: %s %s\n"", tc->hpd_pin, h ? ""H"" : """", lc ? ""LC"" : """"); if (h || lc) drm_kms_helper_hotplug_event(tc->bridge.dev); } regmap_write(tc->regmap, INTSTS_G, val); return IRQ_HANDLED; }"
347----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46807/bad/amdgpu_device.c----amdgpu_device_recover_vram,"static int amdgpu_device_recover_vram(struct amdgpu_device *adev) { struct dma_fence *fence = NULL, *next = NULL; struct amdgpu_bo *shadow; struct amdgpu_bo_vm *vmbo; long r = 1, tmo; if (amdgpu_sriov_runtime(adev)) tmo = msecs_to_jiffies(8000); else tmo = msecs_to_jiffies(100); dev_info(adev->dev, ""recover vram bo from shadow start\n""); mutex_lock(&adev->shadow_list_lock); list_for_each_entry(vmbo, &adev->shadow_list, shadow_list) { if (!vmbo->shadow) continue; shadow = vmbo->shadow; <S2SV_StartVul> if (shadow->tbo.resource->mem_type != TTM_PL_TT || <S2SV_EndVul> shadow->tbo.resource->start == AMDGPU_BO_INVALID_OFFSET || shadow->parent->tbo.resource->mem_type != TTM_PL_VRAM) continue; r = amdgpu_bo_restore_shadow(shadow, &next); if (r) break; if (fence) { tmo = dma_fence_wait_timeout(fence, false, tmo); dma_fence_put(fence); fence = next; if (tmo == 0) { r = -ETIMEDOUT; break; } else if (tmo < 0) { r = tmo; break; } } else { fence = next; } } mutex_unlock(&adev->shadow_list_lock); if (fence) tmo = dma_fence_wait_timeout(fence, false, tmo); dma_fence_put(fence); if (r < 0 || tmo <= 0) { dev_err(adev->dev, ""recover vram bo from shadow failed, r is %ld, tmo is %ld\n"", r, tmo); return -EIO; } dev_info(adev->dev, ""recover vram bo from shadow done\n""); return 0; }","- if (shadow->tbo.resource->mem_type != TTM_PL_TT ||
+ if (!shadow->tbo.resource ||
+ shadow->tbo.resource->mem_type != TTM_PL_TT ||","static int amdgpu_device_recover_vram(struct amdgpu_device *adev) { struct dma_fence *fence = NULL, *next = NULL; struct amdgpu_bo *shadow; struct amdgpu_bo_vm *vmbo; long r = 1, tmo; if (amdgpu_sriov_runtime(adev)) tmo = msecs_to_jiffies(8000); else tmo = msecs_to_jiffies(100); dev_info(adev->dev, ""recover vram bo from shadow start\n""); mutex_lock(&adev->shadow_list_lock); list_for_each_entry(vmbo, &adev->shadow_list, shadow_list) { if (!vmbo->shadow) continue; shadow = vmbo->shadow; if (!shadow->tbo.resource || shadow->tbo.resource->mem_type != TTM_PL_TT || shadow->tbo.resource->start == AMDGPU_BO_INVALID_OFFSET || shadow->parent->tbo.resource->mem_type != TTM_PL_VRAM) continue; r = amdgpu_bo_restore_shadow(shadow, &next); if (r) break; if (fence) { tmo = dma_fence_wait_timeout(fence, false, tmo); dma_fence_put(fence); fence = next; if (tmo == 0) { r = -ETIMEDOUT; break; } else if (tmo < 0) { r = tmo; break; } } else { fence = next; } } mutex_unlock(&adev->shadow_list_lock); if (fence) tmo = dma_fence_wait_timeout(fence, false, tmo); dma_fence_put(fence); if (r < 0 || tmo <= 0) { dev_err(adev->dev, ""recover vram bo from shadow failed, r is %ld, tmo is %ld\n"", r, tmo); return -EIO; } dev_info(adev->dev, ""recover vram bo from shadow done\n""); return 0; }"
1382----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56717/bad/ocelot.c----ocelot_ifh_set_basic,"void ocelot_ifh_set_basic(void *ifh, struct ocelot *ocelot, int port, u32 rew_op, struct sk_buff *skb) { struct ocelot_port *ocelot_port = ocelot->ports[port]; struct net_device *dev = skb->dev; u64 vlan_tci, tag_type; int qos_class; ocelot_xmit_get_vlan_info(skb, ocelot_port->bridge, &vlan_tci, &tag_type); qos_class = netdev_get_num_tc(dev) ? netdev_get_prio_tc_map(dev, skb->priority) : skb->priority; memset(ifh, 0, OCELOT_TAG_LEN); ocelot_ifh_set_bypass(ifh, 1); <S2SV_StartVul> ocelot_ifh_set_src(ifh, BIT_ULL(ocelot->num_phys_ports)); <S2SV_EndVul> ocelot_ifh_set_dest(ifh, BIT_ULL(port)); ocelot_ifh_set_qos_class(ifh, qos_class); ocelot_ifh_set_tag_type(ifh, tag_type); ocelot_ifh_set_vlan_tci(ifh, vlan_tci); if (rew_op) ocelot_ifh_set_rew_op(ifh, rew_op); }","- ocelot_ifh_set_src(ifh, BIT_ULL(ocelot->num_phys_ports));
+ ocelot_ifh_set_src(ifh, ocelot->num_phys_ports);","void ocelot_ifh_set_basic(void *ifh, struct ocelot *ocelot, int port, u32 rew_op, struct sk_buff *skb) { struct ocelot_port *ocelot_port = ocelot->ports[port]; struct net_device *dev = skb->dev; u64 vlan_tci, tag_type; int qos_class; ocelot_xmit_get_vlan_info(skb, ocelot_port->bridge, &vlan_tci, &tag_type); qos_class = netdev_get_num_tc(dev) ? netdev_get_prio_tc_map(dev, skb->priority) : skb->priority; memset(ifh, 0, OCELOT_TAG_LEN); ocelot_ifh_set_bypass(ifh, 1); ocelot_ifh_set_src(ifh, ocelot->num_phys_ports); ocelot_ifh_set_dest(ifh, BIT_ULL(port)); ocelot_ifh_set_qos_class(ifh, qos_class); ocelot_ifh_set_tag_type(ifh, tag_type); ocelot_ifh_set_vlan_tci(ifh, vlan_tci); if (rew_op) ocelot_ifh_set_rew_op(ifh, rew_op); }"
697----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49988/bad/oplock.c----smb_lazy_parent_lease_break_close,"void smb_lazy_parent_lease_break_close(struct ksmbd_file *fp) { struct oplock_info *opinfo; struct ksmbd_inode *p_ci = NULL; rcu_read_lock(); opinfo = rcu_dereference(fp->f_opinfo); rcu_read_unlock(); if (!opinfo || !opinfo->is_lease || opinfo->o_lease->version != 2) return; p_ci = ksmbd_inode_lookup_lock(fp->filp->f_path.dentry->d_parent); if (!p_ci) return; down_read(&p_ci->m_lock); list_for_each_entry(opinfo, &p_ci->m_op_list, op_entry) { if (opinfo->conn == NULL || !opinfo->is_lease) continue; if (opinfo->o_lease->state != SMB2_OPLOCK_LEVEL_NONE) { if (!atomic_inc_not_zero(&opinfo->refcount)) continue; <S2SV_StartVul> atomic_inc(&opinfo->conn->r_count); <S2SV_EndVul> <S2SV_StartVul> if (ksmbd_conn_releasing(opinfo->conn)) { <S2SV_EndVul> <S2SV_StartVul> atomic_dec(&opinfo->conn->r_count); <S2SV_EndVul> continue; <S2SV_StartVul> } <S2SV_EndVul> oplock_break(opinfo, SMB2_OPLOCK_LEVEL_NONE); <S2SV_StartVul> opinfo_conn_put(opinfo); <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> up_read(&p_ci->m_lock); ksmbd_inode_put(p_ci); }","- atomic_inc(&opinfo->conn->r_count);
- if (ksmbd_conn_releasing(opinfo->conn)) {
- atomic_dec(&opinfo->conn->r_count);
- }
- opinfo_conn_put(opinfo);
- }
- }
+ if (ksmbd_conn_releasing(opinfo->conn))
+ opinfo_put(opinfo);","void smb_lazy_parent_lease_break_close(struct ksmbd_file *fp) { struct oplock_info *opinfo; struct ksmbd_inode *p_ci = NULL; rcu_read_lock(); opinfo = rcu_dereference(fp->f_opinfo); rcu_read_unlock(); if (!opinfo || !opinfo->is_lease || opinfo->o_lease->version != 2) return; p_ci = ksmbd_inode_lookup_lock(fp->filp->f_path.dentry->d_parent); if (!p_ci) return; down_read(&p_ci->m_lock); list_for_each_entry(opinfo, &p_ci->m_op_list, op_entry) { if (opinfo->conn == NULL || !opinfo->is_lease) continue; if (opinfo->o_lease->state != SMB2_OPLOCK_LEVEL_NONE) { if (!atomic_inc_not_zero(&opinfo->refcount)) continue; if (ksmbd_conn_releasing(opinfo->conn)) continue; oplock_break(opinfo, SMB2_OPLOCK_LEVEL_NONE); opinfo_put(opinfo); } } up_read(&p_ci->m_lock); ksmbd_inode_put(p_ci); }"
878----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50132/bad/trace_kprobe.c----__trace_kprobe_create,"static int __trace_kprobe_create(int argc, const char *argv[]) { struct trace_kprobe *tk = NULL; int i, len, new_argc = 0, ret = 0; bool is_return = false; char *symbol = NULL, *tmp = NULL; const char **new_argv = NULL; const char *event = NULL, *group = KPROBE_EVENT_SYSTEM; enum probe_print_type ptype; int maxactive = 0; long offset = 0; void *addr = NULL; char buf[MAX_EVENT_NAME_LEN]; char gbuf[MAX_EVENT_NAME_LEN]; char abuf[MAX_BTF_ARGS_LEN]; char *dbuf = NULL; struct traceprobe_parse_context ctx = { .flags = TPARG_FL_KERNEL }; switch (argv[0][0]) { case 'r': is_return = true; break; case 'p': break; default: return -ECANCELED; } if (argc < 2) return -ECANCELED; trace_probe_log_init(""trace_kprobe"", argc, argv); event = strchr(&argv[0][1], ':'); if (event) event++; if (isdigit(argv[0][1])) { if (!is_return) { trace_probe_log_err(1, BAD_MAXACT_TYPE); goto parse_error; } if (event) len = event - &argv[0][1] - 1; else len = strlen(&argv[0][1]); if (len > MAX_EVENT_NAME_LEN - 1) { trace_probe_log_err(1, BAD_MAXACT); goto parse_error; } memcpy(buf, &argv[0][1], len); buf[len] = '\0'; ret = kstrtouint(buf, 0, &maxactive); if (ret || !maxactive) { trace_probe_log_err(1, BAD_MAXACT); goto parse_error; } if (maxactive > KRETPROBE_MAXACTIVE_MAX) { trace_probe_log_err(1, MAXACT_TOO_BIG); goto parse_error; } } if (kstrtoul(argv[1], 0, (unsigned long *)&addr)) { trace_probe_log_set_index(1); if (strchr(argv[1], '/') && strchr(argv[1], ':')) { ret = -ECANCELED; goto error; } symbol = kstrdup(argv[1], GFP_KERNEL); if (!symbol) return -ENOMEM; tmp = strchr(symbol, '%'); if (tmp) { if (!strcmp(tmp, ""%return"")) { *tmp = '\0'; is_return = true; } else { trace_probe_log_err(tmp - symbol, BAD_ADDR_SUFFIX); goto parse_error; } } ret = traceprobe_split_symbol_offset(symbol, &offset); if (ret || offset < 0 || offset > UINT_MAX) { trace_probe_log_err(0, BAD_PROBE_ADDR); goto parse_error; } ret = validate_probe_symbol(symbol); if (ret) { if (ret == -EADDRNOTAVAIL) trace_probe_log_err(0, NON_UNIQ_SYMBOL); else trace_probe_log_err(0, BAD_PROBE_ADDR); goto parse_error; } if (is_return) ctx.flags |= TPARG_FL_RETURN; ret = kprobe_on_func_entry(NULL, symbol, offset); if (ret == 0 && !is_return) ctx.flags |= TPARG_FL_FENTRY; if (ret == -EINVAL && is_return) { trace_probe_log_err(0, BAD_RETPROBE); goto parse_error; } } trace_probe_log_set_index(0); if (event) { ret = traceprobe_parse_event_name(&event, &group, gbuf, event - argv[0]); if (ret) goto parse_error; } if (!event) { if (symbol) snprintf(buf, MAX_EVENT_NAME_LEN, ""%c_%s_%ld"", is_return ? 'r' : 'p', symbol, offset); else snprintf(buf, MAX_EVENT_NAME_LEN, ""%c_0x%p"", is_return ? 'r' : 'p', addr); sanitize_event_name(buf); event = buf; } argc -= 2; argv += 2; ctx.funcname = symbol; new_argv = traceprobe_expand_meta_args(argc, argv, &new_argc, abuf, MAX_BTF_ARGS_LEN, &ctx); if (IS_ERR(new_argv)) { ret = PTR_ERR(new_argv); new_argv = NULL; goto out; } if (new_argv) { argc = new_argc; argv = new_argv; } ret = traceprobe_expand_dentry_args(argc, argv, &dbuf); if (ret) goto out; tk = alloc_trace_kprobe(group, event, addr, symbol, offset, maxactive, argc, is_return); if (IS_ERR(tk)) { ret = PTR_ERR(tk); WARN_ON_ONCE(ret != -ENOMEM); goto out; } <S2SV_StartVul> for (i = 0; i < argc && i < MAX_TRACE_ARGS; i++) { <S2SV_EndVul> trace_probe_log_set_index(i + 2); ctx.offset = 0; ret = traceprobe_parse_probe_arg(&tk->tp, i, argv[i], &ctx); if (ret) goto error; } if (is_return && tk->tp.entry_arg) { tk->rp.entry_handler = trace_kprobe_entry_handler; tk->rp.data_size = traceprobe_get_entry_data_size(&tk->tp); } ptype = is_return ? PROBE_PRINT_RETURN : PROBE_PRINT_NORMAL; ret = traceprobe_set_print_fmt(&tk->tp, ptype); if (ret < 0) goto error; ret = register_trace_kprobe(tk); if (ret) { trace_probe_log_set_index(1); if (ret == -EILSEQ) trace_probe_log_err(0, BAD_INSN_BNDRY); else if (ret == -ENOENT) trace_probe_log_err(0, BAD_PROBE_ADDR); else if (ret != -ENOMEM && ret != -EEXIST) trace_probe_log_err(0, FAIL_REG_PROBE); goto error; } out: traceprobe_finish_parse(&ctx); trace_probe_log_clear(); kfree(new_argv); kfree(symbol); kfree(dbuf); return ret; parse_error: ret = -EINVAL; error: free_trace_kprobe(tk); goto out; }","- for (i = 0; i < argc && i < MAX_TRACE_ARGS; i++) {
+ }
+ if (argc > MAX_TRACE_ARGS) {
+ ret = -E2BIG;
+ goto out;
+ }
+ for (i = 0; i < argc; i++) {","static int __trace_kprobe_create(int argc, const char *argv[]) { struct trace_kprobe *tk = NULL; int i, len, new_argc = 0, ret = 0; bool is_return = false; char *symbol = NULL, *tmp = NULL; const char **new_argv = NULL; const char *event = NULL, *group = KPROBE_EVENT_SYSTEM; enum probe_print_type ptype; int maxactive = 0; long offset = 0; void *addr = NULL; char buf[MAX_EVENT_NAME_LEN]; char gbuf[MAX_EVENT_NAME_LEN]; char abuf[MAX_BTF_ARGS_LEN]; char *dbuf = NULL; struct traceprobe_parse_context ctx = { .flags = TPARG_FL_KERNEL }; switch (argv[0][0]) { case 'r': is_return = true; break; case 'p': break; default: return -ECANCELED; } if (argc < 2) return -ECANCELED; trace_probe_log_init(""trace_kprobe"", argc, argv); event = strchr(&argv[0][1], ':'); if (event) event++; if (isdigit(argv[0][1])) { if (!is_return) { trace_probe_log_err(1, BAD_MAXACT_TYPE); goto parse_error; } if (event) len = event - &argv[0][1] - 1; else len = strlen(&argv[0][1]); if (len > MAX_EVENT_NAME_LEN - 1) { trace_probe_log_err(1, BAD_MAXACT); goto parse_error; } memcpy(buf, &argv[0][1], len); buf[len] = '\0'; ret = kstrtouint(buf, 0, &maxactive); if (ret || !maxactive) { trace_probe_log_err(1, BAD_MAXACT); goto parse_error; } if (maxactive > KRETPROBE_MAXACTIVE_MAX) { trace_probe_log_err(1, MAXACT_TOO_BIG); goto parse_error; } } if (kstrtoul(argv[1], 0, (unsigned long *)&addr)) { trace_probe_log_set_index(1); if (strchr(argv[1], '/') && strchr(argv[1], ':')) { ret = -ECANCELED; goto error; } symbol = kstrdup(argv[1], GFP_KERNEL); if (!symbol) return -ENOMEM; tmp = strchr(symbol, '%'); if (tmp) { if (!strcmp(tmp, ""%return"")) { *tmp = '\0'; is_return = true; } else { trace_probe_log_err(tmp - symbol, BAD_ADDR_SUFFIX); goto parse_error; } } ret = traceprobe_split_symbol_offset(symbol, &offset); if (ret || offset < 0 || offset > UINT_MAX) { trace_probe_log_err(0, BAD_PROBE_ADDR); goto parse_error; } ret = validate_probe_symbol(symbol); if (ret) { if (ret == -EADDRNOTAVAIL) trace_probe_log_err(0, NON_UNIQ_SYMBOL); else trace_probe_log_err(0, BAD_PROBE_ADDR); goto parse_error; } if (is_return) ctx.flags |= TPARG_FL_RETURN; ret = kprobe_on_func_entry(NULL, symbol, offset); if (ret == 0 && !is_return) ctx.flags |= TPARG_FL_FENTRY; if (ret == -EINVAL && is_return) { trace_probe_log_err(0, BAD_RETPROBE); goto parse_error; } } trace_probe_log_set_index(0); if (event) { ret = traceprobe_parse_event_name(&event, &group, gbuf, event - argv[0]); if (ret) goto parse_error; } if (!event) { if (symbol) snprintf(buf, MAX_EVENT_NAME_LEN, ""%c_%s_%ld"", is_return ? 'r' : 'p', symbol, offset); else snprintf(buf, MAX_EVENT_NAME_LEN, ""%c_0x%p"", is_return ? 'r' : 'p', addr); sanitize_event_name(buf); event = buf; } argc -= 2; argv += 2; ctx.funcname = symbol; new_argv = traceprobe_expand_meta_args(argc, argv, &new_argc, abuf, MAX_BTF_ARGS_LEN, &ctx); if (IS_ERR(new_argv)) { ret = PTR_ERR(new_argv); new_argv = NULL; goto out; } if (new_argv) { argc = new_argc; argv = new_argv; } if (argc > MAX_TRACE_ARGS) { ret = -E2BIG; goto out; } ret = traceprobe_expand_dentry_args(argc, argv, &dbuf); if (ret) goto out; tk = alloc_trace_kprobe(group, event, addr, symbol, offset, maxactive, argc, is_return); if (IS_ERR(tk)) { ret = PTR_ERR(tk); WARN_ON_ONCE(ret != -ENOMEM); goto out; } for (i = 0; i < argc; i++) { trace_probe_log_set_index(i + 2); ctx.offset = 0; ret = traceprobe_parse_probe_arg(&tk->tp, i, argv[i], &ctx); if (ret) goto error; } if (is_return && tk->tp.entry_arg) { tk->rp.entry_handler = trace_kprobe_entry_handler; tk->rp.data_size = traceprobe_get_entry_data_size(&tk->tp); } ptype = is_return ? PROBE_PRINT_RETURN : PROBE_PRINT_NORMAL; ret = traceprobe_set_print_fmt(&tk->tp, ptype); if (ret < 0) goto error; ret = register_trace_kprobe(tk); if (ret) { trace_probe_log_set_index(1); if (ret == -EILSEQ) trace_probe_log_err(0, BAD_INSN_BNDRY); else if (ret == -ENOENT) trace_probe_log_err(0, BAD_PROBE_ADDR); else if (ret != -ENOMEM && ret != -EEXIST) trace_probe_log_err(0, FAIL_REG_PROBE); goto error; } out: traceprobe_finish_parse(&ctx); trace_probe_log_clear(); kfree(new_argv); kfree(symbol); kfree(dbuf); return ret; parse_error: ret = -EINVAL; error: free_trace_kprobe(tk); goto out; }"
517----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47752/bad/vdec_h264_req_if.c----vdec_h264_slice_decode,"static int vdec_h264_slice_decode(void *h_vdec, struct mtk_vcodec_mem *bs, struct vdec_fb *unused, bool *res_chg) { struct vdec_h264_slice_inst *inst = h_vdec; const struct v4l2_ctrl_h264_decode_params *dec_params = mtk_vdec_h264_get_ctrl_ptr(inst->ctx, V4L2_CID_STATELESS_H264_DECODE_PARAMS); struct vdec_vpu_inst *vpu = &inst->vpu; struct mtk_video_dec_buf *src_buf_info; struct mtk_video_dec_buf *dst_buf_info; struct vdec_fb *fb; u32 data[2]; u64 y_fb_dma; u64 c_fb_dma; int err; inst->num_nalu++; if (!bs) return vpu_dec_reset(vpu); fb = inst->ctx->dev->vdec_pdata->get_cap_buffer(inst->ctx); src_buf_info = container_of(bs, struct mtk_video_dec_buf, bs_buffer); dst_buf_info = container_of(fb, struct mtk_video_dec_buf, frame_buffer); <S2SV_StartVul> y_fb_dma = fb ? (u64)fb->base_y.dma_addr : 0; <S2SV_EndVul> <S2SV_StartVul> c_fb_dma = fb ? (u64)fb->base_c.dma_addr : 0; <S2SV_EndVul> mtk_vdec_debug(inst->ctx, ""+ [%d] FB y_dma=%llx c_dma=%llx va=%p"", inst->num_nalu, y_fb_dma, c_fb_dma, fb); inst->vsi_ctx.dec.bs_dma = (uint64_t)bs->dma_addr; inst->vsi_ctx.dec.y_fb_dma = y_fb_dma; inst->vsi_ctx.dec.c_fb_dma = c_fb_dma; inst->vsi_ctx.dec.vdec_fb_va = (u64)(uintptr_t)fb; v4l2_m2m_buf_copy_metadata(&src_buf_info->m2m_buf.vb, &dst_buf_info->m2m_buf.vb, true); err = get_vdec_decode_parameters(inst); if (err) goto err_free_fb_out; data[0] = bs->size; data[1] = (dec_params->nal_ref_idc << 5) | ((dec_params->flags & V4L2_H264_DECODE_PARAM_FLAG_IDR_PIC) ? 0x5 : 0x1); *res_chg = inst->vsi_ctx.dec.resolution_changed; if (*res_chg) { mtk_vdec_debug(inst->ctx, ""- resolution changed -""); if (inst->vsi_ctx.dec.realloc_mv_buf) { err = alloc_mv_buf(inst, &inst->ctx->picinfo); inst->vsi_ctx.dec.realloc_mv_buf = false; if (err) goto err_free_fb_out; } *res_chg = false; } memcpy(inst->vpu.vsi, &inst->vsi_ctx, sizeof(inst->vsi_ctx)); err = vpu_dec_start(vpu, data, 2); if (err) goto err_free_fb_out; err = mtk_vcodec_wait_for_done_ctx(inst->ctx, MTK_INST_IRQ_RECEIVED, WAIT_INTR_TIMEOUT_MS, 0); if (err) goto err_free_fb_out; vpu_dec_end(vpu); memcpy(&inst->vsi_ctx, inst->vpu.vsi, sizeof(inst->vsi_ctx)); mtk_vdec_debug(inst->ctx, ""\n - NALU[%d]"", inst->num_nalu); return 0; err_free_fb_out: mtk_vdec_err(inst->ctx, ""\n - NALU[%d] err=%d -\n"", inst->num_nalu, err); return err; }","- y_fb_dma = fb ? (u64)fb->base_y.dma_addr : 0;
- c_fb_dma = fb ? (u64)fb->base_c.dma_addr : 0;
+ if (!fb) {
+ mtk_vdec_err(inst->ctx, ""fb buffer is NULL"");
+ return -ENOMEM;
+ }
+ y_fb_dma = fb->base_y.dma_addr;
+ c_fb_dma = fb->base_c.dma_addr;","static int vdec_h264_slice_decode(void *h_vdec, struct mtk_vcodec_mem *bs, struct vdec_fb *unused, bool *res_chg) { struct vdec_h264_slice_inst *inst = h_vdec; const struct v4l2_ctrl_h264_decode_params *dec_params = mtk_vdec_h264_get_ctrl_ptr(inst->ctx, V4L2_CID_STATELESS_H264_DECODE_PARAMS); struct vdec_vpu_inst *vpu = &inst->vpu; struct mtk_video_dec_buf *src_buf_info; struct mtk_video_dec_buf *dst_buf_info; struct vdec_fb *fb; u32 data[2]; u64 y_fb_dma; u64 c_fb_dma; int err; inst->num_nalu++; if (!bs) return vpu_dec_reset(vpu); fb = inst->ctx->dev->vdec_pdata->get_cap_buffer(inst->ctx); if (!fb) { mtk_vdec_err(inst->ctx, ""fb buffer is NULL""); return -ENOMEM; } src_buf_info = container_of(bs, struct mtk_video_dec_buf, bs_buffer); dst_buf_info = container_of(fb, struct mtk_video_dec_buf, frame_buffer); y_fb_dma = fb->base_y.dma_addr; c_fb_dma = fb->base_c.dma_addr; mtk_vdec_debug(inst->ctx, ""+ [%d] FB y_dma=%llx c_dma=%llx va=%p"", inst->num_nalu, y_fb_dma, c_fb_dma, fb); inst->vsi_ctx.dec.bs_dma = (uint64_t)bs->dma_addr; inst->vsi_ctx.dec.y_fb_dma = y_fb_dma; inst->vsi_ctx.dec.c_fb_dma = c_fb_dma; inst->vsi_ctx.dec.vdec_fb_va = (u64)(uintptr_t)fb; v4l2_m2m_buf_copy_metadata(&src_buf_info->m2m_buf.vb, &dst_buf_info->m2m_buf.vb, true); err = get_vdec_decode_parameters(inst); if (err) goto err_free_fb_out; data[0] = bs->size; data[1] = (dec_params->nal_ref_idc << 5) | ((dec_params->flags & V4L2_H264_DECODE_PARAM_FLAG_IDR_PIC) ? 0x5 : 0x1); *res_chg = inst->vsi_ctx.dec.resolution_changed; if (*res_chg) { mtk_vdec_debug(inst->ctx, ""- resolution changed -""); if (inst->vsi_ctx.dec.realloc_mv_buf) { err = alloc_mv_buf(inst, &inst->ctx->picinfo); inst->vsi_ctx.dec.realloc_mv_buf = false; if (err) goto err_free_fb_out; } *res_chg = false; } memcpy(inst->vpu.vsi, &inst->vsi_ctx, sizeof(inst->vsi_ctx)); err = vpu_dec_start(vpu, data, 2); if (err) goto err_free_fb_out; err = mtk_vcodec_wait_for_done_ctx(inst->ctx, MTK_INST_IRQ_RECEIVED, WAIT_INTR_TIMEOUT_MS, 0); if (err) goto err_free_fb_out; vpu_dec_end(vpu); memcpy(&inst->vsi_ctx, inst->vpu.vsi, sizeof(inst->vsi_ctx)); mtk_vdec_debug(inst->ctx, ""\n - NALU[%d]"", inst->num_nalu); return 0; err_free_fb_out: mtk_vdec_err(inst->ctx, ""\n - NALU[%d] err=%d -\n"", inst->num_nalu, err); return err; }"
261----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46741/bad/fastrpc.c----fastrpc_req_mmap,"static int fastrpc_req_mmap(struct fastrpc_user *fl, char __user *argp) { struct fastrpc_invoke_args args[3] = { [0 ... 2] = { 0 } }; struct fastrpc_buf *buf = NULL; struct fastrpc_mmap_req_msg req_msg; struct fastrpc_mmap_rsp_msg rsp_msg; struct fastrpc_phy_page pages; struct fastrpc_req_mmap req; struct device *dev = fl->sctx->dev; int err; u32 sc; if (copy_from_user(&req, argp, sizeof(req))) return -EFAULT; if (req.flags != ADSP_MMAP_ADD_PAGES && req.flags != ADSP_MMAP_REMOTE_HEAP_ADDR) { dev_err(dev, ""flag not supported 0x%x\n"", req.flags); return -EINVAL; } if (req.vaddrin) { dev_err(dev, ""adding user allocated pages is not supported\n""); return -EINVAL; } if (req.flags == ADSP_MMAP_REMOTE_HEAP_ADDR) err = fastrpc_remote_heap_alloc(fl, dev, req.size, &buf); else err = fastrpc_buf_alloc(fl, dev, req.size, &buf); if (err) { dev_err(dev, ""failed to allocate buffer\n""); return err; } req_msg.pgid = fl->tgid; req_msg.flags = req.flags; req_msg.vaddr = req.vaddrin; req_msg.num = sizeof(pages); args[0].ptr = (u64) (uintptr_t) &req_msg; args[0].length = sizeof(req_msg); pages.addr = buf->phys; pages.size = buf->size; args[1].ptr = (u64) (uintptr_t) &pages; args[1].length = sizeof(pages); args[2].ptr = (u64) (uintptr_t) &rsp_msg; args[2].length = sizeof(rsp_msg); sc = FASTRPC_SCALARS(FASTRPC_RMID_INIT_MMAP, 2, 1); err = fastrpc_internal_invoke(fl, true, FASTRPC_INIT_HANDLE, sc, &args[0]); if (err) { dev_err(dev, ""mmap error (len 0x%08llx)\n"", buf->size); <S2SV_StartVul> goto err_invoke; <S2SV_EndVul> } buf->raddr = (uintptr_t) rsp_msg.vaddr; req.vaddrout = rsp_msg.vaddr; if (req.flags == ADSP_MMAP_REMOTE_HEAP_ADDR && fl->cctx->vmcount) { u64 src_perms = BIT(QCOM_SCM_VMID_HLOS); err = qcom_scm_assign_mem(buf->phys, (u64)buf->size, &src_perms, fl->cctx->vmperms, fl->cctx->vmcount); if (err) { dev_err(fl->sctx->dev, ""Failed to assign memory phys 0x%llx size 0x%llx err %d"", buf->phys, buf->size, err); goto err_assign; } } spin_lock(&fl->lock); list_add_tail(&buf->node, &fl->mmaps); spin_unlock(&fl->lock); if (copy_to_user((void __user *)argp, &req, sizeof(req))) { err = -EFAULT; goto err_assign; } dev_dbg(dev, ""mmap\t\tpt 0x%09lx OK [len 0x%08llx]\n"", buf->raddr, buf->size); return 0; err_assign: fastrpc_req_munmap_impl(fl, buf); <S2SV_StartVul> err_invoke: <S2SV_EndVul> <S2SV_StartVul> fastrpc_buf_free(buf); <S2SV_EndVul> return err; }","- goto err_invoke;
- err_invoke:
- fastrpc_buf_free(buf);
+ fastrpc_buf_free(buf);
+ return err;
+ return err;","static int fastrpc_req_mmap(struct fastrpc_user *fl, char __user *argp) { struct fastrpc_invoke_args args[3] = { [0 ... 2] = { 0 } }; struct fastrpc_buf *buf = NULL; struct fastrpc_mmap_req_msg req_msg; struct fastrpc_mmap_rsp_msg rsp_msg; struct fastrpc_phy_page pages; struct fastrpc_req_mmap req; struct device *dev = fl->sctx->dev; int err; u32 sc; if (copy_from_user(&req, argp, sizeof(req))) return -EFAULT; if (req.flags != ADSP_MMAP_ADD_PAGES && req.flags != ADSP_MMAP_REMOTE_HEAP_ADDR) { dev_err(dev, ""flag not supported 0x%x\n"", req.flags); return -EINVAL; } if (req.vaddrin) { dev_err(dev, ""adding user allocated pages is not supported\n""); return -EINVAL; } if (req.flags == ADSP_MMAP_REMOTE_HEAP_ADDR) err = fastrpc_remote_heap_alloc(fl, dev, req.size, &buf); else err = fastrpc_buf_alloc(fl, dev, req.size, &buf); if (err) { dev_err(dev, ""failed to allocate buffer\n""); return err; } req_msg.pgid = fl->tgid; req_msg.flags = req.flags; req_msg.vaddr = req.vaddrin; req_msg.num = sizeof(pages); args[0].ptr = (u64) (uintptr_t) &req_msg; args[0].length = sizeof(req_msg); pages.addr = buf->phys; pages.size = buf->size; args[1].ptr = (u64) (uintptr_t) &pages; args[1].length = sizeof(pages); args[2].ptr = (u64) (uintptr_t) &rsp_msg; args[2].length = sizeof(rsp_msg); sc = FASTRPC_SCALARS(FASTRPC_RMID_INIT_MMAP, 2, 1); err = fastrpc_internal_invoke(fl, true, FASTRPC_INIT_HANDLE, sc, &args[0]); if (err) { dev_err(dev, ""mmap error (len 0x%08llx)\n"", buf->size); fastrpc_buf_free(buf); return err; } buf->raddr = (uintptr_t) rsp_msg.vaddr; req.vaddrout = rsp_msg.vaddr; if (req.flags == ADSP_MMAP_REMOTE_HEAP_ADDR && fl->cctx->vmcount) { u64 src_perms = BIT(QCOM_SCM_VMID_HLOS); err = qcom_scm_assign_mem(buf->phys, (u64)buf->size, &src_perms, fl->cctx->vmperms, fl->cctx->vmcount); if (err) { dev_err(fl->sctx->dev, ""Failed to assign memory phys 0x%llx size 0x%llx err %d"", buf->phys, buf->size, err); goto err_assign; } } spin_lock(&fl->lock); list_add_tail(&buf->node, &fl->mmaps); spin_unlock(&fl->lock); if (copy_to_user((void __user *)argp, &req, sizeof(req))) { err = -EFAULT; goto err_assign; } dev_dbg(dev, ""mmap\t\tpt 0x%09lx OK [len 0x%08llx]\n"", buf->raddr, buf->size); return 0; err_assign: fastrpc_req_munmap_impl(fl, buf); return err; }"
1094----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53078/bad/drm.c----host1x_drm_probe,"static int host1x_drm_probe(struct host1x_device *dev) { struct device *dma_dev = dev->dev.parent; struct tegra_drm *tegra; struct drm_device *drm; int err; drm = drm_dev_alloc(&tegra_drm_driver, &dev->dev); if (IS_ERR(drm)) return PTR_ERR(drm); tegra = kzalloc(sizeof(*tegra), GFP_KERNEL); if (!tegra) { err = -ENOMEM; goto put; } if (host1x_drm_wants_iommu(dev) && device_iommu_mapped(dma_dev)) { tegra->domain = iommu_paging_domain_alloc(dma_dev); <S2SV_StartVul> if (!tegra->domain) { <S2SV_EndVul> <S2SV_StartVul> err = -ENOMEM; <S2SV_EndVul> goto free; } err = iova_cache_get(); if (err < 0) goto domain; } mutex_init(&tegra->clients_lock); INIT_LIST_HEAD(&tegra->clients); dev_set_drvdata(&dev->dev, drm); drm->dev_private = tegra; tegra->drm = drm; drm_mode_config_init(drm); drm->mode_config.min_width = 0; drm->mode_config.min_height = 0; drm->mode_config.max_width = 0; drm->mode_config.max_height = 0; drm->mode_config.normalize_zpos = true; drm->mode_config.funcs = &tegra_drm_mode_config_funcs; drm->mode_config.helper_private = &tegra_drm_mode_config_helpers; drm_kms_helper_poll_init(drm); err = host1x_device_init(dev); if (err < 0) goto poll; tegra->hmask = drm->mode_config.max_width - 1; tegra->vmask = drm->mode_config.max_height - 1; if (tegra->use_explicit_iommu) { u64 carveout_start, carveout_end, gem_start, gem_end; u64 dma_mask = dma_get_mask(&dev->dev); dma_addr_t start, end; unsigned long order; start = tegra->domain->geometry.aperture_start & dma_mask; end = tegra->domain->geometry.aperture_end & dma_mask; gem_start = start; gem_end = end - CARVEOUT_SZ; carveout_start = gem_end + 1; carveout_end = end; order = __ffs(tegra->domain->pgsize_bitmap); init_iova_domain(&tegra->carveout.domain, 1UL << order, carveout_start >> order); tegra->carveout.shift = iova_shift(&tegra->carveout.domain); tegra->carveout.limit = carveout_end >> tegra->carveout.shift; drm_mm_init(&tegra->mm, gem_start, gem_end - gem_start + 1); mutex_init(&tegra->mm_lock); DRM_DEBUG_DRIVER(""IOMMU apertures:\n""); DRM_DEBUG_DRIVER("" GEM: %#llx-%#llx\n"", gem_start, gem_end); DRM_DEBUG_DRIVER("" Carveout: %#llx-%#llx\n"", carveout_start, carveout_end); } else if (tegra->domain) { iommu_domain_free(tegra->domain); tegra->domain = NULL; iova_cache_put(); } if (tegra->hub) { err = tegra_display_hub_prepare(tegra->hub); if (err < 0) goto device; } drm->max_vblank_count = 0xffffffff; err = drm_vblank_init(drm, drm->mode_config.num_crtc); if (err < 0) goto hub; drm_mode_config_reset(drm); if (drm->mode_config.num_crtc > 0) { err = drm_aperture_remove_framebuffers(&tegra_drm_driver); if (err < 0) goto hub; } else { drm->driver_features &= ~(DRIVER_MODESET | DRIVER_ATOMIC); } err = drm_dev_register(drm, 0); if (err < 0) goto hub; tegra_fbdev_setup(drm); return 0; hub: if (tegra->hub) tegra_display_hub_cleanup(tegra->hub); device: if (tegra->domain) { mutex_destroy(&tegra->mm_lock); drm_mm_takedown(&tegra->mm); put_iova_domain(&tegra->carveout.domain); iova_cache_put(); } host1x_device_exit(dev); poll: drm_kms_helper_poll_fini(drm); drm_mode_config_cleanup(drm); domain: if (tegra->domain) iommu_domain_free(tegra->domain); free: kfree(tegra); put: drm_dev_put(drm); return err; }","- if (!tegra->domain) {
- err = -ENOMEM;
+ if (IS_ERR(tegra->domain)) {
+ err = PTR_ERR(tegra->domain);","static int host1x_drm_probe(struct host1x_device *dev) { struct device *dma_dev = dev->dev.parent; struct tegra_drm *tegra; struct drm_device *drm; int err; drm = drm_dev_alloc(&tegra_drm_driver, &dev->dev); if (IS_ERR(drm)) return PTR_ERR(drm); tegra = kzalloc(sizeof(*tegra), GFP_KERNEL); if (!tegra) { err = -ENOMEM; goto put; } if (host1x_drm_wants_iommu(dev) && device_iommu_mapped(dma_dev)) { tegra->domain = iommu_paging_domain_alloc(dma_dev); if (IS_ERR(tegra->domain)) { err = PTR_ERR(tegra->domain); goto free; } err = iova_cache_get(); if (err < 0) goto domain; } mutex_init(&tegra->clients_lock); INIT_LIST_HEAD(&tegra->clients); dev_set_drvdata(&dev->dev, drm); drm->dev_private = tegra; tegra->drm = drm; drm_mode_config_init(drm); drm->mode_config.min_width = 0; drm->mode_config.min_height = 0; drm->mode_config.max_width = 0; drm->mode_config.max_height = 0; drm->mode_config.normalize_zpos = true; drm->mode_config.funcs = &tegra_drm_mode_config_funcs; drm->mode_config.helper_private = &tegra_drm_mode_config_helpers; drm_kms_helper_poll_init(drm); err = host1x_device_init(dev); if (err < 0) goto poll; tegra->hmask = drm->mode_config.max_width - 1; tegra->vmask = drm->mode_config.max_height - 1; if (tegra->use_explicit_iommu) { u64 carveout_start, carveout_end, gem_start, gem_end; u64 dma_mask = dma_get_mask(&dev->dev); dma_addr_t start, end; unsigned long order; start = tegra->domain->geometry.aperture_start & dma_mask; end = tegra->domain->geometry.aperture_end & dma_mask; gem_start = start; gem_end = end - CARVEOUT_SZ; carveout_start = gem_end + 1; carveout_end = end; order = __ffs(tegra->domain->pgsize_bitmap); init_iova_domain(&tegra->carveout.domain, 1UL << order, carveout_start >> order); tegra->carveout.shift = iova_shift(&tegra->carveout.domain); tegra->carveout.limit = carveout_end >> tegra->carveout.shift; drm_mm_init(&tegra->mm, gem_start, gem_end - gem_start + 1); mutex_init(&tegra->mm_lock); DRM_DEBUG_DRIVER(""IOMMU apertures:\n""); DRM_DEBUG_DRIVER("" GEM: %#llx-%#llx\n"", gem_start, gem_end); DRM_DEBUG_DRIVER("" Carveout: %#llx-%#llx\n"", carveout_start, carveout_end); } else if (tegra->domain) { iommu_domain_free(tegra->domain); tegra->domain = NULL; iova_cache_put(); } if (tegra->hub) { err = tegra_display_hub_prepare(tegra->hub); if (err < 0) goto device; } drm->max_vblank_count = 0xffffffff; err = drm_vblank_init(drm, drm->mode_config.num_crtc); if (err < 0) goto hub; drm_mode_config_reset(drm); if (drm->mode_config.num_crtc > 0) { err = drm_aperture_remove_framebuffers(&tegra_drm_driver); if (err < 0) goto hub; } else { drm->driver_features &= ~(DRIVER_MODESET | DRIVER_ATOMIC); } err = drm_dev_register(drm, 0); if (err < 0) goto hub; tegra_fbdev_setup(drm); return 0; hub: if (tegra->hub) tegra_display_hub_cleanup(tegra->hub); device: if (tegra->domain) { mutex_destroy(&tegra->mm_lock); drm_mm_takedown(&tegra->mm); put_iova_domain(&tegra->carveout.domain); iova_cache_put(); } host1x_device_exit(dev); poll: drm_kms_helper_poll_fini(drm); drm_mode_config_cleanup(drm); domain: if (tegra->domain) iommu_domain_free(tegra->domain); free: kfree(tegra); put: drm_dev_put(drm); return err; }"
277----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46758/bad/lm95234.c----tcrit2_store,"static ssize_t tcrit2_store(struct device *dev, struct device_attribute *attr, const char *buf, size_t count) { struct lm95234_data *data = dev_get_drvdata(dev); int index = to_sensor_dev_attr(attr)->index; long val; int ret = lm95234_update_device(data); if (ret) return ret; ret = kstrtol(buf, 10, &val); if (ret < 0) return ret; <S2SV_StartVul> val = clamp_val(DIV_ROUND_CLOSEST(val, 1000), 0, index ? 255 : 127); <S2SV_EndVul> mutex_lock(&data->update_lock); data->tcrit2[index] = val; i2c_smbus_write_byte_data(data->client, LM95234_REG_TCRIT2(index), val); mutex_unlock(&data->update_lock); return count; }","- val = clamp_val(DIV_ROUND_CLOSEST(val, 1000), 0, index ? 255 : 127);
+ val = DIV_ROUND_CLOSEST(clamp_val(val, 0, (index ? 255 : 127) * 1000),
+ 1000);","static ssize_t tcrit2_store(struct device *dev, struct device_attribute *attr, const char *buf, size_t count) { struct lm95234_data *data = dev_get_drvdata(dev); int index = to_sensor_dev_attr(attr)->index; long val; int ret = lm95234_update_device(data); if (ret) return ret; ret = kstrtol(buf, 10, &val); if (ret < 0) return ret; val = DIV_ROUND_CLOSEST(clamp_val(val, 0, (index ? 255 : 127) * 1000), 1000); mutex_lock(&data->update_lock); data->tcrit2[index] = val; i2c_smbus_write_byte_data(data->client, LM95234_REG_TCRIT2(index), val); mutex_unlock(&data->update_lock); return count; }"
699----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49988/bad/oplock.c----*alloc_opinfo,"static struct oplock_info *alloc_opinfo(struct ksmbd_work *work, u64 id, __u16 Tid) { struct ksmbd_conn *conn = work->conn; struct ksmbd_session *sess = work->sess; struct oplock_info *opinfo; opinfo = kzalloc(sizeof(struct oplock_info), GFP_KERNEL); if (!opinfo) return NULL; opinfo->sess = sess; opinfo->conn = conn; opinfo->level = SMB2_OPLOCK_LEVEL_NONE; opinfo->op_state = OPLOCK_STATE_NONE; opinfo->pending_break = 0; opinfo->fid = id; opinfo->Tid = Tid; INIT_LIST_HEAD(&opinfo->op_entry); INIT_LIST_HEAD(&opinfo->interim_list); init_waitqueue_head(&opinfo->oplock_q); init_waitqueue_head(&opinfo->oplock_brk); atomic_set(&opinfo->refcount, 1); atomic_set(&opinfo->breaking_cnt, 0); return opinfo; <S2SV_StartVul> } <S2SV_EndVul>","- }
+ atomic_inc(&opinfo->conn->refcnt);","static struct oplock_info *alloc_opinfo(struct ksmbd_work *work, u64 id, __u16 Tid) { struct ksmbd_conn *conn = work->conn; struct ksmbd_session *sess = work->sess; struct oplock_info *opinfo; opinfo = kzalloc(sizeof(struct oplock_info), GFP_KERNEL); if (!opinfo) return NULL; opinfo->sess = sess; opinfo->conn = conn; opinfo->level = SMB2_OPLOCK_LEVEL_NONE; opinfo->op_state = OPLOCK_STATE_NONE; opinfo->pending_break = 0; opinfo->fid = id; opinfo->Tid = Tid; INIT_LIST_HEAD(&opinfo->op_entry); INIT_LIST_HEAD(&opinfo->interim_list); init_waitqueue_head(&opinfo->oplock_q); init_waitqueue_head(&opinfo->oplock_brk); atomic_set(&opinfo->refcount, 1); atomic_set(&opinfo->breaking_cnt, 0); atomic_inc(&opinfo->conn->refcnt); return opinfo; }"
1020----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50259/bad/fib.c----nsim_nexthop_bucket_activity_write,"static ssize_t nsim_nexthop_bucket_activity_write(struct file *file, const char __user *user_buf, size_t size, loff_t *ppos) { struct nsim_fib_data *data = file->private_data; struct net *net = devlink_net(data->devlink); struct nsim_nexthop *nexthop; unsigned long *activity; loff_t pos = *ppos; u16 bucket_index; char buf[128]; int err = 0; u32 nhid; if (pos != 0) return -EINVAL; <S2SV_StartVul> if (size > sizeof(buf)) <S2SV_EndVul> return -EINVAL; if (copy_from_user(buf, user_buf, size)) return -EFAULT; if (sscanf(buf, ""%u %hu"", &nhid, &bucket_index) != 2) return -EINVAL; rtnl_lock(); nexthop = rhashtable_lookup_fast(&data->nexthop_ht, &nhid, nsim_nexthop_ht_params); if (!nexthop || !nexthop->is_resilient || bucket_index >= nexthop->occ) { err = -EINVAL; goto out; } activity = bitmap_zalloc(nexthop->occ, GFP_KERNEL); if (!activity) { err = -ENOMEM; goto out; } bitmap_set(activity, bucket_index, 1); nexthop_res_grp_activity_update(net, nhid, nexthop->occ, activity); bitmap_free(activity); out: rtnl_unlock(); *ppos = size; return err ?: size; }","- if (size > sizeof(buf))
+ if (size > sizeof(buf) - 1)
+ buf[size] = 0;","static ssize_t nsim_nexthop_bucket_activity_write(struct file *file, const char __user *user_buf, size_t size, loff_t *ppos) { struct nsim_fib_data *data = file->private_data; struct net *net = devlink_net(data->devlink); struct nsim_nexthop *nexthop; unsigned long *activity; loff_t pos = *ppos; u16 bucket_index; char buf[128]; int err = 0; u32 nhid; if (pos != 0) return -EINVAL; if (size > sizeof(buf) - 1) return -EINVAL; if (copy_from_user(buf, user_buf, size)) return -EFAULT; buf[size] = 0; if (sscanf(buf, ""%u %hu"", &nhid, &bucket_index) != 2) return -EINVAL; rtnl_lock(); nexthop = rhashtable_lookup_fast(&data->nexthop_ht, &nhid, nsim_nexthop_ht_params); if (!nexthop || !nexthop->is_resilient || bucket_index >= nexthop->occ) { err = -EINVAL; goto out; } activity = bitmap_zalloc(nexthop->occ, GFP_KERNEL); if (!activity) { err = -ENOMEM; goto out; } bitmap_set(activity, bucket_index, 1); nexthop_res_grp_activity_update(net, nhid, nexthop->occ, activity); bitmap_free(activity); out: rtnl_unlock(); *ppos = size; return err ?: size; }"
1207----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53208/bad/mgmt.c----mgmt_set_powered_complete,"static void mgmt_set_powered_complete(struct hci_dev *hdev, void *data, int err) { struct mgmt_pending_cmd *cmd = data; struct mgmt_mode *cp; <S2SV_StartVul> if (cmd != pending_find(MGMT_OP_SET_POWERED, hdev)) <S2SV_EndVul> return; cp = cmd->param; bt_dev_dbg(hdev, ""err %d"", err); if (!err) { if (cp->val) { hci_dev_lock(hdev); restart_le_actions(hdev); hci_update_passive_scan(hdev); hci_dev_unlock(hdev); } send_settings_rsp(cmd->sk, cmd->opcode, hdev); if (cp->val) new_settings(hdev, cmd->sk); } else { mgmt_cmd_status(cmd->sk, hdev->id, MGMT_OP_SET_POWERED, mgmt_status(err)); } mgmt_pending_remove(cmd); }","- if (cmd != pending_find(MGMT_OP_SET_POWERED, hdev))
+ if (err == -ECANCELED ||
+ cmd != pending_find(MGMT_OP_SET_POWERED, hdev))
+ cp = cmd->param;","static void mgmt_set_powered_complete(struct hci_dev *hdev, void *data, int err) { struct mgmt_pending_cmd *cmd = data; struct mgmt_mode *cp; if (err == -ECANCELED || cmd != pending_find(MGMT_OP_SET_POWERED, hdev)) return; cp = cmd->param; bt_dev_dbg(hdev, ""err %d"", err); if (!err) { if (cp->val) { hci_dev_lock(hdev); restart_le_actions(hdev); hci_update_passive_scan(hdev); hci_dev_unlock(hdev); } send_settings_rsp(cmd->sk, cmd->opcode, hdev); if (cp->val) new_settings(hdev, cmd->sk); } else { mgmt_cmd_status(cmd->sk, hdev->id, MGMT_OP_SET_POWERED, mgmt_status(err)); } mgmt_pending_remove(cmd); }"
532----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49854/bad/bfq-iosched.c----*bfq_init_rq,"static struct bfq_queue *bfq_init_rq(struct request *rq) { struct request_queue *q = rq->q; struct bio *bio = rq->bio; struct bfq_data *bfqd = q->elevator->elevator_data; struct bfq_io_cq *bic; const int is_sync = rq_is_sync(rq); struct bfq_queue *bfqq; bool new_queue = false; bool bfqq_already_existing = false, split = false; unsigned int a_idx = bfq_actuator_index(bfqd, bio); if (unlikely(!rq->elv.icq)) return NULL; if (RQ_BFQQ(rq)) return RQ_BFQQ(rq); bic = icq_to_bic(rq->elv.icq); bfq_check_ioprio_change(bic, bio); bfq_bic_update_cgroup(bic, bio); bfqq = bfq_get_bfqq_handle_split(bfqd, bic, bio, false, is_sync, &new_queue); if (likely(!new_queue)) { if (bfq_bfqq_coop(bfqq) && bfq_bfqq_split_coop(bfqq) && !bic->bfqq_data[a_idx].stably_merged) { <S2SV_StartVul> struct bfq_queue *old_bfqq = bfqq; <S2SV_EndVul> if (bfq_bfqq_in_large_burst(bfqq)) bic->bfqq_data[a_idx].saved_in_large_burst = true; bfqq = bfq_split_bfqq(bic, bfqq); split = true; if (!bfqq) { bfqq = bfq_get_bfqq_handle_split(bfqd, bic, bio, true, is_sync, NULL); if (unlikely(bfqq == &bfqd->oom_bfqq)) bfqq_already_existing = true; } else bfqq_already_existing = true; if (!bfqq_already_existing) { <S2SV_StartVul> bfqq->waker_bfqq = old_bfqq->waker_bfqq; <S2SV_EndVul> bfqq->tentative_waker_bfqq = NULL; <S2SV_StartVul> if (bfqq->waker_bfqq) <S2SV_EndVul> hlist_add_head(&bfqq->woken_list_node, &bfqq->waker_bfqq->woken_list); } } } bfqq_request_allocated(bfqq); bfqq->ref++; bic->requests++; bfq_log_bfqq(bfqd, bfqq, ""get_request %p: bfqq %p, %d"", rq, bfqq, bfqq->ref); rq->elv.priv[0] = bic; rq->elv.priv[1] = bfqq; if (likely(bfqq != &bfqd->oom_bfqq) && !bfqq->new_bfqq && bfqq_process_refs(bfqq) == 1) { bfqq->bic = bic; if (split) { bfq_bfqq_resume_state(bfqq, bfqd, bic, bfqq_already_existing); } } if (unlikely(bfq_bfqq_just_created(bfqq) && (bfqd->burst_size > 0 || bfq_tot_busy_queues(bfqd) == 0))) bfq_handle_burst(bfqd, bfqq); return bfqq; }","- struct bfq_queue *old_bfqq = bfqq;
- bfqq->waker_bfqq = old_bfqq->waker_bfqq;
- if (bfqq->waker_bfqq)
+ struct bfq_queue *waker_bfqq = bfq_waker_bfqq(bfqq);
+ bfqq->waker_bfqq = waker_bfqq;
+ if (waker_bfqq)
+ }","static struct bfq_queue *bfq_init_rq(struct request *rq) { struct request_queue *q = rq->q; struct bio *bio = rq->bio; struct bfq_data *bfqd = q->elevator->elevator_data; struct bfq_io_cq *bic; const int is_sync = rq_is_sync(rq); struct bfq_queue *bfqq; bool new_queue = false; bool bfqq_already_existing = false, split = false; unsigned int a_idx = bfq_actuator_index(bfqd, bio); if (unlikely(!rq->elv.icq)) return NULL; if (RQ_BFQQ(rq)) return RQ_BFQQ(rq); bic = icq_to_bic(rq->elv.icq); bfq_check_ioprio_change(bic, bio); bfq_bic_update_cgroup(bic, bio); bfqq = bfq_get_bfqq_handle_split(bfqd, bic, bio, false, is_sync, &new_queue); if (likely(!new_queue)) { if (bfq_bfqq_coop(bfqq) && bfq_bfqq_split_coop(bfqq) && !bic->bfqq_data[a_idx].stably_merged) { struct bfq_queue *waker_bfqq = bfq_waker_bfqq(bfqq); if (bfq_bfqq_in_large_burst(bfqq)) bic->bfqq_data[a_idx].saved_in_large_burst = true; bfqq = bfq_split_bfqq(bic, bfqq); split = true; if (!bfqq) { bfqq = bfq_get_bfqq_handle_split(bfqd, bic, bio, true, is_sync, NULL); if (unlikely(bfqq == &bfqd->oom_bfqq)) bfqq_already_existing = true; } else bfqq_already_existing = true; if (!bfqq_already_existing) { bfqq->waker_bfqq = waker_bfqq; bfqq->tentative_waker_bfqq = NULL; if (waker_bfqq) hlist_add_head(&bfqq->woken_list_node, &bfqq->waker_bfqq->woken_list); } } } bfqq_request_allocated(bfqq); bfqq->ref++; bic->requests++; bfq_log_bfqq(bfqd, bfqq, ""get_request %p: bfqq %p, %d"", rq, bfqq, bfqq->ref); rq->elv.priv[0] = bic; rq->elv.priv[1] = bfqq; if (likely(bfqq != &bfqd->oom_bfqq) && !bfqq->new_bfqq && bfqq_process_refs(bfqq) == 1) { bfqq->bic = bic; if (split) { bfq_bfqq_resume_state(bfqq, bfqd, bic, bfqq_already_existing); } } if (unlikely(bfq_bfqq_just_created(bfqq) && (bfqd->burst_size > 0 || bfq_tot_busy_queues(bfqd) == 0))) bfq_handle_burst(bfqd, bfqq); return bfqq; }"
616----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49922/bad/amdgpu_dm.c----amdgpu_dm_atomic_commit_tail,"static void amdgpu_dm_atomic_commit_tail(struct drm_atomic_state *state) { struct drm_device *dev = state->dev; struct amdgpu_device *adev = drm_to_adev(dev); struct amdgpu_display_manager *dm = &adev->dm; struct dm_atomic_state *dm_state; struct dc_state *dc_state = NULL; u32 i, j; struct drm_crtc *crtc; struct drm_crtc_state *old_crtc_state, *new_crtc_state; unsigned long flags; bool wait_for_vblank = true; struct drm_connector *connector; struct drm_connector_state *old_con_state, *new_con_state; struct dm_crtc_state *dm_old_crtc_state, *dm_new_crtc_state; int crtc_disable_count = 0; trace_amdgpu_dm_atomic_commit_tail_begin(state); drm_atomic_helper_update_legacy_modeset_state(dev, state); drm_dp_mst_atomic_wait_for_dependencies(state); dm_state = dm_atomic_get_new_state(state); if (dm_state && dm_state->context) { dc_state = dm_state->context; amdgpu_dm_commit_streams(state, dc_state); } for_each_oldnew_connector_in_state(state, connector, old_con_state, new_con_state, i) { struct dm_connector_state *dm_new_con_state = to_dm_connector_state(new_con_state); struct amdgpu_crtc *acrtc = to_amdgpu_crtc(dm_new_con_state->base.crtc); struct amdgpu_dm_connector *aconnector; if (connector->connector_type == DRM_MODE_CONNECTOR_WRITEBACK) continue; aconnector = to_amdgpu_dm_connector(connector); if (!adev->dm.hdcp_workqueue) continue; pr_debug(""[HDCP_DM] -------------- i : %x ----------\n"", i); if (!connector) continue; pr_debug(""[HDCP_DM] connector->index: %x connect_status: %x dpms: %x\n"", connector->index, connector->status, connector->dpms); pr_debug(""[HDCP_DM] state protection old: %x new: %x\n"", old_con_state->content_protection, new_con_state->content_protection); if (aconnector->dc_sink) { if (aconnector->dc_sink->sink_signal != SIGNAL_TYPE_VIRTUAL && aconnector->dc_sink->sink_signal != SIGNAL_TYPE_NONE) { pr_debug(""[HDCP_DM] pipe_ctx dispname=%s\n"", aconnector->dc_sink->edid_caps.display_name); } } new_crtc_state = NULL; old_crtc_state = NULL; if (acrtc) { new_crtc_state = drm_atomic_get_new_crtc_state(state, &acrtc->base); old_crtc_state = drm_atomic_get_old_crtc_state(state, &acrtc->base); } if (old_crtc_state) pr_debug(""old crtc en: %x a: %x m: %x a-chg: %x c-chg: %x\n"", old_crtc_state->enable, old_crtc_state->active, old_crtc_state->mode_changed, old_crtc_state->active_changed, old_crtc_state->connectors_changed); if (new_crtc_state) pr_debug(""NEW crtc en: %x a: %x m: %x a-chg: %x c-chg: %x\n"", new_crtc_state->enable, new_crtc_state->active, new_crtc_state->mode_changed, new_crtc_state->active_changed, new_crtc_state->connectors_changed); } for_each_oldnew_connector_in_state(state, connector, old_con_state, new_con_state, i) { struct dm_connector_state *dm_new_con_state = to_dm_connector_state(new_con_state); struct amdgpu_crtc *acrtc = to_amdgpu_crtc(dm_new_con_state->base.crtc); struct amdgpu_dm_connector *aconnector = to_amdgpu_dm_connector(connector); if (!adev->dm.hdcp_workqueue) continue; new_crtc_state = NULL; old_crtc_state = NULL; if (acrtc) { new_crtc_state = drm_atomic_get_new_crtc_state(state, &acrtc->base); old_crtc_state = drm_atomic_get_old_crtc_state(state, &acrtc->base); } dm_new_crtc_state = to_dm_crtc_state(new_crtc_state); if (dm_new_crtc_state && dm_new_crtc_state->stream == NULL && connector->state->content_protection == DRM_MODE_CONTENT_PROTECTION_ENABLED) { hdcp_reset_display(adev->dm.hdcp_workqueue, aconnector->dc_link->link_index); new_con_state->content_protection = DRM_MODE_CONTENT_PROTECTION_DESIRED; dm_new_con_state->update_hdcp = true; continue; } if (is_content_protection_different(new_crtc_state, old_crtc_state, new_con_state, old_con_state, connector, adev->dm.hdcp_workqueue)) { bool enable_encryption = false; if (new_con_state->content_protection == DRM_MODE_CONTENT_PROTECTION_DESIRED) enable_encryption = true; if (aconnector->dc_link && aconnector->dc_sink && aconnector->dc_link->type == dc_connection_mst_branch) { struct hdcp_workqueue *hdcp_work = adev->dm.hdcp_workqueue; struct hdcp_workqueue *hdcp_w = &hdcp_work[aconnector->dc_link->link_index]; hdcp_w->hdcp_content_type[connector->index] = new_con_state->hdcp_content_type; hdcp_w->content_protection[connector->index] = new_con_state->content_protection; } if (new_crtc_state && new_crtc_state->mode_changed && new_con_state->content_protection >= DRM_MODE_CONTENT_PROTECTION_DESIRED) enable_encryption = true; DRM_INFO(""[HDCP_DM] hdcp_update_display enable_encryption = %x\n"", enable_encryption); <S2SV_StartVul> hdcp_update_display( <S2SV_EndVul> <S2SV_StartVul> adev->dm.hdcp_workqueue, aconnector->dc_link->link_index, aconnector, <S2SV_EndVul> <S2SV_StartVul> new_con_state->hdcp_content_type, enable_encryption); <S2SV_EndVul> } } for_each_oldnew_connector_in_state(state, connector, old_con_state, new_con_state, i) { struct dm_connector_state *dm_new_con_state = to_dm_connector_state(new_con_state); struct dm_connector_state *dm_old_con_state = to_dm_connector_state(old_con_state); struct amdgpu_crtc *acrtc = to_amdgpu_crtc(dm_new_con_state->base.crtc); struct dc_surface_update *dummy_updates; struct dc_stream_update stream_update; struct dc_info_packet hdr_packet; struct dc_stream_status *status = NULL; bool abm_changed, hdr_changed, scaling_changed; memset(&stream_update, 0, sizeof(stream_update)); if (acrtc) { new_crtc_state = drm_atomic_get_new_crtc_state(state, &acrtc->base); old_crtc_state = drm_atomic_get_old_crtc_state(state, &acrtc->base); } if (!acrtc || drm_atomic_crtc_needs_modeset(new_crtc_state)) continue; dm_new_crtc_state = to_dm_crtc_state(new_crtc_state); dm_old_crtc_state = to_dm_crtc_state(old_crtc_state); scaling_changed = is_scaling_state_different(dm_new_con_state, dm_old_con_state); abm_changed = dm_new_crtc_state->abm_level != dm_old_crtc_state->abm_level; hdr_changed = !drm_connector_atomic_hdr_metadata_equal(old_con_state, new_con_state); if (!scaling_changed && !abm_changed && !hdr_changed) continue; stream_update.stream = dm_new_crtc_state->stream; if (scaling_changed) { update_stream_scaling_settings(&dm_new_con_state->base.crtc->mode, dm_new_con_state, dm_new_crtc_state->stream); stream_update.src = dm_new_crtc_state->stream->src; stream_update.dst = dm_new_crtc_state->stream->dst; } if (abm_changed) { dm_new_crtc_state->stream->abm_level = dm_new_crtc_state->abm_level; stream_update.abm_level = &dm_new_crtc_state->abm_level; } if (hdr_changed) { fill_hdr_info_packet(new_con_state, &hdr_packet); stream_update.hdr_static_metadata = &hdr_packet; } status = dc_stream_get_status(dm_new_crtc_state->stream); if (WARN_ON(!status)) continue; WARN_ON(!status->plane_count); dummy_updates = kzalloc(sizeof(struct dc_surface_update) * MAX_SURFACES, GFP_ATOMIC); if (!dummy_updates) { DRM_ERROR(""Failed to allocate memory for dummy_updates.\n""); continue; } for (j = 0; j < status->plane_count; j++) dummy_updates[j].surface = status->plane_states[0]; sort(dummy_updates, status->plane_count, sizeof(*dummy_updates), dm_plane_layer_index_cmp, NULL); mutex_lock(&dm->dc_lock); dc_exit_ips_for_hw_access(dm->dc); dc_update_planes_and_stream(dm->dc, dummy_updates, status->plane_count, dm_new_crtc_state->stream, &stream_update); mutex_unlock(&dm->dc_lock); kfree(dummy_updates); } for_each_oldnew_crtc_in_state(state, crtc, old_crtc_state, new_crtc_state, i) { struct amdgpu_crtc *acrtc = to_amdgpu_crtc(crtc); #ifdef CONFIG_DEBUG_FS enum amdgpu_dm_pipe_crc_source cur_crc_src; #endif if (old_crtc_state->active && !new_crtc_state->active) crtc_disable_count++; dm_new_crtc_state = to_dm_crtc_state(new_crtc_state); dm_old_crtc_state = to_dm_crtc_state(old_crtc_state); update_stream_irq_parameters(dm, dm_new_crtc_state); #ifdef CONFIG_DEBUG_FS spin_lock_irqsave(&adev_to_drm(adev)->event_lock, flags); cur_crc_src = acrtc->dm_irq_params.crc_src; spin_unlock_irqrestore(&adev_to_drm(adev)->event_lock, flags); #endif if (new_crtc_state->active && (!old_crtc_state->active || drm_atomic_crtc_needs_modeset(new_crtc_state))) { dc_stream_retain(dm_new_crtc_state->stream); acrtc->dm_irq_params.stream = dm_new_crtc_state->stream; manage_dm_interrupts(adev, acrtc, true); } amdgpu_dm_handle_vrr_transition(dm_old_crtc_state, dm_new_crtc_state); #ifdef CONFIG_DEBUG_FS if (new_crtc_state->active && (!old_crtc_state->active || drm_atomic_crtc_needs_modeset(new_crtc_state))) { if (amdgpu_dm_is_valid_crc_source(cur_crc_src)) { #if defined(CONFIG_DRM_AMD_SECURE_DISPLAY) if (amdgpu_dm_crc_window_is_activated(crtc)) { spin_lock_irqsave(&adev_to_drm(adev)->event_lock, flags); acrtc->dm_irq_params.window_param.update_win = true; acrtc->dm_irq_params.window_param.skip_frame_cnt = 2; spin_unlock_irqrestore(&adev_to_drm(adev)->event_lock, flags); } #endif if (amdgpu_dm_crtc_configure_crc_source( crtc, dm_new_crtc_state, cur_crc_src)) drm_dbg_atomic(dev, ""Failed to configure crc source""); } } #endif } for_each_new_crtc_in_state(state, crtc, new_crtc_state, j) if (new_crtc_state->async_flip) wait_for_vblank = false; for_each_new_crtc_in_state(state, crtc, new_crtc_state, j) { dm_new_crtc_state = to_dm_crtc_state(new_crtc_state); if (dm_new_crtc_state->stream) amdgpu_dm_commit_planes(state, dev, dm, crtc, wait_for_vblank); } for_each_new_connector_in_state(state, connector, new_con_state, i) { struct dm_connector_state *dm_new_con_state = to_dm_connector_state(new_con_state); struct amdgpu_crtc *acrtc = to_amdgpu_crtc(dm_new_con_state->base.crtc); if (connector->connector_type != DRM_MODE_CONNECTOR_WRITEBACK) continue; if (!new_con_state->writeback_job) continue; new_crtc_state = drm_atomic_get_new_crtc_state(state, &acrtc->base); if (!new_crtc_state) continue; if (acrtc->wb_enabled) continue; dm_new_crtc_state = to_dm_crtc_state(new_crtc_state); dm_set_writeback(dm, dm_new_crtc_state, connector, new_con_state); acrtc->wb_enabled = true; } amdgpu_dm_commit_audio(dev, state); for (i = 0; i < dm->num_of_edps; i++) { if (dm->backlight_dev[i] && (dm->actual_brightness[i] != dm->brightness[i])) amdgpu_dm_backlight_set_level(dm, i, dm->brightness[i]); } spin_lock_irqsave(&adev_to_drm(adev)->event_lock, flags); for_each_new_crtc_in_state(state, crtc, new_crtc_state, i) { if (new_crtc_state->event) drm_send_event_locked(dev, &new_crtc_state->event->base); new_crtc_state->event = NULL; } spin_unlock_irqrestore(&adev_to_drm(adev)->event_lock, flags); drm_atomic_helper_commit_hw_done(state); if (wait_for_vblank) drm_atomic_helper_wait_for_flip_done(dev, state); drm_atomic_helper_cleanup_planes(dev, state); if (!adev->in_suspend) { if (!adev->mman.keep_stolen_vga_memory) amdgpu_bo_free_kernel(&adev->mman.stolen_vga_memory, NULL, NULL); amdgpu_bo_free_kernel(&adev->mman.stolen_extended_memory, NULL, NULL); } for (i = 0; i < crtc_disable_count; i++) pm_runtime_put_autosuspend(dev->dev); pm_runtime_mark_last_busy(dev->dev); }","- hdcp_update_display(
- adev->dm.hdcp_workqueue, aconnector->dc_link->link_index, aconnector,
- new_con_state->hdcp_content_type, enable_encryption);
+ if (aconnector->dc_link)
+ hdcp_update_display(
+ adev->dm.hdcp_workqueue, aconnector->dc_link->link_index, aconnector,
+ new_con_state->hdcp_content_type, enable_encryption);","static void amdgpu_dm_atomic_commit_tail(struct drm_atomic_state *state) { struct drm_device *dev = state->dev; struct amdgpu_device *adev = drm_to_adev(dev); struct amdgpu_display_manager *dm = &adev->dm; struct dm_atomic_state *dm_state; struct dc_state *dc_state = NULL; u32 i, j; struct drm_crtc *crtc; struct drm_crtc_state *old_crtc_state, *new_crtc_state; unsigned long flags; bool wait_for_vblank = true; struct drm_connector *connector; struct drm_connector_state *old_con_state, *new_con_state; struct dm_crtc_state *dm_old_crtc_state, *dm_new_crtc_state; int crtc_disable_count = 0; trace_amdgpu_dm_atomic_commit_tail_begin(state); drm_atomic_helper_update_legacy_modeset_state(dev, state); drm_dp_mst_atomic_wait_for_dependencies(state); dm_state = dm_atomic_get_new_state(state); if (dm_state && dm_state->context) { dc_state = dm_state->context; amdgpu_dm_commit_streams(state, dc_state); } for_each_oldnew_connector_in_state(state, connector, old_con_state, new_con_state, i) { struct dm_connector_state *dm_new_con_state = to_dm_connector_state(new_con_state); struct amdgpu_crtc *acrtc = to_amdgpu_crtc(dm_new_con_state->base.crtc); struct amdgpu_dm_connector *aconnector; if (connector->connector_type == DRM_MODE_CONNECTOR_WRITEBACK) continue; aconnector = to_amdgpu_dm_connector(connector); if (!adev->dm.hdcp_workqueue) continue; pr_debug(""[HDCP_DM] -------------- i : %x ----------\n"", i); if (!connector) continue; pr_debug(""[HDCP_DM] connector->index: %x connect_status: %x dpms: %x\n"", connector->index, connector->status, connector->dpms); pr_debug(""[HDCP_DM] state protection old: %x new: %x\n"", old_con_state->content_protection, new_con_state->content_protection); if (aconnector->dc_sink) { if (aconnector->dc_sink->sink_signal != SIGNAL_TYPE_VIRTUAL && aconnector->dc_sink->sink_signal != SIGNAL_TYPE_NONE) { pr_debug(""[HDCP_DM] pipe_ctx dispname=%s\n"", aconnector->dc_sink->edid_caps.display_name); } } new_crtc_state = NULL; old_crtc_state = NULL; if (acrtc) { new_crtc_state = drm_atomic_get_new_crtc_state(state, &acrtc->base); old_crtc_state = drm_atomic_get_old_crtc_state(state, &acrtc->base); } if (old_crtc_state) pr_debug(""old crtc en: %x a: %x m: %x a-chg: %x c-chg: %x\n"", old_crtc_state->enable, old_crtc_state->active, old_crtc_state->mode_changed, old_crtc_state->active_changed, old_crtc_state->connectors_changed); if (new_crtc_state) pr_debug(""NEW crtc en: %x a: %x m: %x a-chg: %x c-chg: %x\n"", new_crtc_state->enable, new_crtc_state->active, new_crtc_state->mode_changed, new_crtc_state->active_changed, new_crtc_state->connectors_changed); } for_each_oldnew_connector_in_state(state, connector, old_con_state, new_con_state, i) { struct dm_connector_state *dm_new_con_state = to_dm_connector_state(new_con_state); struct amdgpu_crtc *acrtc = to_amdgpu_crtc(dm_new_con_state->base.crtc); struct amdgpu_dm_connector *aconnector = to_amdgpu_dm_connector(connector); if (!adev->dm.hdcp_workqueue) continue; new_crtc_state = NULL; old_crtc_state = NULL; if (acrtc) { new_crtc_state = drm_atomic_get_new_crtc_state(state, &acrtc->base); old_crtc_state = drm_atomic_get_old_crtc_state(state, &acrtc->base); } dm_new_crtc_state = to_dm_crtc_state(new_crtc_state); if (dm_new_crtc_state && dm_new_crtc_state->stream == NULL && connector->state->content_protection == DRM_MODE_CONTENT_PROTECTION_ENABLED) { hdcp_reset_display(adev->dm.hdcp_workqueue, aconnector->dc_link->link_index); new_con_state->content_protection = DRM_MODE_CONTENT_PROTECTION_DESIRED; dm_new_con_state->update_hdcp = true; continue; } if (is_content_protection_different(new_crtc_state, old_crtc_state, new_con_state, old_con_state, connector, adev->dm.hdcp_workqueue)) { bool enable_encryption = false; if (new_con_state->content_protection == DRM_MODE_CONTENT_PROTECTION_DESIRED) enable_encryption = true; if (aconnector->dc_link && aconnector->dc_sink && aconnector->dc_link->type == dc_connection_mst_branch) { struct hdcp_workqueue *hdcp_work = adev->dm.hdcp_workqueue; struct hdcp_workqueue *hdcp_w = &hdcp_work[aconnector->dc_link->link_index]; hdcp_w->hdcp_content_type[connector->index] = new_con_state->hdcp_content_type; hdcp_w->content_protection[connector->index] = new_con_state->content_protection; } if (new_crtc_state && new_crtc_state->mode_changed && new_con_state->content_protection >= DRM_MODE_CONTENT_PROTECTION_DESIRED) enable_encryption = true; DRM_INFO(""[HDCP_DM] hdcp_update_display enable_encryption = %x\n"", enable_encryption); if (aconnector->dc_link) hdcp_update_display( adev->dm.hdcp_workqueue, aconnector->dc_link->link_index, aconnector, new_con_state->hdcp_content_type, enable_encryption); } } for_each_oldnew_connector_in_state(state, connector, old_con_state, new_con_state, i) { struct dm_connector_state *dm_new_con_state = to_dm_connector_state(new_con_state); struct dm_connector_state *dm_old_con_state = to_dm_connector_state(old_con_state); struct amdgpu_crtc *acrtc = to_amdgpu_crtc(dm_new_con_state->base.crtc); struct dc_surface_update *dummy_updates; struct dc_stream_update stream_update; struct dc_info_packet hdr_packet; struct dc_stream_status *status = NULL; bool abm_changed, hdr_changed, scaling_changed; memset(&stream_update, 0, sizeof(stream_update)); if (acrtc) { new_crtc_state = drm_atomic_get_new_crtc_state(state, &acrtc->base); old_crtc_state = drm_atomic_get_old_crtc_state(state, &acrtc->base); } if (!acrtc || drm_atomic_crtc_needs_modeset(new_crtc_state)) continue; dm_new_crtc_state = to_dm_crtc_state(new_crtc_state); dm_old_crtc_state = to_dm_crtc_state(old_crtc_state); scaling_changed = is_scaling_state_different(dm_new_con_state, dm_old_con_state); abm_changed = dm_new_crtc_state->abm_level != dm_old_crtc_state->abm_level; hdr_changed = !drm_connector_atomic_hdr_metadata_equal(old_con_state, new_con_state); if (!scaling_changed && !abm_changed && !hdr_changed) continue; stream_update.stream = dm_new_crtc_state->stream; if (scaling_changed) { update_stream_scaling_settings(&dm_new_con_state->base.crtc->mode, dm_new_con_state, dm_new_crtc_state->stream); stream_update.src = dm_new_crtc_state->stream->src; stream_update.dst = dm_new_crtc_state->stream->dst; } if (abm_changed) { dm_new_crtc_state->stream->abm_level = dm_new_crtc_state->abm_level; stream_update.abm_level = &dm_new_crtc_state->abm_level; } if (hdr_changed) { fill_hdr_info_packet(new_con_state, &hdr_packet); stream_update.hdr_static_metadata = &hdr_packet; } status = dc_stream_get_status(dm_new_crtc_state->stream); if (WARN_ON(!status)) continue; WARN_ON(!status->plane_count); dummy_updates = kzalloc(sizeof(struct dc_surface_update) * MAX_SURFACES, GFP_ATOMIC); if (!dummy_updates) { DRM_ERROR(""Failed to allocate memory for dummy_updates.\n""); continue; } for (j = 0; j < status->plane_count; j++) dummy_updates[j].surface = status->plane_states[0]; sort(dummy_updates, status->plane_count, sizeof(*dummy_updates), dm_plane_layer_index_cmp, NULL); mutex_lock(&dm->dc_lock); dc_exit_ips_for_hw_access(dm->dc); dc_update_planes_and_stream(dm->dc, dummy_updates, status->plane_count, dm_new_crtc_state->stream, &stream_update); mutex_unlock(&dm->dc_lock); kfree(dummy_updates); } for_each_oldnew_crtc_in_state(state, crtc, old_crtc_state, new_crtc_state, i) { struct amdgpu_crtc *acrtc = to_amdgpu_crtc(crtc); #ifdef CONFIG_DEBUG_FS enum amdgpu_dm_pipe_crc_source cur_crc_src; #endif if (old_crtc_state->active && !new_crtc_state->active) crtc_disable_count++; dm_new_crtc_state = to_dm_crtc_state(new_crtc_state); dm_old_crtc_state = to_dm_crtc_state(old_crtc_state); update_stream_irq_parameters(dm, dm_new_crtc_state); #ifdef CONFIG_DEBUG_FS spin_lock_irqsave(&adev_to_drm(adev)->event_lock, flags); cur_crc_src = acrtc->dm_irq_params.crc_src; spin_unlock_irqrestore(&adev_to_drm(adev)->event_lock, flags); #endif if (new_crtc_state->active && (!old_crtc_state->active || drm_atomic_crtc_needs_modeset(new_crtc_state))) { dc_stream_retain(dm_new_crtc_state->stream); acrtc->dm_irq_params.stream = dm_new_crtc_state->stream; manage_dm_interrupts(adev, acrtc, true); } amdgpu_dm_handle_vrr_transition(dm_old_crtc_state, dm_new_crtc_state); #ifdef CONFIG_DEBUG_FS if (new_crtc_state->active && (!old_crtc_state->active || drm_atomic_crtc_needs_modeset(new_crtc_state))) { if (amdgpu_dm_is_valid_crc_source(cur_crc_src)) { #if defined(CONFIG_DRM_AMD_SECURE_DISPLAY) if (amdgpu_dm_crc_window_is_activated(crtc)) { spin_lock_irqsave(&adev_to_drm(adev)->event_lock, flags); acrtc->dm_irq_params.window_param.update_win = true; acrtc->dm_irq_params.window_param.skip_frame_cnt = 2; spin_unlock_irqrestore(&adev_to_drm(adev)->event_lock, flags); } #endif if (amdgpu_dm_crtc_configure_crc_source( crtc, dm_new_crtc_state, cur_crc_src)) drm_dbg_atomic(dev, ""Failed to configure crc source""); } } #endif } for_each_new_crtc_in_state(state, crtc, new_crtc_state, j) if (new_crtc_state->async_flip) wait_for_vblank = false; for_each_new_crtc_in_state(state, crtc, new_crtc_state, j) { dm_new_crtc_state = to_dm_crtc_state(new_crtc_state); if (dm_new_crtc_state->stream) amdgpu_dm_commit_planes(state, dev, dm, crtc, wait_for_vblank); } for_each_new_connector_in_state(state, connector, new_con_state, i) { struct dm_connector_state *dm_new_con_state = to_dm_connector_state(new_con_state); struct amdgpu_crtc *acrtc = to_amdgpu_crtc(dm_new_con_state->base.crtc); if (connector->connector_type != DRM_MODE_CONNECTOR_WRITEBACK) continue; if (!new_con_state->writeback_job) continue; new_crtc_state = drm_atomic_get_new_crtc_state(state, &acrtc->base); if (!new_crtc_state) continue; if (acrtc->wb_enabled) continue; dm_new_crtc_state = to_dm_crtc_state(new_crtc_state); dm_set_writeback(dm, dm_new_crtc_state, connector, new_con_state); acrtc->wb_enabled = true; } amdgpu_dm_commit_audio(dev, state); for (i = 0; i < dm->num_of_edps; i++) { if (dm->backlight_dev[i] && (dm->actual_brightness[i] != dm->brightness[i])) amdgpu_dm_backlight_set_level(dm, i, dm->brightness[i]); } spin_lock_irqsave(&adev_to_drm(adev)->event_lock, flags); for_each_new_crtc_in_state(state, crtc, new_crtc_state, i) { if (new_crtc_state->event) drm_send_event_locked(dev, &new_crtc_state->event->base); new_crtc_state->event = NULL; } spin_unlock_irqrestore(&adev_to_drm(adev)->event_lock, flags); drm_atomic_helper_commit_hw_done(state); if (wait_for_vblank) drm_atomic_helper_wait_for_flip_done(dev, state); drm_atomic_helper_cleanup_planes(dev, state); if (!adev->in_suspend) { if (!adev->mman.keep_stolen_vga_memory) amdgpu_bo_free_kernel(&adev->mman.stolen_vga_memory, NULL, NULL); amdgpu_bo_free_kernel(&adev->mman.stolen_extended_memory, NULL, NULL); } for (i = 0; i < crtc_disable_count; i++) pm_runtime_put_autosuspend(dev->dev); pm_runtime_mark_last_busy(dev->dev); }"
656----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49957/bad/journal.c----ocfs2_journal_shutdown,"void ocfs2_journal_shutdown(struct ocfs2_super *osb) { struct ocfs2_journal *journal = NULL; int status = 0; struct inode *inode = NULL; int num_running_trans = 0; BUG_ON(!osb); journal = osb->journal; if (!journal) goto done; inode = journal->j_inode; if (journal->j_state != OCFS2_JOURNAL_LOADED) goto done; if (!igrab(inode)) BUG(); <S2SV_StartVul> num_running_trans = atomic_read(&(osb->journal->j_num_trans)); <S2SV_EndVul> trace_ocfs2_journal_shutdown(num_running_trans); journal->j_state = OCFS2_JOURNAL_IN_SHUTDOWN; if (osb->commit_task) { trace_ocfs2_journal_shutdown_wait(osb->commit_task); kthread_stop(osb->commit_task); osb->commit_task = NULL; } <S2SV_StartVul> BUG_ON(atomic_read(&(osb->journal->j_num_trans)) != 0); <S2SV_EndVul> <S2SV_StartVul> if (ocfs2_mount_local(osb)) { <S2SV_EndVul> jbd2_journal_lock_updates(journal->j_journal); status = jbd2_journal_flush(journal->j_journal, 0); jbd2_journal_unlock_updates(journal->j_journal); if (status < 0) mlog_errno(status); } if (!jbd2_journal_destroy(journal->j_journal) && !status) { status = ocfs2_journal_toggle_dirty(osb, 0, 0); if (status < 0) mlog_errno(status); } journal->j_journal = NULL; OCFS2_I(inode)->ip_open_count--; ocfs2_inode_unlock(inode, 1); brelse(journal->j_bh); journal->j_bh = NULL; journal->j_state = OCFS2_JOURNAL_FREE; done: iput(inode); kfree(journal); osb->journal = NULL; }","- num_running_trans = atomic_read(&(osb->journal->j_num_trans));
- BUG_ON(atomic_read(&(osb->journal->j_num_trans)) != 0);
- if (ocfs2_mount_local(osb)) {
+ num_running_trans = atomic_read(&(journal->j_num_trans));
+ BUG_ON(atomic_read(&(journal->j_num_trans)) != 0);
+ if (ocfs2_mount_local(osb) &&
+ (journal->j_journal->j_flags & JBD2_LOADED)) {","void ocfs2_journal_shutdown(struct ocfs2_super *osb) { struct ocfs2_journal *journal = NULL; int status = 0; struct inode *inode = NULL; int num_running_trans = 0; BUG_ON(!osb); journal = osb->journal; if (!journal) goto done; inode = journal->j_inode; if (journal->j_state != OCFS2_JOURNAL_LOADED) goto done; if (!igrab(inode)) BUG(); num_running_trans = atomic_read(&(journal->j_num_trans)); trace_ocfs2_journal_shutdown(num_running_trans); journal->j_state = OCFS2_JOURNAL_IN_SHUTDOWN; if (osb->commit_task) { trace_ocfs2_journal_shutdown_wait(osb->commit_task); kthread_stop(osb->commit_task); osb->commit_task = NULL; } BUG_ON(atomic_read(&(journal->j_num_trans)) != 0); if (ocfs2_mount_local(osb) && (journal->j_journal->j_flags & JBD2_LOADED)) { jbd2_journal_lock_updates(journal->j_journal); status = jbd2_journal_flush(journal->j_journal, 0); jbd2_journal_unlock_updates(journal->j_journal); if (status < 0) mlog_errno(status); } if (!jbd2_journal_destroy(journal->j_journal) && !status) { status = ocfs2_journal_toggle_dirty(osb, 0, 0); if (status < 0) mlog_errno(status); } journal->j_journal = NULL; OCFS2_I(inode)->ip_open_count--; ocfs2_inode_unlock(inode, 1); brelse(journal->j_bh); journal->j_bh = NULL; journal->j_state = OCFS2_JOURNAL_FREE; done: iput(inode); kfree(journal); osb->journal = NULL; }"
1423----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56751/bad/route.c----rt6_remove_exception,"static void rt6_remove_exception(struct rt6_exception_bucket *bucket, struct rt6_exception *rt6_ex) { <S2SV_StartVul> struct fib6_info *from; <S2SV_EndVul> struct net *net; if (!bucket || !rt6_ex) return; net = dev_net(rt6_ex->rt6i->dst.dev); net->ipv6.rt6_stats->fib_rt_cache--; <S2SV_StartVul> from = unrcu_pointer(xchg(&rt6_ex->rt6i->from, NULL)); <S2SV_EndVul> <S2SV_StartVul> fib6_info_release(from); <S2SV_EndVul> dst_dev_put(&rt6_ex->rt6i->dst); hlist_del_rcu(&rt6_ex->hlist); dst_release(&rt6_ex->rt6i->dst); kfree_rcu(rt6_ex, rcu); WARN_ON_ONCE(!bucket->depth); bucket->depth--; }","- struct fib6_info *from;
- from = unrcu_pointer(xchg(&rt6_ex->rt6i->from, NULL));
- fib6_info_release(from);","static void rt6_remove_exception(struct rt6_exception_bucket *bucket, struct rt6_exception *rt6_ex) { struct net *net; if (!bucket || !rt6_ex) return; net = dev_net(rt6_ex->rt6i->dst.dev); net->ipv6.rt6_stats->fib_rt_cache--; dst_dev_put(&rt6_ex->rt6i->dst); hlist_del_rcu(&rt6_ex->hlist); dst_release(&rt6_ex->rt6i->dst); kfree_rcu(rt6_ex, rcu); WARN_ON_ONCE(!bucket->depth); bucket->depth--; }"
1520----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2025-21675/bad/port_sel.c----mlx5_lag_port_sel_create,"int mlx5_lag_port_sel_create(struct mlx5_lag *ldev, enum netdev_lag_hash hash_type, u8 *ports) { struct mlx5_lag_port_sel *port_sel = &ldev->port_sel; int err; set_tt_map(port_sel, hash_type); err = mlx5_lag_create_definers(ldev, hash_type, ports); if (err) <S2SV_StartVul> return err; <S2SV_EndVul> if (port_sel->tunnel) { err = mlx5_lag_create_inner_ttc_table(ldev); if (err) goto destroy_definers; } err = mlx5_lag_create_ttc_table(ldev); if (err) goto destroy_inner; return 0; destroy_inner: if (port_sel->tunnel) mlx5_destroy_ttc_table(port_sel->inner.ttc); destroy_definers: mlx5_lag_destroy_definers(ldev); <S2SV_StartVul> return err; <S2SV_EndVul> }","- return err;
- return err;
+ goto clear_port_sel;
+ clear_port_sel:
+ memset(port_sel, 0, sizeof(*port_sel));","int mlx5_lag_port_sel_create(struct mlx5_lag *ldev, enum netdev_lag_hash hash_type, u8 *ports) { struct mlx5_lag_port_sel *port_sel = &ldev->port_sel; int err; set_tt_map(port_sel, hash_type); err = mlx5_lag_create_definers(ldev, hash_type, ports); if (err) goto clear_port_sel; if (port_sel->tunnel) { err = mlx5_lag_create_inner_ttc_table(ldev); if (err) goto destroy_definers; } err = mlx5_lag_create_ttc_table(ldev); if (err) goto destroy_inner; return 0; destroy_inner: if (port_sel->tunnel) mlx5_destroy_ttc_table(port_sel->inner.ttc); destroy_definers: mlx5_lag_destroy_definers(ldev); clear_port_sel: memset(port_sel, 0, sizeof(*port_sel)); return err; }"
59----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44943/bad/gup.c----gup_huge_pmd,"static int gup_huge_pmd(pmd_t orig, pmd_t *pmdp, unsigned long addr, unsigned long end, unsigned int flags, struct page **pages, int *nr) { struct page *page; struct folio *folio; int refs; if (!pmd_access_permitted(orig, flags & FOLL_WRITE)) return 0; if (pmd_devmap(orig)) { if (unlikely(flags & FOLL_LONGTERM)) return 0; return __gup_device_huge_pmd(orig, pmdp, addr, end, flags, pages, nr); } page = nth_page(pmd_page(orig), (addr & ~PMD_MASK) >> PAGE_SHIFT); refs = record_subpages(page, addr, end, pages + *nr); <S2SV_StartVul> folio = try_grab_folio(page, refs, flags); <S2SV_EndVul> <S2SV_StartVul> if (!folio) <S2SV_EndVul> return 0; if (unlikely(pmd_val(orig) != pmd_val(*pmdp))) { gup_put_folio(folio, refs, flags); return 0; } if (!folio_fast_pin_allowed(folio, flags)) { gup_put_folio(folio, refs, flags); return 0; } if (!pmd_write(orig) && gup_must_unshare(NULL, flags, &folio->page)) { gup_put_folio(folio, refs, flags); return 0; } *nr += refs; folio_set_referenced(folio); return 1; }","- folio = try_grab_folio(page, refs, flags);
- if (!folio)
+ folio = try_grab_folio_fast(page, refs, flags);
+ if (!folio)","static int gup_huge_pmd(pmd_t orig, pmd_t *pmdp, unsigned long addr, unsigned long end, unsigned int flags, struct page **pages, int *nr) { struct page *page; struct folio *folio; int refs; if (!pmd_access_permitted(orig, flags & FOLL_WRITE)) return 0; if (pmd_devmap(orig)) { if (unlikely(flags & FOLL_LONGTERM)) return 0; return __gup_device_huge_pmd(orig, pmdp, addr, end, flags, pages, nr); } page = nth_page(pmd_page(orig), (addr & ~PMD_MASK) >> PAGE_SHIFT); refs = record_subpages(page, addr, end, pages + *nr); folio = try_grab_folio_fast(page, refs, flags); if (!folio) return 0; if (unlikely(pmd_val(orig) != pmd_val(*pmdp))) { gup_put_folio(folio, refs, flags); return 0; } if (!folio_fast_pin_allowed(folio, flags)) { gup_put_folio(folio, refs, flags); return 0; } if (!pmd_write(orig) && gup_must_unshare(NULL, flags, &folio->page)) { gup_put_folio(folio, refs, flags); return 0; } *nr += refs; folio_set_referenced(folio); return 1; }"
617----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49923/bad/dcn20_resource.c----dcn20_fast_validate_bw,"bool dcn20_fast_validate_bw( struct dc *dc, struct dc_state *context, display_e2e_pipe_params_st *pipes, int *pipe_cnt_out, int *pipe_split_from, int *vlevel_out, bool fast_validate) { bool out = false; int split[MAX_PIPES] = { 0 }; int pipe_cnt, i, pipe_idx, vlevel; ASSERT(pipes); if (!pipes) return false; dcn20_merge_pipes_for_validate(dc, context); DC_FP_START(); pipe_cnt = dc->res_pool->funcs->populate_dml_pipes(dc, context, pipes, fast_validate); DC_FP_END(); *pipe_cnt_out = pipe_cnt; if (!pipe_cnt) { out = true; goto validate_out; } vlevel = dml_get_voltage_level(&context->bw_ctx.dml, pipes, pipe_cnt); if (vlevel > context->bw_ctx.dml.soc.num_states) goto validate_fail; <S2SV_StartVul> vlevel = dcn20_validate_apply_pipe_split_flags(dc, context, vlevel, split, NULL); <S2SV_EndVul> for (i = 0; i < MAX_PIPES; i++) pipe_split_from[i] = -1; for (i = 0, pipe_idx = -1; i < dc->res_pool->pipe_count; i++) { struct pipe_ctx *pipe = &context->res_ctx.pipe_ctx[i]; struct pipe_ctx *hsplit_pipe = pipe->bottom_pipe; if (!pipe->stream || pipe_split_from[i] >= 0) continue; pipe_idx++; if (!pipe->top_pipe && !pipe->plane_state && context->bw_ctx.dml.vba.ODMCombineEnabled[pipe_idx]) { hsplit_pipe = dcn20_find_secondary_pipe(dc, &context->res_ctx, dc->res_pool, pipe); ASSERT(hsplit_pipe); if (!dcn20_split_stream_for_odm( dc, &context->res_ctx, pipe, hsplit_pipe)) goto validate_fail; pipe_split_from[hsplit_pipe->pipe_idx] = pipe_idx; dcn20_build_mapped_resource(dc, context, pipe->stream); } if (!pipe->plane_state) continue; if (pipe->top_pipe && pipe->plane_state == pipe->top_pipe->plane_state) continue; if (hsplit_pipe && hsplit_pipe->plane_state != pipe->plane_state && context->bw_ctx.dml.vba.ODMCombineEnabled[pipe_idx]) goto validate_fail; if (split[i] == 2) { if (!hsplit_pipe || hsplit_pipe->plane_state != pipe->plane_state) { hsplit_pipe = dcn20_find_secondary_pipe(dc, &context->res_ctx, dc->res_pool, pipe); ASSERT(hsplit_pipe); if (!hsplit_pipe) { DC_FP_START(); dcn20_fpu_adjust_dppclk(&context->bw_ctx.dml.vba, vlevel, context->bw_ctx.dml.vba.maxMpcComb, pipe_idx, true); DC_FP_END(); continue; } if (context->bw_ctx.dml.vba.ODMCombineEnabled[pipe_idx]) { if (!dcn20_split_stream_for_odm( dc, &context->res_ctx, pipe, hsplit_pipe)) goto validate_fail; dcn20_build_mapped_resource(dc, context, pipe->stream); } else { dcn20_split_stream_for_mpc( &context->res_ctx, dc->res_pool, pipe, hsplit_pipe); resource_build_scaling_params(pipe); resource_build_scaling_params(hsplit_pipe); } pipe_split_from[hsplit_pipe->pipe_idx] = pipe_idx; } } else if (hsplit_pipe && hsplit_pipe->plane_state == pipe->plane_state) { ASSERT(0); } } if (!dcn20_validate_dsc(dc, context)) { context->bw_ctx.dml.vba.ValidationStatus[context->bw_ctx.dml.vba.soc.num_states] = DML_FAIL_DSC_VALIDATION_FAILURE; goto validate_fail; } *vlevel_out = vlevel; out = true; goto validate_out; validate_fail: out = false; validate_out: return out; }","- vlevel = dcn20_validate_apply_pipe_split_flags(dc, context, vlevel, split, NULL);
+ bool merge[MAX_PIPES] = { false };
+ vlevel = dcn20_validate_apply_pipe_split_flags(dc, context, vlevel, split, merge);","bool dcn20_fast_validate_bw( struct dc *dc, struct dc_state *context, display_e2e_pipe_params_st *pipes, int *pipe_cnt_out, int *pipe_split_from, int *vlevel_out, bool fast_validate) { bool out = false; int split[MAX_PIPES] = { 0 }; bool merge[MAX_PIPES] = { false }; int pipe_cnt, i, pipe_idx, vlevel; ASSERT(pipes); if (!pipes) return false; dcn20_merge_pipes_for_validate(dc, context); DC_FP_START(); pipe_cnt = dc->res_pool->funcs->populate_dml_pipes(dc, context, pipes, fast_validate); DC_FP_END(); *pipe_cnt_out = pipe_cnt; if (!pipe_cnt) { out = true; goto validate_out; } vlevel = dml_get_voltage_level(&context->bw_ctx.dml, pipes, pipe_cnt); if (vlevel > context->bw_ctx.dml.soc.num_states) goto validate_fail; vlevel = dcn20_validate_apply_pipe_split_flags(dc, context, vlevel, split, merge); for (i = 0; i < MAX_PIPES; i++) pipe_split_from[i] = -1; for (i = 0, pipe_idx = -1; i < dc->res_pool->pipe_count; i++) { struct pipe_ctx *pipe = &context->res_ctx.pipe_ctx[i]; struct pipe_ctx *hsplit_pipe = pipe->bottom_pipe; if (!pipe->stream || pipe_split_from[i] >= 0) continue; pipe_idx++; if (!pipe->top_pipe && !pipe->plane_state && context->bw_ctx.dml.vba.ODMCombineEnabled[pipe_idx]) { hsplit_pipe = dcn20_find_secondary_pipe(dc, &context->res_ctx, dc->res_pool, pipe); ASSERT(hsplit_pipe); if (!dcn20_split_stream_for_odm( dc, &context->res_ctx, pipe, hsplit_pipe)) goto validate_fail; pipe_split_from[hsplit_pipe->pipe_idx] = pipe_idx; dcn20_build_mapped_resource(dc, context, pipe->stream); } if (!pipe->plane_state) continue; if (pipe->top_pipe && pipe->plane_state == pipe->top_pipe->plane_state) continue; if (hsplit_pipe && hsplit_pipe->plane_state != pipe->plane_state && context->bw_ctx.dml.vba.ODMCombineEnabled[pipe_idx]) goto validate_fail; if (split[i] == 2) { if (!hsplit_pipe || hsplit_pipe->plane_state != pipe->plane_state) { hsplit_pipe = dcn20_find_secondary_pipe(dc, &context->res_ctx, dc->res_pool, pipe); ASSERT(hsplit_pipe); if (!hsplit_pipe) { DC_FP_START(); dcn20_fpu_adjust_dppclk(&context->bw_ctx.dml.vba, vlevel, context->bw_ctx.dml.vba.maxMpcComb, pipe_idx, true); DC_FP_END(); continue; } if (context->bw_ctx.dml.vba.ODMCombineEnabled[pipe_idx]) { if (!dcn20_split_stream_for_odm( dc, &context->res_ctx, pipe, hsplit_pipe)) goto validate_fail; dcn20_build_mapped_resource(dc, context, pipe->stream); } else { dcn20_split_stream_for_mpc( &context->res_ctx, dc->res_pool, pipe, hsplit_pipe); resource_build_scaling_params(pipe); resource_build_scaling_params(hsplit_pipe); } pipe_split_from[hsplit_pipe->pipe_idx] = pipe_idx; } } else if (hsplit_pipe && hsplit_pipe->plane_state == pipe->plane_state) { ASSERT(0); } } if (!dcn20_validate_dsc(dc, context)) { context->bw_ctx.dml.vba.ValidationStatus[context->bw_ctx.dml.vba.soc.num_states] = DML_FAIL_DSC_VALIDATION_FAILURE; goto validate_fail; } *vlevel_out = vlevel; out = true; goto validate_out; validate_fail: out = false; validate_out: return out; }"
280----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46759/bad/adc128d818.c----adc128_temp_store,"static ssize_t adc128_temp_store(struct device *dev, struct device_attribute *attr, const char *buf, size_t count) { struct adc128_data *data = dev_get_drvdata(dev); int index = to_sensor_dev_attr(attr)->index; long val; int err; s8 regval; err = kstrtol(buf, 10, &val); if (err < 0) return err; mutex_lock(&data->update_lock); <S2SV_StartVul> regval = clamp_val(DIV_ROUND_CLOSEST(val, 1000), -128, 127); <S2SV_EndVul> data->temp[index] = regval << 1; i2c_smbus_write_byte_data(data->client, index == 1 ? ADC128_REG_TEMP_MAX : ADC128_REG_TEMP_HYST, regval); mutex_unlock(&data->update_lock); return count; }","- regval = clamp_val(DIV_ROUND_CLOSEST(val, 1000), -128, 127);
+ regval = DIV_ROUND_CLOSEST(clamp_val(val, -128000, 127000), 1000);","static ssize_t adc128_temp_store(struct device *dev, struct device_attribute *attr, const char *buf, size_t count) { struct adc128_data *data = dev_get_drvdata(dev); int index = to_sensor_dev_attr(attr)->index; long val; int err; s8 regval; err = kstrtol(buf, 10, &val); if (err < 0) return err; mutex_lock(&data->update_lock); regval = DIV_ROUND_CLOSEST(clamp_val(val, -128000, 127000), 1000); data->temp[index] = regval << 1; i2c_smbus_write_byte_data(data->client, index == 1 ? ADC128_REG_TEMP_MAX : ADC128_REG_TEMP_HYST, regval); mutex_unlock(&data->update_lock); return count; }"
271----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46750/bad/pci.c----pci_bus_unlock,"static void pci_bus_unlock(struct pci_bus *bus) { struct pci_dev *dev; list_for_each_entry(dev, &bus->devices, bus_list) { if (dev->subordinate) pci_bus_unlock(dev->subordinate); <S2SV_StartVul> pci_dev_unlock(dev); <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul>","- pci_dev_unlock(dev);
- }
- }
+ else
+ pci_dev_unlock(dev);
+ pci_dev_unlock(bus->self);","static void pci_bus_unlock(struct pci_bus *bus) { struct pci_dev *dev; list_for_each_entry(dev, &bus->devices, bus_list) { if (dev->subordinate) pci_bus_unlock(dev->subordinate); else pci_dev_unlock(dev); } pci_dev_unlock(bus->self); }"
211----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46693/bad/qcom_battmgr.c----qcom_battmgr_probe,"static int qcom_battmgr_probe(struct auxiliary_device *adev, const struct auxiliary_device_id *id) { struct power_supply_config psy_cfg_supply = {}; struct power_supply_config psy_cfg = {}; const struct of_device_id *match; struct qcom_battmgr *battmgr; struct device *dev = &adev->dev; battmgr = devm_kzalloc(dev, sizeof(*battmgr), GFP_KERNEL); if (!battmgr) return -ENOMEM; battmgr->dev = dev; psy_cfg.drv_data = battmgr; psy_cfg.of_node = adev->dev.of_node; psy_cfg_supply.drv_data = battmgr; psy_cfg_supply.of_node = adev->dev.of_node; psy_cfg_supply.supplied_to = qcom_battmgr_battery; psy_cfg_supply.num_supplicants = 1; INIT_WORK(&battmgr->enable_work, qcom_battmgr_enable_worker); mutex_init(&battmgr->lock); init_completion(&battmgr->ack); match = of_match_device(qcom_battmgr_of_variants, dev->parent); if (match) battmgr->variant = (unsigned long)match->data; else battmgr->variant = QCOM_BATTMGR_SM8350; if (battmgr->variant == QCOM_BATTMGR_SC8280XP) { battmgr->bat_psy = devm_power_supply_register(dev, &sc8280xp_bat_psy_desc, &psy_cfg); if (IS_ERR(battmgr->bat_psy)) return dev_err_probe(dev, PTR_ERR(battmgr->bat_psy), ""failed to register battery power supply\n""); battmgr->ac_psy = devm_power_supply_register(dev, &sc8280xp_ac_psy_desc, &psy_cfg_supply); if (IS_ERR(battmgr->ac_psy)) return dev_err_probe(dev, PTR_ERR(battmgr->ac_psy), ""failed to register AC power supply\n""); battmgr->usb_psy = devm_power_supply_register(dev, &sc8280xp_usb_psy_desc, &psy_cfg_supply); if (IS_ERR(battmgr->usb_psy)) return dev_err_probe(dev, PTR_ERR(battmgr->usb_psy), ""failed to register USB power supply\n""); battmgr->wls_psy = devm_power_supply_register(dev, &sc8280xp_wls_psy_desc, &psy_cfg_supply); if (IS_ERR(battmgr->wls_psy)) return dev_err_probe(dev, PTR_ERR(battmgr->wls_psy), ""failed to register wireless charing power supply\n""); } else { battmgr->bat_psy = devm_power_supply_register(dev, &sm8350_bat_psy_desc, &psy_cfg); if (IS_ERR(battmgr->bat_psy)) return dev_err_probe(dev, PTR_ERR(battmgr->bat_psy), ""failed to register battery power supply\n""); battmgr->usb_psy = devm_power_supply_register(dev, &sm8350_usb_psy_desc, &psy_cfg_supply); if (IS_ERR(battmgr->usb_psy)) return dev_err_probe(dev, PTR_ERR(battmgr->usb_psy), ""failed to register USB power supply\n""); battmgr->wls_psy = devm_power_supply_register(dev, &sm8350_wls_psy_desc, &psy_cfg_supply); if (IS_ERR(battmgr->wls_psy)) return dev_err_probe(dev, PTR_ERR(battmgr->wls_psy), ""failed to register wireless charing power supply\n""); } <S2SV_StartVul> battmgr->client = devm_pmic_glink_register_client(dev, <S2SV_EndVul> <S2SV_StartVul> PMIC_GLINK_OWNER_BATTMGR, <S2SV_EndVul> <S2SV_StartVul> qcom_battmgr_callback, <S2SV_EndVul> <S2SV_StartVul> qcom_battmgr_pdr_notify, <S2SV_EndVul> <S2SV_StartVul> battmgr); <S2SV_EndVul> <S2SV_StartVul> return PTR_ERR_OR_ZERO(battmgr->client); <S2SV_EndVul> }","- battmgr->client = devm_pmic_glink_register_client(dev,
- PMIC_GLINK_OWNER_BATTMGR,
- qcom_battmgr_callback,
- qcom_battmgr_pdr_notify,
- battmgr);
- return PTR_ERR_OR_ZERO(battmgr->client);
+ battmgr->client = devm_pmic_glink_client_alloc(dev, PMIC_GLINK_OWNER_BATTMGR,
+ qcom_battmgr_callback,
+ qcom_battmgr_pdr_notify,
+ battmgr);
+ if (IS_ERR(battmgr->client))
+ return PTR_ERR(battmgr->client);
+ pmic_glink_client_register(battmgr->client);
+ return 0;","static int qcom_battmgr_probe(struct auxiliary_device *adev, const struct auxiliary_device_id *id) { struct power_supply_config psy_cfg_supply = {}; struct power_supply_config psy_cfg = {}; const struct of_device_id *match; struct qcom_battmgr *battmgr; struct device *dev = &adev->dev; battmgr = devm_kzalloc(dev, sizeof(*battmgr), GFP_KERNEL); if (!battmgr) return -ENOMEM; battmgr->dev = dev; psy_cfg.drv_data = battmgr; psy_cfg.of_node = adev->dev.of_node; psy_cfg_supply.drv_data = battmgr; psy_cfg_supply.of_node = adev->dev.of_node; psy_cfg_supply.supplied_to = qcom_battmgr_battery; psy_cfg_supply.num_supplicants = 1; INIT_WORK(&battmgr->enable_work, qcom_battmgr_enable_worker); mutex_init(&battmgr->lock); init_completion(&battmgr->ack); match = of_match_device(qcom_battmgr_of_variants, dev->parent); if (match) battmgr->variant = (unsigned long)match->data; else battmgr->variant = QCOM_BATTMGR_SM8350; if (battmgr->variant == QCOM_BATTMGR_SC8280XP) { battmgr->bat_psy = devm_power_supply_register(dev, &sc8280xp_bat_psy_desc, &psy_cfg); if (IS_ERR(battmgr->bat_psy)) return dev_err_probe(dev, PTR_ERR(battmgr->bat_psy), ""failed to register battery power supply\n""); battmgr->ac_psy = devm_power_supply_register(dev, &sc8280xp_ac_psy_desc, &psy_cfg_supply); if (IS_ERR(battmgr->ac_psy)) return dev_err_probe(dev, PTR_ERR(battmgr->ac_psy), ""failed to register AC power supply\n""); battmgr->usb_psy = devm_power_supply_register(dev, &sc8280xp_usb_psy_desc, &psy_cfg_supply); if (IS_ERR(battmgr->usb_psy)) return dev_err_probe(dev, PTR_ERR(battmgr->usb_psy), ""failed to register USB power supply\n""); battmgr->wls_psy = devm_power_supply_register(dev, &sc8280xp_wls_psy_desc, &psy_cfg_supply); if (IS_ERR(battmgr->wls_psy)) return dev_err_probe(dev, PTR_ERR(battmgr->wls_psy), ""failed to register wireless charing power supply\n""); } else { battmgr->bat_psy = devm_power_supply_register(dev, &sm8350_bat_psy_desc, &psy_cfg); if (IS_ERR(battmgr->bat_psy)) return dev_err_probe(dev, PTR_ERR(battmgr->bat_psy), ""failed to register battery power supply\n""); battmgr->usb_psy = devm_power_supply_register(dev, &sm8350_usb_psy_desc, &psy_cfg_supply); if (IS_ERR(battmgr->usb_psy)) return dev_err_probe(dev, PTR_ERR(battmgr->usb_psy), ""failed to register USB power supply\n""); battmgr->wls_psy = devm_power_supply_register(dev, &sm8350_wls_psy_desc, &psy_cfg_supply); if (IS_ERR(battmgr->wls_psy)) return dev_err_probe(dev, PTR_ERR(battmgr->wls_psy), ""failed to register wireless charing power supply\n""); } battmgr->client = devm_pmic_glink_client_alloc(dev, PMIC_GLINK_OWNER_BATTMGR, qcom_battmgr_callback, qcom_battmgr_pdr_notify, battmgr); if (IS_ERR(battmgr->client)) return PTR_ERR(battmgr->client); pmic_glink_client_register(battmgr->client); return 0; }"
109----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44967/bad/mgag200_i2c.c----mgag200_i2c_init,"int mgag200_i2c_init(struct mga_device *mdev, struct mga_i2c_chan *i2c) { struct drm_device *dev = &mdev->base; const struct mgag200_device_info *info = mdev->info; int ret; WREG_DAC(MGA1064_GEN_IO_CTL2, 1); WREG_DAC(MGA1064_GEN_IO_DATA, 0xff); WREG_DAC(MGA1064_GEN_IO_CTL, 0); i2c->data = BIT(info->i2c.data_bit); i2c->clock = BIT(info->i2c.clock_bit); i2c->adapter.owner = THIS_MODULE; i2c->adapter.class = I2C_CLASS_DDC; i2c->adapter.dev.parent = dev->dev; i2c->dev = dev; i2c_set_adapdata(&i2c->adapter, i2c); snprintf(i2c->adapter.name, sizeof(i2c->adapter.name), ""mga i2c""); i2c->adapter.algo_data = &i2c->bit; i2c->bit.udelay = 10; i2c->bit.timeout = usecs_to_jiffies(2200); i2c->bit.data = i2c; i2c->bit.setsda = mga_gpio_setsda; i2c->bit.setscl = mga_gpio_setscl; i2c->bit.getsda = mga_gpio_getsda; i2c->bit.getscl = mga_gpio_getscl; ret = i2c_bit_add_bus(&i2c->adapter); if (ret) return ret; <S2SV_StartVul> return devm_add_action_or_reset(dev->dev, mgag200_i2c_release, i2c); <S2SV_EndVul> }","- return devm_add_action_or_reset(dev->dev, mgag200_i2c_release, i2c);
+ return drmm_add_action_or_reset(dev, mgag200_i2c_release, i2c);","int mgag200_i2c_init(struct mga_device *mdev, struct mga_i2c_chan *i2c) { struct drm_device *dev = &mdev->base; const struct mgag200_device_info *info = mdev->info; int ret; WREG_DAC(MGA1064_GEN_IO_CTL2, 1); WREG_DAC(MGA1064_GEN_IO_DATA, 0xff); WREG_DAC(MGA1064_GEN_IO_CTL, 0); i2c->data = BIT(info->i2c.data_bit); i2c->clock = BIT(info->i2c.clock_bit); i2c->adapter.owner = THIS_MODULE; i2c->adapter.class = I2C_CLASS_DDC; i2c->adapter.dev.parent = dev->dev; i2c->dev = dev; i2c_set_adapdata(&i2c->adapter, i2c); snprintf(i2c->adapter.name, sizeof(i2c->adapter.name), ""mga i2c""); i2c->adapter.algo_data = &i2c->bit; i2c->bit.udelay = 10; i2c->bit.timeout = usecs_to_jiffies(2200); i2c->bit.data = i2c; i2c->bit.setsda = mga_gpio_setsda; i2c->bit.setscl = mga_gpio_setscl; i2c->bit.getsda = mga_gpio_getsda; i2c->bit.getscl = mga_gpio_getscl; ret = i2c_bit_add_bus(&i2c->adapter); if (ret) return ret; return drmm_add_action_or_reset(dev, mgag200_i2c_release, i2c); }"
273----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46756/bad/w83627ehf.c----store_target_temp,"store_target_temp(struct device *dev, struct device_attribute *attr, const char *buf, size_t count) { struct w83627ehf_data *data = dev_get_drvdata(dev); struct sensor_device_attribute *sensor_attr = to_sensor_dev_attr(attr); int nr = sensor_attr->index; long val; int err; err = kstrtol(buf, 10, &val); if (err < 0) return err; <S2SV_StartVul> val = clamp_val(DIV_ROUND_CLOSEST(val, 1000), 0, 127); <S2SV_EndVul> mutex_lock(&data->update_lock); data->target_temp[nr] = val; w83627ehf_write_value(data, W83627EHF_REG_TARGET[nr], val); mutex_unlock(&data->update_lock); return count; }","- val = clamp_val(DIV_ROUND_CLOSEST(val, 1000), 0, 127);
+ val = DIV_ROUND_CLOSEST(clamp_val(val, 0, 127000), 1000);","store_target_temp(struct device *dev, struct device_attribute *attr, const char *buf, size_t count) { struct w83627ehf_data *data = dev_get_drvdata(dev); struct sensor_device_attribute *sensor_attr = to_sensor_dev_attr(attr); int nr = sensor_attr->index; long val; int err; err = kstrtol(buf, 10, &val); if (err < 0) return err; val = DIV_ROUND_CLOSEST(clamp_val(val, 0, 127000), 1000); mutex_lock(&data->update_lock); data->target_temp[nr] = val; w83627ehf_write_value(data, W83627EHF_REG_TARGET[nr], val); mutex_unlock(&data->update_lock); return count; }"
282----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46760/bad/usb.c----rtw_usb_probe,"int rtw_usb_probe(struct usb_interface *intf, const struct usb_device_id *id) { struct rtw_dev *rtwdev; struct ieee80211_hw *hw; struct rtw_usb *rtwusb; int drv_data_size; int ret; drv_data_size = sizeof(struct rtw_dev) + sizeof(struct rtw_usb); hw = ieee80211_alloc_hw(drv_data_size, &rtw_ops); if (!hw) return -ENOMEM; rtwdev = hw->priv; rtwdev->hw = hw; rtwdev->dev = &intf->dev; rtwdev->chip = (struct rtw_chip_info *)id->driver_info; rtwdev->hci.ops = &rtw_usb_ops; rtwdev->hci.type = RTW_HCI_TYPE_USB; rtwusb = rtw_get_usb_priv(rtwdev); rtwusb->rtwdev = rtwdev; ret = rtw_usb_alloc_rx_bufs(rtwusb); if (ret) goto err_release_hw; ret = rtw_core_init(rtwdev); if (ret) goto err_free_rx_bufs; ret = rtw_usb_intf_init(rtwdev, intf); if (ret) { rtw_err(rtwdev, ""failed to init USB interface\n""); goto err_deinit_core; } ret = rtw_usb_init_tx(rtwdev); if (ret) { rtw_err(rtwdev, ""failed to init USB TX\n""); goto err_destroy_usb; } ret = rtw_usb_init_rx(rtwdev); if (ret) { rtw_err(rtwdev, ""failed to init USB RX\n""); goto err_destroy_txwq; } ret = rtw_chip_info_setup(rtwdev); if (ret) { rtw_err(rtwdev, ""failed to setup chip information\n""); goto err_destroy_rxwq; } ret = rtw_register_hw(rtwdev, rtwdev->hw); if (ret) { rtw_err(rtwdev, ""failed to register hw\n""); goto err_destroy_rxwq; } <S2SV_StartVul> return 0; <S2SV_EndVul> err_destroy_rxwq: rtw_usb_deinit_rx(rtwdev); err_destroy_txwq: rtw_usb_deinit_tx(rtwdev); err_destroy_usb: rtw_usb_intf_deinit(rtwdev, intf); err_deinit_core: rtw_core_deinit(rtwdev); err_free_rx_bufs: rtw_usb_free_rx_bufs(rtwusb); err_release_hw: ieee80211_free_hw(hw); return ret; }","- return 0;
+ }
+ rtw_usb_setup_rx(rtwdev);
+ return 0;","int rtw_usb_probe(struct usb_interface *intf, const struct usb_device_id *id) { struct rtw_dev *rtwdev; struct ieee80211_hw *hw; struct rtw_usb *rtwusb; int drv_data_size; int ret; drv_data_size = sizeof(struct rtw_dev) + sizeof(struct rtw_usb); hw = ieee80211_alloc_hw(drv_data_size, &rtw_ops); if (!hw) return -ENOMEM; rtwdev = hw->priv; rtwdev->hw = hw; rtwdev->dev = &intf->dev; rtwdev->chip = (struct rtw_chip_info *)id->driver_info; rtwdev->hci.ops = &rtw_usb_ops; rtwdev->hci.type = RTW_HCI_TYPE_USB; rtwusb = rtw_get_usb_priv(rtwdev); rtwusb->rtwdev = rtwdev; ret = rtw_usb_alloc_rx_bufs(rtwusb); if (ret) goto err_release_hw; ret = rtw_core_init(rtwdev); if (ret) goto err_free_rx_bufs; ret = rtw_usb_intf_init(rtwdev, intf); if (ret) { rtw_err(rtwdev, ""failed to init USB interface\n""); goto err_deinit_core; } ret = rtw_usb_init_tx(rtwdev); if (ret) { rtw_err(rtwdev, ""failed to init USB TX\n""); goto err_destroy_usb; } ret = rtw_usb_init_rx(rtwdev); if (ret) { rtw_err(rtwdev, ""failed to init USB RX\n""); goto err_destroy_txwq; } ret = rtw_chip_info_setup(rtwdev); if (ret) { rtw_err(rtwdev, ""failed to setup chip information\n""); goto err_destroy_rxwq; } ret = rtw_register_hw(rtwdev, rtwdev->hw); if (ret) { rtw_err(rtwdev, ""failed to register hw\n""); goto err_destroy_rxwq; } rtw_usb_setup_rx(rtwdev); return 0; err_destroy_rxwq: rtw_usb_deinit_rx(rtwdev); err_destroy_txwq: rtw_usb_deinit_tx(rtwdev); err_destroy_usb: rtw_usb_intf_deinit(rtwdev, intf); err_deinit_core: rtw_core_deinit(rtwdev); err_free_rx_bufs: rtw_usb_free_rx_bufs(rtwusb); err_release_hw: ieee80211_free_hw(hw); return ret; }"
902----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50149/bad/xe_guc_submit.c----guc_exec_queue_timedout_job,"guc_exec_queue_timedout_job(struct drm_sched_job *drm_job) { struct xe_sched_job *job = to_xe_sched_job(drm_job); struct xe_sched_job *tmp_job; struct xe_exec_queue *q = job->q; struct xe_gpu_scheduler *sched = &q->guc->sched; struct xe_guc *guc = exec_queue_to_guc(q); const char *process_name = ""no process""; int err = -ETIME; pid_t pid = -1; int i = 0; bool wedged, skip_timeout_check; if (test_bit(DMA_FENCE_FLAG_SIGNALED_BIT, &job->fence->flags)) { <S2SV_StartVul> guc_exec_queue_free_job(drm_job); <S2SV_EndVul> return DRM_GPU_SCHED_STAT_NOMINAL; } xe_sched_submission_stop(sched); skip_timeout_check = exec_queue_reset(q) || exec_queue_killed_or_banned_or_wedged(q) || exec_queue_destroyed(q); if (!skip_timeout_check && !xe_sched_job_started(job)) goto rearm; wedged = guc_submit_hint_wedged(exec_queue_to_guc(q)); if (!wedged && exec_queue_registered(q)) { int ret; if (exec_queue_reset(q)) err = -EIO; if (!exec_queue_destroyed(q)) { ret = wait_event_timeout(guc->ct.wq, !exec_queue_pending_enable(q) || guc_read_stopped(guc), HZ * 5); if (!ret || guc_read_stopped(guc)) goto trigger_reset; set_exec_queue_check_timeout(q); disable_scheduling(q, skip_timeout_check); } smp_rmb(); ret = wait_event_timeout(guc->ct.wq, !exec_queue_pending_disable(q) || guc_read_stopped(guc), HZ * 5); if (!ret || guc_read_stopped(guc)) { trigger_reset: if (!ret) xe_gt_warn(guc_to_gt(guc), ""Schedule disable failed to respond""); set_exec_queue_extra_ref(q); xe_exec_queue_get(q); set_exec_queue_banned(q); xe_gt_reset_async(q->gt); xe_sched_tdr_queue_imm(sched); goto rearm; } } if (!wedged && !skip_timeout_check && !check_timeout(q, job) && !exec_queue_reset(q) && exec_queue_registered(q)) { clear_exec_queue_check_timeout(q); goto sched_enable; } if (q->vm && q->vm->xef) { process_name = q->vm->xef->process_name; pid = q->vm->xef->pid; } xe_gt_notice(guc_to_gt(guc), ""Timedout job: seqno=%u, lrc_seqno=%u, guc_id=%d, flags=0x%lx in %s [%d]"", xe_sched_job_seqno(job), xe_sched_job_lrc_seqno(job), q->guc->id, q->flags, process_name, pid); trace_xe_sched_job_timedout(job); if (!exec_queue_killed(q)) xe_devcoredump(job); xe_gt_WARN(q->gt, q->flags & EXEC_QUEUE_FLAG_KERNEL, ""Kernel-submitted job timed out\n""); xe_gt_WARN(q->gt, q->flags & EXEC_QUEUE_FLAG_VM && !exec_queue_killed(q), ""VM job timed out on non-killed execqueue\n""); if (!wedged && (q->flags & EXEC_QUEUE_FLAG_KERNEL || (q->flags & EXEC_QUEUE_FLAG_VM && !exec_queue_killed(q)))) { if (!xe_sched_invalidate_job(job, 2)) { clear_exec_queue_check_timeout(q); xe_gt_reset_async(q->gt); goto rearm; } } set_exec_queue_banned(q); if (!wedged && exec_queue_registered(q) && !exec_queue_destroyed(q)) { set_exec_queue_extra_ref(q); xe_exec_queue_get(q); __deregister_exec_queue(guc, q); } xe_hw_fence_irq_stop(q->fence_irq); xe_sched_add_pending_job(sched, job); xe_sched_submission_start(sched); xe_guc_exec_queue_trigger_cleanup(q); spin_lock(&sched->base.job_list_lock); list_for_each_entry(tmp_job, &sched->base.pending_list, drm.list) xe_sched_job_set_error(tmp_job, !i++ ? err : -ECANCELED); spin_unlock(&sched->base.job_list_lock); xe_hw_fence_irq_start(q->fence_irq); return DRM_GPU_SCHED_STAT_NOMINAL; sched_enable: enable_scheduling(q); rearm: xe_sched_add_pending_job(sched, job); xe_sched_submission_start(sched); return DRM_GPU_SCHED_STAT_NOMINAL; }","- guc_exec_queue_free_job(drm_job);
+ xe_sched_add_pending_job(sched, job);
+ xe_sched_submission_start(sched);","guc_exec_queue_timedout_job(struct drm_sched_job *drm_job) { struct xe_sched_job *job = to_xe_sched_job(drm_job); struct xe_sched_job *tmp_job; struct xe_exec_queue *q = job->q; struct xe_gpu_scheduler *sched = &q->guc->sched; struct xe_guc *guc = exec_queue_to_guc(q); const char *process_name = ""no process""; int err = -ETIME; pid_t pid = -1; int i = 0; bool wedged, skip_timeout_check; if (test_bit(DMA_FENCE_FLAG_SIGNALED_BIT, &job->fence->flags)) { xe_sched_add_pending_job(sched, job); xe_sched_submission_start(sched); return DRM_GPU_SCHED_STAT_NOMINAL; } xe_sched_submission_stop(sched); skip_timeout_check = exec_queue_reset(q) || exec_queue_killed_or_banned_or_wedged(q) || exec_queue_destroyed(q); if (!skip_timeout_check && !xe_sched_job_started(job)) goto rearm; wedged = guc_submit_hint_wedged(exec_queue_to_guc(q)); if (!wedged && exec_queue_registered(q)) { int ret; if (exec_queue_reset(q)) err = -EIO; if (!exec_queue_destroyed(q)) { ret = wait_event_timeout(guc->ct.wq, !exec_queue_pending_enable(q) || guc_read_stopped(guc), HZ * 5); if (!ret || guc_read_stopped(guc)) goto trigger_reset; set_exec_queue_check_timeout(q); disable_scheduling(q, skip_timeout_check); } smp_rmb(); ret = wait_event_timeout(guc->ct.wq, !exec_queue_pending_disable(q) || guc_read_stopped(guc), HZ * 5); if (!ret || guc_read_stopped(guc)) { trigger_reset: if (!ret) xe_gt_warn(guc_to_gt(guc), ""Schedule disable failed to respond""); set_exec_queue_extra_ref(q); xe_exec_queue_get(q); set_exec_queue_banned(q); xe_gt_reset_async(q->gt); xe_sched_tdr_queue_imm(sched); goto rearm; } } if (!wedged && !skip_timeout_check && !check_timeout(q, job) && !exec_queue_reset(q) && exec_queue_registered(q)) { clear_exec_queue_check_timeout(q); goto sched_enable; } if (q->vm && q->vm->xef) { process_name = q->vm->xef->process_name; pid = q->vm->xef->pid; } xe_gt_notice(guc_to_gt(guc), ""Timedout job: seqno=%u, lrc_seqno=%u, guc_id=%d, flags=0x%lx in %s [%d]"", xe_sched_job_seqno(job), xe_sched_job_lrc_seqno(job), q->guc->id, q->flags, process_name, pid); trace_xe_sched_job_timedout(job); if (!exec_queue_killed(q)) xe_devcoredump(job); xe_gt_WARN(q->gt, q->flags & EXEC_QUEUE_FLAG_KERNEL, ""Kernel-submitted job timed out\n""); xe_gt_WARN(q->gt, q->flags & EXEC_QUEUE_FLAG_VM && !exec_queue_killed(q), ""VM job timed out on non-killed execqueue\n""); if (!wedged && (q->flags & EXEC_QUEUE_FLAG_KERNEL || (q->flags & EXEC_QUEUE_FLAG_VM && !exec_queue_killed(q)))) { if (!xe_sched_invalidate_job(job, 2)) { clear_exec_queue_check_timeout(q); xe_gt_reset_async(q->gt); goto rearm; } } set_exec_queue_banned(q); if (!wedged && exec_queue_registered(q) && !exec_queue_destroyed(q)) { set_exec_queue_extra_ref(q); xe_exec_queue_get(q); __deregister_exec_queue(guc, q); } xe_hw_fence_irq_stop(q->fence_irq); xe_sched_add_pending_job(sched, job); xe_sched_submission_start(sched); xe_guc_exec_queue_trigger_cleanup(q); spin_lock(&sched->base.job_list_lock); list_for_each_entry(tmp_job, &sched->base.pending_list, drm.list) xe_sched_job_set_error(tmp_job, !i++ ? err : -ECANCELED); spin_unlock(&sched->base.job_list_lock); xe_hw_fence_irq_start(q->fence_irq); return DRM_GPU_SCHED_STAT_NOMINAL; sched_enable: enable_scheduling(q); rearm: xe_sched_add_pending_job(sched, job); xe_sched_submission_start(sched); return DRM_GPU_SCHED_STAT_NOMINAL; }"
1366----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56702/bad/btf.c----btf_ctx_access,"bool btf_ctx_access(int off, int size, enum bpf_access_type type, const struct bpf_prog *prog, struct bpf_insn_access_aux *info) { const struct btf_type *t = prog->aux->attach_func_proto; struct bpf_prog *tgt_prog = prog->aux->dst_prog; struct btf *btf = bpf_prog_get_target_btf(prog); const char *tname = prog->aux->attach_func_name; struct bpf_verifier_log *log = info->log; const struct btf_param *args; const char *tag_value; u32 nr_args, arg; int i, ret; if (off % 8) { bpf_log(log, ""func '%s' offset %d is not multiple of 8\n"", tname, off); return false; } arg = get_ctx_arg_idx(btf, t, off); args = (const struct btf_param *)(t + 1); nr_args = t ? btf_type_vlen(t) : MAX_BPF_FUNC_REG_ARGS; if (prog->aux->attach_btf_trace) { args++; nr_args--; } if (arg > nr_args) { bpf_log(log, ""func '%s' doesn't have %d-th argument\n"", tname, arg + 1); return false; } if (arg == nr_args) { switch (prog->expected_attach_type) { case BPF_LSM_MAC: info->is_retval = true; fallthrough; case BPF_LSM_CGROUP: case BPF_TRACE_FEXIT: if (!t) return true; t = btf_type_by_id(btf, t->type); break; case BPF_MODIFY_RETURN: if (!t) return false; t = btf_type_skip_modifiers(btf, t->type, NULL); if (!btf_type_is_small_int(t)) { bpf_log(log, ""ret type %s not allowed for fmod_ret\n"", btf_type_str(t)); return false; } break; default: bpf_log(log, ""func '%s' doesn't have %d-th argument\n"", tname, arg + 1); return false; } } else { if (!t) return true; t = btf_type_by_id(btf, args[arg].type); } while (btf_type_is_modifier(t)) t = btf_type_by_id(btf, t->type); if (btf_type_is_small_int(t) || btf_is_any_enum(t) || __btf_type_is_struct(t)) return true; if (!btf_type_is_ptr(t)) { bpf_log(log, ""func '%s' arg%d '%s' has type %s. Only pointer access is allowed\n"", tname, arg, __btf_name_by_offset(btf, t->name_off), btf_type_str(t)); return false; } for (i = 0; i < prog->aux->ctx_arg_info_size; i++) { const struct bpf_ctx_arg_aux *ctx_arg_info = &prog->aux->ctx_arg_info[i]; u32 type, flag; type = base_type(ctx_arg_info->reg_type); flag = type_flag(ctx_arg_info->reg_type); if (ctx_arg_info->offset == off && type == PTR_TO_BUF && (flag & PTR_MAYBE_NULL)) { info->reg_type = ctx_arg_info->reg_type; return true; } } if (t->type == 0) return true; if (is_int_ptr(btf, t)) return true; for (i = 0; i < prog->aux->ctx_arg_info_size; i++) { const struct bpf_ctx_arg_aux *ctx_arg_info = &prog->aux->ctx_arg_info[i]; if (ctx_arg_info->offset == off) { if (!ctx_arg_info->btf_id) { bpf_log(log,""invalid btf_id for context argument offset %u\n"", off); return false; } info->reg_type = ctx_arg_info->reg_type; info->btf = ctx_arg_info->btf ? : btf_vmlinux; info->btf_id = ctx_arg_info->btf_id; return true; } } info->reg_type = PTR_TO_BTF_ID; if (prog_args_trusted(prog)) info->reg_type |= PTR_TRUSTED; <S2SV_StartVul> if (btf_param_match_suffix(btf, &args[arg], ""__nullable"")) <S2SV_EndVul> info->reg_type |= PTR_MAYBE_NULL; if (tgt_prog) { enum bpf_prog_type tgt_type; if (tgt_prog->type == BPF_PROG_TYPE_EXT) tgt_type = tgt_prog->aux->saved_dst_prog_type; else tgt_type = tgt_prog->type; ret = btf_translate_to_vmlinux(log, btf, t, tgt_type, arg); if (ret > 0) { info->btf = btf_vmlinux; info->btf_id = ret; return true; } else { return false; } } info->btf = btf; info->btf_id = t->type; t = btf_type_by_id(btf, t->type); if (btf_type_is_type_tag(t)) { tag_value = __btf_name_by_offset(btf, t->name_off); if (strcmp(tag_value, ""user"") == 0) info->reg_type |= MEM_USER; if (strcmp(tag_value, ""percpu"") == 0) info->reg_type |= MEM_PERCPU; } while (btf_type_is_modifier(t)) { info->btf_id = t->type; t = btf_type_by_id(btf, t->type); } if (!btf_type_is_struct(t)) { bpf_log(log, ""func '%s' arg%d type %s is not a struct\n"", tname, arg, btf_type_str(t)); return false; } bpf_log(log, ""func '%s' arg%d has btf_id %d type %s '%s'\n"", tname, arg, info->btf_id, btf_type_str(t), __btf_name_by_offset(btf, t->name_off)); return true; }","- if (btf_param_match_suffix(btf, &args[arg], ""__nullable""))
+ if (bpf_prog_is_raw_tp(prog))
+ info->reg_type |= PTR_MAYBE_NULL;
+ else if (btf_param_match_suffix(btf, &args[arg], ""__nullable""))
+ info->reg_type |= PTR_MAYBE_NULL;","bool btf_ctx_access(int off, int size, enum bpf_access_type type, const struct bpf_prog *prog, struct bpf_insn_access_aux *info) { const struct btf_type *t = prog->aux->attach_func_proto; struct bpf_prog *tgt_prog = prog->aux->dst_prog; struct btf *btf = bpf_prog_get_target_btf(prog); const char *tname = prog->aux->attach_func_name; struct bpf_verifier_log *log = info->log; const struct btf_param *args; const char *tag_value; u32 nr_args, arg; int i, ret; if (off % 8) { bpf_log(log, ""func '%s' offset %d is not multiple of 8\n"", tname, off); return false; } arg = get_ctx_arg_idx(btf, t, off); args = (const struct btf_param *)(t + 1); nr_args = t ? btf_type_vlen(t) : MAX_BPF_FUNC_REG_ARGS; if (prog->aux->attach_btf_trace) { args++; nr_args--; } if (arg > nr_args) { bpf_log(log, ""func '%s' doesn't have %d-th argument\n"", tname, arg + 1); return false; } if (arg == nr_args) { switch (prog->expected_attach_type) { case BPF_LSM_MAC: info->is_retval = true; fallthrough; case BPF_LSM_CGROUP: case BPF_TRACE_FEXIT: if (!t) return true; t = btf_type_by_id(btf, t->type); break; case BPF_MODIFY_RETURN: if (!t) return false; t = btf_type_skip_modifiers(btf, t->type, NULL); if (!btf_type_is_small_int(t)) { bpf_log(log, ""ret type %s not allowed for fmod_ret\n"", btf_type_str(t)); return false; } break; default: bpf_log(log, ""func '%s' doesn't have %d-th argument\n"", tname, arg + 1); return false; } } else { if (!t) return true; t = btf_type_by_id(btf, args[arg].type); } while (btf_type_is_modifier(t)) t = btf_type_by_id(btf, t->type); if (btf_type_is_small_int(t) || btf_is_any_enum(t) || __btf_type_is_struct(t)) return true; if (!btf_type_is_ptr(t)) { bpf_log(log, ""func '%s' arg%d '%s' has type %s. Only pointer access is allowed\n"", tname, arg, __btf_name_by_offset(btf, t->name_off), btf_type_str(t)); return false; } for (i = 0; i < prog->aux->ctx_arg_info_size; i++) { const struct bpf_ctx_arg_aux *ctx_arg_info = &prog->aux->ctx_arg_info[i]; u32 type, flag; type = base_type(ctx_arg_info->reg_type); flag = type_flag(ctx_arg_info->reg_type); if (ctx_arg_info->offset == off && type == PTR_TO_BUF && (flag & PTR_MAYBE_NULL)) { info->reg_type = ctx_arg_info->reg_type; return true; } } if (t->type == 0) return true; if (is_int_ptr(btf, t)) return true; for (i = 0; i < prog->aux->ctx_arg_info_size; i++) { const struct bpf_ctx_arg_aux *ctx_arg_info = &prog->aux->ctx_arg_info[i]; if (ctx_arg_info->offset == off) { if (!ctx_arg_info->btf_id) { bpf_log(log,""invalid btf_id for context argument offset %u\n"", off); return false; } info->reg_type = ctx_arg_info->reg_type; info->btf = ctx_arg_info->btf ? : btf_vmlinux; info->btf_id = ctx_arg_info->btf_id; return true; } } info->reg_type = PTR_TO_BTF_ID; if (prog_args_trusted(prog)) info->reg_type |= PTR_TRUSTED; if (bpf_prog_is_raw_tp(prog)) info->reg_type |= PTR_MAYBE_NULL; else if (btf_param_match_suffix(btf, &args[arg], ""__nullable"")) info->reg_type |= PTR_MAYBE_NULL; if (tgt_prog) { enum bpf_prog_type tgt_type; if (tgt_prog->type == BPF_PROG_TYPE_EXT) tgt_type = tgt_prog->aux->saved_dst_prog_type; else tgt_type = tgt_prog->type; ret = btf_translate_to_vmlinux(log, btf, t, tgt_type, arg); if (ret > 0) { info->btf = btf_vmlinux; info->btf_id = ret; return true; } else { return false; } } info->btf = btf; info->btf_id = t->type; t = btf_type_by_id(btf, t->type); if (btf_type_is_type_tag(t)) { tag_value = __btf_name_by_offset(btf, t->name_off); if (strcmp(tag_value, ""user"") == 0) info->reg_type |= MEM_USER; if (strcmp(tag_value, ""percpu"") == 0) info->reg_type |= MEM_PERCPU; } while (btf_type_is_modifier(t)) { info->btf_id = t->type; t = btf_type_by_id(btf, t->type); } if (!btf_type_is_struct(t)) { bpf_log(log, ""func '%s' arg%d type %s is not a struct\n"", tname, arg, btf_type_str(t)); return false; } bpf_log(log, ""func '%s' arg%d has btf_id %d type %s '%s'\n"", tname, arg, info->btf_id, btf_type_str(t), __btf_name_by_offset(btf, t->name_off)); return true; }"
62----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44943/bad/gup.c----gup_huge_pud,"static int gup_huge_pud(pud_t orig, pud_t *pudp, unsigned long addr, unsigned long end, unsigned int flags, struct page **pages, int *nr) { struct page *page; struct folio *folio; int refs; if (!pud_access_permitted(orig, flags & FOLL_WRITE)) return 0; if (pud_devmap(orig)) { if (unlikely(flags & FOLL_LONGTERM)) return 0; return __gup_device_huge_pud(orig, pudp, addr, end, flags, pages, nr); } page = nth_page(pud_page(orig), (addr & ~PUD_MASK) >> PAGE_SHIFT); refs = record_subpages(page, addr, end, pages + *nr); <S2SV_StartVul> folio = try_grab_folio(page, refs, flags); <S2SV_EndVul> <S2SV_StartVul> if (!folio) <S2SV_EndVul> return 0; if (unlikely(pud_val(orig) != pud_val(*pudp))) { gup_put_folio(folio, refs, flags); return 0; } if (!folio_fast_pin_allowed(folio, flags)) { gup_put_folio(folio, refs, flags); return 0; } if (!pud_write(orig) && gup_must_unshare(NULL, flags, &folio->page)) { gup_put_folio(folio, refs, flags); return 0; } *nr += refs; folio_set_referenced(folio); return 1; }","- folio = try_grab_folio(page, refs, flags);
- if (!folio)
+ folio = try_grab_folio_fast(page, refs, flags);
+ if (!folio)","static int gup_huge_pud(pud_t orig, pud_t *pudp, unsigned long addr, unsigned long end, unsigned int flags, struct page **pages, int *nr) { struct page *page; struct folio *folio; int refs; if (!pud_access_permitted(orig, flags & FOLL_WRITE)) return 0; if (pud_devmap(orig)) { if (unlikely(flags & FOLL_LONGTERM)) return 0; return __gup_device_huge_pud(orig, pudp, addr, end, flags, pages, nr); } page = nth_page(pud_page(orig), (addr & ~PUD_MASK) >> PAGE_SHIFT); refs = record_subpages(page, addr, end, pages + *nr); folio = try_grab_folio_fast(page, refs, flags); if (!folio) return 0; if (unlikely(pud_val(orig) != pud_val(*pudp))) { gup_put_folio(folio, refs, flags); return 0; } if (!folio_fast_pin_allowed(folio, flags)) { gup_put_folio(folio, refs, flags); return 0; } if (!pud_write(orig) && gup_must_unshare(NULL, flags, &folio->page)) { gup_put_folio(folio, refs, flags); return 0; } *nr += refs; folio_set_referenced(folio); return 1; }"
1428----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56756/bad/pci.c----__nvme_alloc_host_mem,"static int __nvme_alloc_host_mem(struct nvme_dev *dev, u64 preferred, u32 chunk_size) { struct nvme_host_mem_buf_desc *descs; <S2SV_StartVul> u32 max_entries, len; <S2SV_EndVul> dma_addr_t descs_dma; int i = 0; void **bufs; u64 size, tmp; tmp = (preferred + chunk_size - 1); do_div(tmp, chunk_size); max_entries = tmp; if (dev->ctrl.hmmaxd && dev->ctrl.hmmaxd < max_entries) max_entries = dev->ctrl.hmmaxd; <S2SV_StartVul> descs = dma_alloc_coherent(dev->dev, max_entries * sizeof(*descs), <S2SV_EndVul> <S2SV_StartVul> &descs_dma, GFP_KERNEL); <S2SV_EndVul> if (!descs) goto out; bufs = kcalloc(max_entries, sizeof(*bufs), GFP_KERNEL); if (!bufs) goto out_free_descs; for (size = 0; size < preferred && i < max_entries; size += len) { dma_addr_t dma_addr; len = min_t(u64, chunk_size, preferred - size); bufs[i] = dma_alloc_attrs(dev->dev, len, &dma_addr, GFP_KERNEL, DMA_ATTR_NO_KERNEL_MAPPING | DMA_ATTR_NO_WARN); if (!bufs[i]) break; descs[i].addr = cpu_to_le64(dma_addr); descs[i].size = cpu_to_le32(len / NVME_CTRL_PAGE_SIZE); i++; } if (!size) goto out_free_bufs; dev->nr_host_mem_descs = i; dev->host_mem_size = size; dev->host_mem_descs = descs; dev->host_mem_descs_dma = descs_dma; dev->host_mem_desc_bufs = bufs; return 0; out_free_bufs: while (--i >= 0) { size_t size = le32_to_cpu(descs[i].size) * NVME_CTRL_PAGE_SIZE; dma_free_attrs(dev->dev, size, bufs[i], le64_to_cpu(descs[i].addr), DMA_ATTR_NO_KERNEL_MAPPING | DMA_ATTR_NO_WARN); } kfree(bufs); out_free_descs: <S2SV_StartVul> dma_free_coherent(dev->dev, max_entries * sizeof(*descs), descs, <S2SV_EndVul> <S2SV_StartVul> descs_dma); <S2SV_EndVul> out: dev->host_mem_descs = NULL; return -ENOMEM; }","- u32 max_entries, len;
- descs = dma_alloc_coherent(dev->dev, max_entries * sizeof(*descs),
- &descs_dma, GFP_KERNEL);
- dma_free_coherent(dev->dev, max_entries * sizeof(*descs), descs,
- descs_dma);
+ u32 max_entries, len, descs_size;
+ descs_size = max_entries * sizeof(*descs);
+ descs = dma_alloc_coherent(dev->dev, descs_size, &descs_dma,
+ GFP_KERNEL);
+ dev->host_mem_descs_size = descs_size;
+ dma_free_coherent(dev->dev, descs_size, descs, descs_dma);","static int __nvme_alloc_host_mem(struct nvme_dev *dev, u64 preferred, u32 chunk_size) { struct nvme_host_mem_buf_desc *descs; u32 max_entries, len, descs_size; dma_addr_t descs_dma; int i = 0; void **bufs; u64 size, tmp; tmp = (preferred + chunk_size - 1); do_div(tmp, chunk_size); max_entries = tmp; if (dev->ctrl.hmmaxd && dev->ctrl.hmmaxd < max_entries) max_entries = dev->ctrl.hmmaxd; descs_size = max_entries * sizeof(*descs); descs = dma_alloc_coherent(dev->dev, descs_size, &descs_dma, GFP_KERNEL); if (!descs) goto out; bufs = kcalloc(max_entries, sizeof(*bufs), GFP_KERNEL); if (!bufs) goto out_free_descs; for (size = 0; size < preferred && i < max_entries; size += len) { dma_addr_t dma_addr; len = min_t(u64, chunk_size, preferred - size); bufs[i] = dma_alloc_attrs(dev->dev, len, &dma_addr, GFP_KERNEL, DMA_ATTR_NO_KERNEL_MAPPING | DMA_ATTR_NO_WARN); if (!bufs[i]) break; descs[i].addr = cpu_to_le64(dma_addr); descs[i].size = cpu_to_le32(len / NVME_CTRL_PAGE_SIZE); i++; } if (!size) goto out_free_bufs; dev->nr_host_mem_descs = i; dev->host_mem_size = size; dev->host_mem_descs = descs; dev->host_mem_descs_dma = descs_dma; dev->host_mem_descs_size = descs_size; dev->host_mem_desc_bufs = bufs; return 0; out_free_bufs: while (--i >= 0) { size_t size = le32_to_cpu(descs[i].size) * NVME_CTRL_PAGE_SIZE; dma_free_attrs(dev->dev, size, bufs[i], le64_to_cpu(descs[i].addr), DMA_ATTR_NO_KERNEL_MAPPING | DMA_ATTR_NO_WARN); } kfree(bufs); out_free_descs: dma_free_coherent(dev->dev, descs_size, descs, descs_dma); out: dev->host_mem_descs = NULL; return -ENOMEM; }"
717----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49992/bad/ltdc.c----ltdc_crtc_init,"static int ltdc_crtc_init(struct drm_device *ddev, struct drm_crtc *crtc) { struct ltdc_device *ldev = ddev->dev_private; struct drm_plane *primary, *overlay; int supported_rotations = DRM_MODE_ROTATE_0 | DRM_MODE_REFLECT_X | DRM_MODE_REFLECT_Y; unsigned int i; int ret; primary = ltdc_plane_create(ddev, DRM_PLANE_TYPE_PRIMARY, 0); if (!primary) { DRM_ERROR(""Can not create primary plane\n""); return -EINVAL; } if (ldev->caps.dynamic_zorder) drm_plane_create_zpos_property(primary, 0, 0, ldev->caps.nb_layers - 1); else drm_plane_create_zpos_immutable_property(primary, 0); if (ldev->caps.plane_rotation) drm_plane_create_rotation_property(primary, DRM_MODE_ROTATE_0, supported_rotations); if (ldev->caps.crc) <S2SV_StartVul> ret = drm_crtc_init_with_planes(ddev, crtc, primary, NULL, <S2SV_EndVul> <S2SV_StartVul> &ltdc_crtc_with_crc_support_funcs, NULL); <S2SV_EndVul> else <S2SV_StartVul> ret = drm_crtc_init_with_planes(ddev, crtc, primary, NULL, <S2SV_EndVul> <S2SV_StartVul> &ltdc_crtc_funcs, NULL); <S2SV_EndVul> <S2SV_StartVul> if (ret) { <S2SV_EndVul> DRM_ERROR(""Can not initialize CRTC\n""); <S2SV_StartVul> goto cleanup; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> drm_crtc_helper_add(crtc, &ltdc_crtc_helper_funcs); drm_mode_crtc_set_gamma_size(crtc, CLUT_SIZE); drm_crtc_enable_color_mgmt(crtc, 0, false, CLUT_SIZE); DRM_DEBUG_DRIVER(""CRTC:%d created\n"", crtc->base.id); for (i = 1; i < ldev->caps.nb_layers; i++) { overlay = ltdc_plane_create(ddev, DRM_PLANE_TYPE_OVERLAY, i); if (!overlay) { <S2SV_StartVul> ret = -ENOMEM; <S2SV_EndVul> DRM_ERROR(""Can not create overlay plane %d\n"", i); <S2SV_StartVul> goto cleanup; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> if (ldev->caps.dynamic_zorder) drm_plane_create_zpos_property(overlay, i, 0, ldev->caps.nb_layers - 1); else drm_plane_create_zpos_immutable_property(overlay, i); if (ldev->caps.plane_rotation) drm_plane_create_rotation_property(overlay, DRM_MODE_ROTATE_0, supported_rotations); } return 0; <S2SV_StartVul> cleanup: <S2SV_EndVul> <S2SV_StartVul> ltdc_plane_destroy_all(ddev); <S2SV_EndVul> <S2SV_StartVul> return ret; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul>","- ret = drm_crtc_init_with_planes(ddev, crtc, primary, NULL,
- &ltdc_crtc_with_crc_support_funcs, NULL);
- ret = drm_crtc_init_with_planes(ddev, crtc, primary, NULL,
- &ltdc_crtc_funcs, NULL);
- if (ret) {
- goto cleanup;
- }
- ret = -ENOMEM;
- goto cleanup;
- }
- cleanup:
- ltdc_plane_destroy_all(ddev);
- return ret;
- }
+ ret = drmm_crtc_init_with_planes(ddev, crtc, primary, NULL,
+ &ltdc_crtc_with_crc_support_funcs, NULL);
+ ret = drmm_crtc_init_with_planes(ddev, crtc, primary, NULL,
+ &ltdc_crtc_funcs, NULL);
+ return ret;
+ return -ENOMEM;","static int ltdc_crtc_init(struct drm_device *ddev, struct drm_crtc *crtc) { struct ltdc_device *ldev = ddev->dev_private; struct drm_plane *primary, *overlay; int supported_rotations = DRM_MODE_ROTATE_0 | DRM_MODE_REFLECT_X | DRM_MODE_REFLECT_Y; unsigned int i; int ret; primary = ltdc_plane_create(ddev, DRM_PLANE_TYPE_PRIMARY, 0); if (!primary) { DRM_ERROR(""Can not create primary plane\n""); return -EINVAL; } if (ldev->caps.dynamic_zorder) drm_plane_create_zpos_property(primary, 0, 0, ldev->caps.nb_layers - 1); else drm_plane_create_zpos_immutable_property(primary, 0); if (ldev->caps.plane_rotation) drm_plane_create_rotation_property(primary, DRM_MODE_ROTATE_0, supported_rotations); if (ldev->caps.crc) ret = drmm_crtc_init_with_planes(ddev, crtc, primary, NULL, &ltdc_crtc_with_crc_support_funcs, NULL); else ret = drmm_crtc_init_with_planes(ddev, crtc, primary, NULL, &ltdc_crtc_funcs, NULL); if (ret) { DRM_ERROR(""Can not initialize CRTC\n""); return ret; } drm_crtc_helper_add(crtc, &ltdc_crtc_helper_funcs); drm_mode_crtc_set_gamma_size(crtc, CLUT_SIZE); drm_crtc_enable_color_mgmt(crtc, 0, false, CLUT_SIZE); DRM_DEBUG_DRIVER(""CRTC:%d created\n"", crtc->base.id); for (i = 1; i < ldev->caps.nb_layers; i++) { overlay = ltdc_plane_create(ddev, DRM_PLANE_TYPE_OVERLAY, i); if (!overlay) { DRM_ERROR(""Can not create overlay plane %d\n"", i); return -ENOMEM; } if (ldev->caps.dynamic_zorder) drm_plane_create_zpos_property(overlay, i, 0, ldev->caps.nb_layers - 1); else drm_plane_create_zpos_immutable_property(overlay, i); if (ldev->caps.plane_rotation) drm_plane_create_rotation_property(overlay, DRM_MODE_ROTATE_0, supported_rotations); } return 0; }"
987----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50226/bad/region.c----cxl_region_decode_reset,"static int cxl_region_decode_reset(struct cxl_region *cxlr, int count) { struct cxl_region_params *p = &cxlr->params; int i, rc = 0; rc = cxl_region_invalidate_memregion(cxlr); if (rc) return rc; for (i = count - 1; i >= 0; i--) { struct cxl_endpoint_decoder *cxled = p->targets[i]; struct cxl_memdev *cxlmd = cxled_to_memdev(cxled); struct cxl_port *iter = cxled_to_port(cxled); struct cxl_dev_state *cxlds = cxlmd->cxlds; struct cxl_ep *ep; if (cxlds->rcd) goto endpoint_reset; while (!is_cxl_root(to_cxl_port(iter->dev.parent))) iter = to_cxl_port(iter->dev.parent); for (ep = cxl_ep_load(iter, cxlmd); iter; iter = ep->next, ep = cxl_ep_load(iter, cxlmd)) { struct cxl_region_ref *cxl_rr; struct cxl_decoder *cxld; cxl_rr = cxl_rr_load(iter, cxlr); cxld = cxl_rr->decoder; if (cxld->reset) <S2SV_StartVul> rc = cxld->reset(cxld); <S2SV_EndVul> <S2SV_StartVul> if (rc) <S2SV_EndVul> <S2SV_StartVul> return rc; <S2SV_EndVul> set_bit(CXL_REGION_F_NEEDS_RESET, &cxlr->flags); } endpoint_reset: <S2SV_StartVul> rc = cxled->cxld.reset(&cxled->cxld); <S2SV_EndVul> <S2SV_StartVul> if (rc) <S2SV_EndVul> <S2SV_StartVul> return rc; <S2SV_EndVul> set_bit(CXL_REGION_F_NEEDS_RESET, &cxlr->flags); } clear_bit(CXL_REGION_F_NEEDS_RESET, &cxlr->flags); <S2SV_StartVul> return 0; <S2SV_EndVul> }","- rc = cxld->reset(cxld);
- if (rc)
- return rc;
- rc = cxled->cxld.reset(&cxled->cxld);
- if (rc)
- return rc;
- return 0;
+ char *buf)","#include <linux/memregion.h> #include <linux/genalloc.h> #include <linux/device.h> #include <linux/module.h> #include <linux/slab.h> #include <linux/uuid.h> #include <linux/sort.h> #include <linux/idr.h> #include <cxlmem.h> #include <cxl.h> #include ""core.h"" static struct cxl_region *to_cxl_region(struct device *dev); static ssize_t uuid_show(struct device *dev, struct device_attribute *attr, char *buf) { struct cxl_region *cxlr = to_cxl_region(dev); struct cxl_region_params *p = &cxlr->params; ssize_t rc; rc = down_read_interruptible(&cxl_region_rwsem); if (rc) return rc; if (cxlr->mode != CXL_DECODER_PMEM) rc = sysfs_emit(buf, ""\n""); else rc = sysfs_emit(buf, ""%pUb\n"", &p->uuid); up_read(&cxl_region_rwsem); return rc; }"
960----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50202/bad/namei.c----nilfs_rename,"static int nilfs_rename(struct mnt_idmap *idmap, struct inode *old_dir, struct dentry *old_dentry, struct inode *new_dir, struct dentry *new_dentry, unsigned int flags) { struct inode *old_inode = d_inode(old_dentry); struct inode *new_inode = d_inode(new_dentry); struct page *dir_page = NULL; struct nilfs_dir_entry *dir_de = NULL; struct page *old_page; struct nilfs_dir_entry *old_de; struct nilfs_transaction_info ti; int err; if (flags & ~RENAME_NOREPLACE) return -EINVAL; err = nilfs_transaction_begin(old_dir->i_sb, &ti, 1); if (unlikely(err)) return err; <S2SV_StartVul> err = -ENOENT; <S2SV_EndVul> old_de = nilfs_find_entry(old_dir, &old_dentry->d_name, &old_page); <S2SV_StartVul> if (!old_de) <S2SV_EndVul> goto out; if (S_ISDIR(old_inode->i_mode)) { err = -EIO; dir_de = nilfs_dotdot(old_inode, &dir_page); if (!dir_de) goto out_old; } if (new_inode) { struct page *new_page; struct nilfs_dir_entry *new_de; err = -ENOTEMPTY; if (dir_de && !nilfs_empty_dir(new_inode)) goto out_dir; <S2SV_StartVul> err = -ENOENT; <S2SV_EndVul> <S2SV_StartVul> new_de = nilfs_find_entry(new_dir, &new_dentry->d_name, &new_page); <S2SV_EndVul> <S2SV_StartVul> if (!new_de) <S2SV_EndVul> goto out_dir; nilfs_set_link(new_dir, new_de, new_page, old_inode); nilfs_mark_inode_dirty(new_dir); inode_set_ctime_current(new_inode); if (dir_de) drop_nlink(new_inode); drop_nlink(new_inode); nilfs_mark_inode_dirty(new_inode); } else { err = nilfs_add_link(new_dentry, old_inode); if (err) goto out_dir; if (dir_de) { inc_nlink(new_dir); nilfs_mark_inode_dirty(new_dir); } } inode_set_ctime_current(old_inode); nilfs_delete_entry(old_de, old_page); if (dir_de) { nilfs_set_link(old_inode, dir_de, dir_page, new_dir); drop_nlink(old_dir); } nilfs_mark_inode_dirty(old_dir); nilfs_mark_inode_dirty(old_inode); err = nilfs_transaction_commit(old_dir->i_sb); return err; out_dir: if (dir_de) { kunmap(dir_page); put_page(dir_page); } out_old: kunmap(old_page); put_page(old_page); out: nilfs_transaction_abort(old_dir->i_sb); return err; }","- err = -ENOENT;
- if (!old_de)
- err = -ENOENT;
- new_de = nilfs_find_entry(new_dir, &new_dentry->d_name, &new_page);
- if (!new_de)
+ if (IS_ERR(old_de)) {
+ err = PTR_ERR(old_de);
+ }
+ new_de = nilfs_find_entry(new_dir, &new_dentry->d_name,
+ &new_page);
+ if (IS_ERR(new_de)) {
+ err = PTR_ERR(new_de);
+ }","static int nilfs_rename(struct mnt_idmap *idmap, struct inode *old_dir, struct dentry *old_dentry, struct inode *new_dir, struct dentry *new_dentry, unsigned int flags) { struct inode *old_inode = d_inode(old_dentry); struct inode *new_inode = d_inode(new_dentry); struct page *dir_page = NULL; struct nilfs_dir_entry *dir_de = NULL; struct page *old_page; struct nilfs_dir_entry *old_de; struct nilfs_transaction_info ti; int err; if (flags & ~RENAME_NOREPLACE) return -EINVAL; err = nilfs_transaction_begin(old_dir->i_sb, &ti, 1); if (unlikely(err)) return err; old_de = nilfs_find_entry(old_dir, &old_dentry->d_name, &old_page); if (IS_ERR(old_de)) { err = PTR_ERR(old_de); goto out; } if (S_ISDIR(old_inode->i_mode)) { err = -EIO; dir_de = nilfs_dotdot(old_inode, &dir_page); if (!dir_de) goto out_old; } if (new_inode) { struct page *new_page; struct nilfs_dir_entry *new_de; err = -ENOTEMPTY; if (dir_de && !nilfs_empty_dir(new_inode)) goto out_dir; new_de = nilfs_find_entry(new_dir, &new_dentry->d_name, &new_page); if (IS_ERR(new_de)) { err = PTR_ERR(new_de); goto out_dir; } nilfs_set_link(new_dir, new_de, new_page, old_inode); nilfs_mark_inode_dirty(new_dir); inode_set_ctime_current(new_inode); if (dir_de) drop_nlink(new_inode); drop_nlink(new_inode); nilfs_mark_inode_dirty(new_inode); } else { err = nilfs_add_link(new_dentry, old_inode); if (err) goto out_dir; if (dir_de) { inc_nlink(new_dir); nilfs_mark_inode_dirty(new_dir); } } inode_set_ctime_current(old_inode); nilfs_delete_entry(old_de, old_page); if (dir_de) { nilfs_set_link(old_inode, dir_de, dir_page, new_dir); drop_nlink(old_dir); } nilfs_mark_inode_dirty(old_dir); nilfs_mark_inode_dirty(old_inode); err = nilfs_transaction_commit(old_dir->i_sb); return err; out_dir: if (dir_de) { kunmap(dir_page); put_page(dir_page); } out_old: kunmap(old_page); put_page(old_page); out: nilfs_transaction_abort(old_dir->i_sb); return err; }"
231----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46709/bad/vmwgfx_blit.c----vmw_bo_cpu_blit,"int vmw_bo_cpu_blit(struct ttm_buffer_object *dst, u32 dst_offset, u32 dst_stride, struct ttm_buffer_object *src, u32 src_offset, u32 src_stride, u32 w, u32 h, struct vmw_diff_cpy *diff) { struct ttm_operation_ctx ctx = { .interruptible = false, .no_wait_gpu = false }; u32 j, initial_line = dst_offset / dst_stride; struct vmw_bo_blit_line_data d = {0}; int ret = 0; struct page **dst_pages = NULL; struct page **src_pages = NULL; if (!(dst->pin_count)) dma_resv_assert_held(dst->base.resv); if (!(src->pin_count)) dma_resv_assert_held(src->base.resv); if (!ttm_tt_is_populated(dst->ttm)) { ret = dst->bdev->funcs->ttm_tt_populate(dst->bdev, dst->ttm, &ctx); if (ret) return ret; } if (!ttm_tt_is_populated(src->ttm)) { ret = src->bdev->funcs->ttm_tt_populate(src->bdev, src->ttm, &ctx); if (ret) return ret; } if (!src->ttm->pages && src->ttm->sg) { src_pages = kvmalloc_array(src->ttm->num_pages, sizeof(struct page *), GFP_KERNEL); if (!src_pages) return -ENOMEM; ret = drm_prime_sg_to_page_array(src->ttm->sg, src_pages, src->ttm->num_pages); if (ret) goto out; } if (!dst->ttm->pages && dst->ttm->sg) { dst_pages = kvmalloc_array(dst->ttm->num_pages, sizeof(struct page *), GFP_KERNEL); if (!dst_pages) { ret = -ENOMEM; goto out; } ret = drm_prime_sg_to_page_array(dst->ttm->sg, dst_pages, dst->ttm->num_pages); if (ret) goto out; } d.mapped_dst = 0; d.mapped_src = 0; d.dst_addr = NULL; d.src_addr = NULL; d.dst_pages = dst->ttm->pages ? dst->ttm->pages : dst_pages; d.src_pages = src->ttm->pages ? src->ttm->pages : src_pages; d.dst_num_pages = PFN_UP(dst->resource->size); d.src_num_pages = PFN_UP(src->resource->size); d.dst_prot = ttm_io_prot(dst, dst->resource, PAGE_KERNEL); d.src_prot = ttm_io_prot(src, src->resource, PAGE_KERNEL); d.diff = diff; for (j = 0; j < h; ++j) { diff->line = j + initial_line; <S2SV_StartVul> diff->line_offset = dst_offset % dst_stride; <S2SV_EndVul> ret = vmw_bo_cpu_blit_line(&d, dst_offset, src_offset, w); if (ret) goto out; dst_offset += dst_stride; src_offset += src_stride; } out: if (d.src_addr) kunmap_atomic(d.src_addr); if (d.dst_addr) kunmap_atomic(d.dst_addr); if (src_pages) kvfree(src_pages); if (dst_pages) kvfree(dst_pages); <S2SV_StartVul> return ret; <S2SV_EndVul> }","- diff->line_offset = dst_offset % dst_stride;
- return ret;","#include ""vmwgfx_drv.h"" #include ""vmwgfx_bo.h"" #include <linux/highmem.h> #define VMW_FIND_FIRST_DIFF(_type) \ static size_t vmw_find_first_diff_ ## _type \ (const _type * dst, const _type * src, size_t size)\ { \ size_t i; \ \ for (i = 0; i < size; i += sizeof(_type)) { \ if (*dst++ != *src++) \ break; \ } \ \ return i; \ }"
153----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-45011/bad/xillyusb.c----setup_channels,"static int setup_channels(struct xillyusb_dev *xdev, __le16 *chandesc, int num_channels) { <S2SV_StartVul> struct xillyusb_channel *chan; <S2SV_EndVul> int i; chan = kcalloc(num_channels, sizeof(*chan), GFP_KERNEL); if (!chan) return -ENOMEM; <S2SV_StartVul> xdev->channels = chan; <S2SV_EndVul> for (i = 0; i < num_channels; i++, chan++) { unsigned int in_desc = le16_to_cpu(*chandesc++); unsigned int out_desc = le16_to_cpu(*chandesc++); chan->xdev = xdev; mutex_init(&chan->in_mutex); mutex_init(&chan->out_mutex); mutex_init(&chan->lock); init_waitqueue_head(&chan->flushq); chan->chan_idx = i; if (in_desc & 0x80) { chan->readable = 1; chan->in_synchronous = !!(in_desc & 0x40); chan->in_seekable = !!(in_desc & 0x20); chan->in_log2_element_size = in_desc & 0x0f; chan->in_log2_fifo_size = ((in_desc >> 8) & 0x1f) + 16; } if ((out_desc & 0x80) && i < 14) { chan->writable = 1; chan->out_synchronous = !!(out_desc & 0x40); chan->out_seekable = !!(out_desc & 0x20); chan->out_log2_element_size = out_desc & 0x0f; chan->out_log2_fifo_size = ((out_desc >> 8) & 0x1f) + 16; } } return 0; }","- struct xillyusb_channel *chan;
- xdev->channels = chan;
+ struct usb_device *udev = xdev->udev;
+ struct xillyusb_channel *chan, *new_channels;
+ new_channels = chan;
+ if (usb_pipe_type_check(udev,
+ usb_sndbulkpipe(udev, i + 2))) {
+ dev_err(xdev->dev,
+ ""Missing BULK OUT endpoint %d\n"",
+ i + 2);
+ kfree(new_channels);
+ return -ENODEV;
+ }
+ }
+ xdev->channels = new_channels;
+ }","static int setup_channels(struct xillyusb_dev *xdev, __le16 *chandesc, int num_channels) { struct usb_device *udev = xdev->udev; struct xillyusb_channel *chan, *new_channels; int i; chan = kcalloc(num_channels, sizeof(*chan), GFP_KERNEL); if (!chan) return -ENOMEM; new_channels = chan; for (i = 0; i < num_channels; i++, chan++) { unsigned int in_desc = le16_to_cpu(*chandesc++); unsigned int out_desc = le16_to_cpu(*chandesc++); chan->xdev = xdev; mutex_init(&chan->in_mutex); mutex_init(&chan->out_mutex); mutex_init(&chan->lock); init_waitqueue_head(&chan->flushq); chan->chan_idx = i; if (in_desc & 0x80) { chan->readable = 1; chan->in_synchronous = !!(in_desc & 0x40); chan->in_seekable = !!(in_desc & 0x20); chan->in_log2_element_size = in_desc & 0x0f; chan->in_log2_fifo_size = ((in_desc >> 8) & 0x1f) + 16; } if ((out_desc & 0x80) && i < 14) { if (usb_pipe_type_check(udev, usb_sndbulkpipe(udev, i + 2))) { dev_err(xdev->dev, ""Missing BULK OUT endpoint %d\n"", i + 2); kfree(new_channels); return -ENODEV; } chan->writable = 1; chan->out_synchronous = !!(out_desc & 0x40); chan->out_seekable = !!(out_desc & 0x20); chan->out_log2_element_size = out_desc & 0x0f; chan->out_log2_fifo_size = ((out_desc >> 8) & 0x1f) + 16; } } xdev->channels = new_channels; return 0; }"
1375----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56710/bad/file.c----ceph_direct_read_write,"ceph_direct_read_write(struct kiocb *iocb, struct iov_iter *iter, struct ceph_snap_context *snapc, struct ceph_cap_flush **pcf) { struct file *file = iocb->ki_filp; struct inode *inode = file_inode(file); struct ceph_inode_info *ci = ceph_inode(inode); struct ceph_fs_client *fsc = ceph_inode_to_fs_client(inode); struct ceph_client_metric *metric = &fsc->mdsc->metric; struct ceph_vino vino; struct ceph_osd_request *req; struct bio_vec *bvecs; struct ceph_aio_request *aio_req = NULL; int num_pages = 0; int flags; int ret = 0; struct timespec64 mtime = current_time(inode); size_t count = iov_iter_count(iter); loff_t pos = iocb->ki_pos; bool write = iov_iter_rw(iter) == WRITE; bool should_dirty = !write && user_backed_iter(iter); bool sparse = ceph_test_mount_opt(fsc, SPARSEREAD); if (write && ceph_snap(file_inode(file)) != CEPH_NOSNAP) return -EROFS; dout(""sync_direct_%s on file %p %lld~%u snapc %p seq %lld\n"", (write ? ""write"" : ""read""), file, pos, (unsigned)count, snapc, snapc ? snapc->seq : 0); if (write) { int ret2; ceph_fscache_invalidate(inode, true); ret2 = invalidate_inode_pages2_range(inode->i_mapping, pos >> PAGE_SHIFT, (pos + count - 1) >> PAGE_SHIFT); if (ret2 < 0) dout(""invalidate_inode_pages2_range returned %d\n"", ret2); flags = CEPH_OSD_FLAG_WRITE; } else { flags = CEPH_OSD_FLAG_READ; } while (iov_iter_count(iter) > 0) { u64 size = iov_iter_count(iter); ssize_t len; struct ceph_osd_req_op *op; int readop = sparse ? CEPH_OSD_OP_SPARSE_READ : CEPH_OSD_OP_READ; int extent_cnt; if (write) size = min_t(u64, size, fsc->mount_options->wsize); else size = min_t(u64, size, fsc->mount_options->rsize); vino = ceph_vino(inode); req = ceph_osdc_new_request(&fsc->client->osdc, &ci->i_layout, vino, pos, &size, 0, 1, write ? CEPH_OSD_OP_WRITE : readop, flags, snapc, ci->i_truncate_seq, ci->i_truncate_size, false); if (IS_ERR(req)) { ret = PTR_ERR(req); break; <S2SV_StartVul> } <S2SV_EndVul> len = iter_get_bvecs_alloc(iter, size, &bvecs, &num_pages); if (len < 0) { <S2SV_StartVul> ceph_osdc_put_request(req); <S2SV_EndVul> ret = len; break; } if (len != size) osd_req_op_extent_update(req, 0, len); if (pos == iocb->ki_pos && !is_sync_kiocb(iocb) && (len == count || pos + count <= i_size_read(inode))) { aio_req = kzalloc(sizeof(*aio_req), GFP_KERNEL); if (aio_req) { aio_req->iocb = iocb; aio_req->write = write; aio_req->should_dirty = should_dirty; INIT_LIST_HEAD(&aio_req->osd_reqs); if (write) { aio_req->mtime = mtime; swap(aio_req->prealloc_cf, *pcf); } } } if (write) { truncate_inode_pages_range(inode->i_mapping, pos, PAGE_ALIGN(pos + len) - 1); req->r_mtime = mtime; <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> osd_req_op_extent_osd_data_bvecs(req, 0, bvecs, num_pages, len); <S2SV_EndVul> <S2SV_StartVul> op = &req->r_ops[0]; <S2SV_EndVul> <S2SV_StartVul> if (sparse) { <S2SV_EndVul> <S2SV_StartVul> extent_cnt = __ceph_sparse_read_ext_count(inode, size); <S2SV_EndVul> <S2SV_StartVul> ret = ceph_alloc_sparse_ext_map(op, extent_cnt); <S2SV_EndVul> <S2SV_StartVul> if (ret) { <S2SV_EndVul> <S2SV_StartVul> ceph_osdc_put_request(req); <S2SV_EndVul> <S2SV_StartVul> break; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> if (aio_req) { aio_req->total_len += len; aio_req->num_reqs++; atomic_inc(&aio_req->pending_reqs); req->r_callback = ceph_aio_complete_req; req->r_inode = inode; req->r_priv = aio_req; list_add_tail(&req->r_private_item, &aio_req->osd_reqs); pos += len; continue; } ceph_osdc_start_request(req->r_osdc, req); ret = ceph_osdc_wait_request(&fsc->client->osdc, req); if (write) ceph_update_write_metrics(metric, req->r_start_latency, req->r_end_latency, len, ret); else ceph_update_read_metrics(metric, req->r_start_latency, req->r_end_latency, len, ret); size = i_size_read(inode); if (!write) { if (sparse && ret >= 0) ret = ceph_sparse_ext_map_end(op); else if (ret == -ENOENT) ret = 0; if (ret >= 0 && ret < len && pos + ret < size) { struct iov_iter i; int zlen = min_t(size_t, len - ret, size - pos - ret); iov_iter_bvec(&i, ITER_DEST, bvecs, num_pages, len); iov_iter_advance(&i, ret); iov_iter_zero(zlen, &i); ret += zlen; } if (ret >= 0) len = ret; } put_bvecs(bvecs, num_pages, should_dirty); ceph_osdc_put_request(req); if (ret < 0) break; pos += len; if (!write && pos >= size) break; if (write && pos > size) { if (ceph_inode_set_size(inode, pos)) ceph_check_caps(ceph_inode(inode), CHECK_CAPS_AUTHONLY); } } if (aio_req) { LIST_HEAD(osd_reqs); if (aio_req->num_reqs == 0) { kfree(aio_req); return ret; } ceph_get_cap_refs(ci, write ? CEPH_CAP_FILE_WR : CEPH_CAP_FILE_RD); list_splice(&aio_req->osd_reqs, &osd_reqs); inode_dio_begin(inode); while (!list_empty(&osd_reqs)) { req = list_first_entry(&osd_reqs, struct ceph_osd_request, r_private_item); list_del_init(&req->r_private_item); if (ret >= 0) ceph_osdc_start_request(req->r_osdc, req); if (ret < 0) { req->r_result = ret; ceph_aio_complete_req(req); } } return -EIOCBQUEUED; } if (ret != -EOLDSNAPC && pos > iocb->ki_pos) { ret = pos - iocb->ki_pos; iocb->ki_pos = pos; } return ret; }","- }
- ceph_osdc_put_request(req);
- }
- osd_req_op_extent_osd_data_bvecs(req, 0, bvecs, num_pages, len);
- op = &req->r_ops[0];
- if (sparse) {
- extent_cnt = __ceph_sparse_read_ext_count(inode, size);
- ret = ceph_alloc_sparse_ext_map(op, extent_cnt);
- if (ret) {
- ceph_osdc_put_request(req);
- break;
- }
- }
+ }
+ op = &req->r_ops[0];
+ if (sparse) {
+ extent_cnt = __ceph_sparse_read_ext_count(inode, size);
+ ret = ceph_alloc_sparse_ext_map(op, extent_cnt);
+ if (ret) {
+ ceph_osdc_put_request(req);
+ break;
+ }
+ }
+ ceph_osdc_put_request(req);
+ osd_req_op_extent_osd_data_bvecs(req, 0, bvecs, num_pages, len);
+ }","ceph_direct_read_write(struct kiocb *iocb, struct iov_iter *iter, struct ceph_snap_context *snapc, struct ceph_cap_flush **pcf) { struct file *file = iocb->ki_filp; struct inode *inode = file_inode(file); struct ceph_inode_info *ci = ceph_inode(inode); struct ceph_fs_client *fsc = ceph_inode_to_fs_client(inode); struct ceph_client_metric *metric = &fsc->mdsc->metric; struct ceph_vino vino; struct ceph_osd_request *req; struct bio_vec *bvecs; struct ceph_aio_request *aio_req = NULL; int num_pages = 0; int flags; int ret = 0; struct timespec64 mtime = current_time(inode); size_t count = iov_iter_count(iter); loff_t pos = iocb->ki_pos; bool write = iov_iter_rw(iter) == WRITE; bool should_dirty = !write && user_backed_iter(iter); bool sparse = ceph_test_mount_opt(fsc, SPARSEREAD); if (write && ceph_snap(file_inode(file)) != CEPH_NOSNAP) return -EROFS; dout(""sync_direct_%s on file %p %lld~%u snapc %p seq %lld\n"", (write ? ""write"" : ""read""), file, pos, (unsigned)count, snapc, snapc ? snapc->seq : 0); if (write) { int ret2; ceph_fscache_invalidate(inode, true); ret2 = invalidate_inode_pages2_range(inode->i_mapping, pos >> PAGE_SHIFT, (pos + count - 1) >> PAGE_SHIFT); if (ret2 < 0) dout(""invalidate_inode_pages2_range returned %d\n"", ret2); flags = CEPH_OSD_FLAG_WRITE; } else { flags = CEPH_OSD_FLAG_READ; } while (iov_iter_count(iter) > 0) { u64 size = iov_iter_count(iter); ssize_t len; struct ceph_osd_req_op *op; int readop = sparse ? CEPH_OSD_OP_SPARSE_READ : CEPH_OSD_OP_READ; int extent_cnt; if (write) size = min_t(u64, size, fsc->mount_options->wsize); else size = min_t(u64, size, fsc->mount_options->rsize); vino = ceph_vino(inode); req = ceph_osdc_new_request(&fsc->client->osdc, &ci->i_layout, vino, pos, &size, 0, 1, write ? CEPH_OSD_OP_WRITE : readop, flags, snapc, ci->i_truncate_seq, ci->i_truncate_size, false); if (IS_ERR(req)) { ret = PTR_ERR(req); break; } op = &req->r_ops[0]; if (sparse) { extent_cnt = __ceph_sparse_read_ext_count(inode, size); ret = ceph_alloc_sparse_ext_map(op, extent_cnt); if (ret) { ceph_osdc_put_request(req); break; } } len = iter_get_bvecs_alloc(iter, size, &bvecs, &num_pages); if (len < 0) { ceph_osdc_put_request(req); ret = len; break; } if (len != size) osd_req_op_extent_update(req, 0, len); osd_req_op_extent_osd_data_bvecs(req, 0, bvecs, num_pages, len); if (pos == iocb->ki_pos && !is_sync_kiocb(iocb) && (len == count || pos + count <= i_size_read(inode))) { aio_req = kzalloc(sizeof(*aio_req), GFP_KERNEL); if (aio_req) { aio_req->iocb = iocb; aio_req->write = write; aio_req->should_dirty = should_dirty; INIT_LIST_HEAD(&aio_req->osd_reqs); if (write) { aio_req->mtime = mtime; swap(aio_req->prealloc_cf, *pcf); } } } if (write) { truncate_inode_pages_range(inode->i_mapping, pos, PAGE_ALIGN(pos + len) - 1); req->r_mtime = mtime; } if (aio_req) { aio_req->total_len += len; aio_req->num_reqs++; atomic_inc(&aio_req->pending_reqs); req->r_callback = ceph_aio_complete_req; req->r_inode = inode; req->r_priv = aio_req; list_add_tail(&req->r_private_item, &aio_req->osd_reqs); pos += len; continue; } ceph_osdc_start_request(req->r_osdc, req); ret = ceph_osdc_wait_request(&fsc->client->osdc, req); if (write) ceph_update_write_metrics(metric, req->r_start_latency, req->r_end_latency, len, ret); else ceph_update_read_metrics(metric, req->r_start_latency, req->r_end_latency, len, ret); size = i_size_read(inode); if (!write) { if (sparse && ret >= 0) ret = ceph_sparse_ext_map_end(op); else if (ret == -ENOENT) ret = 0; if (ret >= 0 && ret < len && pos + ret < size) { struct iov_iter i; int zlen = min_t(size_t, len - ret, size - pos - ret); iov_iter_bvec(&i, ITER_DEST, bvecs, num_pages, len); iov_iter_advance(&i, ret); iov_iter_zero(zlen, &i); ret += zlen; } if (ret >= 0) len = ret; } put_bvecs(bvecs, num_pages, should_dirty); ceph_osdc_put_request(req); if (ret < 0) break; pos += len; if (!write && pos >= size) break; if (write && pos > size) { if (ceph_inode_set_size(inode, pos)) ceph_check_caps(ceph_inode(inode), CHECK_CAPS_AUTHONLY); } } if (aio_req) { LIST_HEAD(osd_reqs); if (aio_req->num_reqs == 0) { kfree(aio_req); return ret; } ceph_get_cap_refs(ci, write ? CEPH_CAP_FILE_WR : CEPH_CAP_FILE_RD); list_splice(&aio_req->osd_reqs, &osd_reqs); inode_dio_begin(inode); while (!list_empty(&osd_reqs)) { req = list_first_entry(&osd_reqs, struct ceph_osd_request, r_private_item); list_del_init(&req->r_private_item); if (ret >= 0) ceph_osdc_start_request(req->r_osdc, req); if (ret < 0) { req->r_result = ret; ceph_aio_complete_req(req); } } return -EIOCBQUEUED; } if (ret != -EOLDSNAPC && pos > iocb->ki_pos) { ret = pos - iocb->ki_pos; iocb->ki_pos = pos; } return ret; }"
1383----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56718/bad/smc_core.c----smcr_link_down_cond_sched,"void smcr_link_down_cond_sched(struct smc_link *lnk) { if (smc_link_downing(&lnk->state)) { trace_smcr_link_down(lnk, __builtin_return_address(0)); <S2SV_StartVul> schedule_work(&lnk->link_down_wrk); <S2SV_EndVul> } }","- schedule_work(&lnk->link_down_wrk);
+ if (!schedule_work(&lnk->link_down_wrk))
+ smcr_link_put(lnk);","void smcr_link_down_cond_sched(struct smc_link *lnk) { if (smc_link_downing(&lnk->state)) { trace_smcr_link_down(lnk, __builtin_return_address(0)); smcr_link_hold(lnk); if (!schedule_work(&lnk->link_down_wrk)) smcr_link_put(lnk); } }"
769----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50046/bad/nfs42proc.c----handle_async_copy,"static int handle_async_copy(struct nfs42_copy_res *res, struct nfs_server *dst_server, struct nfs_server *src_server, struct file *src, struct file *dst, nfs4_stateid *src_stateid, bool *restart) { struct nfs4_copy_state *copy, *tmp_copy; int status = NFS4_OK; bool found_pending = false; struct nfs_open_context *dst_ctx = nfs_file_open_context(dst); struct nfs_open_context *src_ctx = nfs_file_open_context(src); copy = kzalloc(sizeof(struct nfs4_copy_state), GFP_NOFS); if (!copy) return -ENOMEM; spin_lock(&dst_server->nfs_client->cl_lock); list_for_each_entry(tmp_copy, &dst_server->nfs_client->pending_cb_stateids, copies) { if (memcmp(&res->write_res.stateid, &tmp_copy->stateid, NFS4_STATEID_SIZE)) continue; found_pending = true; list_del(&tmp_copy->copies); break; } if (found_pending) { spin_unlock(&dst_server->nfs_client->cl_lock); kfree(copy); copy = tmp_copy; goto out; } memcpy(&copy->stateid, &res->write_res.stateid, NFS4_STATEID_SIZE); init_completion(&copy->completion); copy->parent_dst_state = dst_ctx->state; copy->parent_src_state = src_ctx->state; list_add_tail(&copy->copies, &dst_server->ss_copies); spin_unlock(&dst_server->nfs_client->cl_lock); if (dst_server != src_server) { spin_lock(&src_server->nfs_client->cl_lock); <S2SV_StartVul> list_add_tail(&copy->src_copies, &src_server->ss_copies); <S2SV_EndVul> spin_unlock(&src_server->nfs_client->cl_lock); } status = wait_for_completion_interruptible(&copy->completion); spin_lock(&dst_server->nfs_client->cl_lock); list_del_init(&copy->copies); spin_unlock(&dst_server->nfs_client->cl_lock); if (dst_server != src_server) { spin_lock(&src_server->nfs_client->cl_lock); list_del_init(&copy->src_copies); spin_unlock(&src_server->nfs_client->cl_lock); } if (status == -ERESTARTSYS) { goto out_cancel; } else if (copy->flags || copy->error == NFS4ERR_PARTNER_NO_AUTH) { status = -EAGAIN; *restart = true; goto out_cancel; } out: res->write_res.count = copy->count; memcpy(&res->write_res.verifier, &copy->verf, sizeof(copy->verf)); status = -copy->error; out_free: kfree(copy); return status; out_cancel: nfs42_do_offload_cancel_async(dst, &copy->stateid); if (!nfs42_files_from_same_server(src, dst)) nfs42_do_offload_cancel_async(src, src_stateid); goto out_free; }","- list_add_tail(&copy->src_copies, &src_server->ss_copies);
+ list_add_tail(&copy->src_copies, &src_server->ss_src_copies);","static int handle_async_copy(struct nfs42_copy_res *res, struct nfs_server *dst_server, struct nfs_server *src_server, struct file *src, struct file *dst, nfs4_stateid *src_stateid, bool *restart) { struct nfs4_copy_state *copy, *tmp_copy; int status = NFS4_OK; bool found_pending = false; struct nfs_open_context *dst_ctx = nfs_file_open_context(dst); struct nfs_open_context *src_ctx = nfs_file_open_context(src); copy = kzalloc(sizeof(struct nfs4_copy_state), GFP_NOFS); if (!copy) return -ENOMEM; spin_lock(&dst_server->nfs_client->cl_lock); list_for_each_entry(tmp_copy, &dst_server->nfs_client->pending_cb_stateids, copies) { if (memcmp(&res->write_res.stateid, &tmp_copy->stateid, NFS4_STATEID_SIZE)) continue; found_pending = true; list_del(&tmp_copy->copies); break; } if (found_pending) { spin_unlock(&dst_server->nfs_client->cl_lock); kfree(copy); copy = tmp_copy; goto out; } memcpy(&copy->stateid, &res->write_res.stateid, NFS4_STATEID_SIZE); init_completion(&copy->completion); copy->parent_dst_state = dst_ctx->state; copy->parent_src_state = src_ctx->state; list_add_tail(&copy->copies, &dst_server->ss_copies); spin_unlock(&dst_server->nfs_client->cl_lock); if (dst_server != src_server) { spin_lock(&src_server->nfs_client->cl_lock); list_add_tail(&copy->src_copies, &src_server->ss_src_copies); spin_unlock(&src_server->nfs_client->cl_lock); } status = wait_for_completion_interruptible(&copy->completion); spin_lock(&dst_server->nfs_client->cl_lock); list_del_init(&copy->copies); spin_unlock(&dst_server->nfs_client->cl_lock); if (dst_server != src_server) { spin_lock(&src_server->nfs_client->cl_lock); list_del_init(&copy->src_copies); spin_unlock(&src_server->nfs_client->cl_lock); } if (status == -ERESTARTSYS) { goto out_cancel; } else if (copy->flags || copy->error == NFS4ERR_PARTNER_NO_AUTH) { status = -EAGAIN; *restart = true; goto out_cancel; } out: res->write_res.count = copy->count; memcpy(&res->write_res.verifier, &copy->verf, sizeof(copy->verf)); status = -copy->error; out_free: kfree(copy); return status; out_cancel: nfs42_do_offload_cancel_async(dst, &copy->stateid); if (!nfs42_files_from_same_server(src, dst)) nfs42_do_offload_cancel_async(src, src_stateid); goto out_free; }"
909----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50154/bad/inet_connection_sock.c----reqsk_queue_unlink,"static bool reqsk_queue_unlink(struct request_sock *req) { struct sock *sk = req_to_sk(req); bool found = false; if (sk_hashed(sk)) { struct inet_hashinfo *hashinfo = tcp_or_dccp_get_hashinfo(sk); spinlock_t *lock = inet_ehash_lockp(hashinfo, req->rsk_hash); spin_lock(lock); found = __sk_nulls_del_node_init_rcu(sk); spin_unlock(lock); } <S2SV_StartVul> if (timer_pending(&req->rsk_timer) && del_timer_sync(&req->rsk_timer)) <S2SV_EndVul> <S2SV_StartVul> reqsk_put(req); <S2SV_EndVul> return found; }","- if (timer_pending(&req->rsk_timer) && del_timer_sync(&req->rsk_timer))
- reqsk_put(req);
+ }
+ }","static bool reqsk_queue_unlink(struct request_sock *req) { struct sock *sk = req_to_sk(req); bool found = false; if (sk_hashed(sk)) { struct inet_hashinfo *hashinfo = tcp_or_dccp_get_hashinfo(sk); spinlock_t *lock = inet_ehash_lockp(hashinfo, req->rsk_hash); spin_lock(lock); found = __sk_nulls_del_node_init_rcu(sk); spin_unlock(lock); } return found; }"
1068----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53052/bad/io_uring.c----io_write,"static int io_write(struct io_kiocb *req, unsigned int issue_flags) { struct iovec inline_vecs[UIO_FASTIOV], *iovec = inline_vecs; struct kiocb *kiocb = &req->rw.kiocb; struct iov_iter __iter, *iter = &__iter; struct io_async_rw *rw = req->async_data; bool force_nonblock = issue_flags & IO_URING_F_NONBLOCK; struct iov_iter_state __state, *state; ssize_t ret, ret2; loff_t *ppos; if (rw) { iter = &rw->iter; state = &rw->iter_state; iov_iter_restore(iter, state); iovec = NULL; } else { ret = io_import_iovec(WRITE, req, &iovec, iter, !force_nonblock); if (ret < 0) return ret; state = &__state; iov_iter_save_state(iter, state); } req->result = iov_iter_count(iter); if (!force_nonblock) kiocb->ki_flags &= ~IOCB_NOWAIT; else kiocb->ki_flags |= IOCB_NOWAIT; if (force_nonblock && !io_file_supports_nowait(req, WRITE)) goto copy_iov; if (force_nonblock && !(kiocb->ki_flags & IOCB_DIRECT) && (req->flags & REQ_F_ISREG)) goto copy_iov; ppos = io_kiocb_update_pos(req); ret = rw_verify_area(WRITE, req->file, ppos, req->result); if (unlikely(ret)) goto out_free; if (req->flags & REQ_F_ISREG) kiocb_start_write(kiocb); kiocb->ki_flags |= IOCB_WRITE; if (req->file->f_op->write_iter) ret2 = call_write_iter(req->file, kiocb, iter); else if (req->file->f_op->write) <S2SV_StartVul> ret2 = loop_rw_iter(WRITE, req, iter); <S2SV_EndVul> <S2SV_StartVul> else <S2SV_EndVul> ret2 = -EINVAL; if (req->flags & REQ_F_REISSUE) { req->flags &= ~REQ_F_REISSUE; ret2 = -EAGAIN; } if (ret2 == -EOPNOTSUPP && (kiocb->ki_flags & IOCB_NOWAIT)) ret2 = -EAGAIN; if (ret2 == -EAGAIN && (req->flags & REQ_F_NOWAIT)) goto done; if (!force_nonblock || ret2 != -EAGAIN) { if ((req->ctx->flags & IORING_SETUP_IOPOLL) && ret2 == -EAGAIN) goto copy_iov; done: kiocb_done(kiocb, ret2, issue_flags); } else { copy_iov: iov_iter_restore(iter, state); ret = io_setup_async_rw(req, iovec, inline_vecs, iter, false); if (!ret) { if (kiocb->ki_flags & IOCB_WRITE) io_req_end_write(req); return -EAGAIN; } return ret; } out_free: if (iovec) kfree(iovec); return ret; }","- ret2 = loop_rw_iter(WRITE, req, iter);
- else
+ struct io_async_rw *rw = req->async_data;
+ struct iov_iter_state __state, *state;
+ ssize_t ret, ret2;
+ loff_t *ppos;
+ iter = &rw->iter;
+ state = &rw->iter_state;
+ iov_iter_restore(iter, state);
+ iovec = NULL;
+ } else {
+ ret = io_import_iovec(WRITE, req, &iovec, iter, !force_nonblock);
+ return ret;
+ state = &__state;
+ iov_iter_save_state(iter, state);
+ }
+ req->result = iov_iter_count(iter);
+ kiocb->ki_flags &= ~IOCB_NOWAIT;
+ if ((req->ctx->flags & IORING_SETUP_IOPOLL) && ret2 == -EAGAIN)","static int io_write(struct io_kiocb *req, unsigned int issue_flags) { struct iovec inline_vecs[UIO_FASTIOV], *iovec = inline_vecs; struct kiocb *kiocb = &req->rw.kiocb; struct iov_iter __iter, *iter = &__iter; struct io_async_rw *rw = req->async_data; bool force_nonblock = issue_flags & IO_URING_F_NONBLOCK; struct iov_iter_state __state, *state; ssize_t ret, ret2; loff_t *ppos; if (rw) { iter = &rw->iter; state = &rw->iter_state; iov_iter_restore(iter, state); iovec = NULL; } else { ret = io_import_iovec(WRITE, req, &iovec, iter, !force_nonblock); if (ret < 0) return ret; state = &__state; iov_iter_save_state(iter, state); } req->result = iov_iter_count(iter); if (!force_nonblock) kiocb->ki_flags &= ~IOCB_NOWAIT; else kiocb->ki_flags |= IOCB_NOWAIT; if (force_nonblock && !io_file_supports_nowait(req, WRITE)) goto copy_iov; if (force_nonblock && !(kiocb->ki_flags & IOCB_DIRECT) && (req->flags & REQ_F_ISREG)) goto copy_iov; ppos = io_kiocb_update_pos(req); ret = rw_verify_area(WRITE, req->file, ppos, req->result); if (unlikely(ret)) goto out_free; if (unlikely(!io_kiocb_start_write(req, kiocb))) goto copy_iov; kiocb->ki_flags |= IOCB_WRITE; if (req->file->f_op->write_iter) ret2 = call_write_iter(req->file, kiocb, iter); else if (req->file->f_op->write) ret2 = loop_rw_iter(WRITE, req, iter); else ret2 = -EINVAL; if (req->flags & REQ_F_REISSUE) { req->flags &= ~REQ_F_REISSUE; ret2 = -EAGAIN; } if (ret2 == -EOPNOTSUPP && (kiocb->ki_flags & IOCB_NOWAIT)) ret2 = -EAGAIN; if (ret2 == -EAGAIN && (req->flags & REQ_F_NOWAIT)) goto done; if (!force_nonblock || ret2 != -EAGAIN) { if ((req->ctx->flags & IORING_SETUP_IOPOLL) && ret2 == -EAGAIN) goto copy_iov; done: kiocb_done(kiocb, ret2, issue_flags); } else { copy_iov: iov_iter_restore(iter, state); ret = io_setup_async_rw(req, iovec, inline_vecs, iter, false); if (!ret) { if (kiocb->ki_flags & IOCB_WRITE) io_req_end_write(req); return -EAGAIN; } return ret; } out_free: if (iovec) kfree(iovec); return ret; }"
880----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50133/bad/process.c----stack_top,unsigned long stack_top(void) { unsigned long top = TASK_SIZE & PAGE_MASK; <S2SV_StartVul> top -= PAGE_ALIGN(current->thread.vdso->size); <S2SV_EndVul> <S2SV_StartVul> top -= VVAR_SIZE; <S2SV_EndVul> <S2SV_StartVul> if (current->flags & PF_RANDOMIZE) <S2SV_EndVul> <S2SV_StartVul> top -= VDSO_RANDOMIZE_SIZE; <S2SV_EndVul> return top; },"- top -= PAGE_ALIGN(current->thread.vdso->size);
- top -= VVAR_SIZE;
- if (current->flags & PF_RANDOMIZE)
- top -= VDSO_RANDOMIZE_SIZE;
+ if (current->thread.vdso) {
+ top -= PAGE_ALIGN(current->thread.vdso->size);
+ top -= VVAR_SIZE;
+ if (current->flags & PF_RANDOMIZE)
+ top -= VDSO_RANDOMIZE_SIZE;
+ }
+ }",unsigned long stack_top(void) { unsigned long top = TASK_SIZE & PAGE_MASK; if (current->thread.vdso) { top -= PAGE_ALIGN(current->thread.vdso->size); top -= VVAR_SIZE; if (current->flags & PF_RANDOMIZE) top -= VDSO_RANDOMIZE_SIZE; } return top; }
69----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44943/bad/huge_memory.c----*follow_trans_huge_pmd,"struct page *follow_trans_huge_pmd(struct vm_area_struct *vma, unsigned long addr, pmd_t *pmd, unsigned int flags) { struct mm_struct *mm = vma->vm_mm; struct page *page; int ret; assert_spin_locked(pmd_lockptr(mm, pmd)); page = pmd_page(*pmd); VM_BUG_ON_PAGE(!PageHead(page) && !is_zone_device_page(page), page); if ((flags & FOLL_WRITE) && !can_follow_write_pmd(*pmd, page, vma, flags)) return NULL; if ((flags & FOLL_DUMP) && is_huge_zero_pmd(*pmd)) return ERR_PTR(-EFAULT); if (pmd_protnone(*pmd) && !gup_can_follow_protnone(vma, flags)) return NULL; if (!pmd_write(*pmd) && gup_must_unshare(vma, flags, page)) return ERR_PTR(-EMLINK); VM_BUG_ON_PAGE((flags & FOLL_PIN) && PageAnon(page) && !PageAnonExclusive(page), page); <S2SV_StartVul> ret = try_grab_page(page, flags); <S2SV_EndVul> if (ret) return ERR_PTR(ret); if (flags & FOLL_TOUCH) touch_pmd(vma, addr, pmd, flags & FOLL_WRITE); page += (addr & ~HPAGE_PMD_MASK) >> PAGE_SHIFT; VM_BUG_ON_PAGE(!PageCompound(page) && !is_zone_device_page(page), page); return page; }","- ret = try_grab_page(page, flags);
+ ret = try_grab_folio(page_folio(page), 1, flags);","struct page *follow_trans_huge_pmd(struct vm_area_struct *vma, unsigned long addr, pmd_t *pmd, unsigned int flags) { struct mm_struct *mm = vma->vm_mm; struct page *page; int ret; assert_spin_locked(pmd_lockptr(mm, pmd)); page = pmd_page(*pmd); VM_BUG_ON_PAGE(!PageHead(page) && !is_zone_device_page(page), page); if ((flags & FOLL_WRITE) && !can_follow_write_pmd(*pmd, page, vma, flags)) return NULL; if ((flags & FOLL_DUMP) && is_huge_zero_pmd(*pmd)) return ERR_PTR(-EFAULT); if (pmd_protnone(*pmd) && !gup_can_follow_protnone(vma, flags)) return NULL; if (!pmd_write(*pmd) && gup_must_unshare(vma, flags, page)) return ERR_PTR(-EMLINK); VM_BUG_ON_PAGE((flags & FOLL_PIN) && PageAnon(page) && !PageAnonExclusive(page), page); ret = try_grab_folio(page_folio(page), 1, flags); if (ret) return ERR_PTR(ret); if (flags & FOLL_TOUCH) touch_pmd(vma, addr, pmd, flags & FOLL_WRITE); page += (addr & ~HPAGE_PMD_MASK) >> PAGE_SHIFT; VM_BUG_ON_PAGE(!PageCompound(page) && !is_zone_device_page(page), page); return page; }"
1241----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-54460/bad/iso.c----iso_sock_listen,"static int iso_sock_listen(struct socket *sock, int backlog) { struct sock *sk = sock->sk; int err = 0; BT_DBG(""sk %p backlog %d"", sk, backlog); lock_sock(sk); if (sk->sk_state != BT_BOUND) { err = -EBADFD; goto done; } if (sk->sk_type != SOCK_SEQPACKET) { err = -EINVAL; goto done; } <S2SV_StartVul> if (!bacmp(&iso_pi(sk)->dst, BDADDR_ANY)) <S2SV_EndVul> err = iso_listen_cis(sk); <S2SV_StartVul> else <S2SV_EndVul> err = iso_listen_bis(sk); if (err) goto done; sk->sk_max_ack_backlog = backlog; sk->sk_ack_backlog = 0; sk->sk_state = BT_LISTEN; done: release_sock(sk); return err; }","- if (!bacmp(&iso_pi(sk)->dst, BDADDR_ANY))
- else
+ sock_hold(sk);
+ lock_sock(sk);
+ }
+ if (!bacmp(&iso_pi(sk)->dst, BDADDR_ANY)) {
+ } else {
+ release_sock(sk);
+ lock_sock(sk);
+ }
+ release_sock(sk);
+ sock_put(sk);
+ }","static int iso_sock_listen(struct socket *sock, int backlog) { struct sock *sk = sock->sk; int err = 0; BT_DBG(""sk %p backlog %d"", sk, backlog); sock_hold(sk); lock_sock(sk); if (sk->sk_state != BT_BOUND) { err = -EBADFD; goto done; } if (sk->sk_type != SOCK_SEQPACKET) { err = -EINVAL; goto done; } if (!bacmp(&iso_pi(sk)->dst, BDADDR_ANY)) { err = iso_listen_cis(sk); } else { release_sock(sk); err = iso_listen_bis(sk); lock_sock(sk); } if (err) goto done; sk->sk_max_ack_backlog = backlog; sk->sk_ack_backlog = 0; sk->sk_state = BT_LISTEN; done: release_sock(sk); sock_put(sk); return err; }"
1007----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50245/bad/namei.c----*ntfs_lookup,"static struct dentry *ntfs_lookup(struct inode *dir, struct dentry *dentry, u32 flags) { struct ntfs_inode *ni = ntfs_i(dir); struct cpu_str *uni = __getname(); struct inode *inode; int err; if (!uni) inode = ERR_PTR(-ENOMEM); else { err = ntfs_nls_to_utf16(ni->mi.sbi, dentry->d_name.name, dentry->d_name.len, uni, NTFS_NAME_LEN, UTF16_HOST_ENDIAN); if (err < 0) inode = ERR_PTR(err); else { <S2SV_StartVul> ni_lock(ni); <S2SV_EndVul> inode = dir_search_u(dir, uni, NULL); ni_unlock(ni); } __putname(uni); } if (!IS_ERR_OR_NULL(inode) && !inode->i_op) { iput(inode); inode = ERR_PTR(-EINVAL); } return d_splice_alias(inode, dentry); }","- ni_lock(ni);
+ ni_lock_dir(ni);","static struct dentry *ntfs_lookup(struct inode *dir, struct dentry *dentry, u32 flags) { struct ntfs_inode *ni = ntfs_i(dir); struct cpu_str *uni = __getname(); struct inode *inode; int err; if (!uni) inode = ERR_PTR(-ENOMEM); else { err = ntfs_nls_to_utf16(ni->mi.sbi, dentry->d_name.name, dentry->d_name.len, uni, NTFS_NAME_LEN, UTF16_HOST_ENDIAN); if (err < 0) inode = ERR_PTR(err); else { ni_lock_dir(ni); inode = dir_search_u(dir, uni, NULL); ni_unlock(ni); } __putname(uni); } if (!IS_ERR_OR_NULL(inode) && !inode->i_op) { iput(inode); inode = ERR_PTR(-EINVAL); } return d_splice_alias(inode, dentry); }"
634----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49939/bad/mac80211.c----rtw89_ops_add_interface,"static int rtw89_ops_add_interface(struct ieee80211_hw *hw, struct ieee80211_vif *vif) { struct rtw89_dev *rtwdev = hw->priv; struct rtw89_vif *rtwvif = (struct rtw89_vif *)vif->drv_priv; int ret = 0; rtw89_debug(rtwdev, RTW89_DBG_STATE, ""add vif %pM type %d, p2p %d\n"", vif->addr, vif->type, vif->p2p); mutex_lock(&rtwdev->mutex); rtw89_leave_ips_by_hwflags(rtwdev); if (RTW89_CHK_FW_FEATURE(BEACON_FILTER, &rtwdev->fw)) vif->driver_flags |= IEEE80211_VIF_BEACON_FILTER | IEEE80211_VIF_SUPPORTS_CQM_RSSI; rtwvif->rtwdev = rtwdev; rtwvif->roc.state = RTW89_ROC_IDLE; rtwvif->offchan = false; <S2SV_StartVul> list_add_tail(&rtwvif->list, &rtwdev->rtwvifs_list); <S2SV_EndVul> INIT_WORK(&rtwvif->update_beacon_work, rtw89_core_update_beacon_work); INIT_DELAYED_WORK(&rtwvif->roc.roc_work, rtw89_roc_work); rtw89_leave_ps_mode(rtwdev); rtw89_traffic_stats_init(rtwdev, &rtwvif->stats); rtw89_vif_type_mapping(vif, false); rtwvif->port = rtw89_core_acquire_bit_map(rtwdev->hw_port, RTW89_PORT_NUM); if (rtwvif->port == RTW89_PORT_NUM) { ret = -ENOSPC; list_del_init(&rtwvif->list); goto out; } rtwvif->bcn_hit_cond = 0; rtwvif->mac_idx = RTW89_MAC_0; rtwvif->phy_idx = RTW89_PHY_0; rtwvif->sub_entity_idx = RTW89_SUB_ENTITY_0; rtwvif->chanctx_assigned = false; rtwvif->hit_rule = 0; rtwvif->reg_6ghz_power = RTW89_REG_6GHZ_POWER_DFLT; ether_addr_copy(rtwvif->mac_addr, vif->addr); INIT_LIST_HEAD(&rtwvif->general_pkt_list); ret = rtw89_mac_add_vif(rtwdev, rtwvif); if (ret) { rtw89_core_release_bit_map(rtwdev->hw_port, rtwvif->port); list_del_init(&rtwvif->list); goto out; } rtw89_core_txq_init(rtwdev, vif->txq); rtw89_btc_ntfy_role_info(rtwdev, rtwvif, NULL, BTC_ROLE_START); rtw89_recalc_lps(rtwdev); out: mutex_unlock(&rtwdev->mutex); return ret; }","- list_add_tail(&rtwvif->list, &rtwdev->rtwvifs_list);
+ if (!rtw89_rtwvif_in_list(rtwdev, rtwvif))
+ list_add_tail(&rtwvif->list, &rtwdev->rtwvifs_list);","static int rtw89_ops_add_interface(struct ieee80211_hw *hw, struct ieee80211_vif *vif) { struct rtw89_dev *rtwdev = hw->priv; struct rtw89_vif *rtwvif = (struct rtw89_vif *)vif->drv_priv; int ret = 0; rtw89_debug(rtwdev, RTW89_DBG_STATE, ""add vif %pM type %d, p2p %d\n"", vif->addr, vif->type, vif->p2p); mutex_lock(&rtwdev->mutex); rtw89_leave_ips_by_hwflags(rtwdev); if (RTW89_CHK_FW_FEATURE(BEACON_FILTER, &rtwdev->fw)) vif->driver_flags |= IEEE80211_VIF_BEACON_FILTER | IEEE80211_VIF_SUPPORTS_CQM_RSSI; rtwvif->rtwdev = rtwdev; rtwvif->roc.state = RTW89_ROC_IDLE; rtwvif->offchan = false; if (!rtw89_rtwvif_in_list(rtwdev, rtwvif)) list_add_tail(&rtwvif->list, &rtwdev->rtwvifs_list); INIT_WORK(&rtwvif->update_beacon_work, rtw89_core_update_beacon_work); INIT_DELAYED_WORK(&rtwvif->roc.roc_work, rtw89_roc_work); rtw89_leave_ps_mode(rtwdev); rtw89_traffic_stats_init(rtwdev, &rtwvif->stats); rtw89_vif_type_mapping(vif, false); rtwvif->port = rtw89_core_acquire_bit_map(rtwdev->hw_port, RTW89_PORT_NUM); if (rtwvif->port == RTW89_PORT_NUM) { ret = -ENOSPC; list_del_init(&rtwvif->list); goto out; } rtwvif->bcn_hit_cond = 0; rtwvif->mac_idx = RTW89_MAC_0; rtwvif->phy_idx = RTW89_PHY_0; rtwvif->sub_entity_idx = RTW89_SUB_ENTITY_0; rtwvif->chanctx_assigned = false; rtwvif->hit_rule = 0; rtwvif->reg_6ghz_power = RTW89_REG_6GHZ_POWER_DFLT; ether_addr_copy(rtwvif->mac_addr, vif->addr); INIT_LIST_HEAD(&rtwvif->general_pkt_list); ret = rtw89_mac_add_vif(rtwdev, rtwvif); if (ret) { rtw89_core_release_bit_map(rtwdev->hw_port, rtwvif->port); list_del_init(&rtwvif->list); goto out; } rtw89_core_txq_init(rtwdev, vif->txq); rtw89_btc_ntfy_role_info(rtwdev, rtwvif, NULL, BTC_ROLE_START); rtw89_recalc_lps(rtwdev); out: mutex_unlock(&rtwdev->mutex); return ret; }"
1459----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-57798/bad/drm_dp_mst_topology.c----drm_dp_mst_handle_up_req,"static int drm_dp_mst_handle_up_req(struct drm_dp_mst_topology_mgr *mgr) { struct drm_dp_pending_up_req *up_req; if (!drm_dp_get_one_sb_msg(mgr, true, NULL)) <S2SV_StartVul> goto out; <S2SV_EndVul> if (!mgr->up_req_recv.have_eomt) return 0; up_req = kzalloc(sizeof(*up_req), GFP_KERNEL); if (!up_req) return -ENOMEM; INIT_LIST_HEAD(&up_req->next); drm_dp_sideband_parse_req(mgr, &mgr->up_req_recv, &up_req->msg); if (up_req->msg.req_type != DP_CONNECTION_STATUS_NOTIFY && up_req->msg.req_type != DP_RESOURCE_STATUS_NOTIFY) { drm_dbg_kms(mgr->dev, ""Received unknown up req type, ignoring: %x\n"", up_req->msg.req_type); kfree(up_req); <S2SV_StartVul> goto out; <S2SV_EndVul> } <S2SV_StartVul> drm_dp_send_up_ack_reply(mgr, mgr->mst_primary, up_req->msg.req_type, <S2SV_EndVul> false); if (up_req->msg.req_type == DP_CONNECTION_STATUS_NOTIFY) { const struct drm_dp_connection_status_notify *conn_stat = &up_req->msg.u.conn_stat; bool handle_csn; drm_dbg_kms(mgr->dev, ""Got CSN: pn: %d ldps:%d ddps: %d mcs: %d ip: %d pdt: %d\n"", conn_stat->port_number, conn_stat->legacy_device_plug_status, conn_stat->displayport_device_plug_status, conn_stat->message_capability_status, conn_stat->input_port, conn_stat->peer_device_type); mutex_lock(&mgr->probe_lock); <S2SV_StartVul> handle_csn = mgr->mst_primary->link_address_sent; <S2SV_EndVul> mutex_unlock(&mgr->probe_lock); if (!handle_csn) { drm_dbg_kms(mgr->dev, ""Got CSN before finish topology probing. Skip it.""); kfree(up_req); <S2SV_StartVul> goto out; <S2SV_EndVul> } } else if (up_req->msg.req_type == DP_RESOURCE_STATUS_NOTIFY) { const struct drm_dp_resource_status_notify *res_stat = &up_req->msg.u.resource_stat; drm_dbg_kms(mgr->dev, ""Got RSN: pn: %d avail_pbn %d\n"", res_stat->port_number, res_stat->available_pbn); } up_req->hdr = mgr->up_req_recv.initial_hdr; mutex_lock(&mgr->up_req_lock); list_add_tail(&up_req->next, &mgr->up_req_list); mutex_unlock(&mgr->up_req_lock); queue_work(system_long_wq, &mgr->up_req_work); <S2SV_StartVul> out: <S2SV_EndVul> memset(&mgr->up_req_recv, 0, sizeof(struct drm_dp_sideband_msg_rx)); return 0; }","- goto out;
- goto out;
- drm_dp_send_up_ack_reply(mgr, mgr->mst_primary, up_req->msg.req_type,
- handle_csn = mgr->mst_primary->link_address_sent;
- goto out;
- out:
+ struct drm_dp_mst_branch *mst_primary;
+ goto out_clear_reply;
+ kfree(up_req);
+ goto out_clear_reply;
+ }
+ mutex_lock(&mgr->lock);
+ mst_primary = mgr->mst_primary;
+ if (!mst_primary || !drm_dp_mst_topology_try_get_mstb(mst_primary)) {
+ mutex_unlock(&mgr->lock);
+ kfree(up_req);
+ goto out_clear_reply;
+ }
+ mutex_unlock(&mgr->lock);
+ drm_dp_send_up_ack_reply(mgr, mst_primary, up_req->msg.req_type,
+ handle_csn = mst_primary->link_address_sent;
+ kfree(up_req);
+ goto out_put_primary;
+ }
+ out_put_primary:
+ drm_dp_mst_topology_put_mstb(mst_primary);
+ out_clear_reply:
+ }","static int drm_dp_mst_handle_up_req(struct drm_dp_mst_topology_mgr *mgr) { struct drm_dp_pending_up_req *up_req; struct drm_dp_mst_branch *mst_primary; if (!drm_dp_get_one_sb_msg(mgr, true, NULL)) goto out_clear_reply; if (!mgr->up_req_recv.have_eomt) return 0; up_req = kzalloc(sizeof(*up_req), GFP_KERNEL); if (!up_req) return -ENOMEM; INIT_LIST_HEAD(&up_req->next); drm_dp_sideband_parse_req(mgr, &mgr->up_req_recv, &up_req->msg); if (up_req->msg.req_type != DP_CONNECTION_STATUS_NOTIFY && up_req->msg.req_type != DP_RESOURCE_STATUS_NOTIFY) { drm_dbg_kms(mgr->dev, ""Received unknown up req type, ignoring: %x\n"", up_req->msg.req_type); kfree(up_req); goto out_clear_reply; } mutex_lock(&mgr->lock); mst_primary = mgr->mst_primary; if (!mst_primary || !drm_dp_mst_topology_try_get_mstb(mst_primary)) { mutex_unlock(&mgr->lock); kfree(up_req); goto out_clear_reply; } mutex_unlock(&mgr->lock); drm_dp_send_up_ack_reply(mgr, mst_primary, up_req->msg.req_type, false); if (up_req->msg.req_type == DP_CONNECTION_STATUS_NOTIFY) { const struct drm_dp_connection_status_notify *conn_stat = &up_req->msg.u.conn_stat; bool handle_csn; drm_dbg_kms(mgr->dev, ""Got CSN: pn: %d ldps:%d ddps: %d mcs: %d ip: %d pdt: %d\n"", conn_stat->port_number, conn_stat->legacy_device_plug_status, conn_stat->displayport_device_plug_status, conn_stat->message_capability_status, conn_stat->input_port, conn_stat->peer_device_type); mutex_lock(&mgr->probe_lock); handle_csn = mst_primary->link_address_sent; mutex_unlock(&mgr->probe_lock); if (!handle_csn) { drm_dbg_kms(mgr->dev, ""Got CSN before finish topology probing. Skip it.""); kfree(up_req); goto out_put_primary; } } else if (up_req->msg.req_type == DP_RESOURCE_STATUS_NOTIFY) { const struct drm_dp_resource_status_notify *res_stat = &up_req->msg.u.resource_stat; drm_dbg_kms(mgr->dev, ""Got RSN: pn: %d avail_pbn %d\n"", res_stat->port_number, res_stat->available_pbn); } up_req->hdr = mgr->up_req_recv.initial_hdr; mutex_lock(&mgr->up_req_lock); list_add_tail(&up_req->next, &mgr->up_req_list); mutex_unlock(&mgr->up_req_lock); queue_work(system_long_wq, &mgr->up_req_work); out_put_primary: drm_dp_mst_topology_put_mstb(mst_primary); out_clear_reply: memset(&mgr->up_req_recv, 0, sizeof(struct drm_dp_sideband_msg_rx)); return 0; }"
670----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49970/bad/dcn401_resource.c----*dcn401_stream_encoder_create,"static struct stream_encoder *dcn401_stream_encoder_create( enum engine_id eng_id, struct dc_context *ctx) { struct dcn10_stream_encoder *enc1; struct vpg *vpg; struct afmt *afmt; int vpg_inst; int afmt_inst; if (eng_id <= ENGINE_ID_DIGF) { vpg_inst = eng_id; afmt_inst = eng_id; } else return NULL; enc1 = kzalloc(sizeof(struct dcn10_stream_encoder), GFP_KERNEL); vpg = dcn401_vpg_create(ctx, vpg_inst); afmt = dcn401_afmt_create(ctx, afmt_inst); <S2SV_StartVul> if (!enc1 || !vpg || !afmt) { <S2SV_EndVul> kfree(enc1); kfree(vpg); kfree(afmt); return NULL; } #undef REG_STRUCT #define REG_STRUCT stream_enc_regs stream_enc_regs_init(0), stream_enc_regs_init(1), stream_enc_regs_init(2), stream_enc_regs_init(3); dcn401_dio_stream_encoder_construct(enc1, ctx, ctx->dc_bios, eng_id, vpg, afmt, &stream_enc_regs[eng_id], &se_shift, &se_mask); return &enc1->base; }","- if (!enc1 || !vpg || !afmt) {
+ if (!enc1 || !vpg || !afmt || eng_id >= ARRAY_SIZE(stream_enc_regs)) {","static struct stream_encoder *dcn401_stream_encoder_create( enum engine_id eng_id, struct dc_context *ctx) { struct dcn10_stream_encoder *enc1; struct vpg *vpg; struct afmt *afmt; int vpg_inst; int afmt_inst; if (eng_id <= ENGINE_ID_DIGF) { vpg_inst = eng_id; afmt_inst = eng_id; } else return NULL; enc1 = kzalloc(sizeof(struct dcn10_stream_encoder), GFP_KERNEL); vpg = dcn401_vpg_create(ctx, vpg_inst); afmt = dcn401_afmt_create(ctx, afmt_inst); if (!enc1 || !vpg || !afmt || eng_id >= ARRAY_SIZE(stream_enc_regs)) { kfree(enc1); kfree(vpg); kfree(afmt); return NULL; } #undef REG_STRUCT #define REG_STRUCT stream_enc_regs stream_enc_regs_init(0), stream_enc_regs_init(1), stream_enc_regs_init(2), stream_enc_regs_init(3); dcn401_dio_stream_encoder_construct(enc1, ctx, ctx->dc_bios, eng_id, vpg, afmt, &stream_enc_regs[eng_id], &se_shift, &se_mask); return &enc1->base; }"
1435----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56760/bad/irqdomain.c----pci_msi_domain_supports,"bool pci_msi_domain_supports(struct pci_dev *pdev, unsigned int feature_mask, enum support_mode mode) { struct msi_domain_info *info; struct irq_domain *domain; unsigned int supported; domain = dev_get_msi_domain(&pdev->dev); <S2SV_StartVul> if (!domain || !irq_domain_is_hierarchy(domain)) <S2SV_EndVul> <S2SV_StartVul> return mode == ALLOW_LEGACY; <S2SV_EndVul> if (!irq_domain_is_msi_parent(domain)) { info = domain->host_data; supported = info->flags; } else { supported = domain->msi_parent_ops->supported_flags; } return (supported & feature_mask) == feature_mask; }","- if (!domain || !irq_domain_is_hierarchy(domain))
- return mode == ALLOW_LEGACY;
+ if (!domain || !irq_domain_is_hierarchy(domain)) {
+ if (IS_ENABLED(CONFIG_PCI_MSI_ARCH_FALLBACKS))
+ return mode == ALLOW_LEGACY;
+ return false;
+ }","bool pci_msi_domain_supports(struct pci_dev *pdev, unsigned int feature_mask, enum support_mode mode) { struct msi_domain_info *info; struct irq_domain *domain; unsigned int supported; domain = dev_get_msi_domain(&pdev->dev); if (!domain || !irq_domain_is_hierarchy(domain)) { if (IS_ENABLED(CONFIG_PCI_MSI_ARCH_FALLBACKS)) return mode == ALLOW_LEGACY; return false; } if (!irq_domain_is_msi_parent(domain)) { info = domain->host_data; supported = info->flags; } else { supported = domain->msi_parent_ops->supported_flags; } return (supported & feature_mask) == feature_mask; }"
893----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50143/bad/inode.c----udf_current_aext,"int udf_current_aext(struct inode *inode, struct extent_position *epos, struct kernel_lb_addr *eloc, uint32_t *elen, int8_t *etype, int inc) { int alen; uint8_t *ptr; struct short_ad *sad; struct long_ad *lad; struct udf_inode_info *iinfo = UDF_I(inode); if (!epos->bh) { if (!epos->offset) epos->offset = udf_file_entry_alloc_offset(inode); ptr = iinfo->i_data + epos->offset - udf_file_entry_alloc_offset(inode) + iinfo->i_lenEAttr; alen = udf_file_entry_alloc_offset(inode) + iinfo->i_lenAlloc; } else { if (!epos->offset) epos->offset = sizeof(struct allocExtDesc); ptr = epos->bh->b_data + epos->offset; alen = sizeof(struct allocExtDesc) + le32_to_cpu(((struct allocExtDesc *)epos->bh->b_data)-> lengthAllocDescs); } switch (iinfo->i_alloc_type) { case ICBTAG_FLAG_AD_SHORT: sad = udf_get_fileshortad(ptr, alen, &epos->offset, inc); if (!sad) return 0; *etype = le32_to_cpu(sad->extLength) >> 30; eloc->logicalBlockNum = le32_to_cpu(sad->extPosition); eloc->partitionReferenceNum = iinfo->i_location.partitionReferenceNum; *elen = le32_to_cpu(sad->extLength) & UDF_EXTENT_LENGTH_MASK; break; case ICBTAG_FLAG_AD_LONG: lad = udf_get_filelongad(ptr, alen, &epos->offset, inc); if (!lad) return 0; *etype = le32_to_cpu(lad->extLength) >> 30; *eloc = lelb_to_cpu(lad->extLocation); *elen = le32_to_cpu(lad->extLength) & UDF_EXTENT_LENGTH_MASK; break; default: udf_debug(""alloc_type = %u unsupported\n"", iinfo->i_alloc_type); return -EINVAL; } return 1; <S2SV_StartVul> } <S2SV_EndVul>","- }
+ default:
+ udf_debug(""alloc_type = %u unsupported\n"", iinfo->i_alloc_type);
+ }","int udf_current_aext(struct inode *inode, struct extent_position *epos, struct kernel_lb_addr *eloc, uint32_t *elen, int8_t *etype, int inc) { int alen; uint8_t *ptr; struct short_ad *sad; struct long_ad *lad; struct udf_inode_info *iinfo = UDF_I(inode); if (!epos->bh) { if (!epos->offset) epos->offset = udf_file_entry_alloc_offset(inode); ptr = iinfo->i_data + epos->offset - udf_file_entry_alloc_offset(inode) + iinfo->i_lenEAttr; alen = udf_file_entry_alloc_offset(inode) + iinfo->i_lenAlloc; } else { struct allocExtDesc *header = (struct allocExtDesc *)epos->bh->b_data; if (!epos->offset) epos->offset = sizeof(struct allocExtDesc); ptr = epos->bh->b_data + epos->offset; if (check_add_overflow(sizeof(struct allocExtDesc), le32_to_cpu(header->lengthAllocDescs), &alen)) return -1; } switch (iinfo->i_alloc_type) { case ICBTAG_FLAG_AD_SHORT: sad = udf_get_fileshortad(ptr, alen, &epos->offset, inc); if (!sad) return 0; *etype = le32_to_cpu(sad->extLength) >> 30; eloc->logicalBlockNum = le32_to_cpu(sad->extPosition); eloc->partitionReferenceNum = iinfo->i_location.partitionReferenceNum; *elen = le32_to_cpu(sad->extLength) & UDF_EXTENT_LENGTH_MASK; break; case ICBTAG_FLAG_AD_LONG: lad = udf_get_filelongad(ptr, alen, &epos->offset, inc); if (!lad) return 0; *etype = le32_to_cpu(lad->extLength) >> 30; *eloc = lelb_to_cpu(lad->extLocation); *elen = le32_to_cpu(lad->extLength) & UDF_EXTENT_LENGTH_MASK; break; default: udf_debug(""alloc_type = %u unsupported\n"", iinfo->i_alloc_type); return -EINVAL; } return 1; }"
272----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46756/bad/w83627ehf.c----store_tolerance,"store_tolerance(struct device *dev, struct device_attribute *attr, const char *buf, size_t count) { struct w83627ehf_data *data = dev_get_drvdata(dev); struct sensor_device_attribute *sensor_attr = to_sensor_dev_attr(attr); int nr = sensor_attr->index; u16 reg; long val; int err; err = kstrtol(buf, 10, &val); if (err < 0) return err; <S2SV_StartVul> val = clamp_val(DIV_ROUND_CLOSEST(val, 1000), 0, 15); <S2SV_EndVul> mutex_lock(&data->update_lock); reg = w83627ehf_read_value(data, W83627EHF_REG_TOLERANCE[nr]); if (nr == 1) reg = (reg & 0x0f) | (val << 4); else reg = (reg & 0xf0) | val; w83627ehf_write_value(data, W83627EHF_REG_TOLERANCE[nr], reg); data->tolerance[nr] = val; mutex_unlock(&data->update_lock); return count; }","- val = clamp_val(DIV_ROUND_CLOSEST(val, 1000), 0, 15);
+ val = DIV_ROUND_CLOSEST(clamp_val(val, 0, 15000), 1000);","store_tolerance(struct device *dev, struct device_attribute *attr, const char *buf, size_t count) { struct w83627ehf_data *data = dev_get_drvdata(dev); struct sensor_device_attribute *sensor_attr = to_sensor_dev_attr(attr); int nr = sensor_attr->index; u16 reg; long val; int err; err = kstrtol(buf, 10, &val); if (err < 0) return err; val = DIV_ROUND_CLOSEST(clamp_val(val, 0, 15000), 1000); mutex_lock(&data->update_lock); reg = w83627ehf_read_value(data, W83627EHF_REG_TOLERANCE[nr]); if (nr == 1) reg = (reg & 0x0f) | (val << 4); else reg = (reg & 0xf0) | val; w83627ehf_write_value(data, W83627EHF_REG_TOLERANCE[nr], reg); data->tolerance[nr] = val; mutex_unlock(&data->update_lock); return count; }"
688----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49983/bad/extents.c----ext4_ext_replay_update_ex,"int ext4_ext_replay_update_ex(struct inode *inode, ext4_lblk_t start, int len, int unwritten, ext4_fsblk_t pblk) { <S2SV_StartVul> struct ext4_ext_path *path = NULL, *ppath; <S2SV_EndVul> struct ext4_extent *ex; int ret; path = ext4_find_extent(inode, start, NULL, 0); if (IS_ERR(path)) return PTR_ERR(path); ex = path[path->p_depth].p_ext; if (!ex) { ret = -EFSCORRUPTED; goto out; } if (le32_to_cpu(ex->ee_block) != start || ext4_ext_get_actual_len(ex) != len) { <S2SV_StartVul> ppath = path; <S2SV_EndVul> down_write(&EXT4_I(inode)->i_data_sem); <S2SV_StartVul> ret = ext4_force_split_extent_at(NULL, inode, &ppath, start, 1); <S2SV_EndVul> up_write(&EXT4_I(inode)->i_data_sem); if (ret) goto out; <S2SV_StartVul> kfree(path); <S2SV_EndVul> <S2SV_StartVul> path = ext4_find_extent(inode, start, NULL, 0); <S2SV_EndVul> if (IS_ERR(path)) <S2SV_StartVul> return -1; <S2SV_EndVul> <S2SV_StartVul> ppath = path; <S2SV_EndVul> ex = path[path->p_depth].p_ext; WARN_ON(le32_to_cpu(ex->ee_block) != start); if (ext4_ext_get_actual_len(ex) != len) { down_write(&EXT4_I(inode)->i_data_sem); <S2SV_StartVul> ret = ext4_force_split_extent_at(NULL, inode, &ppath, <S2SV_EndVul> start + len, 1); up_write(&EXT4_I(inode)->i_data_sem); if (ret) goto out; <S2SV_StartVul> kfree(path); <S2SV_EndVul> <S2SV_StartVul> path = ext4_find_extent(inode, start, NULL, 0); <S2SV_EndVul> if (IS_ERR(path)) <S2SV_StartVul> return -EINVAL; <S2SV_EndVul> ex = path[path->p_depth].p_ext; } } if (unwritten) ext4_ext_mark_unwritten(ex); else ext4_ext_mark_initialized(ex); ext4_ext_store_pblock(ex, pblk); down_write(&EXT4_I(inode)->i_data_sem); ret = ext4_ext_dirty(NULL, inode, &path[path->p_depth]); up_write(&EXT4_I(inode)->i_data_sem); out: ext4_ext_drop_refs(path); kfree(path); ext4_mark_inode_dirty(NULL, inode); return ret; }","- struct ext4_ext_path *path = NULL, *ppath;
- ppath = path;
- ret = ext4_force_split_extent_at(NULL, inode, &ppath, start, 1);
- kfree(path);
- path = ext4_find_extent(inode, start, NULL, 0);
- return -1;
- ppath = path;
- ret = ext4_force_split_extent_at(NULL, inode, &ppath,
- kfree(path);
- path = ext4_find_extent(inode, start, NULL, 0);
- return -EINVAL;
+ struct ext4_ext_path *path;
+ ret = ext4_force_split_extent_at(NULL, inode, &path, start, 1);
+ path = ext4_find_extent(inode, start, &path, 0);
+ return PTR_ERR(path);
+ ret = ext4_force_split_extent_at(NULL, inode, &path,
+ path = ext4_find_extent(inode, start, &path, 0);
+ return PTR_ERR(path);","int ext4_ext_replay_update_ex(struct inode *inode, ext4_lblk_t start, int len, int unwritten, ext4_fsblk_t pblk) { struct ext4_ext_path *path; struct ext4_extent *ex; int ret; path = ext4_find_extent(inode, start, NULL, 0); if (IS_ERR(path)) return PTR_ERR(path); ex = path[path->p_depth].p_ext; if (!ex) { ret = -EFSCORRUPTED; goto out; } if (le32_to_cpu(ex->ee_block) != start || ext4_ext_get_actual_len(ex) != len) { down_write(&EXT4_I(inode)->i_data_sem); ret = ext4_force_split_extent_at(NULL, inode, &path, start, 1); up_write(&EXT4_I(inode)->i_data_sem); if (ret) goto out; path = ext4_find_extent(inode, start, &path, 0); if (IS_ERR(path)) return PTR_ERR(path); ex = path[path->p_depth].p_ext; WARN_ON(le32_to_cpu(ex->ee_block) != start); if (ext4_ext_get_actual_len(ex) != len) { down_write(&EXT4_I(inode)->i_data_sem); ret = ext4_force_split_extent_at(NULL, inode, &path, start + len, 1); up_write(&EXT4_I(inode)->i_data_sem); if (ret) goto out; path = ext4_find_extent(inode, start, &path, 0); if (IS_ERR(path)) return PTR_ERR(path); ex = path[path->p_depth].p_ext; } } if (unwritten) ext4_ext_mark_unwritten(ex); else ext4_ext_mark_initialized(ex); ext4_ext_store_pblock(ex, pblk); down_write(&EXT4_I(inode)->i_data_sem); ret = ext4_ext_dirty(NULL, inode, &path[path->p_depth]); up_write(&EXT4_I(inode)->i_data_sem); out: ext4_ext_drop_refs(path); kfree(path); ext4_mark_inode_dirty(NULL, inode); return ret; }"
67----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44943/bad/hugetlb.c----*hugetlb_follow_page_mask,"struct page *hugetlb_follow_page_mask(struct vm_area_struct *vma, unsigned long address, unsigned int flags, unsigned int *page_mask) { struct hstate *h = hstate_vma(vma); struct mm_struct *mm = vma->vm_mm; unsigned long haddr = address & huge_page_mask(h); struct page *page = NULL; spinlock_t *ptl; pte_t *pte, entry; int ret; hugetlb_vma_lock_read(vma); pte = hugetlb_walk(vma, haddr, huge_page_size(h)); if (!pte) goto out_unlock; ptl = huge_pte_lock(h, mm, pte); entry = huge_ptep_get(pte); if (pte_present(entry)) { page = pte_page(entry); if (!huge_pte_write(entry)) { if (flags & FOLL_WRITE) { page = NULL; goto out; } if (gup_must_unshare(vma, flags, page)) { page = ERR_PTR(-EMLINK); goto out; } } page = nth_page(page, ((address & ~huge_page_mask(h)) >> PAGE_SHIFT)); <S2SV_StartVul> ret = try_grab_page(page, flags); <S2SV_EndVul> if (WARN_ON_ONCE(ret)) { page = ERR_PTR(ret); goto out; } *page_mask = (1U << huge_page_order(h)) - 1; } out: spin_unlock(ptl); out_unlock: hugetlb_vma_unlock_read(vma); if (!page && (flags & FOLL_DUMP) && !hugetlbfs_pagecache_present(h, vma, address)) page = ERR_PTR(-EFAULT); return page; }","- ret = try_grab_page(page, flags);
+ ret = try_grab_folio(page_folio(page), 1, flags);","struct page *hugetlb_follow_page_mask(struct vm_area_struct *vma, unsigned long address, unsigned int flags, unsigned int *page_mask) { struct hstate *h = hstate_vma(vma); struct mm_struct *mm = vma->vm_mm; unsigned long haddr = address & huge_page_mask(h); struct page *page = NULL; spinlock_t *ptl; pte_t *pte, entry; int ret; hugetlb_vma_lock_read(vma); pte = hugetlb_walk(vma, haddr, huge_page_size(h)); if (!pte) goto out_unlock; ptl = huge_pte_lock(h, mm, pte); entry = huge_ptep_get(pte); if (pte_present(entry)) { page = pte_page(entry); if (!huge_pte_write(entry)) { if (flags & FOLL_WRITE) { page = NULL; goto out; } if (gup_must_unshare(vma, flags, page)) { page = ERR_PTR(-EMLINK); goto out; } } page = nth_page(page, ((address & ~huge_page_mask(h)) >> PAGE_SHIFT)); ret = try_grab_folio(page_folio(page), 1, flags); if (WARN_ON_ONCE(ret)) { page = ERR_PTR(ret); goto out; } *page_mask = (1U << huge_page_order(h)) - 1; } out: spin_unlock(ptl); out_unlock: hugetlb_vma_unlock_read(vma); if (!page && (flags & FOLL_DUMP) && !hugetlbfs_pagecache_present(h, vma, address)) page = ERR_PTR(-EFAULT); return page; }"
1186----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53173/bad/nfs4proc.c----nfs4_open_release,"static void nfs4_open_release(void *calldata) { struct nfs4_opendata *data = calldata; struct nfs4_state *state = NULL; if (!data->cancelled) <S2SV_StartVul> goto out_free; <S2SV_EndVul> <S2SV_StartVul> if (data->rpc_status != 0 || !data->rpc_done) <S2SV_EndVul> <S2SV_StartVul> goto out_free; <S2SV_EndVul> if (data->o_res.rflags & NFS4_OPEN_RESULT_CONFIRM) <S2SV_StartVul> goto out_free; <S2SV_EndVul> state = nfs4_opendata_to_nfs4_state(data); if (!IS_ERR(state)) nfs4_close_state(state, data->o_arg.fmode); out_free: nfs4_opendata_put(data); }","- goto out_free;
- if (data->rpc_status != 0 || !data->rpc_done)
- goto out_free;
- goto out_free;
+ if (data->rpc_status != 0 || !data->rpc_done) {
+ nfs_release_seqid(data->o_arg.seqid);
+ goto out_free;
+ }
+ goto out_free;
+ goto out_free;","static void nfs4_open_release(void *calldata) { struct nfs4_opendata *data = calldata; struct nfs4_state *state = NULL; if (data->rpc_status != 0 || !data->rpc_done) { nfs_release_seqid(data->o_arg.seqid); goto out_free; } if (!data->cancelled) goto out_free; if (data->o_res.rflags & NFS4_OPEN_RESULT_CONFIRM) goto out_free; state = nfs4_opendata_to_nfs4_state(data); if (!IS_ERR(state)) nfs4_close_state(state, data->o_arg.fmode); out_free: nfs4_opendata_put(data); }"
594----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49916/bad/dcn401_hwseq.c----dcn401_init_hw,"void dcn401_init_hw(struct dc *dc) { struct abm **abms = dc->res_pool->multiple_abms; struct dce_hwseq *hws = dc->hwseq; struct dc_bios *dcb = dc->ctx->dc_bios; struct resource_pool *res_pool = dc->res_pool; int i; int edp_num; uint32_t backlight = MAX_BACKLIGHT_LEVEL; uint32_t user_level = MAX_BACKLIGHT_LEVEL; <S2SV_StartVul> if (dc->clk_mgr && dc->clk_mgr->funcs->init_clocks) { <S2SV_EndVul> dc->clk_mgr->funcs->init_clocks(dc->clk_mgr); dc->caps.dcmode_power_limits_present = (dc->clk_mgr->bw_params->clk_table.num_entries_per_clk.num_dcfclk_levels && dc->clk_mgr->bw_params->dc_mode_limit.dcfclk_mhz) || (dc->clk_mgr->bw_params->clk_table.num_entries_per_clk.num_dispclk_levels && dc->clk_mgr->bw_params->dc_mode_limit.dispclk_mhz) || (dc->clk_mgr->bw_params->clk_table.num_entries_per_clk.num_dtbclk_levels && dc->clk_mgr->bw_params->dc_mode_limit.dtbclk_mhz) || (dc->clk_mgr->bw_params->clk_table.num_entries_per_clk.num_fclk_levels && dc->clk_mgr->bw_params->dc_mode_limit.fclk_mhz) || (dc->clk_mgr->bw_params->clk_table.num_entries_per_clk.num_memclk_levels && dc->clk_mgr->bw_params->dc_mode_limit.memclk_mhz) || (dc->clk_mgr->bw_params->clk_table.num_entries_per_clk.num_socclk_levels && dc->clk_mgr->bw_params->dc_mode_limit.socclk_mhz); } if (res_pool->dccg->funcs->dccg_init) res_pool->dccg->funcs->dccg_init(res_pool->dccg); if (dc->debug.enable_mem_low_power.bits.optc) { REG_SET_2(ODM_MEM_PWR_CTRL3, 0, ODM_MEM_UNASSIGNED_PWR_MODE, 3, ODM_MEM_VBLANK_PWR_MODE, 1); } if (dc->debug.enable_mem_low_power.bits.vga) { REG_UPDATE(MMHUBBUB_MEM_PWR_CNTL, VGA_MEM_PWR_FORCE, 1); } if (dc->ctx->dc_bios->fw_info_valid) { res_pool->ref_clocks.xtalin_clock_inKhz = dc->ctx->dc_bios->fw_info.pll_info.crystal_frequency; if (res_pool->hubbub) { (res_pool->dccg->funcs->get_dccg_ref_freq)(res_pool->dccg, dc->ctx->dc_bios->fw_info.pll_info.crystal_frequency, &res_pool->ref_clocks.dccg_ref_clock_inKhz); (res_pool->hubbub->funcs->get_dchub_ref_freq)(res_pool->hubbub, res_pool->ref_clocks.dccg_ref_clock_inKhz, &res_pool->ref_clocks.dchub_ref_clock_inKhz); } else { res_pool->ref_clocks.dccg_ref_clock_inKhz = res_pool->ref_clocks.xtalin_clock_inKhz; res_pool->ref_clocks.dchub_ref_clock_inKhz = res_pool->ref_clocks.xtalin_clock_inKhz; } } else ASSERT_CRITICAL(false); for (i = 0; i < dc->link_count; i++) { struct dc_link *link = dc->links[i]; link->link_enc->funcs->hw_init(link->link_enc); if (link->link_enc->funcs->is_dig_enabled && link->link_enc->funcs->is_dig_enabled(link->link_enc)) { link->link_status.link_active = true; link->phy_state.symclk_state = SYMCLK_ON_TX_ON; if (link->link_enc->funcs->fec_is_active && link->link_enc->funcs->fec_is_active(link->link_enc)) link->fec_state = dc_link_fec_enabled; } } if (hws->funcs.enable_power_gating_plane) hws->funcs.enable_power_gating_plane(dc->hwseq, true); dc->link_srv->blank_all_dp_displays(dc); if (dcb->funcs->is_accelerated_mode(dcb) || !dc->config.seamless_boot_edp_requested) { if (dc->hwss.enable_accelerated_mode && dc->debug.disable_boot_optimizations) dc->hwss.enable_accelerated_mode(dc, dc->current_state); else hws->funcs.init_pipes(dc, dc->current_state); if (dc->res_pool->hubbub->funcs->allow_self_refresh_control) dc->res_pool->hubbub->funcs->allow_self_refresh_control(dc->res_pool->hubbub, !dc->res_pool->hubbub->ctx->dc->debug.disable_stutter); dcn401_initialize_min_clocks(dc); dc_allow_idle_optimizations(dc, false); dc_allow_idle_optimizations(dc, true); } if (!dc->config.seamless_boot_edp_requested) { struct dc_link *edp_links[MAX_NUM_EDP]; struct dc_link *edp_link; dc_get_edp_links(dc, edp_links, &edp_num); if (edp_num) { for (i = 0; i < edp_num; i++) { edp_link = edp_links[i]; if (edp_link->link_enc->funcs->is_dig_enabled && edp_link->link_enc->funcs->is_dig_enabled(edp_link->link_enc) && dc->hwss.edp_backlight_control && dc->hwss.power_down && dc->hwss.edp_power_control) { dc->hwss.edp_backlight_control(edp_link, false); dc->hwss.power_down(dc); dc->hwss.edp_power_control(edp_link, false); } } } else { for (i = 0; i < dc->link_count; i++) { struct dc_link *link = dc->links[i]; if (link->link_enc->funcs->is_dig_enabled && link->link_enc->funcs->is_dig_enabled(link->link_enc) && dc->hwss.power_down) { dc->hwss.power_down(dc); break; } } } } for (i = 0; i < res_pool->audio_count; i++) { struct audio *audio = res_pool->audios[i]; audio->funcs->hw_init(audio); } for (i = 0; i < dc->link_count; i++) { struct dc_link *link = dc->links[i]; if (link->panel_cntl) { backlight = link->panel_cntl->funcs->hw_init(link->panel_cntl); user_level = link->panel_cntl->stored_backlight_registers.USER_LEVEL; } } for (i = 0; i < dc->res_pool->pipe_count; i++) { if (abms[i] != NULL && abms[i]->funcs != NULL) abms[i]->funcs->abm_init(abms[i], backlight, user_level); } REG_WRITE(DIO_MEM_PWR_CTRL, 0); if (!dc->debug.disable_clock_gate) { REG_WRITE(DCCG_GATE_DISABLE_CNTL, 0); REG_WRITE(DCCG_GATE_DISABLE_CNTL2, 0); REG_UPDATE(DCFCLK_CNTL, DCFCLK_GATE_DIS, 0); } dcn401_setup_hpo_hw_control(hws, true); if (!dcb->funcs->is_accelerated_mode(dcb) && dc->res_pool->hubbub->funcs->init_watermarks) dc->res_pool->hubbub->funcs->init_watermarks(dc->res_pool->hubbub); <S2SV_StartVul> if (dc->clk_mgr->funcs->notify_wm_ranges) <S2SV_EndVul> dc->clk_mgr->funcs->notify_wm_ranges(dc->clk_mgr); if (dc->clk_mgr->funcs->set_hard_max_memclk && !dc->clk_mgr->dc_mode_softmax_enabled) dc->clk_mgr->funcs->set_hard_max_memclk(dc->clk_mgr); if (dc->res_pool->hubbub->funcs->force_pstate_change_control) dc->res_pool->hubbub->funcs->force_pstate_change_control( dc->res_pool->hubbub, false, false); if (dc->res_pool->hubbub->funcs->init_crb) dc->res_pool->hubbub->funcs->init_crb(dc->res_pool->hubbub); if (dc->res_pool->hubbub->funcs->set_request_limit && dc->config.sdpif_request_limit_words_per_umc > 0) dc->res_pool->hubbub->funcs->set_request_limit(dc->res_pool->hubbub, dc->ctx->dc_bios->vram_info.num_chans, dc->config.sdpif_request_limit_words_per_umc); if (dc->ctx->dmub_srv) { dc_dmub_srv_query_caps_cmd(dc->ctx->dmub_srv); dc->caps.dmub_caps.psr = dc->ctx->dmub_srv->dmub->feature_caps.psr; dc->caps.dmub_caps.mclk_sw = dc->ctx->dmub_srv->dmub->feature_caps.fw_assisted_mclk_switch_ver > 0; dc->caps.dmub_caps.fams_ver = dc->ctx->dmub_srv->dmub->feature_caps.fw_assisted_mclk_switch_ver; dc->debug.fams2_config.bits.enable &= dc->ctx->dmub_srv->dmub->feature_caps.fw_assisted_mclk_switch_ver == 2; if (!dc->debug.fams2_config.bits.enable && dc->res_pool->funcs->update_bw_bounding_box) { <S2SV_StartVul> dc->res_pool->funcs->update_bw_bounding_box(dc, dc->clk_mgr->bw_params); <S2SV_EndVul> } } }","- if (dc->clk_mgr && dc->clk_mgr->funcs->init_clocks) {
- if (dc->clk_mgr->funcs->notify_wm_ranges)
- dc->res_pool->funcs->update_bw_bounding_box(dc, dc->clk_mgr->bw_params);
+ if (dc->clk_mgr && dc->clk_mgr->funcs && dc->clk_mgr->funcs->init_clocks) {
+ if (dc->clk_mgr && dc->clk_mgr->funcs && dc->clk_mgr->funcs->notify_wm_ranges)
+ if (dc->clk_mgr)
+ dc->res_pool->funcs->update_bw_bounding_box(dc,
+ dc->clk_mgr->bw_params);","void dcn401_init_hw(struct dc *dc) { struct abm **abms = dc->res_pool->multiple_abms; struct dce_hwseq *hws = dc->hwseq; struct dc_bios *dcb = dc->ctx->dc_bios; struct resource_pool *res_pool = dc->res_pool; int i; int edp_num; uint32_t backlight = MAX_BACKLIGHT_LEVEL; uint32_t user_level = MAX_BACKLIGHT_LEVEL; if (dc->clk_mgr && dc->clk_mgr->funcs && dc->clk_mgr->funcs->init_clocks) { dc->clk_mgr->funcs->init_clocks(dc->clk_mgr); dc->caps.dcmode_power_limits_present = (dc->clk_mgr->bw_params->clk_table.num_entries_per_clk.num_dcfclk_levels && dc->clk_mgr->bw_params->dc_mode_limit.dcfclk_mhz) || (dc->clk_mgr->bw_params->clk_table.num_entries_per_clk.num_dispclk_levels && dc->clk_mgr->bw_params->dc_mode_limit.dispclk_mhz) || (dc->clk_mgr->bw_params->clk_table.num_entries_per_clk.num_dtbclk_levels && dc->clk_mgr->bw_params->dc_mode_limit.dtbclk_mhz) || (dc->clk_mgr->bw_params->clk_table.num_entries_per_clk.num_fclk_levels && dc->clk_mgr->bw_params->dc_mode_limit.fclk_mhz) || (dc->clk_mgr->bw_params->clk_table.num_entries_per_clk.num_memclk_levels && dc->clk_mgr->bw_params->dc_mode_limit.memclk_mhz) || (dc->clk_mgr->bw_params->clk_table.num_entries_per_clk.num_socclk_levels && dc->clk_mgr->bw_params->dc_mode_limit.socclk_mhz); } if (res_pool->dccg->funcs->dccg_init) res_pool->dccg->funcs->dccg_init(res_pool->dccg); if (dc->debug.enable_mem_low_power.bits.optc) { REG_SET_2(ODM_MEM_PWR_CTRL3, 0, ODM_MEM_UNASSIGNED_PWR_MODE, 3, ODM_MEM_VBLANK_PWR_MODE, 1); } if (dc->debug.enable_mem_low_power.bits.vga) { REG_UPDATE(MMHUBBUB_MEM_PWR_CNTL, VGA_MEM_PWR_FORCE, 1); } if (dc->ctx->dc_bios->fw_info_valid) { res_pool->ref_clocks.xtalin_clock_inKhz = dc->ctx->dc_bios->fw_info.pll_info.crystal_frequency; if (res_pool->hubbub) { (res_pool->dccg->funcs->get_dccg_ref_freq)(res_pool->dccg, dc->ctx->dc_bios->fw_info.pll_info.crystal_frequency, &res_pool->ref_clocks.dccg_ref_clock_inKhz); (res_pool->hubbub->funcs->get_dchub_ref_freq)(res_pool->hubbub, res_pool->ref_clocks.dccg_ref_clock_inKhz, &res_pool->ref_clocks.dchub_ref_clock_inKhz); } else { res_pool->ref_clocks.dccg_ref_clock_inKhz = res_pool->ref_clocks.xtalin_clock_inKhz; res_pool->ref_clocks.dchub_ref_clock_inKhz = res_pool->ref_clocks.xtalin_clock_inKhz; } } else ASSERT_CRITICAL(false); for (i = 0; i < dc->link_count; i++) { struct dc_link *link = dc->links[i]; link->link_enc->funcs->hw_init(link->link_enc); if (link->link_enc->funcs->is_dig_enabled && link->link_enc->funcs->is_dig_enabled(link->link_enc)) { link->link_status.link_active = true; link->phy_state.symclk_state = SYMCLK_ON_TX_ON; if (link->link_enc->funcs->fec_is_active && link->link_enc->funcs->fec_is_active(link->link_enc)) link->fec_state = dc_link_fec_enabled; } } if (hws->funcs.enable_power_gating_plane) hws->funcs.enable_power_gating_plane(dc->hwseq, true); dc->link_srv->blank_all_dp_displays(dc); if (dcb->funcs->is_accelerated_mode(dcb) || !dc->config.seamless_boot_edp_requested) { if (dc->hwss.enable_accelerated_mode && dc->debug.disable_boot_optimizations) dc->hwss.enable_accelerated_mode(dc, dc->current_state); else hws->funcs.init_pipes(dc, dc->current_state); if (dc->res_pool->hubbub->funcs->allow_self_refresh_control) dc->res_pool->hubbub->funcs->allow_self_refresh_control(dc->res_pool->hubbub, !dc->res_pool->hubbub->ctx->dc->debug.disable_stutter); dcn401_initialize_min_clocks(dc); dc_allow_idle_optimizations(dc, false); dc_allow_idle_optimizations(dc, true); } if (!dc->config.seamless_boot_edp_requested) { struct dc_link *edp_links[MAX_NUM_EDP]; struct dc_link *edp_link; dc_get_edp_links(dc, edp_links, &edp_num); if (edp_num) { for (i = 0; i < edp_num; i++) { edp_link = edp_links[i]; if (edp_link->link_enc->funcs->is_dig_enabled && edp_link->link_enc->funcs->is_dig_enabled(edp_link->link_enc) && dc->hwss.edp_backlight_control && dc->hwss.power_down && dc->hwss.edp_power_control) { dc->hwss.edp_backlight_control(edp_link, false); dc->hwss.power_down(dc); dc->hwss.edp_power_control(edp_link, false); } } } else { for (i = 0; i < dc->link_count; i++) { struct dc_link *link = dc->links[i]; if (link->link_enc->funcs->is_dig_enabled && link->link_enc->funcs->is_dig_enabled(link->link_enc) && dc->hwss.power_down) { dc->hwss.power_down(dc); break; } } } } for (i = 0; i < res_pool->audio_count; i++) { struct audio *audio = res_pool->audios[i]; audio->funcs->hw_init(audio); } for (i = 0; i < dc->link_count; i++) { struct dc_link *link = dc->links[i]; if (link->panel_cntl) { backlight = link->panel_cntl->funcs->hw_init(link->panel_cntl); user_level = link->panel_cntl->stored_backlight_registers.USER_LEVEL; } } for (i = 0; i < dc->res_pool->pipe_count; i++) { if (abms[i] != NULL && abms[i]->funcs != NULL) abms[i]->funcs->abm_init(abms[i], backlight, user_level); } REG_WRITE(DIO_MEM_PWR_CTRL, 0); if (!dc->debug.disable_clock_gate) { REG_WRITE(DCCG_GATE_DISABLE_CNTL, 0); REG_WRITE(DCCG_GATE_DISABLE_CNTL2, 0); REG_UPDATE(DCFCLK_CNTL, DCFCLK_GATE_DIS, 0); } dcn401_setup_hpo_hw_control(hws, true); if (!dcb->funcs->is_accelerated_mode(dcb) && dc->res_pool->hubbub->funcs->init_watermarks) dc->res_pool->hubbub->funcs->init_watermarks(dc->res_pool->hubbub); if (dc->clk_mgr && dc->clk_mgr->funcs && dc->clk_mgr->funcs->notify_wm_ranges) dc->clk_mgr->funcs->notify_wm_ranges(dc->clk_mgr); if (dc->clk_mgr->funcs->set_hard_max_memclk && !dc->clk_mgr->dc_mode_softmax_enabled) dc->clk_mgr->funcs->set_hard_max_memclk(dc->clk_mgr); if (dc->res_pool->hubbub->funcs->force_pstate_change_control) dc->res_pool->hubbub->funcs->force_pstate_change_control( dc->res_pool->hubbub, false, false); if (dc->res_pool->hubbub->funcs->init_crb) dc->res_pool->hubbub->funcs->init_crb(dc->res_pool->hubbub); if (dc->res_pool->hubbub->funcs->set_request_limit && dc->config.sdpif_request_limit_words_per_umc > 0) dc->res_pool->hubbub->funcs->set_request_limit(dc->res_pool->hubbub, dc->ctx->dc_bios->vram_info.num_chans, dc->config.sdpif_request_limit_words_per_umc); if (dc->ctx->dmub_srv) { dc_dmub_srv_query_caps_cmd(dc->ctx->dmub_srv); dc->caps.dmub_caps.psr = dc->ctx->dmub_srv->dmub->feature_caps.psr; dc->caps.dmub_caps.mclk_sw = dc->ctx->dmub_srv->dmub->feature_caps.fw_assisted_mclk_switch_ver > 0; dc->caps.dmub_caps.fams_ver = dc->ctx->dmub_srv->dmub->feature_caps.fw_assisted_mclk_switch_ver; dc->debug.fams2_config.bits.enable &= dc->ctx->dmub_srv->dmub->feature_caps.fw_assisted_mclk_switch_ver == 2; if (!dc->debug.fams2_config.bits.enable && dc->res_pool->funcs->update_bw_bounding_box) { if (dc->clk_mgr) dc->res_pool->funcs->update_bw_bounding_box(dc, dc->clk_mgr->bw_params); } } }"
174----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-45026/bad/dasd_3990_erp.c----dasd_3990_erp_file_prot,"dasd_3990_erp_file_prot(struct dasd_ccw_req * erp) { struct dasd_device *device = erp->startdev; <S2SV_StartVul> if (!test_bit(DASD_CQR_SUPPRESS_FP, &erp->flags)) <S2SV_EndVul> <S2SV_StartVul> dev_err(&device->cdev->dev, <S2SV_EndVul> <S2SV_StartVul> ""Accessing the DASD failed because of a hardware error\n""); <S2SV_EndVul> return dasd_3990_erp_cleanup(erp, DASD_CQR_FAILED); }","- if (!test_bit(DASD_CQR_SUPPRESS_FP, &erp->flags))
- dev_err(&device->cdev->dev,
- ""Accessing the DASD failed because of a hardware error\n"");
+ dev_err(&device->cdev->dev,
+ ""Accessing the DASD failed because of a hardware error\n"");","dasd_3990_erp_file_prot(struct dasd_ccw_req * erp) { struct dasd_device *device = erp->startdev; dev_err(&device->cdev->dev, ""Accessing the DASD failed because of a hardware error\n""); return dasd_3990_erp_cleanup(erp, DASD_CQR_FAILED); }"
1216----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53226/bad/hns_roce_mr.c----hns_roce_map_mr_sg,"int hns_roce_map_mr_sg(struct ib_mr *ibmr, struct scatterlist *sg, int sg_nents, unsigned int *sg_offset) { struct hns_roce_dev *hr_dev = to_hr_dev(ibmr->device); struct ib_device *ibdev = &hr_dev->ib_dev; struct hns_roce_mr *mr = to_hr_mr(ibmr); struct hns_roce_mtr *mtr = &mr->pbl_mtr; int ret, sg_num = 0; if (!IS_ALIGNED(*sg_offset, HNS_ROCE_FRMR_ALIGN_SIZE) || ibmr->page_size < HNS_HW_PAGE_SIZE || ibmr->page_size > HNS_HW_MAX_PAGE_SIZE) return sg_num; mr->npages = 0; mr->page_list = kvcalloc(mr->pbl_mtr.hem_cfg.buf_pg_count, sizeof(dma_addr_t), GFP_KERNEL); if (!mr->page_list) return sg_num; <S2SV_StartVul> sg_num = ib_sg_to_pages(ibmr, sg, sg_nents, sg_offset, hns_roce_set_page); <S2SV_EndVul> if (sg_num < 1) { ibdev_err(ibdev, ""failed to store sg pages %u %u, cnt = %d.\n"", mr->npages, mr->pbl_mtr.hem_cfg.buf_pg_count, sg_num); goto err_page_list; } mtr->hem_cfg.region[0].offset = 0; mtr->hem_cfg.region[0].count = mr->npages; mtr->hem_cfg.region[0].hopnum = mr->pbl_hop_num; mtr->hem_cfg.region_count = 1; ret = hns_roce_mtr_map(hr_dev, mtr, mr->page_list, mr->npages); if (ret) { ibdev_err(ibdev, ""failed to map sg mtr, ret = %d.\n"", ret); sg_num = 0; } else { mr->pbl_mtr.hem_cfg.buf_pg_shift = (u32)ilog2(ibmr->page_size); } err_page_list: kvfree(mr->page_list); mr->page_list = NULL; return sg_num; }","- sg_num = ib_sg_to_pages(ibmr, sg, sg_nents, sg_offset, hns_roce_set_page);
+ sg_num = ib_sg_to_pages(ibmr, sg, sg_nents, sg_offset_p, hns_roce_set_page);","int hns_roce_map_mr_sg(struct ib_mr *ibmr, struct scatterlist *sg, int sg_nents, unsigned int *sg_offset_p) { unsigned int sg_offset = sg_offset_p ? *sg_offset_p : 0; struct hns_roce_dev *hr_dev = to_hr_dev(ibmr->device); struct ib_device *ibdev = &hr_dev->ib_dev; struct hns_roce_mr *mr = to_hr_mr(ibmr); struct hns_roce_mtr *mtr = &mr->pbl_mtr; int ret, sg_num = 0; if (!IS_ALIGNED(sg_offset, HNS_ROCE_FRMR_ALIGN_SIZE) || ibmr->page_size < HNS_HW_PAGE_SIZE || ibmr->page_size > HNS_HW_MAX_PAGE_SIZE) return sg_num; mr->npages = 0; mr->page_list = kvcalloc(mr->pbl_mtr.hem_cfg.buf_pg_count, sizeof(dma_addr_t), GFP_KERNEL); if (!mr->page_list) return sg_num; sg_num = ib_sg_to_pages(ibmr, sg, sg_nents, sg_offset_p, hns_roce_set_page); if (sg_num < 1) { ibdev_err(ibdev, ""failed to store sg pages %u %u, cnt = %d.\n"", mr->npages, mr->pbl_mtr.hem_cfg.buf_pg_count, sg_num); goto err_page_list; } mtr->hem_cfg.region[0].offset = 0; mtr->hem_cfg.region[0].count = mr->npages; mtr->hem_cfg.region[0].hopnum = mr->pbl_hop_num; mtr->hem_cfg.region_count = 1; ret = hns_roce_mtr_map(hr_dev, mtr, mr->page_list, mr->npages); if (ret) { ibdev_err(ibdev, ""failed to map sg mtr, ret = %d.\n"", ret); sg_num = 0; } else { mr->pbl_mtr.hem_cfg.buf_pg_shift = (u32)ilog2(ibmr->page_size); } err_page_list: kvfree(mr->page_list); mr->page_list = NULL; return sg_num; }"
979----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50214/bad/drm_connector_test.c----drm_test_drm_hdmi_compute_mode_clock_rgb_12bpc,"static void drm_test_drm_hdmi_compute_mode_clock_rgb_12bpc(struct kunit *test) { struct drm_connector_init_priv *priv = test->priv; const struct drm_display_mode *mode; unsigned long long rate; struct drm_device *drm = &priv->drm; <S2SV_StartVul> mode = drm_display_mode_from_cea_vic(drm, 16); <S2SV_EndVul> KUNIT_ASSERT_NOT_NULL(test, mode); KUNIT_ASSERT_FALSE(test, mode->flags & DRM_MODE_FLAG_DBLCLK); rate = drm_hdmi_compute_mode_clock(mode, 12, HDMI_COLORSPACE_RGB); KUNIT_ASSERT_GT(test, rate, 0); KUNIT_EXPECT_EQ(test, mode->clock * 1500, rate); }","- mode = drm_display_mode_from_cea_vic(drm, 16);
+ mode = drm_kunit_display_mode_from_cea_vic(test, drm, 16);","static void drm_test_drm_hdmi_compute_mode_clock_rgb_12bpc(struct kunit *test) { struct drm_connector_init_priv *priv = test->priv; const struct drm_display_mode *mode; unsigned long long rate; struct drm_device *drm = &priv->drm; mode = drm_kunit_display_mode_from_cea_vic(test, drm, 16); KUNIT_ASSERT_NOT_NULL(test, mode); KUNIT_ASSERT_FALSE(test, mode->flags & DRM_MODE_FLAG_DBLCLK); rate = drm_hdmi_compute_mode_clock(mode, 12, HDMI_COLORSPACE_RGB); KUNIT_ASSERT_GT(test, rate, 0); KUNIT_EXPECT_EQ(test, mode->clock * 1500, rate); }"
48----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44932/bad/idpf_lib.c----idpf_vport_open,"static int idpf_vport_open(struct idpf_vport *vport) { struct idpf_netdev_priv *np = netdev_priv(vport->netdev); struct idpf_adapter *adapter = vport->adapter; struct idpf_vport_config *vport_config; int err; if (np->state != __IDPF_VPORT_DOWN) return -EBUSY; netif_carrier_off(vport->netdev); <S2SV_StartVul> err = idpf_vport_queues_alloc(vport); <S2SV_EndVul> <S2SV_StartVul> if (err) <S2SV_EndVul> <S2SV_StartVul> return err; <S2SV_EndVul> err = idpf_vport_intr_alloc(vport); if (err) { dev_err(&adapter->pdev->dev, ""Failed to allocate interrupts for vport %u: %d\n"", vport->vport_id, err); <S2SV_StartVul> goto queues_rel; <S2SV_EndVul> } err = idpf_vport_queue_ids_init(vport); if (err) { dev_err(&adapter->pdev->dev, ""Failed to initialize queue ids for vport %u: %d\n"", vport->vport_id, err); <S2SV_StartVul> goto intr_rel; <S2SV_EndVul> } err = idpf_vport_intr_init(vport); if (err) { dev_err(&adapter->pdev->dev, ""Failed to initialize interrupts for vport %u: %d\n"", vport->vport_id, err); <S2SV_StartVul> goto intr_rel; <S2SV_EndVul> } err = idpf_rx_bufs_init_all(vport); if (err) { dev_err(&adapter->pdev->dev, ""Failed to initialize RX buffers for vport %u: %d\n"", vport->vport_id, err); <S2SV_StartVul> goto intr_rel; <S2SV_EndVul> } err = idpf_queue_reg_init(vport); if (err) { dev_err(&adapter->pdev->dev, ""Failed to initialize queue registers for vport %u: %d\n"", vport->vport_id, err); <S2SV_StartVul> goto intr_rel; <S2SV_EndVul> } idpf_rx_init_buf_tail(vport); idpf_vport_intr_ena(vport); err = idpf_send_config_queues_msg(vport); if (err) { dev_err(&adapter->pdev->dev, ""Failed to configure queues for vport %u, %d\n"", vport->vport_id, err); goto intr_deinit; } err = idpf_send_map_unmap_queue_vector_msg(vport, true); if (err) { dev_err(&adapter->pdev->dev, ""Failed to map queue vectors for vport %u: %d\n"", vport->vport_id, err); goto intr_deinit; } err = idpf_send_enable_queues_msg(vport); if (err) { dev_err(&adapter->pdev->dev, ""Failed to enable queues for vport %u: %d\n"", vport->vport_id, err); goto unmap_queue_vectors; } err = idpf_send_enable_vport_msg(vport); if (err) { dev_err(&adapter->pdev->dev, ""Failed to enable vport %u: %d\n"", vport->vport_id, err); err = -EAGAIN; goto disable_queues; } idpf_restore_features(vport); vport_config = adapter->vport_config[vport->idx]; if (vport_config->user_config.rss_data.rss_lut) err = idpf_config_rss(vport); else err = idpf_init_rss(vport); if (err) { dev_err(&adapter->pdev->dev, ""Failed to initialize RSS for vport %u: %d\n"", vport->vport_id, err); goto disable_vport; } err = idpf_up_complete(vport); if (err) { dev_err(&adapter->pdev->dev, ""Failed to complete interface up for vport %u: %d\n"", vport->vport_id, err); goto deinit_rss; } return 0; deinit_rss: idpf_deinit_rss(vport); disable_vport: idpf_send_disable_vport_msg(vport); disable_queues: idpf_send_disable_queues_msg(vport); unmap_queue_vectors: idpf_send_map_unmap_queue_vector_msg(vport, false); intr_deinit: idpf_vport_intr_deinit(vport); <S2SV_StartVul> intr_rel: <S2SV_EndVul> <S2SV_StartVul> idpf_vport_intr_rel(vport); <S2SV_EndVul> queues_rel: idpf_vport_queues_rel(vport); <S2SV_StartVul> return err; <S2SV_EndVul> }","- err = idpf_vport_queues_alloc(vport);
- if (err)
- return err;
- goto queues_rel;
- goto intr_rel;
- goto intr_rel;
- goto intr_rel;
- goto intr_rel;
- intr_rel:
- idpf_vport_intr_rel(vport);
- return err;
+ return err;
+ err = idpf_vport_queues_alloc(vport);
+ if (err)
+ goto intr_rel;
+ goto queues_rel;
+ goto queues_rel;
+ goto queues_rel;
+ goto queues_rel;
+ intr_rel:
+ idpf_vport_intr_rel(vport);
+ return err;","static int idpf_vport_open(struct idpf_vport *vport) { struct idpf_netdev_priv *np = netdev_priv(vport->netdev); struct idpf_adapter *adapter = vport->adapter; struct idpf_vport_config *vport_config; int err; if (np->state != __IDPF_VPORT_DOWN) return -EBUSY; netif_carrier_off(vport->netdev); err = idpf_vport_intr_alloc(vport); if (err) { dev_err(&adapter->pdev->dev, ""Failed to allocate interrupts for vport %u: %d\n"", vport->vport_id, err); return err; } err = idpf_vport_queues_alloc(vport); if (err) goto intr_rel; err = idpf_vport_queue_ids_init(vport); if (err) { dev_err(&adapter->pdev->dev, ""Failed to initialize queue ids for vport %u: %d\n"", vport->vport_id, err); goto queues_rel; } err = idpf_vport_intr_init(vport); if (err) { dev_err(&adapter->pdev->dev, ""Failed to initialize interrupts for vport %u: %d\n"", vport->vport_id, err); goto queues_rel; } err = idpf_rx_bufs_init_all(vport); if (err) { dev_err(&adapter->pdev->dev, ""Failed to initialize RX buffers for vport %u: %d\n"", vport->vport_id, err); goto queues_rel; } err = idpf_queue_reg_init(vport); if (err) { dev_err(&adapter->pdev->dev, ""Failed to initialize queue registers for vport %u: %d\n"", vport->vport_id, err); goto queues_rel; } idpf_rx_init_buf_tail(vport); idpf_vport_intr_ena(vport); err = idpf_send_config_queues_msg(vport); if (err) { dev_err(&adapter->pdev->dev, ""Failed to configure queues for vport %u, %d\n"", vport->vport_id, err); goto intr_deinit; } err = idpf_send_map_unmap_queue_vector_msg(vport, true); if (err) { dev_err(&adapter->pdev->dev, ""Failed to map queue vectors for vport %u: %d\n"", vport->vport_id, err); goto intr_deinit; } err = idpf_send_enable_queues_msg(vport); if (err) { dev_err(&adapter->pdev->dev, ""Failed to enable queues for vport %u: %d\n"", vport->vport_id, err); goto unmap_queue_vectors; } err = idpf_send_enable_vport_msg(vport); if (err) { dev_err(&adapter->pdev->dev, ""Failed to enable vport %u: %d\n"", vport->vport_id, err); err = -EAGAIN; goto disable_queues; } idpf_restore_features(vport); vport_config = adapter->vport_config[vport->idx]; if (vport_config->user_config.rss_data.rss_lut) err = idpf_config_rss(vport); else err = idpf_init_rss(vport); if (err) { dev_err(&adapter->pdev->dev, ""Failed to initialize RSS for vport %u: %d\n"", vport->vport_id, err); goto disable_vport; } err = idpf_up_complete(vport); if (err) { dev_err(&adapter->pdev->dev, ""Failed to complete interface up for vport %u: %d\n"", vport->vport_id, err); goto deinit_rss; } return 0; deinit_rss: idpf_deinit_rss(vport); disable_vport: idpf_send_disable_vport_msg(vport); disable_queues: idpf_send_disable_queues_msg(vport); unmap_queue_vectors: idpf_send_map_unmap_queue_vector_msg(vport, false); intr_deinit: idpf_vport_intr_deinit(vport); queues_rel: idpf_vport_queues_rel(vport); intr_rel: idpf_vport_intr_rel(vport); return err; }"
1306----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56604/bad/sock.c----*rfcomm_sock_alloc,"static struct sock *rfcomm_sock_alloc(struct net *net, struct socket *sock, int proto, gfp_t prio, int kern) { struct rfcomm_dlc *d; struct sock *sk; <S2SV_StartVul> sk = bt_sock_alloc(net, sock, &rfcomm_proto, proto, prio, kern); <S2SV_EndVul> <S2SV_StartVul> if (!sk) <S2SV_EndVul> return NULL; <S2SV_StartVul> d = rfcomm_dlc_alloc(prio); <S2SV_EndVul> <S2SV_StartVul> if (!d) { <S2SV_EndVul> <S2SV_StartVul> sk_free(sk); <S2SV_EndVul> return NULL; } d->data_ready = rfcomm_sk_data_ready; d->state_change = rfcomm_sk_state_change; rfcomm_pi(sk)->dlc = d; d->owner = sk; sk->sk_destruct = rfcomm_sock_destruct; sk->sk_sndtimeo = RFCOMM_CONN_TIMEOUT; sk->sk_sndbuf = RFCOMM_MAX_CREDITS * RFCOMM_DEFAULT_MTU * 10; sk->sk_rcvbuf = RFCOMM_MAX_CREDITS * RFCOMM_DEFAULT_MTU * 10; bt_sock_link(&rfcomm_sk_list, sk); BT_DBG(""sk %p"", sk); return sk; }","- sk = bt_sock_alloc(net, sock, &rfcomm_proto, proto, prio, kern);
- if (!sk)
- d = rfcomm_dlc_alloc(prio);
- if (!d) {
- sk_free(sk);
+ d = rfcomm_dlc_alloc(prio);
+ if (!d)
+ sk = bt_sock_alloc(net, sock, &rfcomm_proto, proto, prio, kern);
+ if (!sk) {
+ rfcomm_dlc_free(d);","static struct sock *rfcomm_sock_alloc(struct net *net, struct socket *sock, int proto, gfp_t prio, int kern) { struct rfcomm_dlc *d; struct sock *sk; d = rfcomm_dlc_alloc(prio); if (!d) return NULL; sk = bt_sock_alloc(net, sock, &rfcomm_proto, proto, prio, kern); if (!sk) { rfcomm_dlc_free(d); return NULL; } d->data_ready = rfcomm_sk_data_ready; d->state_change = rfcomm_sk_state_change; rfcomm_pi(sk)->dlc = d; d->owner = sk; sk->sk_destruct = rfcomm_sock_destruct; sk->sk_sndtimeo = RFCOMM_CONN_TIMEOUT; sk->sk_sndbuf = RFCOMM_MAX_CREDITS * RFCOMM_DEFAULT_MTU * 10; sk->sk_rcvbuf = RFCOMM_MAX_CREDITS * RFCOMM_DEFAULT_MTU * 10; bt_sock_link(&rfcomm_sk_list, sk); BT_DBG(""sk %p"", sk); return sk; }"
95----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44963/bad/ctree.c----insert_new_root,"static noinline int insert_new_root(struct btrfs_trans_handle *trans, struct btrfs_root *root, struct btrfs_path *path, int level) { u64 lower_gen; struct extent_buffer *lower; struct extent_buffer *c; struct extent_buffer *old; struct btrfs_disk_key lower_key; int ret; BUG_ON(path->nodes[level]); BUG_ON(path->nodes[level-1] != root->node); lower = path->nodes[level-1]; if (level == 1) btrfs_item_key(lower, &lower_key, 0); else btrfs_node_key(lower, &lower_key, 0); c = btrfs_alloc_tree_block(trans, root, 0, btrfs_root_id(root), &lower_key, level, root->node->start, 0, 0, BTRFS_NESTING_NEW_ROOT); if (IS_ERR(c)) return PTR_ERR(c); root_add_used_bytes(root); btrfs_set_header_nritems(c, 1); btrfs_set_node_key(c, &lower_key, 0); btrfs_set_node_blockptr(c, 0, lower->start); lower_gen = btrfs_header_generation(lower); WARN_ON(lower_gen != trans->transid); btrfs_set_node_ptr_generation(c, 0, lower_gen); btrfs_mark_buffer_dirty(trans, c); old = root->node; ret = btrfs_tree_mod_log_insert_root(root->node, c, false); if (ret < 0) { <S2SV_StartVul> btrfs_free_tree_block(trans, btrfs_root_id(root), c, 0, 1); <S2SV_EndVul> btrfs_tree_unlock(c); free_extent_buffer(c); return ret; } rcu_assign_pointer(root->node, c); free_extent_buffer(old); add_root_to_dirty_list(root); atomic_inc(&c->refs); path->nodes[level] = c; path->locks[level] = BTRFS_WRITE_LOCK; path->slots[level] = 0; return 0; }","- btrfs_free_tree_block(trans, btrfs_root_id(root), c, 0, 1);
+ if (ret < 0) {
+ int ret2;
+ ret2 = btrfs_free_tree_block(trans, btrfs_root_id(root), c, 0, 1);
+ if (ret2 < 0)
+ btrfs_abort_transaction(trans, ret2);
+ return ret;","static noinline int insert_new_root(struct btrfs_trans_handle *trans, struct btrfs_root *root, struct btrfs_path *path, int level) { u64 lower_gen; struct extent_buffer *lower; struct extent_buffer *c; struct extent_buffer *old; struct btrfs_disk_key lower_key; int ret; BUG_ON(path->nodes[level]); BUG_ON(path->nodes[level-1] != root->node); lower = path->nodes[level-1]; if (level == 1) btrfs_item_key(lower, &lower_key, 0); else btrfs_node_key(lower, &lower_key, 0); c = btrfs_alloc_tree_block(trans, root, 0, btrfs_root_id(root), &lower_key, level, root->node->start, 0, 0, BTRFS_NESTING_NEW_ROOT); if (IS_ERR(c)) return PTR_ERR(c); root_add_used_bytes(root); btrfs_set_header_nritems(c, 1); btrfs_set_node_key(c, &lower_key, 0); btrfs_set_node_blockptr(c, 0, lower->start); lower_gen = btrfs_header_generation(lower); WARN_ON(lower_gen != trans->transid); btrfs_set_node_ptr_generation(c, 0, lower_gen); btrfs_mark_buffer_dirty(trans, c); old = root->node; ret = btrfs_tree_mod_log_insert_root(root->node, c, false); if (ret < 0) { int ret2; ret2 = btrfs_free_tree_block(trans, btrfs_root_id(root), c, 0, 1); if (ret2 < 0) btrfs_abort_transaction(trans, ret2); btrfs_tree_unlock(c); free_extent_buffer(c); return ret; } rcu_assign_pointer(root->node, c); free_extent_buffer(old); add_root_to_dirty_list(root); atomic_inc(&c->refs); path->nodes[level] = c; path->locks[level] = BTRFS_WRITE_LOCK; path->slots[level] = 0; return 0; }"
66----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44943/bad/gup.c----gup_hugepte,"static int gup_hugepte(pte_t *ptep, unsigned long sz, unsigned long addr, unsigned long end, unsigned int flags, struct page **pages, int *nr) { unsigned long pte_end; struct page *page; struct folio *folio; pte_t pte; int refs; pte_end = (addr + sz) & ~(sz-1); if (pte_end < end) end = pte_end; pte = huge_ptep_get(ptep); if (!pte_access_permitted(pte, flags & FOLL_WRITE)) return 0; VM_BUG_ON(!pfn_valid(pte_pfn(pte))); page = nth_page(pte_page(pte), (addr & (sz - 1)) >> PAGE_SHIFT); refs = record_subpages(page, addr, end, pages + *nr); <S2SV_StartVul> folio = try_grab_folio(page, refs, flags); <S2SV_EndVul> <S2SV_StartVul> if (!folio) <S2SV_EndVul> return 0; if (unlikely(pte_val(pte) != pte_val(ptep_get(ptep)))) { gup_put_folio(folio, refs, flags); return 0; } if (!folio_fast_pin_allowed(folio, flags)) { gup_put_folio(folio, refs, flags); return 0; } if (!pte_write(pte) && gup_must_unshare(NULL, flags, &folio->page)) { gup_put_folio(folio, refs, flags); return 0; } *nr += refs; folio_set_referenced(folio); return 1; }","- folio = try_grab_folio(page, refs, flags);
- if (!folio)
+ folio = try_grab_folio_fast(page, refs, flags);
+ if (!folio)","static int gup_hugepte(pte_t *ptep, unsigned long sz, unsigned long addr, unsigned long end, unsigned int flags, struct page **pages, int *nr) { unsigned long pte_end; struct page *page; struct folio *folio; pte_t pte; int refs; pte_end = (addr + sz) & ~(sz-1); if (pte_end < end) end = pte_end; pte = huge_ptep_get(ptep); if (!pte_access_permitted(pte, flags & FOLL_WRITE)) return 0; VM_BUG_ON(!pfn_valid(pte_pfn(pte))); page = nth_page(pte_page(pte), (addr & (sz - 1)) >> PAGE_SHIFT); refs = record_subpages(page, addr, end, pages + *nr); folio = try_grab_folio_fast(page, refs, flags); if (!folio) return 0; if (unlikely(pte_val(pte) != pte_val(ptep_get(ptep)))) { gup_put_folio(folio, refs, flags); return 0; } if (!folio_fast_pin_allowed(folio, flags)) { gup_put_folio(folio, refs, flags); return 0; } if (!pte_write(pte) && gup_must_unshare(NULL, flags, &folio->page)) { gup_put_folio(folio, refs, flags); return 0; } *nr += refs; folio_set_referenced(folio); return 1; }"
497----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47737/bad/nfs4idmap.c----idmap_id_to_name,"static __be32 idmap_id_to_name(struct xdr_stream *xdr, struct svc_rqst *rqstp, int type, u32 id) { struct ent *item, key = { .id = id, .type = type, }; __be32 *p; int ret; struct nfsd_net *nn = net_generic(SVC_NET(rqstp), nfsd_net_id); strscpy(key.authname, rqst_authname(rqstp), sizeof(key.authname)); ret = idmap_lookup(rqstp, idtoname_lookup, &key, nn->idtoname_cache, &item); if (ret == -ENOENT) return encode_ascii_id(xdr, id); if (ret) return nfserrno(ret); ret = strlen(item->name); WARN_ON_ONCE(ret > IDMAP_NAMESZ); p = xdr_reserve_space(xdr, ret + 4); <S2SV_StartVul> if (!p) <S2SV_EndVul> <S2SV_StartVul> return nfserr_resource; <S2SV_EndVul> <S2SV_StartVul> p = xdr_encode_opaque(p, item->name, ret); <S2SV_EndVul> cache_put(&item->h, nn->idtoname_cache); <S2SV_StartVul> return 0; <S2SV_EndVul> }","- if (!p)
- return nfserr_resource;
- p = xdr_encode_opaque(p, item->name, ret);
- return 0;
+ __be32 status = nfs_ok;
+ if (unlikely(!p)) {
+ status = nfserr_resource;
+ goto out_put;
+ }
+ xdr_encode_opaque(p, item->name, ret);
+ out_put:
+ return status;
+ }","static __be32 idmap_id_to_name(struct xdr_stream *xdr, struct svc_rqst *rqstp, int type, u32 id) { struct ent *item, key = { .id = id, .type = type, }; __be32 status = nfs_ok; __be32 *p; int ret; struct nfsd_net *nn = net_generic(SVC_NET(rqstp), nfsd_net_id); strscpy(key.authname, rqst_authname(rqstp), sizeof(key.authname)); ret = idmap_lookup(rqstp, idtoname_lookup, &key, nn->idtoname_cache, &item); if (ret == -ENOENT) return encode_ascii_id(xdr, id); if (ret) return nfserrno(ret); ret = strlen(item->name); WARN_ON_ONCE(ret > IDMAP_NAMESZ); p = xdr_reserve_space(xdr, ret + 4); if (unlikely(!p)) { status = nfserr_resource; goto out_put; } xdr_encode_opaque(p, item->name, ret); out_put: cache_put(&item->h, nn->idtoname_cache); return status; }"
762----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50038/bad/xt_RATEEST.c----xt_rateest_tg_init,static int __init xt_rateest_tg_init(void) { int err = register_pernet_subsys(&xt_rateest_net_ops); if (err) return err; <S2SV_StartVul> return xt_register_target(&xt_rateest_tg_reg); <S2SV_EndVul> },"- return xt_register_target(&xt_rateest_tg_reg);
+ return xt_register_targets(xt_rateest_tg_reg, ARRAY_SIZE(xt_rateest_tg_reg));","static int __init xt_rateest_tg_init(void) { int err = register_pernet_subsys(&xt_rateest_net_ops); if (err) return err; return xt_register_targets(xt_rateest_tg_reg, ARRAY_SIZE(xt_rateest_tg_reg)); }"
1416----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56744/bad/super.c----f2fs_handle_critical_error,"void f2fs_handle_critical_error(struct f2fs_sb_info *sbi, unsigned char reason, bool irq_context) { struct super_block *sb = sbi->sb; bool shutdown = reason == STOP_CP_REASON_SHUTDOWN; bool continue_fs = !shutdown && F2FS_OPTION(sbi).errors == MOUNT_ERRORS_CONTINUE; set_ckpt_flags(sbi, CP_ERROR_FLAG); if (!f2fs_hw_is_readonly(sbi)) { save_stop_reason(sbi, reason); <S2SV_StartVul> if (irq_context && !shutdown) <S2SV_EndVul> <S2SV_StartVul> schedule_work(&sbi->s_error_work); <S2SV_EndVul> <S2SV_StartVul> else <S2SV_EndVul> <S2SV_StartVul> f2fs_record_stop_reason(sbi); <S2SV_EndVul> } if (F2FS_OPTION(sbi).errors == MOUNT_ERRORS_PANIC && !shutdown && !system_going_down() && !is_sbi_flag_set(sbi, SBI_IS_SHUTDOWN)) panic(""F2FS-fs (device %s): panic forced after error\n"", sb->s_id); if (shutdown) set_sbi_flag(sbi, SBI_IS_SHUTDOWN); if (continue_fs || f2fs_readonly(sb) || shutdown) { f2fs_warn(sbi, ""Stopped filesystem due to reason: %d"", reason); return; } f2fs_warn(sbi, ""Remounting filesystem read-only""); }","- if (irq_context && !shutdown)
- schedule_work(&sbi->s_error_work);
- else
- f2fs_record_stop_reason(sbi);
+ #include <linux/statfs.h>
+ #include <linux/buffer_head.h>
+ #include <linux/kthread.h>
+ #include <linux/parser.h>
+ #include <linux/mount.h>
+ #include <linux/seq_file.h>
+ #include <linux/exportfs.h>","#include <linux/module.h> #include <linux/init.h> #include <linux/fs.h> #include <linux/fs_context.h> #include <linux/sched/mm.h> #include <linux/statfs.h> #include <linux/buffer_head.h> #include <linux/kthread.h> #include <linux/parser.h> #include <linux/mount.h> #include <linux/seq_file.h> #include <linux/proc_fs.h> #include <linux/random.h> #include <linux/exportfs.h> #include <linux/blkdev.h> #include <linux/quotaops.h> #include <linux/f2fs_fs.h> #include <linux/sysfs.h> #include <linux/quota.h> #include <linux/unicode.h> #include <linux/part_stat.h> #include <linux/zstd.h> #include <linux/lz4.h> #include ""f2fs.h"" #include ""node.h"" #include ""segment.h"" #include ""xattr.h"" #include ""gc.h"" #include ""iostat.h"" #define CREATE_TRACE_POINTS #include <trace/events/f2fs.h> static struct kmem_cache *f2fs_inode_cachep; #ifdef CONFIG_F2FS_FAULT_INJECTION const char *f2fs_fault_name[FAULT_MAX] = { [FAULT_KMALLOC] = ""kmalloc"", [FAULT_KVMALLOC] = ""kvmalloc"", [FAULT_PAGE_ALLOC] = ""page alloc"", [FAULT_PAGE_GET] = ""page get"", [FAULT_ALLOC_NID] = ""alloc nid"", [FAULT_ORPHAN] = ""orphan"", [FAULT_BLOCK] = ""no more block"", [FAULT_DIR_DEPTH] = ""too big dir depth"", [FAULT_EVICT_INODE] = ""evict_inode fail"", [FAULT_TRUNCATE] = ""truncate fail"", [FAULT_READ_IO] = ""read IO error"", [FAULT_CHECKPOINT] = ""checkpoint error"", [FAULT_DISCARD] = ""discard error"", [FAULT_WRITE_IO] = ""write IO error"", [FAULT_SLAB_ALLOC] = ""slab alloc"", [FAULT_DQUOT_INIT] = ""dquot initialize"", [FAULT_LOCK_OP] = ""lock_op"", [FAULT_BLKADDR_VALIDITY] = ""invalid blkaddr"", [FAULT_BLKADDR_CONSISTENCE] = ""inconsistent blkaddr"", [FAULT_NO_SEGMENT] = ""no free segment"", };"
1376----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56710/bad/file.c----__ceph_sync_read,"ssize_t __ceph_sync_read(struct inode *inode, loff_t *ki_pos, struct iov_iter *to, int *retry_op, u64 *last_objver) { struct ceph_inode_info *ci = ceph_inode(inode); struct ceph_fs_client *fsc = ceph_inode_to_fs_client(inode); struct ceph_osd_client *osdc = &fsc->client->osdc; ssize_t ret; u64 off = *ki_pos; u64 len = iov_iter_count(to); u64 i_size = i_size_read(inode); bool sparse = IS_ENCRYPTED(inode) || ceph_test_mount_opt(fsc, SPARSEREAD); u64 objver = 0; dout(""sync_read on inode %p %llx~%llx\n"", inode, *ki_pos, len); if (ceph_inode_is_shutdown(inode)) return -EIO; if (!len || !i_size) return 0; ret = filemap_write_and_wait_range(inode->i_mapping, off, off + len - 1); if (ret < 0) return ret; ret = 0; while ((len = iov_iter_count(to)) > 0) { struct ceph_osd_request *req; struct page **pages; int num_pages; size_t page_off; bool more; int idx = 0; size_t left; struct ceph_osd_req_op *op; u64 read_off = off; u64 read_len = len; int extent_cnt; ceph_fscrypt_adjust_off_and_len(inode, &read_off, &read_len); dout(""sync_read orig %llu~%llu reading %llu~%llu"", off, len, read_off, read_len); req = ceph_osdc_new_request(osdc, &ci->i_layout, ci->i_vino, read_off, &read_len, 0, 1, sparse ? CEPH_OSD_OP_SPARSE_READ : CEPH_OSD_OP_READ, CEPH_OSD_FLAG_READ, NULL, ci->i_truncate_seq, ci->i_truncate_size, false); if (IS_ERR(req)) { ret = PTR_ERR(req); break; } if (off + len > read_off + read_len) len = read_off + read_len - off; more = len < iov_iter_count(to); num_pages = calc_pages_for(read_off, read_len); page_off = offset_in_page(off); pages = ceph_alloc_page_vector(num_pages, GFP_KERNEL); if (IS_ERR(pages)) { ceph_osdc_put_request(req); ret = PTR_ERR(pages); break; } osd_req_op_extent_osd_data_pages(req, 0, pages, read_len, offset_in_page(read_off), false, true); <S2SV_StartVul> op = &req->r_ops[0]; <S2SV_EndVul> <S2SV_StartVul> if (sparse) { <S2SV_EndVul> <S2SV_StartVul> extent_cnt = __ceph_sparse_read_ext_count(inode, read_len); <S2SV_EndVul> <S2SV_StartVul> ret = ceph_alloc_sparse_ext_map(op, extent_cnt); <S2SV_EndVul> <S2SV_StartVul> if (ret) { <S2SV_EndVul> <S2SV_StartVul> ceph_osdc_put_request(req); <S2SV_EndVul> <S2SV_StartVul> break; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> ceph_osdc_start_request(osdc, req); ret = ceph_osdc_wait_request(osdc, req); ceph_update_read_metrics(&fsc->mdsc->metric, req->r_start_latency, req->r_end_latency, read_len, ret); if (ret > 0) objver = req->r_version; i_size = i_size_read(inode); dout(""sync_read %llu~%llu got %zd i_size %llu%s\n"", off, len, ret, i_size, (more ? "" MORE"" : """")); if (sparse && ret >= 0) ret = ceph_sparse_ext_map_end(op); else if (ret == -ENOENT) ret = 0; if (ret < 0) { ceph_osdc_put_request(req); if (ret == -EBLOCKLISTED) fsc->blocklisted = true; break; } if (IS_ENCRYPTED(inode)) { int fret; fret = ceph_fscrypt_decrypt_extents(inode, pages, read_off, op->extent.sparse_ext, op->extent.sparse_ext_cnt); if (fret < 0) { ret = fret; ceph_osdc_put_request(req); break; } fret -= (off - read_off); fret = max(fret, 0); ret = min_t(ssize_t, fret, len); } if (ret < len && (off + ret < i_size)) { int zlen = min(len - ret, i_size - off - ret); int zoff = page_off + ret; dout(""sync_read zero gap %llu~%llu\n"", off + ret, off + ret + zlen); ceph_zero_page_vector_range(zoff, zlen, pages); ret += zlen; } if (off + ret > i_size) left = (i_size > off) ? i_size - off : 0; else left = ret; while (left > 0) { size_t plen, copied; plen = min_t(size_t, left, PAGE_SIZE - page_off); SetPageUptodate(pages[idx]); copied = copy_page_to_iter(pages[idx++], page_off, plen, to); off += copied; left -= copied; page_off = 0; if (copied < plen) { ret = -EFAULT; break; } } ceph_osdc_put_request(req); if (off >= i_size || !more) break; } if (ret > 0) { if (off >= i_size) { *retry_op = CHECK_EOF; ret = i_size - *ki_pos; *ki_pos = i_size; } else { ret = off - *ki_pos; *ki_pos = off; } if (last_objver) *last_objver = objver; } dout(""sync_read result %zd retry_op %d\n"", ret, *retry_op); return ret; }","- op = &req->r_ops[0];
- if (sparse) {
- extent_cnt = __ceph_sparse_read_ext_count(inode, read_len);
- ret = ceph_alloc_sparse_ext_map(op, extent_cnt);
- if (ret) {
- ceph_osdc_put_request(req);
- break;
- }
- }
+ op = &req->r_ops[0];
+ if (sparse) {
+ extent_cnt = __ceph_sparse_read_ext_count(inode, read_len);
+ ret = ceph_alloc_sparse_ext_map(op, extent_cnt);
+ if (ret) {
+ ceph_osdc_put_request(req);
+ break;
+ }
+ }","ssize_t __ceph_sync_read(struct inode *inode, loff_t *ki_pos, struct iov_iter *to, int *retry_op, u64 *last_objver) { struct ceph_inode_info *ci = ceph_inode(inode); struct ceph_fs_client *fsc = ceph_inode_to_fs_client(inode); struct ceph_osd_client *osdc = &fsc->client->osdc; ssize_t ret; u64 off = *ki_pos; u64 len = iov_iter_count(to); u64 i_size = i_size_read(inode); bool sparse = IS_ENCRYPTED(inode) || ceph_test_mount_opt(fsc, SPARSEREAD); u64 objver = 0; dout(""sync_read on inode %p %llx~%llx\n"", inode, *ki_pos, len); if (ceph_inode_is_shutdown(inode)) return -EIO; if (!len || !i_size) return 0; ret = filemap_write_and_wait_range(inode->i_mapping, off, off + len - 1); if (ret < 0) return ret; ret = 0; while ((len = iov_iter_count(to)) > 0) { struct ceph_osd_request *req; struct page **pages; int num_pages; size_t page_off; bool more; int idx = 0; size_t left; struct ceph_osd_req_op *op; u64 read_off = off; u64 read_len = len; int extent_cnt; ceph_fscrypt_adjust_off_and_len(inode, &read_off, &read_len); dout(""sync_read orig %llu~%llu reading %llu~%llu"", off, len, read_off, read_len); req = ceph_osdc_new_request(osdc, &ci->i_layout, ci->i_vino, read_off, &read_len, 0, 1, sparse ? CEPH_OSD_OP_SPARSE_READ : CEPH_OSD_OP_READ, CEPH_OSD_FLAG_READ, NULL, ci->i_truncate_seq, ci->i_truncate_size, false); if (IS_ERR(req)) { ret = PTR_ERR(req); break; } if (off + len > read_off + read_len) len = read_off + read_len - off; more = len < iov_iter_count(to); op = &req->r_ops[0]; if (sparse) { extent_cnt = __ceph_sparse_read_ext_count(inode, read_len); ret = ceph_alloc_sparse_ext_map(op, extent_cnt); if (ret) { ceph_osdc_put_request(req); break; } } num_pages = calc_pages_for(read_off, read_len); page_off = offset_in_page(off); pages = ceph_alloc_page_vector(num_pages, GFP_KERNEL); if (IS_ERR(pages)) { ceph_osdc_put_request(req); ret = PTR_ERR(pages); break; } osd_req_op_extent_osd_data_pages(req, 0, pages, read_len, offset_in_page(read_off), false, true); ceph_osdc_start_request(osdc, req); ret = ceph_osdc_wait_request(osdc, req); ceph_update_read_metrics(&fsc->mdsc->metric, req->r_start_latency, req->r_end_latency, read_len, ret); if (ret > 0) objver = req->r_version; i_size = i_size_read(inode); dout(""sync_read %llu~%llu got %zd i_size %llu%s\n"", off, len, ret, i_size, (more ? "" MORE"" : """")); if (sparse && ret >= 0) ret = ceph_sparse_ext_map_end(op); else if (ret == -ENOENT) ret = 0; if (ret < 0) { ceph_osdc_put_request(req); if (ret == -EBLOCKLISTED) fsc->blocklisted = true; break; } if (IS_ENCRYPTED(inode)) { int fret; fret = ceph_fscrypt_decrypt_extents(inode, pages, read_off, op->extent.sparse_ext, op->extent.sparse_ext_cnt); if (fret < 0) { ret = fret; ceph_osdc_put_request(req); break; } fret -= (off - read_off); fret = max(fret, 0); ret = min_t(ssize_t, fret, len); } if (ret < len && (off + ret < i_size)) { int zlen = min(len - ret, i_size - off - ret); int zoff = page_off + ret; dout(""sync_read zero gap %llu~%llu\n"", off + ret, off + ret + zlen); ceph_zero_page_vector_range(zoff, zlen, pages); ret += zlen; } if (off + ret > i_size) left = (i_size > off) ? i_size - off : 0; else left = ret; while (left > 0) { size_t plen, copied; plen = min_t(size_t, left, PAGE_SIZE - page_off); SetPageUptodate(pages[idx]); copied = copy_page_to_iter(pages[idx++], page_off, plen, to); off += copied; left -= copied; page_off = 0; if (copied < plen) { ret = -EFAULT; break; } } ceph_osdc_put_request(req); if (off >= i_size || !more) break; } if (ret > 0) { if (off >= i_size) { *retry_op = CHECK_EOF; ret = i_size - *ki_pos; *ki_pos = i_size; } else { ret = off - *ki_pos; *ki_pos = off; } if (last_objver) *last_objver = objver; } dout(""sync_read result %zd retry_op %d\n"", ret, *retry_op); return ret; }"
758----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50036/bad/dst.c----dst_release_immediate,"void dst_release_immediate(struct dst_entry *dst) { if (dst) { int newrefcnt; newrefcnt = atomic_dec_return(&dst->__refcnt); if (WARN_ONCE(newrefcnt < 0, ""dst_release_immediate underflow"")) net_warn_ratelimited(""%s: dst:%p refcnt:%d\n"", __func__, dst, newrefcnt); <S2SV_StartVul> if (!newrefcnt) <S2SV_EndVul> dst_destroy(dst); } }","- if (!newrefcnt)
+ if (!newrefcnt){
+ dst_count_dec(dst);
+ }
+ }
+ }","void dst_release_immediate(struct dst_entry *dst) { if (dst) { int newrefcnt; newrefcnt = atomic_dec_return(&dst->__refcnt); if (WARN_ONCE(newrefcnt < 0, ""dst_release_immediate underflow"")) net_warn_ratelimited(""%s: dst:%p refcnt:%d\n"", __func__, dst, newrefcnt); if (!newrefcnt){ dst_count_dec(dst); dst_destroy(dst); } } }"
76----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44951/bad/sc16is7xx.c----sc16is7xx_handle_rx,"static void sc16is7xx_handle_rx(struct uart_port *port, unsigned int rxlen, unsigned int iir) { <S2SV_StartVul> struct sc16is7xx_port *s = dev_get_drvdata(port->dev); <S2SV_EndVul> unsigned int lsr = 0, bytes_read, i; bool read_lsr = (iir == SC16IS7XX_IIR_RLSE_SRC) ? true : false; u8 ch, flag; <S2SV_StartVul> if (unlikely(rxlen >= sizeof(s->buf))) { <S2SV_EndVul> dev_warn_ratelimited(port->dev, ""ttySC%i: Possible RX FIFO overrun: %d\n"", port->line, rxlen); port->icount.buf_overrun++; <S2SV_StartVul> rxlen = sizeof(s->buf); <S2SV_EndVul> } while (rxlen) { if (read_lsr) { lsr = sc16is7xx_port_read(port, SC16IS7XX_LSR_REG); if (!(lsr & SC16IS7XX_LSR_FIFOE_BIT)) read_lsr = false; } else lsr = 0; if (read_lsr) { <S2SV_StartVul> s->buf[0] = sc16is7xx_port_read(port, SC16IS7XX_RHR_REG); <S2SV_EndVul> bytes_read = 1; } else { <S2SV_StartVul> sc16is7xx_fifo_read(port, s->buf, rxlen); <S2SV_EndVul> bytes_read = rxlen; } lsr &= SC16IS7XX_LSR_BRK_ERROR_MASK; port->icount.rx++; flag = TTY_NORMAL; if (unlikely(lsr)) { if (lsr & SC16IS7XX_LSR_BI_BIT) { port->icount.brk++; if (uart_handle_break(port)) continue; } else if (lsr & SC16IS7XX_LSR_PE_BIT) port->icount.parity++; else if (lsr & SC16IS7XX_LSR_FE_BIT) port->icount.frame++; else if (lsr & SC16IS7XX_LSR_OE_BIT) port->icount.overrun++; lsr &= port->read_status_mask; if (lsr & SC16IS7XX_LSR_BI_BIT) flag = TTY_BREAK; else if (lsr & SC16IS7XX_LSR_PE_BIT) flag = TTY_PARITY; else if (lsr & SC16IS7XX_LSR_FE_BIT) flag = TTY_FRAME; else if (lsr & SC16IS7XX_LSR_OE_BIT) flag = TTY_OVERRUN; } for (i = 0; i < bytes_read; ++i) { <S2SV_StartVul> ch = s->buf[i]; <S2SV_EndVul> if (uart_handle_sysrq_char(port, ch)) continue; if (lsr & port->ignore_status_mask) continue; uart_insert_char(port, lsr, SC16IS7XX_LSR_OE_BIT, ch, flag); } rxlen -= bytes_read; } tty_flip_buffer_push(&port->state->port); }","- struct sc16is7xx_port *s = dev_get_drvdata(port->dev);
- if (unlikely(rxlen >= sizeof(s->buf))) {
- rxlen = sizeof(s->buf);
- s->buf[0] = sc16is7xx_port_read(port, SC16IS7XX_RHR_REG);
- sc16is7xx_fifo_read(port, s->buf, rxlen);
- ch = s->buf[i];
+ struct sc16is7xx_one *one = to_sc16is7xx_one(port, port);
+ if (unlikely(rxlen >= sizeof(one->buf))) {
+ rxlen = sizeof(one->buf);
+ one->buf[0] = sc16is7xx_port_read(port, SC16IS7XX_RHR_REG);
+ sc16is7xx_fifo_read(port, one->buf, rxlen);
+ ch = one->buf[i];","static void sc16is7xx_handle_rx(struct uart_port *port, unsigned int rxlen, unsigned int iir) { struct sc16is7xx_one *one = to_sc16is7xx_one(port, port); unsigned int lsr = 0, bytes_read, i; bool read_lsr = (iir == SC16IS7XX_IIR_RLSE_SRC) ? true : false; u8 ch, flag; if (unlikely(rxlen >= sizeof(one->buf))) { dev_warn_ratelimited(port->dev, ""ttySC%i: Possible RX FIFO overrun: %d\n"", port->line, rxlen); port->icount.buf_overrun++; rxlen = sizeof(one->buf); } while (rxlen) { if (read_lsr) { lsr = sc16is7xx_port_read(port, SC16IS7XX_LSR_REG); if (!(lsr & SC16IS7XX_LSR_FIFOE_BIT)) read_lsr = false; } else lsr = 0; if (read_lsr) { one->buf[0] = sc16is7xx_port_read(port, SC16IS7XX_RHR_REG); bytes_read = 1; } else { sc16is7xx_fifo_read(port, one->buf, rxlen); bytes_read = rxlen; } lsr &= SC16IS7XX_LSR_BRK_ERROR_MASK; port->icount.rx++; flag = TTY_NORMAL; if (unlikely(lsr)) { if (lsr & SC16IS7XX_LSR_BI_BIT) { port->icount.brk++; if (uart_handle_break(port)) continue; } else if (lsr & SC16IS7XX_LSR_PE_BIT) port->icount.parity++; else if (lsr & SC16IS7XX_LSR_FE_BIT) port->icount.frame++; else if (lsr & SC16IS7XX_LSR_OE_BIT) port->icount.overrun++; lsr &= port->read_status_mask; if (lsr & SC16IS7XX_LSR_BI_BIT) flag = TTY_BREAK; else if (lsr & SC16IS7XX_LSR_PE_BIT) flag = TTY_PARITY; else if (lsr & SC16IS7XX_LSR_FE_BIT) flag = TTY_FRAME; else if (lsr & SC16IS7XX_LSR_OE_BIT) flag = TTY_OVERRUN; } for (i = 0; i < bytes_read; ++i) { ch = one->buf[i]; if (uart_handle_sysrq_char(port, ch)) continue; if (lsr & port->ignore_status_mask) continue; uart_insert_char(port, lsr, SC16IS7XX_LSR_OE_BIT, ch, flag); } rxlen -= bytes_read; } tty_flip_buffer_push(&port->state->port); }"
969----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50211/bad/directory.c----udf_fiiter_init,"int udf_fiiter_init(struct udf_fileident_iter *iter, struct inode *dir, loff_t pos) { struct udf_inode_info *iinfo = UDF_I(dir); int err = 0; iter->dir = dir; iter->bh[0] = iter->bh[1] = NULL; iter->pos = pos; iter->elen = 0; iter->epos.bh = NULL; iter->name = NULL; iter->namebuf = kmalloc(UDF_NAME_LEN_CS0, GFP_KERNEL | __GFP_NOFAIL); if (iinfo->i_alloc_type == ICBTAG_FLAG_AD_IN_ICB) { err = udf_copy_fi(iter); goto out; } <S2SV_StartVul> if (inode_bmap(dir, iter->pos >> dir->i_blkbits, &iter->epos, <S2SV_EndVul> <S2SV_StartVul> &iter->eloc, &iter->elen, &iter->loffset) != <S2SV_EndVul> <S2SV_StartVul> (EXT_RECORDED_ALLOCATED >> 30)) { <S2SV_EndVul> if (pos == dir->i_size) return 0; udf_err(dir->i_sb, ""position %llu not allocated in directory (ino %lu)\n"", (unsigned long long)pos, dir->i_ino); err = -EFSCORRUPTED; goto out; } err = udf_fiiter_load_bhs(iter); if (err < 0) goto out; err = udf_copy_fi(iter); out: if (err < 0) udf_fiiter_release(iter); return err; }","- if (inode_bmap(dir, iter->pos >> dir->i_blkbits, &iter->epos,
- &iter->eloc, &iter->elen, &iter->loffset) !=
- (EXT_RECORDED_ALLOCATED >> 30)) {
+ int8_t etype;
+ err = inode_bmap(dir, iter->pos >> dir->i_blkbits, &iter->epos,
+ &iter->eloc, &iter->elen, &iter->loffset, &etype);
+ if (err <= 0 || etype != (EXT_RECORDED_ALLOCATED >> 30)) {","int udf_fiiter_init(struct udf_fileident_iter *iter, struct inode *dir, loff_t pos) { struct udf_inode_info *iinfo = UDF_I(dir); int err = 0; int8_t etype; iter->dir = dir; iter->bh[0] = iter->bh[1] = NULL; iter->pos = pos; iter->elen = 0; iter->epos.bh = NULL; iter->name = NULL; iter->namebuf = kmalloc(UDF_NAME_LEN_CS0, GFP_KERNEL | __GFP_NOFAIL); if (iinfo->i_alloc_type == ICBTAG_FLAG_AD_IN_ICB) { err = udf_copy_fi(iter); goto out; } err = inode_bmap(dir, iter->pos >> dir->i_blkbits, &iter->epos, &iter->eloc, &iter->elen, &iter->loffset, &etype); if (err <= 0 || etype != (EXT_RECORDED_ALLOCATED >> 30)) { if (pos == dir->i_size) return 0; udf_err(dir->i_sb, ""position %llu not allocated in directory (ino %lu)\n"", (unsigned long long)pos, dir->i_ino); err = -EFSCORRUPTED; goto out; } err = udf_fiiter_load_bhs(iter); if (err < 0) goto out; err = udf_copy_fi(iter); out: if (err < 0) udf_fiiter_release(iter); return err; }"
193----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46679/bad/net-sysfs.c----speed_show,"static ssize_t speed_show(struct device *dev, struct device_attribute *attr, char *buf) { struct net_device *netdev = to_net_dev(dev); int ret = -EINVAL; if (!netdev->ethtool_ops->get_link_ksettings) return ret; if (!rtnl_trylock()) return restart_syscall(); <S2SV_StartVul> if (netif_running(netdev) && netif_device_present(netdev)) { <S2SV_EndVul> struct ethtool_link_ksettings cmd; if (!__ethtool_get_link_ksettings(netdev, &cmd)) ret = sprintf(buf, fmt_dec, cmd.base.speed); } rtnl_unlock(); return ret; }","- if (netif_running(netdev) && netif_device_present(netdev)) {
+ if (netif_running(netdev)) {","static ssize_t speed_show(struct device *dev, struct device_attribute *attr, char *buf) { struct net_device *netdev = to_net_dev(dev); int ret = -EINVAL; if (!netdev->ethtool_ops->get_link_ksettings) return ret; if (!rtnl_trylock()) return restart_syscall(); if (netif_running(netdev)) { struct ethtool_link_ksettings cmd; if (!__ethtool_get_link_ksettings(netdev, &cmd)) ret = sprintf(buf, fmt_dec, cmd.base.speed); } rtnl_unlock(); return ret; }"
591----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49913/bad/dc.c----commit_planes_for_stream,"static void commit_planes_for_stream(struct dc *dc, struct dc_surface_update *srf_updates, int surface_count, struct dc_stream_state *stream, struct dc_stream_update *stream_update, enum surface_update_type update_type, struct dc_state *context) { int i, j; struct pipe_ctx *top_pipe_to_program = NULL; bool should_lock_all_pipes = (update_type != UPDATE_TYPE_FAST); bool subvp_prev_use = false; subvp_prev_use = false; dc_z10_restore(dc); if (update_type == UPDATE_TYPE_FULL) wait_for_outstanding_hw_updates(dc, context); if (get_seamless_boot_stream_count(context) > 0 && surface_count > 0) { if (stream->apply_seamless_boot_optimization) { stream->apply_seamless_boot_optimization = false; if (get_seamless_boot_stream_count(context) == 0) dc->optimized_required = true; } } if (update_type == UPDATE_TYPE_FULL) { dc_allow_idle_optimizations(dc, false); if (get_seamless_boot_stream_count(context) == 0) dc->hwss.prepare_bandwidth(dc, context); if (dc->hwss.update_dsc_pg) dc->hwss.update_dsc_pg(dc, context, false); context_clock_trace(dc, context); } for (j = 0; j < dc->res_pool->pipe_count; j++) { struct pipe_ctx *pipe_ctx = &context->res_ctx.pipe_ctx[j]; if (!pipe_ctx->top_pipe && !pipe_ctx->prev_odm_pipe && pipe_ctx->stream && pipe_ctx->stream == stream) { top_pipe_to_program = pipe_ctx; } } for (i = 0; i < dc->res_pool->pipe_count; i++) { struct pipe_ctx *old_pipe = &dc->current_state->res_ctx.pipe_ctx[i]; subvp_prev_use |= (old_pipe->stream && old_pipe->stream->mall_stream_config.type == SUBVP_PHANTOM); if (subvp_prev_use) break; } if (stream->test_pattern.type != DP_TEST_PATTERN_VIDEO_MODE) { struct pipe_ctx *mpcc_pipe; struct pipe_ctx *odm_pipe; for (mpcc_pipe = top_pipe_to_program; mpcc_pipe; mpcc_pipe = mpcc_pipe->bottom_pipe) for (odm_pipe = mpcc_pipe; odm_pipe; odm_pipe = odm_pipe->next_odm_pipe) odm_pipe->ttu_regs.min_ttu_vblank = MAX_TTU; } if ((update_type != UPDATE_TYPE_FAST) && stream->update_flags.bits.dsc_changed) if (top_pipe_to_program && top_pipe_to_program->stream_res.tg->funcs->lock_doublebuffer_enable) { if (should_use_dmub_lock(stream->link)) { union dmub_hw_lock_flags hw_locks = { 0 }; struct dmub_hw_lock_inst_flags inst_flags = { 0 }; hw_locks.bits.lock_dig = 1; inst_flags.dig_inst = top_pipe_to_program->stream_res.tg->inst; dmub_hw_lock_mgr_cmd(dc->ctx->dmub_srv, true, &hw_locks, &inst_flags); } else top_pipe_to_program->stream_res.tg->funcs->lock_doublebuffer_enable( top_pipe_to_program->stream_res.tg); } if (should_lock_all_pipes && dc->hwss.interdependent_update_lock) { if (dc->hwss.subvp_pipe_control_lock) dc->hwss.subvp_pipe_control_lock(dc, context, true, should_lock_all_pipes, NULL, subvp_prev_use); dc->hwss.interdependent_update_lock(dc, context, true); } else { if (dc->hwss.subvp_pipe_control_lock) dc->hwss.subvp_pipe_control_lock(dc, context, true, should_lock_all_pipes, top_pipe_to_program, subvp_prev_use); dc->hwss.pipe_control_lock(dc, top_pipe_to_program, true); } if (update_type != UPDATE_TYPE_FAST) { for (i = 0; i < dc->res_pool->pipe_count; i++) { struct pipe_ctx *new_pipe = &context->res_ctx.pipe_ctx[i]; if ((new_pipe->stream && new_pipe->stream->mall_stream_config.type == SUBVP_PHANTOM) || subvp_prev_use) { dc->hwss.apply_ctx_to_hw(dc, context); break; } } } dc_dmub_update_dirty_rect(dc, surface_count, stream, srf_updates, context); if (update_type != UPDATE_TYPE_FAST) { for (i = 0; i < dc->res_pool->pipe_count; i++) { struct pipe_ctx *new_pipe = &context->res_ctx.pipe_ctx[i]; if ((new_pipe->stream && new_pipe->stream->mall_stream_config.type == SUBVP_PHANTOM) || subvp_prev_use) { dc->hwss.apply_ctx_to_hw(dc, context); break; } } } if (stream_update) commit_planes_do_stream_update(dc, stream, stream_update, update_type, context); if (surface_count == 0) { if (dc->hwss.apply_ctx_for_surface) dc->hwss.apply_ctx_for_surface(dc, stream, 0, context); if (dc->hwss.program_front_end_for_ctx) dc->hwss.program_front_end_for_ctx(dc, context); if (should_lock_all_pipes && dc->hwss.interdependent_update_lock) { dc->hwss.interdependent_update_lock(dc, context, false); } else { dc->hwss.pipe_control_lock(dc, top_pipe_to_program, false); } dc->hwss.post_unlock_program_front_end(dc, context); if (update_type != UPDATE_TYPE_FAST) if (dc->hwss.commit_subvp_config) dc->hwss.commit_subvp_config(dc, context); if (should_lock_all_pipes && dc->hwss.interdependent_update_lock) { if (dc->hwss.subvp_pipe_control_lock) dc->hwss.subvp_pipe_control_lock(dc, context, false, should_lock_all_pipes, NULL, subvp_prev_use); } else { if (dc->hwss.subvp_pipe_control_lock) dc->hwss.subvp_pipe_control_lock(dc, context, false, should_lock_all_pipes, NULL, subvp_prev_use); } return; } if (!IS_DIAG_DC(dc->ctx->dce_environment)) { for (i = 0; i < surface_count; i++) { struct dc_plane_state *plane_state = srf_updates[i].surface; for (j = 0; j < dc->res_pool->pipe_count; j++) { struct pipe_ctx *pipe_ctx = &context->res_ctx.pipe_ctx[j]; if (!pipe_ctx->plane_state) continue; if (should_update_pipe_for_plane(context, pipe_ctx, plane_state)) continue; pipe_ctx->plane_state->triplebuffer_flips = false; if (update_type == UPDATE_TYPE_FAST && dc->hwss.program_triplebuffer != NULL && !pipe_ctx->plane_state->flip_immediate && dc->debug.enable_tri_buf) { pipe_ctx->plane_state->triplebuffer_flips = true; } } if (update_type == UPDATE_TYPE_FULL) { plane_state->flip_immediate = false; } } } for (j = 0; j < dc->res_pool->pipe_count; j++) { struct pipe_ctx *pipe_ctx = &context->res_ctx.pipe_ctx[j]; if (!pipe_ctx->top_pipe && !pipe_ctx->prev_odm_pipe && should_update_pipe_for_stream(context, pipe_ctx, stream)) { struct dc_stream_status *stream_status = NULL; if (!pipe_ctx->plane_state) continue; if (update_type == UPDATE_TYPE_FAST) continue; ASSERT(!pipe_ctx->plane_state->triplebuffer_flips); if (dc->hwss.program_triplebuffer != NULL && dc->debug.enable_tri_buf) { dc->hwss.program_triplebuffer( dc, pipe_ctx, pipe_ctx->plane_state->triplebuffer_flips); } stream_status = stream_get_status(context, pipe_ctx->stream); if (dc->hwss.apply_ctx_for_surface) dc->hwss.apply_ctx_for_surface( dc, pipe_ctx->stream, stream_status->plane_count, context); } } if (dc->hwss.program_front_end_for_ctx && update_type != UPDATE_TYPE_FAST) { dc->hwss.program_front_end_for_ctx(dc, context); if (dc->debug.validate_dml_output) { for (i = 0; i < dc->res_pool->pipe_count; i++) { struct pipe_ctx *cur_pipe = &context->res_ctx.pipe_ctx[i]; if (cur_pipe->stream == NULL) continue; cur_pipe->plane_res.hubp->funcs->validate_dml_output( cur_pipe->plane_res.hubp, dc->ctx, &context->res_ctx.pipe_ctx[i].rq_regs, &context->res_ctx.pipe_ctx[i].dlg_regs, &context->res_ctx.pipe_ctx[i].ttu_regs); } } } if (update_type == UPDATE_TYPE_FAST) { if (dc->hwss.set_flip_control_gsl) for (i = 0; i < surface_count; i++) { struct dc_plane_state *plane_state = srf_updates[i].surface; for (j = 0; j < dc->res_pool->pipe_count; j++) { struct pipe_ctx *pipe_ctx = &context->res_ctx.pipe_ctx[j]; if (!should_update_pipe_for_stream(context, pipe_ctx, stream)) continue; if (!should_update_pipe_for_plane(context, pipe_ctx, plane_state)) continue; dc->hwss.set_flip_control_gsl(pipe_ctx, pipe_ctx->plane_state->flip_immediate); } } for (i = 0; i < surface_count; i++) { struct dc_plane_state *plane_state = srf_updates[i].surface; for (j = 0; j < dc->res_pool->pipe_count; j++) { struct pipe_ctx *pipe_ctx = &context->res_ctx.pipe_ctx[j]; if (!should_update_pipe_for_stream(context, pipe_ctx, stream)) continue; if (!should_update_pipe_for_plane(context, pipe_ctx, plane_state)) continue; if (dc->hwss.program_triplebuffer != NULL && dc->debug.enable_tri_buf) { dc->hwss.program_triplebuffer( dc, pipe_ctx, pipe_ctx->plane_state->triplebuffer_flips); } if (pipe_ctx->plane_state->update_flags.bits.addr_update) dc->hwss.update_plane_addr(dc, pipe_ctx); } } } if (should_lock_all_pipes && dc->hwss.interdependent_update_lock) { dc->hwss.interdependent_update_lock(dc, context, false); } else { dc->hwss.pipe_control_lock(dc, top_pipe_to_program, false); } if ((update_type != UPDATE_TYPE_FAST) && stream->update_flags.bits.dsc_changed) <S2SV_StartVul> if (top_pipe_to_program->stream_res.tg->funcs->lock_doublebuffer_enable) { <S2SV_EndVul> top_pipe_to_program->stream_res.tg->funcs->wait_for_state( top_pipe_to_program->stream_res.tg, CRTC_STATE_VACTIVE); top_pipe_to_program->stream_res.tg->funcs->wait_for_state( top_pipe_to_program->stream_res.tg, CRTC_STATE_VBLANK); top_pipe_to_program->stream_res.tg->funcs->wait_for_state( top_pipe_to_program->stream_res.tg, CRTC_STATE_VACTIVE); if (should_use_dmub_lock(stream->link)) { union dmub_hw_lock_flags hw_locks = { 0 }; struct dmub_hw_lock_inst_flags inst_flags = { 0 }; hw_locks.bits.lock_dig = 1; inst_flags.dig_inst = top_pipe_to_program->stream_res.tg->inst; dmub_hw_lock_mgr_cmd(dc->ctx->dmub_srv, false, &hw_locks, &inst_flags); } else top_pipe_to_program->stream_res.tg->funcs->lock_doublebuffer_disable( top_pipe_to_program->stream_res.tg); } if (update_type != UPDATE_TYPE_FAST) dc->hwss.post_unlock_program_front_end(dc, context); if (update_type != UPDATE_TYPE_FAST) if (dc->hwss.commit_subvp_config) dc->hwss.commit_subvp_config(dc, context); if (update_type != UPDATE_TYPE_FAST) if (dc->hwss.commit_subvp_config) dc->hwss.commit_subvp_config(dc, context); if (should_lock_all_pipes && dc->hwss.interdependent_update_lock) { if (dc->hwss.subvp_pipe_control_lock) dc->hwss.subvp_pipe_control_lock(dc, context, false, should_lock_all_pipes, NULL, subvp_prev_use); } else { if (dc->hwss.subvp_pipe_control_lock) dc->hwss.subvp_pipe_control_lock(dc, context, false, should_lock_all_pipes, top_pipe_to_program, subvp_prev_use); } for (j = 0; j < dc->res_pool->pipe_count; j++) { struct pipe_ctx *pipe_ctx = &context->res_ctx.pipe_ctx[j]; if (!pipe_ctx->plane_state) continue; if (pipe_ctx->bottom_pipe || pipe_ctx->next_odm_pipe || !pipe_ctx->stream || !should_update_pipe_for_stream(context, pipe_ctx, stream) || !pipe_ctx->plane_state->update_flags.bits.addr_update || pipe_ctx->plane_state->skip_manual_trigger) continue; if (pipe_ctx->stream_res.tg->funcs->program_manual_trigger) pipe_ctx->stream_res.tg->funcs->program_manual_trigger(pipe_ctx->stream_res.tg); } }","- if (top_pipe_to_program->stream_res.tg->funcs->lock_doublebuffer_enable) {
+ if (top_pipe_to_program &&
+ top_pipe_to_program->stream_res.tg->funcs->lock_doublebuffer_enable) {","static void commit_planes_for_stream(struct dc *dc, struct dc_surface_update *srf_updates, int surface_count, struct dc_stream_state *stream, struct dc_stream_update *stream_update, enum surface_update_type update_type, struct dc_state *context) { int i, j; struct pipe_ctx *top_pipe_to_program = NULL; bool should_lock_all_pipes = (update_type != UPDATE_TYPE_FAST); bool subvp_prev_use = false; subvp_prev_use = false; dc_z10_restore(dc); if (update_type == UPDATE_TYPE_FULL) wait_for_outstanding_hw_updates(dc, context); if (get_seamless_boot_stream_count(context) > 0 && surface_count > 0) { if (stream->apply_seamless_boot_optimization) { stream->apply_seamless_boot_optimization = false; if (get_seamless_boot_stream_count(context) == 0) dc->optimized_required = true; } } if (update_type == UPDATE_TYPE_FULL) { dc_allow_idle_optimizations(dc, false); if (get_seamless_boot_stream_count(context) == 0) dc->hwss.prepare_bandwidth(dc, context); if (dc->hwss.update_dsc_pg) dc->hwss.update_dsc_pg(dc, context, false); context_clock_trace(dc, context); } for (j = 0; j < dc->res_pool->pipe_count; j++) { struct pipe_ctx *pipe_ctx = &context->res_ctx.pipe_ctx[j]; if (!pipe_ctx->top_pipe && !pipe_ctx->prev_odm_pipe && pipe_ctx->stream && pipe_ctx->stream == stream) { top_pipe_to_program = pipe_ctx; } } for (i = 0; i < dc->res_pool->pipe_count; i++) { struct pipe_ctx *old_pipe = &dc->current_state->res_ctx.pipe_ctx[i]; subvp_prev_use |= (old_pipe->stream && old_pipe->stream->mall_stream_config.type == SUBVP_PHANTOM); if (subvp_prev_use) break; } if (stream->test_pattern.type != DP_TEST_PATTERN_VIDEO_MODE) { struct pipe_ctx *mpcc_pipe; struct pipe_ctx *odm_pipe; for (mpcc_pipe = top_pipe_to_program; mpcc_pipe; mpcc_pipe = mpcc_pipe->bottom_pipe) for (odm_pipe = mpcc_pipe; odm_pipe; odm_pipe = odm_pipe->next_odm_pipe) odm_pipe->ttu_regs.min_ttu_vblank = MAX_TTU; } if ((update_type != UPDATE_TYPE_FAST) && stream->update_flags.bits.dsc_changed) if (top_pipe_to_program && top_pipe_to_program->stream_res.tg->funcs->lock_doublebuffer_enable) { if (should_use_dmub_lock(stream->link)) { union dmub_hw_lock_flags hw_locks = { 0 }; struct dmub_hw_lock_inst_flags inst_flags = { 0 }; hw_locks.bits.lock_dig = 1; inst_flags.dig_inst = top_pipe_to_program->stream_res.tg->inst; dmub_hw_lock_mgr_cmd(dc->ctx->dmub_srv, true, &hw_locks, &inst_flags); } else top_pipe_to_program->stream_res.tg->funcs->lock_doublebuffer_enable( top_pipe_to_program->stream_res.tg); } if (should_lock_all_pipes && dc->hwss.interdependent_update_lock) { if (dc->hwss.subvp_pipe_control_lock) dc->hwss.subvp_pipe_control_lock(dc, context, true, should_lock_all_pipes, NULL, subvp_prev_use); dc->hwss.interdependent_update_lock(dc, context, true); } else { if (dc->hwss.subvp_pipe_control_lock) dc->hwss.subvp_pipe_control_lock(dc, context, true, should_lock_all_pipes, top_pipe_to_program, subvp_prev_use); dc->hwss.pipe_control_lock(dc, top_pipe_to_program, true); } if (update_type != UPDATE_TYPE_FAST) { for (i = 0; i < dc->res_pool->pipe_count; i++) { struct pipe_ctx *new_pipe = &context->res_ctx.pipe_ctx[i]; if ((new_pipe->stream && new_pipe->stream->mall_stream_config.type == SUBVP_PHANTOM) || subvp_prev_use) { dc->hwss.apply_ctx_to_hw(dc, context); break; } } } dc_dmub_update_dirty_rect(dc, surface_count, stream, srf_updates, context); if (update_type != UPDATE_TYPE_FAST) { for (i = 0; i < dc->res_pool->pipe_count; i++) { struct pipe_ctx *new_pipe = &context->res_ctx.pipe_ctx[i]; if ((new_pipe->stream && new_pipe->stream->mall_stream_config.type == SUBVP_PHANTOM) || subvp_prev_use) { dc->hwss.apply_ctx_to_hw(dc, context); break; } } } if (stream_update) commit_planes_do_stream_update(dc, stream, stream_update, update_type, context); if (surface_count == 0) { if (dc->hwss.apply_ctx_for_surface) dc->hwss.apply_ctx_for_surface(dc, stream, 0, context); if (dc->hwss.program_front_end_for_ctx) dc->hwss.program_front_end_for_ctx(dc, context); if (should_lock_all_pipes && dc->hwss.interdependent_update_lock) { dc->hwss.interdependent_update_lock(dc, context, false); } else { dc->hwss.pipe_control_lock(dc, top_pipe_to_program, false); } dc->hwss.post_unlock_program_front_end(dc, context); if (update_type != UPDATE_TYPE_FAST) if (dc->hwss.commit_subvp_config) dc->hwss.commit_subvp_config(dc, context); if (should_lock_all_pipes && dc->hwss.interdependent_update_lock) { if (dc->hwss.subvp_pipe_control_lock) dc->hwss.subvp_pipe_control_lock(dc, context, false, should_lock_all_pipes, NULL, subvp_prev_use); } else { if (dc->hwss.subvp_pipe_control_lock) dc->hwss.subvp_pipe_control_lock(dc, context, false, should_lock_all_pipes, NULL, subvp_prev_use); } return; } if (!IS_DIAG_DC(dc->ctx->dce_environment)) { for (i = 0; i < surface_count; i++) { struct dc_plane_state *plane_state = srf_updates[i].surface; for (j = 0; j < dc->res_pool->pipe_count; j++) { struct pipe_ctx *pipe_ctx = &context->res_ctx.pipe_ctx[j]; if (!pipe_ctx->plane_state) continue; if (should_update_pipe_for_plane(context, pipe_ctx, plane_state)) continue; pipe_ctx->plane_state->triplebuffer_flips = false; if (update_type == UPDATE_TYPE_FAST && dc->hwss.program_triplebuffer != NULL && !pipe_ctx->plane_state->flip_immediate && dc->debug.enable_tri_buf) { pipe_ctx->plane_state->triplebuffer_flips = true; } } if (update_type == UPDATE_TYPE_FULL) { plane_state->flip_immediate = false; } } } for (j = 0; j < dc->res_pool->pipe_count; j++) { struct pipe_ctx *pipe_ctx = &context->res_ctx.pipe_ctx[j]; if (!pipe_ctx->top_pipe && !pipe_ctx->prev_odm_pipe && should_update_pipe_for_stream(context, pipe_ctx, stream)) { struct dc_stream_status *stream_status = NULL; if (!pipe_ctx->plane_state) continue; if (update_type == UPDATE_TYPE_FAST) continue; ASSERT(!pipe_ctx->plane_state->triplebuffer_flips); if (dc->hwss.program_triplebuffer != NULL && dc->debug.enable_tri_buf) { dc->hwss.program_triplebuffer( dc, pipe_ctx, pipe_ctx->plane_state->triplebuffer_flips); } stream_status = stream_get_status(context, pipe_ctx->stream); if (dc->hwss.apply_ctx_for_surface) dc->hwss.apply_ctx_for_surface( dc, pipe_ctx->stream, stream_status->plane_count, context); } } if (dc->hwss.program_front_end_for_ctx && update_type != UPDATE_TYPE_FAST) { dc->hwss.program_front_end_for_ctx(dc, context); if (dc->debug.validate_dml_output) { for (i = 0; i < dc->res_pool->pipe_count; i++) { struct pipe_ctx *cur_pipe = &context->res_ctx.pipe_ctx[i]; if (cur_pipe->stream == NULL) continue; cur_pipe->plane_res.hubp->funcs->validate_dml_output( cur_pipe->plane_res.hubp, dc->ctx, &context->res_ctx.pipe_ctx[i].rq_regs, &context->res_ctx.pipe_ctx[i].dlg_regs, &context->res_ctx.pipe_ctx[i].ttu_regs); } } } if (update_type == UPDATE_TYPE_FAST) { if (dc->hwss.set_flip_control_gsl) for (i = 0; i < surface_count; i++) { struct dc_plane_state *plane_state = srf_updates[i].surface; for (j = 0; j < dc->res_pool->pipe_count; j++) { struct pipe_ctx *pipe_ctx = &context->res_ctx.pipe_ctx[j]; if (!should_update_pipe_for_stream(context, pipe_ctx, stream)) continue; if (!should_update_pipe_for_plane(context, pipe_ctx, plane_state)) continue; dc->hwss.set_flip_control_gsl(pipe_ctx, pipe_ctx->plane_state->flip_immediate); } } for (i = 0; i < surface_count; i++) { struct dc_plane_state *plane_state = srf_updates[i].surface; for (j = 0; j < dc->res_pool->pipe_count; j++) { struct pipe_ctx *pipe_ctx = &context->res_ctx.pipe_ctx[j]; if (!should_update_pipe_for_stream(context, pipe_ctx, stream)) continue; if (!should_update_pipe_for_plane(context, pipe_ctx, plane_state)) continue; if (dc->hwss.program_triplebuffer != NULL && dc->debug.enable_tri_buf) { dc->hwss.program_triplebuffer( dc, pipe_ctx, pipe_ctx->plane_state->triplebuffer_flips); } if (pipe_ctx->plane_state->update_flags.bits.addr_update) dc->hwss.update_plane_addr(dc, pipe_ctx); } } } if (should_lock_all_pipes && dc->hwss.interdependent_update_lock) { dc->hwss.interdependent_update_lock(dc, context, false); } else { dc->hwss.pipe_control_lock(dc, top_pipe_to_program, false); } if ((update_type != UPDATE_TYPE_FAST) && stream->update_flags.bits.dsc_changed) if (top_pipe_to_program && top_pipe_to_program->stream_res.tg->funcs->lock_doublebuffer_enable) { top_pipe_to_program->stream_res.tg->funcs->wait_for_state( top_pipe_to_program->stream_res.tg, CRTC_STATE_VACTIVE); top_pipe_to_program->stream_res.tg->funcs->wait_for_state( top_pipe_to_program->stream_res.tg, CRTC_STATE_VBLANK); top_pipe_to_program->stream_res.tg->funcs->wait_for_state( top_pipe_to_program->stream_res.tg, CRTC_STATE_VACTIVE); if (should_use_dmub_lock(stream->link)) { union dmub_hw_lock_flags hw_locks = { 0 }; struct dmub_hw_lock_inst_flags inst_flags = { 0 }; hw_locks.bits.lock_dig = 1; inst_flags.dig_inst = top_pipe_to_program->stream_res.tg->inst; dmub_hw_lock_mgr_cmd(dc->ctx->dmub_srv, false, &hw_locks, &inst_flags); } else top_pipe_to_program->stream_res.tg->funcs->lock_doublebuffer_disable( top_pipe_to_program->stream_res.tg); } if (update_type != UPDATE_TYPE_FAST) dc->hwss.post_unlock_program_front_end(dc, context); if (update_type != UPDATE_TYPE_FAST) if (dc->hwss.commit_subvp_config) dc->hwss.commit_subvp_config(dc, context); if (update_type != UPDATE_TYPE_FAST) if (dc->hwss.commit_subvp_config) dc->hwss.commit_subvp_config(dc, context); if (should_lock_all_pipes && dc->hwss.interdependent_update_lock) { if (dc->hwss.subvp_pipe_control_lock) dc->hwss.subvp_pipe_control_lock(dc, context, false, should_lock_all_pipes, NULL, subvp_prev_use); } else { if (dc->hwss.subvp_pipe_control_lock) dc->hwss.subvp_pipe_control_lock(dc, context, false, should_lock_all_pipes, top_pipe_to_program, subvp_prev_use); } for (j = 0; j < dc->res_pool->pipe_count; j++) { struct pipe_ctx *pipe_ctx = &context->res_ctx.pipe_ctx[j]; if (!pipe_ctx->plane_state) continue; if (pipe_ctx->bottom_pipe || pipe_ctx->next_odm_pipe || !pipe_ctx->stream || !should_update_pipe_for_stream(context, pipe_ctx, stream) || !pipe_ctx->plane_state->update_flags.bits.addr_update || pipe_ctx->plane_state->skip_manual_trigger) continue; if (pipe_ctx->stream_res.tg->funcs->program_manual_trigger) pipe_ctx->stream_res.tg->funcs->program_manual_trigger(pipe_ctx->stream_res.tg); } }"
872----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50129/bad/pse_core.c----of_pse_match_pi,"static int of_pse_match_pi(struct pse_controller_dev *pcdev, struct device_node *np) { int i; <S2SV_StartVul> for (i = 0; i <= pcdev->nr_lines; i++) { <S2SV_EndVul> if (pcdev->pi[i].np == np) return i; } return -EINVAL; }","- for (i = 0; i <= pcdev->nr_lines; i++) {
+ for (i = 0; i < pcdev->nr_lines; i++) {","static int of_pse_match_pi(struct pse_controller_dev *pcdev, struct device_node *np) { int i; for (i = 0; i < pcdev->nr_lines; i++) { if (pcdev->pi[i].np == np) return i; } return -EINVAL; }"
604----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49920/bad/link_hwss_dio.c----reset_dio_stream_encoder,"void reset_dio_stream_encoder(struct pipe_ctx *pipe_ctx) { struct link_encoder *link_enc = link_enc_cfg_get_link_enc(pipe_ctx->stream->link); struct stream_encoder *stream_enc = pipe_ctx->stream_res.stream_enc; <S2SV_StartVul> if (stream_enc && stream_enc->funcs->disable_fifo) <S2SV_EndVul> stream_enc->funcs->disable_fifo(stream_enc); if (stream_enc->funcs->set_input_mode) stream_enc->funcs->set_input_mode(stream_enc, 0); if (stream_enc->funcs->enable_stream) stream_enc->funcs->enable_stream(stream_enc, pipe_ctx->stream->signal, false); link_enc->funcs->connect_dig_be_to_fe( link_enc, pipe_ctx->stream_res.stream_enc->id, false); if (dc_is_dp_signal(pipe_ctx->stream->signal)) pipe_ctx->stream->ctx->dc->link_srv->dp_trace_source_sequence( pipe_ctx->stream->link, DPCD_SOURCE_SEQ_AFTER_DISCONNECT_DIG_FE_BE); }","- if (stream_enc && stream_enc->funcs->disable_fifo)
+ if (!stream_enc)
+ return;
+ if (stream_enc->funcs->disable_fifo)","void reset_dio_stream_encoder(struct pipe_ctx *pipe_ctx) { struct link_encoder *link_enc = link_enc_cfg_get_link_enc(pipe_ctx->stream->link); struct stream_encoder *stream_enc = pipe_ctx->stream_res.stream_enc; if (!stream_enc) return; if (stream_enc->funcs->disable_fifo) stream_enc->funcs->disable_fifo(stream_enc); if (stream_enc->funcs->set_input_mode) stream_enc->funcs->set_input_mode(stream_enc, 0); if (stream_enc->funcs->enable_stream) stream_enc->funcs->enable_stream(stream_enc, pipe_ctx->stream->signal, false); link_enc->funcs->connect_dig_be_to_fe( link_enc, pipe_ctx->stream_res.stream_enc->id, false); if (dc_is_dp_signal(pipe_ctx->stream->signal)) pipe_ctx->stream->ctx->dc->link_srv->dp_trace_source_sequence( pipe_ctx->stream->link, DPCD_SOURCE_SEQ_AFTER_DISCONNECT_DIG_FE_BE); }"
1470----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-57887/bad/adv7511_drv.c----adv7511_probe,"static int adv7511_probe(struct i2c_client *i2c) { struct adv7511_link_config link_config; struct adv7511 *adv7511; struct device *dev = &i2c->dev; unsigned int val; int ret; if (!dev->of_node) return -EINVAL; adv7511 = devm_kzalloc(dev, sizeof(*adv7511), GFP_KERNEL); if (!adv7511) return -ENOMEM; adv7511->i2c_main = i2c; adv7511->powered = false; adv7511->status = connector_status_disconnected; adv7511->info = i2c_get_match_data(i2c); memset(&link_config, 0, sizeof(link_config)); ret = drm_of_find_panel_or_bridge(dev->of_node, 1, -1, NULL, &adv7511->next_bridge); if (ret && ret != -ENODEV) return ret; if (adv7511->info->link_config) ret = adv7511_parse_dt(dev->of_node, &link_config); else ret = adv7533_parse_dt(dev->of_node, adv7511); if (ret) return ret; ret = adv7511_init_regulators(adv7511); <S2SV_StartVul> if (ret) <S2SV_EndVul> <S2SV_StartVul> return dev_err_probe(dev, ret, ""failed to init regulators\n""); <S2SV_EndVul> adv7511->gpio_pd = devm_gpiod_get_optional(dev, ""pd"", GPIOD_OUT_HIGH); if (IS_ERR(adv7511->gpio_pd)) { ret = PTR_ERR(adv7511->gpio_pd); goto uninit_regulators; } if (adv7511->gpio_pd) { usleep_range(5000, 6000); gpiod_set_value_cansleep(adv7511->gpio_pd, 0); } adv7511->regmap = devm_regmap_init_i2c(i2c, &adv7511_regmap_config); if (IS_ERR(adv7511->regmap)) { ret = PTR_ERR(adv7511->regmap); goto uninit_regulators; } ret = regmap_read(adv7511->regmap, ADV7511_REG_CHIP_REVISION, &val); if (ret) goto uninit_regulators; dev_dbg(dev, ""Rev. %d\n"", val); if (adv7511->info->type == ADV7511) ret = regmap_register_patch(adv7511->regmap, adv7511_fixed_registers, ARRAY_SIZE(adv7511_fixed_registers)); else ret = adv7533_patch_registers(adv7511); if (ret) goto uninit_regulators; adv7511_packet_disable(adv7511, 0xffff); adv7511->i2c_edid = i2c_new_ancillary_device(i2c, ""edid"", ADV7511_EDID_I2C_ADDR_DEFAULT); if (IS_ERR(adv7511->i2c_edid)) { ret = PTR_ERR(adv7511->i2c_edid); goto uninit_regulators; } regmap_write(adv7511->regmap, ADV7511_REG_EDID_I2C_ADDR, adv7511->i2c_edid->addr << 1); adv7511->i2c_packet = i2c_new_ancillary_device(i2c, ""packet"", ADV7511_PACKET_I2C_ADDR_DEFAULT); if (IS_ERR(adv7511->i2c_packet)) { ret = PTR_ERR(adv7511->i2c_packet); goto err_i2c_unregister_edid; } regmap_write(adv7511->regmap, ADV7511_REG_PACKET_I2C_ADDR, adv7511->i2c_packet->addr << 1); ret = adv7511_init_cec_regmap(adv7511); if (ret) goto err_i2c_unregister_packet; INIT_WORK(&adv7511->hpd_work, adv7511_hpd_work); adv7511_power_off(adv7511); i2c_set_clientdata(i2c, adv7511); if (adv7511->info->link_config) adv7511_set_link_config(adv7511, &link_config); ret = adv7511_cec_init(dev, adv7511); if (ret) goto err_unregister_cec; adv7511->bridge.funcs = &adv7511_bridge_funcs; adv7511->bridge.ops = DRM_BRIDGE_OP_DETECT | DRM_BRIDGE_OP_EDID; if (adv7511->i2c_main->irq) adv7511->bridge.ops |= DRM_BRIDGE_OP_HPD; adv7511->bridge.of_node = dev->of_node; adv7511->bridge.type = DRM_MODE_CONNECTOR_HDMIA; drm_bridge_add(&adv7511->bridge); adv7511_audio_init(dev, adv7511); if (i2c->irq) { init_waitqueue_head(&adv7511->wq); ret = devm_request_threaded_irq(dev, i2c->irq, NULL, adv7511_irq_handler, IRQF_ONESHOT | IRQF_SHARED, dev_name(dev), adv7511); if (ret) goto err_unregister_audio; } if (adv7511->info->has_dsi) { ret = adv7533_attach_dsi(adv7511); if (ret) goto err_unregister_audio; } return 0; err_unregister_audio: adv7511_audio_exit(adv7511); drm_bridge_remove(&adv7511->bridge); err_unregister_cec: cec_unregister_adapter(adv7511->cec_adap); i2c_unregister_device(adv7511->i2c_cec); clk_disable_unprepare(adv7511->cec_clk); err_i2c_unregister_packet: i2c_unregister_device(adv7511->i2c_packet); err_i2c_unregister_edid: i2c_unregister_device(adv7511->i2c_edid); uninit_regulators: adv7511_uninit_regulators(adv7511); return ret; }","- if (ret)
- return dev_err_probe(dev, ret, ""failed to init regulators\n"");
+ if (ret) {
+ dev_err_probe(dev, ret, ""failed to init regulators\n"");
+ goto err_of_node_put;
+ }
+ err_of_node_put:
+ of_node_put(adv7511->host_node);
+ }","static int adv7511_probe(struct i2c_client *i2c) { struct adv7511_link_config link_config; struct adv7511 *adv7511; struct device *dev = &i2c->dev; unsigned int val; int ret; if (!dev->of_node) return -EINVAL; adv7511 = devm_kzalloc(dev, sizeof(*adv7511), GFP_KERNEL); if (!adv7511) return -ENOMEM; adv7511->i2c_main = i2c; adv7511->powered = false; adv7511->status = connector_status_disconnected; adv7511->info = i2c_get_match_data(i2c); memset(&link_config, 0, sizeof(link_config)); ret = drm_of_find_panel_or_bridge(dev->of_node, 1, -1, NULL, &adv7511->next_bridge); if (ret && ret != -ENODEV) return ret; if (adv7511->info->link_config) ret = adv7511_parse_dt(dev->of_node, &link_config); else ret = adv7533_parse_dt(dev->of_node, adv7511); if (ret) return ret; ret = adv7511_init_regulators(adv7511); if (ret) { dev_err_probe(dev, ret, ""failed to init regulators\n""); goto err_of_node_put; } adv7511->gpio_pd = devm_gpiod_get_optional(dev, ""pd"", GPIOD_OUT_HIGH); if (IS_ERR(adv7511->gpio_pd)) { ret = PTR_ERR(adv7511->gpio_pd); goto uninit_regulators; } if (adv7511->gpio_pd) { usleep_range(5000, 6000); gpiod_set_value_cansleep(adv7511->gpio_pd, 0); } adv7511->regmap = devm_regmap_init_i2c(i2c, &adv7511_regmap_config); if (IS_ERR(adv7511->regmap)) { ret = PTR_ERR(adv7511->regmap); goto uninit_regulators; } ret = regmap_read(adv7511->regmap, ADV7511_REG_CHIP_REVISION, &val); if (ret) goto uninit_regulators; dev_dbg(dev, ""Rev. %d\n"", val); if (adv7511->info->type == ADV7511) ret = regmap_register_patch(adv7511->regmap, adv7511_fixed_registers, ARRAY_SIZE(adv7511_fixed_registers)); else ret = adv7533_patch_registers(adv7511); if (ret) goto uninit_regulators; adv7511_packet_disable(adv7511, 0xffff); adv7511->i2c_edid = i2c_new_ancillary_device(i2c, ""edid"", ADV7511_EDID_I2C_ADDR_DEFAULT); if (IS_ERR(adv7511->i2c_edid)) { ret = PTR_ERR(adv7511->i2c_edid); goto uninit_regulators; } regmap_write(adv7511->regmap, ADV7511_REG_EDID_I2C_ADDR, adv7511->i2c_edid->addr << 1); adv7511->i2c_packet = i2c_new_ancillary_device(i2c, ""packet"", ADV7511_PACKET_I2C_ADDR_DEFAULT); if (IS_ERR(adv7511->i2c_packet)) { ret = PTR_ERR(adv7511->i2c_packet); goto err_i2c_unregister_edid; } regmap_write(adv7511->regmap, ADV7511_REG_PACKET_I2C_ADDR, adv7511->i2c_packet->addr << 1); ret = adv7511_init_cec_regmap(adv7511); if (ret) goto err_i2c_unregister_packet; INIT_WORK(&adv7511->hpd_work, adv7511_hpd_work); adv7511_power_off(adv7511); i2c_set_clientdata(i2c, adv7511); if (adv7511->info->link_config) adv7511_set_link_config(adv7511, &link_config); ret = adv7511_cec_init(dev, adv7511); if (ret) goto err_unregister_cec; adv7511->bridge.funcs = &adv7511_bridge_funcs; adv7511->bridge.ops = DRM_BRIDGE_OP_DETECT | DRM_BRIDGE_OP_EDID; if (adv7511->i2c_main->irq) adv7511->bridge.ops |= DRM_BRIDGE_OP_HPD; adv7511->bridge.of_node = dev->of_node; adv7511->bridge.type = DRM_MODE_CONNECTOR_HDMIA; drm_bridge_add(&adv7511->bridge); adv7511_audio_init(dev, adv7511); if (i2c->irq) { init_waitqueue_head(&adv7511->wq); ret = devm_request_threaded_irq(dev, i2c->irq, NULL, adv7511_irq_handler, IRQF_ONESHOT | IRQF_SHARED, dev_name(dev), adv7511); if (ret) goto err_unregister_audio; } if (adv7511->info->has_dsi) { ret = adv7533_attach_dsi(adv7511); if (ret) goto err_unregister_audio; } return 0; err_unregister_audio: adv7511_audio_exit(adv7511); drm_bridge_remove(&adv7511->bridge); err_unregister_cec: cec_unregister_adapter(adv7511->cec_adap); i2c_unregister_device(adv7511->i2c_cec); clk_disable_unprepare(adv7511->cec_clk); err_i2c_unregister_packet: i2c_unregister_device(adv7511->i2c_packet); err_i2c_unregister_edid: i2c_unregister_device(adv7511->i2c_edid); uninit_regulators: adv7511_uninit_regulators(adv7511); err_of_node_put: of_node_put(adv7511->host_node); return ret; }"
1183----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53170/bad/blk-sysfs.c----blk_register_queue,"int blk_register_queue(struct gendisk *disk) { struct request_queue *q = disk->queue; int ret; mutex_lock(&q->sysfs_dir_lock); kobject_init(&disk->queue_kobj, &blk_queue_ktype); ret = kobject_add(&disk->queue_kobj, &disk_to_dev(disk)->kobj, ""queue""); if (ret < 0) goto out_put_queue_kobj; if (queue_is_mq(q)) { ret = blk_mq_sysfs_register(disk); if (ret) goto out_put_queue_kobj; } mutex_lock(&q->sysfs_lock); mutex_lock(&q->debugfs_mutex); q->debugfs_dir = debugfs_create_dir(disk->disk_name, blk_debugfs_root); if (queue_is_mq(q)) blk_mq_debugfs_register(q); mutex_unlock(&q->debugfs_mutex); ret = disk_register_independent_access_ranges(disk); if (ret) goto out_debugfs_remove; if (q->elevator) { ret = elv_register_queue(q, false); if (ret) goto out_unregister_ia_ranges; } ret = blk_crypto_sysfs_register(disk); if (ret) goto out_elv_unregister; blk_queue_flag_set(QUEUE_FLAG_REGISTERED, q); wbt_enable_default(disk); kobject_uevent(&disk->queue_kobj, KOBJ_ADD); if (q->elevator) kobject_uevent(&q->elevator->kobj, KOBJ_ADD); mutex_unlock(&q->sysfs_lock); mutex_unlock(&q->sysfs_dir_lock); <S2SV_StartVul> if (!blk_queue_init_done(q)) { <S2SV_EndVul> <S2SV_StartVul> blk_queue_flag_set(QUEUE_FLAG_INIT_DONE, q); <S2SV_EndVul> <S2SV_StartVul> percpu_ref_switch_to_percpu(&q->q_usage_counter); <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> return ret; out_elv_unregister: elv_unregister_queue(q); out_unregister_ia_ranges: disk_unregister_independent_access_ranges(disk); out_debugfs_remove: blk_debugfs_remove(disk); mutex_unlock(&q->sysfs_lock); out_put_queue_kobj: kobject_put(&disk->queue_kobj); mutex_unlock(&q->sysfs_dir_lock); return ret; }","- if (!blk_queue_init_done(q)) {
- blk_queue_flag_set(QUEUE_FLAG_INIT_DONE, q);
- percpu_ref_switch_to_percpu(&q->q_usage_counter);
- }
+ blk_queue_flag_set(QUEUE_FLAG_INIT_DONE, q);
+ percpu_ref_switch_to_percpu(&q->q_usage_counter);","int blk_register_queue(struct gendisk *disk) { struct request_queue *q = disk->queue; int ret; mutex_lock(&q->sysfs_dir_lock); kobject_init(&disk->queue_kobj, &blk_queue_ktype); ret = kobject_add(&disk->queue_kobj, &disk_to_dev(disk)->kobj, ""queue""); if (ret < 0) goto out_put_queue_kobj; if (queue_is_mq(q)) { ret = blk_mq_sysfs_register(disk); if (ret) goto out_put_queue_kobj; } mutex_lock(&q->sysfs_lock); mutex_lock(&q->debugfs_mutex); q->debugfs_dir = debugfs_create_dir(disk->disk_name, blk_debugfs_root); if (queue_is_mq(q)) blk_mq_debugfs_register(q); mutex_unlock(&q->debugfs_mutex); ret = disk_register_independent_access_ranges(disk); if (ret) goto out_debugfs_remove; if (q->elevator) { ret = elv_register_queue(q, false); if (ret) goto out_unregister_ia_ranges; } ret = blk_crypto_sysfs_register(disk); if (ret) goto out_elv_unregister; blk_queue_flag_set(QUEUE_FLAG_REGISTERED, q); wbt_enable_default(disk); kobject_uevent(&disk->queue_kobj, KOBJ_ADD); if (q->elevator) kobject_uevent(&q->elevator->kobj, KOBJ_ADD); mutex_unlock(&q->sysfs_lock); mutex_unlock(&q->sysfs_dir_lock); blk_queue_flag_set(QUEUE_FLAG_INIT_DONE, q); percpu_ref_switch_to_percpu(&q->q_usage_counter); return ret; out_elv_unregister: elv_unregister_queue(q); out_unregister_ia_ranges: disk_unregister_independent_access_ranges(disk); out_debugfs_remove: blk_debugfs_remove(disk); mutex_unlock(&q->sysfs_lock); out_put_queue_kobj: kobject_put(&disk->queue_kobj); mutex_unlock(&q->sysfs_dir_lock); return ret; }"
101----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44963/bad/ioctl.c----create_subvol,"static noinline int create_subvol(struct mnt_idmap *idmap, struct inode *dir, struct dentry *dentry, struct btrfs_qgroup_inherit *inherit) { struct btrfs_fs_info *fs_info = inode_to_fs_info(dir); struct btrfs_trans_handle *trans; struct btrfs_key key; struct btrfs_root_item *root_item; struct btrfs_inode_item *inode_item; struct extent_buffer *leaf; struct btrfs_root *root = BTRFS_I(dir)->root; struct btrfs_root *new_root; struct btrfs_block_rsv block_rsv; struct timespec64 cur_time = current_time(dir); struct btrfs_new_inode_args new_inode_args = { .dir = dir, .dentry = dentry, .subvol = true, }; unsigned int trans_num_items; int ret; dev_t anon_dev; u64 objectid; u64 qgroup_reserved = 0; root_item = kzalloc(sizeof(*root_item), GFP_KERNEL); if (!root_item) return -ENOMEM; ret = btrfs_get_free_objectid(fs_info->tree_root, &objectid); if (ret) goto out_root_item; if (btrfs_qgroup_level(objectid)) { ret = -ENOSPC; goto out_root_item; } ret = get_anon_bdev(&anon_dev); if (ret < 0) goto out_root_item; new_inode_args.inode = btrfs_new_subvol_inode(idmap, dir); if (!new_inode_args.inode) { ret = -ENOMEM; goto out_anon_dev; } ret = btrfs_new_inode_prepare(&new_inode_args, &trans_num_items); if (ret) goto out_inode; trans_num_items += create_subvol_num_items(inherit); btrfs_init_block_rsv(&block_rsv, BTRFS_BLOCK_RSV_TEMP); ret = btrfs_subvolume_reserve_metadata(root, &block_rsv, trans_num_items, false); if (ret) goto out_new_inode_args; qgroup_reserved = block_rsv.qgroup_rsv_reserved; trans = btrfs_start_transaction(root, 0); if (IS_ERR(trans)) { ret = PTR_ERR(trans); goto out_release_rsv; } btrfs_qgroup_convert_reserved_meta(root, qgroup_reserved); qgroup_reserved = 0; trans->block_rsv = &block_rsv; trans->bytes_reserved = block_rsv.size; ret = btrfs_qgroup_inherit(trans, 0, objectid, btrfs_root_id(root), inherit); if (ret) goto out; leaf = btrfs_alloc_tree_block(trans, root, 0, objectid, NULL, 0, 0, 0, 0, BTRFS_NESTING_NORMAL); if (IS_ERR(leaf)) { ret = PTR_ERR(leaf); goto out; } btrfs_mark_buffer_dirty(trans, leaf); inode_item = &root_item->inode; btrfs_set_stack_inode_generation(inode_item, 1); btrfs_set_stack_inode_size(inode_item, 3); btrfs_set_stack_inode_nlink(inode_item, 1); btrfs_set_stack_inode_nbytes(inode_item, fs_info->nodesize); btrfs_set_stack_inode_mode(inode_item, S_IFDIR | 0755); btrfs_set_root_flags(root_item, 0); btrfs_set_root_limit(root_item, 0); btrfs_set_stack_inode_flags(inode_item, BTRFS_INODE_ROOT_ITEM_INIT); btrfs_set_root_bytenr(root_item, leaf->start); btrfs_set_root_generation(root_item, trans->transid); btrfs_set_root_level(root_item, 0); btrfs_set_root_refs(root_item, 1); btrfs_set_root_used(root_item, leaf->len); btrfs_set_root_last_snapshot(root_item, 0); btrfs_set_root_generation_v2(root_item, btrfs_root_generation(root_item)); generate_random_guid(root_item->uuid); btrfs_set_stack_timespec_sec(&root_item->otime, cur_time.tv_sec); btrfs_set_stack_timespec_nsec(&root_item->otime, cur_time.tv_nsec); root_item->ctime = root_item->otime; btrfs_set_root_ctransid(root_item, trans->transid); btrfs_set_root_otransid(root_item, trans->transid); btrfs_tree_unlock(leaf); btrfs_set_root_dirid(root_item, BTRFS_FIRST_FREE_OBJECTID); key.objectid = objectid; key.offset = 0; key.type = BTRFS_ROOT_ITEM_KEY; ret = btrfs_insert_root(trans, fs_info->tree_root, &key, root_item); if (ret) { btrfs_tree_lock(leaf); btrfs_clear_buffer_dirty(trans, leaf); btrfs_tree_unlock(leaf); btrfs_free_tree_block(trans, objectid, leaf, 0, 1); free_extent_buffer(leaf); goto out; } free_extent_buffer(leaf); leaf = NULL; new_root = btrfs_get_new_fs_root(fs_info, objectid, &anon_dev); if (IS_ERR(new_root)) { ret = PTR_ERR(new_root); btrfs_abort_transaction(trans, ret); goto out; } anon_dev = 0; BTRFS_I(new_inode_args.inode)->root = new_root; ret = btrfs_record_root_in_trans(trans, new_root); if (ret) { btrfs_abort_transaction(trans, ret); goto out; <S2SV_StartVul> } <S2SV_EndVul> ret = btrfs_uuid_tree_add(trans, root_item->uuid, BTRFS_UUID_KEY_SUBVOL, objectid); if (ret) { btrfs_abort_transaction(trans, ret); goto out; } ret = btrfs_create_new_inode(trans, &new_inode_args); if (ret) { btrfs_abort_transaction(trans, ret); goto out; } btrfs_record_new_subvolume(trans, BTRFS_I(dir)); d_instantiate_new(dentry, new_inode_args.inode); new_inode_args.inode = NULL; out: trans->block_rsv = NULL; trans->bytes_reserved = 0; btrfs_end_transaction(trans); out_release_rsv: btrfs_block_rsv_release(fs_info, &block_rsv, (u64)-1, NULL); if (qgroup_reserved) btrfs_qgroup_free_meta_prealloc(root, qgroup_reserved); out_new_inode_args: btrfs_new_inode_args_destroy(&new_inode_args); out_inode: iput(new_inode_args.inode); out_anon_dev: if (anon_dev) free_anon_bdev(anon_dev); out_root_item: kfree(root_item); return ret; }","- }
+ if (IS_ERR(new_root)) {
+ btrfs_abort_transaction(trans, ret);
+ goto out;
+ }","static noinline int create_subvol(struct mnt_idmap *idmap, struct inode *dir, struct dentry *dentry, struct btrfs_qgroup_inherit *inherit) { struct btrfs_fs_info *fs_info = inode_to_fs_info(dir); struct btrfs_trans_handle *trans; struct btrfs_key key; struct btrfs_root_item *root_item; struct btrfs_inode_item *inode_item; struct extent_buffer *leaf; struct btrfs_root *root = BTRFS_I(dir)->root; struct btrfs_root *new_root; struct btrfs_block_rsv block_rsv; struct timespec64 cur_time = current_time(dir); struct btrfs_new_inode_args new_inode_args = { .dir = dir, .dentry = dentry, .subvol = true, }; unsigned int trans_num_items; int ret; dev_t anon_dev; u64 objectid; u64 qgroup_reserved = 0; root_item = kzalloc(sizeof(*root_item), GFP_KERNEL); if (!root_item) return -ENOMEM; ret = btrfs_get_free_objectid(fs_info->tree_root, &objectid); if (ret) goto out_root_item; if (btrfs_qgroup_level(objectid)) { ret = -ENOSPC; goto out_root_item; } ret = get_anon_bdev(&anon_dev); if (ret < 0) goto out_root_item; new_inode_args.inode = btrfs_new_subvol_inode(idmap, dir); if (!new_inode_args.inode) { ret = -ENOMEM; goto out_anon_dev; } ret = btrfs_new_inode_prepare(&new_inode_args, &trans_num_items); if (ret) goto out_inode; trans_num_items += create_subvol_num_items(inherit); btrfs_init_block_rsv(&block_rsv, BTRFS_BLOCK_RSV_TEMP); ret = btrfs_subvolume_reserve_metadata(root, &block_rsv, trans_num_items, false); if (ret) goto out_new_inode_args; qgroup_reserved = block_rsv.qgroup_rsv_reserved; trans = btrfs_start_transaction(root, 0); if (IS_ERR(trans)) { ret = PTR_ERR(trans); goto out_release_rsv; } btrfs_qgroup_convert_reserved_meta(root, qgroup_reserved); qgroup_reserved = 0; trans->block_rsv = &block_rsv; trans->bytes_reserved = block_rsv.size; ret = btrfs_qgroup_inherit(trans, 0, objectid, btrfs_root_id(root), inherit); if (ret) goto out; leaf = btrfs_alloc_tree_block(trans, root, 0, objectid, NULL, 0, 0, 0, 0, BTRFS_NESTING_NORMAL); if (IS_ERR(leaf)) { ret = PTR_ERR(leaf); goto out; } btrfs_mark_buffer_dirty(trans, leaf); inode_item = &root_item->inode; btrfs_set_stack_inode_generation(inode_item, 1); btrfs_set_stack_inode_size(inode_item, 3); btrfs_set_stack_inode_nlink(inode_item, 1); btrfs_set_stack_inode_nbytes(inode_item, fs_info->nodesize); btrfs_set_stack_inode_mode(inode_item, S_IFDIR | 0755); btrfs_set_root_flags(root_item, 0); btrfs_set_root_limit(root_item, 0); btrfs_set_stack_inode_flags(inode_item, BTRFS_INODE_ROOT_ITEM_INIT); btrfs_set_root_bytenr(root_item, leaf->start); btrfs_set_root_generation(root_item, trans->transid); btrfs_set_root_level(root_item, 0); btrfs_set_root_refs(root_item, 1); btrfs_set_root_used(root_item, leaf->len); btrfs_set_root_last_snapshot(root_item, 0); btrfs_set_root_generation_v2(root_item, btrfs_root_generation(root_item)); generate_random_guid(root_item->uuid); btrfs_set_stack_timespec_sec(&root_item->otime, cur_time.tv_sec); btrfs_set_stack_timespec_nsec(&root_item->otime, cur_time.tv_nsec); root_item->ctime = root_item->otime; btrfs_set_root_ctransid(root_item, trans->transid); btrfs_set_root_otransid(root_item, trans->transid); btrfs_tree_unlock(leaf); btrfs_set_root_dirid(root_item, BTRFS_FIRST_FREE_OBJECTID); key.objectid = objectid; key.offset = 0; key.type = BTRFS_ROOT_ITEM_KEY; ret = btrfs_insert_root(trans, fs_info->tree_root, &key, root_item); if (ret) { int ret2; btrfs_tree_lock(leaf); btrfs_clear_buffer_dirty(trans, leaf); btrfs_tree_unlock(leaf); ret2 = btrfs_free_tree_block(trans, objectid, leaf, 0, 1); if (ret2 < 0) btrfs_abort_transaction(trans, ret2); free_extent_buffer(leaf); goto out; } free_extent_buffer(leaf); leaf = NULL; new_root = btrfs_get_new_fs_root(fs_info, objectid, &anon_dev); if (IS_ERR(new_root)) { ret = PTR_ERR(new_root); btrfs_abort_transaction(trans, ret); goto out; } anon_dev = 0; BTRFS_I(new_inode_args.inode)->root = new_root; ret = btrfs_record_root_in_trans(trans, new_root); if (ret) { btrfs_abort_transaction(trans, ret); goto out; } ret = btrfs_uuid_tree_add(trans, root_item->uuid, BTRFS_UUID_KEY_SUBVOL, objectid); if (ret) { btrfs_abort_transaction(trans, ret); goto out; } ret = btrfs_create_new_inode(trans, &new_inode_args); if (ret) { btrfs_abort_transaction(trans, ret); goto out; } btrfs_record_new_subvolume(trans, BTRFS_I(dir)); d_instantiate_new(dentry, new_inode_args.inode); new_inode_args.inode = NULL; out: trans->block_rsv = NULL; trans->bytes_reserved = 0; btrfs_end_transaction(trans); out_release_rsv: btrfs_block_rsv_release(fs_info, &block_rsv, (u64)-1, NULL); if (qgroup_reserved) btrfs_qgroup_free_meta_prealloc(root, qgroup_reserved); out_new_inode_args: btrfs_new_inode_args_destroy(&new_inode_args); out_inode: iput(new_inode_args.inode); out_anon_dev: if (anon_dev) free_anon_bdev(anon_dev); out_root_item: kfree(root_item); return ret; }"
836----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50100/bad/dummy_hcd.c----dummy_timer,"static enum hrtimer_restart dummy_timer(struct hrtimer *t) { struct dummy_hcd *dum_hcd = from_timer(dum_hcd, t, timer); struct dummy *dum = dum_hcd->dum; struct urbp *urbp, *tmp; unsigned long flags; int limit, total; int i; switch (dum->gadget.speed) { case USB_SPEED_LOW: total = 8 * 12 ; break; case USB_SPEED_FULL: total = 64 * 19 ; break; case USB_SPEED_HIGH: total = 512 * 13 * 8 ; break; case USB_SPEED_SUPER: total = 490000; break; default: dev_err(dummy_dev(dum_hcd), ""bogus device speed\n""); total = 0; break; } spin_lock_irqsave(&dum->lock, flags); if (!dum_hcd->udev) { dev_err(dummy_dev(dum_hcd), ""timer fired with no URBs pending?\n""); spin_unlock_irqrestore(&dum->lock, flags); return HRTIMER_NORESTART; } dum_hcd->next_frame_urbp = NULL; for (i = 0; i < DUMMY_ENDPOINTS; i++) { if (!ep_info[i].name) break; dum->ep[i].already_seen = 0; } restart: list_for_each_entry_safe(urbp, tmp, &dum_hcd->urbp_list, urbp_list) { struct urb *urb; struct dummy_request *req; u8 address; struct dummy_ep *ep = NULL; int status = -EINPROGRESS; if (urbp == dum_hcd->next_frame_urbp) break; urb = urbp->urb; if (urb->unlinked) goto return_urb; else if (dum_hcd->rh_state != DUMMY_RH_RUNNING) continue; if (total <= 0) continue; address = usb_pipeendpoint (urb->pipe); if (usb_urb_dir_in(urb)) address |= USB_DIR_IN; ep = find_endpoint(dum, address); if (!ep) { dev_dbg(dummy_dev(dum_hcd), ""no ep configured for urb %p\n"", urb); status = -EPROTO; goto return_urb; } if (ep->already_seen) continue; ep->already_seen = 1; if (ep == &dum->ep[0] && urb->error_count) { ep->setup_stage = 1; urb->error_count = 0; } if (ep->halted && !ep->setup_stage) { dev_dbg(dummy_dev(dum_hcd), ""ep %s halted, urb %p\n"", ep->ep.name, urb); status = -EPIPE; goto return_urb; } if (ep == &dum->ep[0] && ep->setup_stage) { struct usb_ctrlrequest setup; int value; setup = *(struct usb_ctrlrequest *) urb->setup_packet; list_for_each_entry(req, &ep->queue, queue) { list_del_init(&req->queue); req->req.status = -EOVERFLOW; dev_dbg(udc_dev(dum), ""stale req = %p\n"", req); spin_unlock(&dum->lock); usb_gadget_giveback_request(&ep->ep, &req->req); spin_lock(&dum->lock); ep->already_seen = 0; goto restart; } ep->last_io = jiffies; ep->setup_stage = 0; ep->halted = 0; value = handle_control_request(dum_hcd, urb, &setup, &status); if (value > 0) { ++dum->callback_usage; spin_unlock(&dum->lock); value = dum->driver->setup(&dum->gadget, &setup); spin_lock(&dum->lock); --dum->callback_usage; if (value >= 0) { limit = 64*1024; goto treat_control_like_bulk; } } if (value < 0) { if (value != -EOPNOTSUPP) dev_dbg(udc_dev(dum), ""setup --> %d\n"", value); status = -EPIPE; urb->actual_length = 0; } goto return_urb; } limit = total; switch (usb_pipetype(urb->pipe)) { case PIPE_ISOCHRONOUS: limit = max(limit, periodic_bytes(dum, ep)); status = -EINVAL; break; case PIPE_INTERRUPT: limit = max(limit, periodic_bytes(dum, ep)); fallthrough; default: treat_control_like_bulk: ep->last_io = jiffies; total -= transfer(dum_hcd, urb, ep, limit, &status); break; } if (status == -EINPROGRESS) continue; return_urb: list_del(&urbp->urbp_list); kfree(urbp); if (ep) ep->already_seen = ep->setup_stage = 0; usb_hcd_unlink_urb_from_ep(dummy_hcd_to_hcd(dum_hcd), urb); spin_unlock(&dum->lock); usb_hcd_giveback_urb(dummy_hcd_to_hcd(dum_hcd), urb, status); spin_lock(&dum->lock); goto restart; } if (list_empty(&dum_hcd->urbp_list)) { usb_put_dev(dum_hcd->udev); dum_hcd->udev = NULL; <S2SV_StartVul> } else if (dum_hcd->rh_state == DUMMY_RH_RUNNING) { <S2SV_EndVul> hrtimer_start(&dum_hcd->timer, ns_to_ktime(DUMMY_TIMER_INT_NSECS), HRTIMER_MODE_REL_SOFT); } spin_unlock_irqrestore(&dum->lock, flags); return HRTIMER_NORESTART; }","- } else if (dum_hcd->rh_state == DUMMY_RH_RUNNING) {
+ dum_hcd->timer_pending = 0;
+ } else if (!dum_hcd->timer_pending &&
+ dum_hcd->rh_state == DUMMY_RH_RUNNING) {
+ dum_hcd->timer_pending = 1;
+ }","static enum hrtimer_restart dummy_timer(struct hrtimer *t) { struct dummy_hcd *dum_hcd = from_timer(dum_hcd, t, timer); struct dummy *dum = dum_hcd->dum; struct urbp *urbp, *tmp; unsigned long flags; int limit, total; int i; switch (dum->gadget.speed) { case USB_SPEED_LOW: total = 8 * 12 ; break; case USB_SPEED_FULL: total = 64 * 19 ; break; case USB_SPEED_HIGH: total = 512 * 13 * 8 ; break; case USB_SPEED_SUPER: total = 490000; break; default: dev_err(dummy_dev(dum_hcd), ""bogus device speed\n""); total = 0; break; } spin_lock_irqsave(&dum->lock, flags); dum_hcd->timer_pending = 0; if (!dum_hcd->udev) { dev_err(dummy_dev(dum_hcd), ""timer fired with no URBs pending?\n""); spin_unlock_irqrestore(&dum->lock, flags); return HRTIMER_NORESTART; } dum_hcd->next_frame_urbp = NULL; for (i = 0; i < DUMMY_ENDPOINTS; i++) { if (!ep_info[i].name) break; dum->ep[i].already_seen = 0; } restart: list_for_each_entry_safe(urbp, tmp, &dum_hcd->urbp_list, urbp_list) { struct urb *urb; struct dummy_request *req; u8 address; struct dummy_ep *ep = NULL; int status = -EINPROGRESS; if (urbp == dum_hcd->next_frame_urbp) break; urb = urbp->urb; if (urb->unlinked) goto return_urb; else if (dum_hcd->rh_state != DUMMY_RH_RUNNING) continue; if (total <= 0) continue; address = usb_pipeendpoint (urb->pipe); if (usb_urb_dir_in(urb)) address |= USB_DIR_IN; ep = find_endpoint(dum, address); if (!ep) { dev_dbg(dummy_dev(dum_hcd), ""no ep configured for urb %p\n"", urb); status = -EPROTO; goto return_urb; } if (ep->already_seen) continue; ep->already_seen = 1; if (ep == &dum->ep[0] && urb->error_count) { ep->setup_stage = 1; urb->error_count = 0; } if (ep->halted && !ep->setup_stage) { dev_dbg(dummy_dev(dum_hcd), ""ep %s halted, urb %p\n"", ep->ep.name, urb); status = -EPIPE; goto return_urb; } if (ep == &dum->ep[0] && ep->setup_stage) { struct usb_ctrlrequest setup; int value; setup = *(struct usb_ctrlrequest *) urb->setup_packet; list_for_each_entry(req, &ep->queue, queue) { list_del_init(&req->queue); req->req.status = -EOVERFLOW; dev_dbg(udc_dev(dum), ""stale req = %p\n"", req); spin_unlock(&dum->lock); usb_gadget_giveback_request(&ep->ep, &req->req); spin_lock(&dum->lock); ep->already_seen = 0; goto restart; } ep->last_io = jiffies; ep->setup_stage = 0; ep->halted = 0; value = handle_control_request(dum_hcd, urb, &setup, &status); if (value > 0) { ++dum->callback_usage; spin_unlock(&dum->lock); value = dum->driver->setup(&dum->gadget, &setup); spin_lock(&dum->lock); --dum->callback_usage; if (value >= 0) { limit = 64*1024; goto treat_control_like_bulk; } } if (value < 0) { if (value != -EOPNOTSUPP) dev_dbg(udc_dev(dum), ""setup --> %d\n"", value); status = -EPIPE; urb->actual_length = 0; } goto return_urb; } limit = total; switch (usb_pipetype(urb->pipe)) { case PIPE_ISOCHRONOUS: limit = max(limit, periodic_bytes(dum, ep)); status = -EINVAL; break; case PIPE_INTERRUPT: limit = max(limit, periodic_bytes(dum, ep)); fallthrough; default: treat_control_like_bulk: ep->last_io = jiffies; total -= transfer(dum_hcd, urb, ep, limit, &status); break; } if (status == -EINPROGRESS) continue; return_urb: list_del(&urbp->urbp_list); kfree(urbp); if (ep) ep->already_seen = ep->setup_stage = 0; usb_hcd_unlink_urb_from_ep(dummy_hcd_to_hcd(dum_hcd), urb); spin_unlock(&dum->lock); usb_hcd_giveback_urb(dummy_hcd_to_hcd(dum_hcd), urb, status); spin_lock(&dum->lock); goto restart; } if (list_empty(&dum_hcd->urbp_list)) { usb_put_dev(dum_hcd->udev); dum_hcd->udev = NULL; } else if (!dum_hcd->timer_pending && dum_hcd->rh_state == DUMMY_RH_RUNNING) { dum_hcd->timer_pending = 1; hrtimer_start(&dum_hcd->timer, ns_to_ktime(DUMMY_TIMER_INT_NSECS), HRTIMER_MODE_REL_SOFT); } spin_unlock_irqrestore(&dum->lock, flags); return HRTIMER_NORESTART; }"
1160----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53136/bad/shmem.c----shmem_getattr,"static int shmem_getattr(const struct path *path, struct kstat *stat, u32 request_mask, unsigned int query_flags) { struct inode *inode = path->dentry->d_inode; struct shmem_inode_info *info = SHMEM_I(inode); struct shmem_sb_info *sb_info = SHMEM_SB(inode->i_sb); if (info->alloced - info->swapped != inode->i_mapping->nrpages) { spin_lock_irq(&info->lock); shmem_recalc_inode(inode); spin_unlock_irq(&info->lock); } <S2SV_StartVul> inode_lock_shared(inode); <S2SV_EndVul> generic_fillattr(inode, stat); <S2SV_StartVul> inode_unlock_shared(inode); <S2SV_EndVul> if (is_huge_enabled(sb_info)) stat->blksize = HPAGE_PMD_SIZE; return 0; }","- inode_lock_shared(inode);
- inode_unlock_shared(inode);","static int shmem_getattr(const struct path *path, struct kstat *stat, u32 request_mask, unsigned int query_flags) { struct inode *inode = path->dentry->d_inode; struct shmem_inode_info *info = SHMEM_I(inode); struct shmem_sb_info *sb_info = SHMEM_SB(inode->i_sb); if (info->alloced - info->swapped != inode->i_mapping->nrpages) { spin_lock_irq(&info->lock); shmem_recalc_inode(inode); spin_unlock_irq(&info->lock); } generic_fillattr(inode, stat); if (is_huge_enabled(sb_info)) stat->blksize = HPAGE_PMD_SIZE; return 0; }"
106----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44964/bad/idpf_lib.c----idpf_open,"static int idpf_open(struct net_device *netdev) { struct idpf_vport *vport; int err; idpf_vport_ctrl_lock(netdev); vport = idpf_netdev_to_vport(netdev); <S2SV_StartVul> err = idpf_vport_open(vport, true); <S2SV_EndVul> idpf_vport_ctrl_unlock(netdev); return err; }","- err = idpf_vport_open(vport, true);
+ err = idpf_vport_open(vport);",static int idpf_open(struct net_device *netdev) { struct idpf_vport *vport; int err; idpf_vport_ctrl_lock(netdev); vport = idpf_netdev_to_vport(netdev); err = idpf_vport_open(vport); idpf_vport_ctrl_unlock(netdev); return err; }
1151----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53130/bad/btnode.c----nilfs_btnode_create_block,"nilfs_btnode_create_block(struct address_space *btnc, __u64 blocknr) { struct inode *inode = btnc->host; struct buffer_head *bh; bh = nilfs_grab_buffer(inode, btnc, blocknr, BIT(BH_NILFS_Node)); if (unlikely(!bh)) return ERR_PTR(-ENOMEM); if (unlikely(buffer_mapped(bh) || buffer_uptodate(bh) || buffer_dirty(bh))) { nilfs_error(inode->i_sb, ""state inconsistency probably due to duplicate use of b-tree node block address %llu (ino=%lu)"", (unsigned long long)blocknr, inode->i_ino); goto failed; } memset(bh->b_data, 0, i_blocksize(inode)); <S2SV_StartVul> bh->b_bdev = inode->i_sb->s_bdev; <S2SV_EndVul> bh->b_blocknr = blocknr; set_buffer_mapped(bh); set_buffer_uptodate(bh); unlock_page(bh->b_page); put_page(bh->b_page); return bh; failed: unlock_page(bh->b_page); put_page(bh->b_page); brelse(bh); return ERR_PTR(-EIO); }",- bh->b_bdev = inode->i_sb->s_bdev;,"nilfs_btnode_create_block(struct address_space *btnc, __u64 blocknr) { struct inode *inode = btnc->host; struct buffer_head *bh; bh = nilfs_grab_buffer(inode, btnc, blocknr, BIT(BH_NILFS_Node)); if (unlikely(!bh)) return ERR_PTR(-ENOMEM); if (unlikely(buffer_mapped(bh) || buffer_uptodate(bh) || buffer_dirty(bh))) { nilfs_error(inode->i_sb, ""state inconsistency probably due to duplicate use of b-tree node block address %llu (ino=%lu)"", (unsigned long long)blocknr, inode->i_ino); goto failed; } memset(bh->b_data, 0, i_blocksize(inode)); bh->b_blocknr = blocknr; set_buffer_mapped(bh); set_buffer_uptodate(bh); unlock_page(bh->b_page); put_page(bh->b_page); return bh; failed: unlock_page(bh->b_page); put_page(bh->b_page); brelse(bh); return ERR_PTR(-EIO); }"
933----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50182/bad/secretmem.c----SYSCALL_DEFINE1,"SYSCALL_DEFINE1(memfd_secret, unsigned int, flags) { struct file *file; int fd, err; BUILD_BUG_ON(SECRETMEM_FLAGS_MASK & O_CLOEXEC); <S2SV_StartVul> if (!secretmem_enable) <S2SV_EndVul> return -ENOSYS; if (flags & ~(SECRETMEM_FLAGS_MASK | O_CLOEXEC)) return -EINVAL; if (atomic_read(&secretmem_users) < 0) return -ENFILE; fd = get_unused_fd_flags(flags & O_CLOEXEC); if (fd < 0) return fd; file = secretmem_file_create(flags); if (IS_ERR(file)) { err = PTR_ERR(file); goto err_put_fd; } file->f_flags |= O_LARGEFILE; atomic_inc(&secretmem_users); fd_install(fd, file); return fd; err_put_fd: put_unused_fd(fd); return err; }","- if (!secretmem_enable)
+ if (!secretmem_enable || !can_set_direct_map())","SYSCALL_DEFINE1(memfd_secret, unsigned int, flags) { struct file *file; int fd, err; BUILD_BUG_ON(SECRETMEM_FLAGS_MASK & O_CLOEXEC); if (!secretmem_enable || !can_set_direct_map()) return -ENOSYS; if (flags & ~(SECRETMEM_FLAGS_MASK | O_CLOEXEC)) return -EINVAL; if (atomic_read(&secretmem_users) < 0) return -ENFILE; fd = get_unused_fd_flags(flags & O_CLOEXEC); if (fd < 0) return fd; file = secretmem_file_create(flags); if (IS_ERR(file)) { err = PTR_ERR(file); goto err_put_fd; } file->f_flags |= O_LARGEFILE; atomic_inc(&secretmem_users); fd_install(fd, file); return fd; err_put_fd: put_unused_fd(fd); return err; }"
209----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46693/bad/pmic_glink.c----*devm_pmic_glink_register_client,"struct pmic_glink_client *devm_pmic_glink_register_client(struct device *dev, unsigned int id, void (*cb)(const void *, size_t, void *), void (*pdr)(void *, int), void *priv) { struct pmic_glink_client *client; struct pmic_glink *pg = dev_get_drvdata(dev->parent); unsigned long flags; client = devres_alloc(_devm_pmic_glink_release_client, sizeof(*client), GFP_KERNEL); if (!client) return ERR_PTR(-ENOMEM); client->pg = pg; client->id = id; client->cb = cb; client->pdr_notify = pdr; client->priv = priv; mutex_lock(&pg->state_lock); spin_lock_irqsave(&pg->client_lock, flags); list_add(&client->node, &pg->clients); client->pdr_notify(client->priv, pg->client_state); spin_unlock_irqrestore(&pg->client_lock, flags); mutex_unlock(&pg->state_lock); <S2SV_StartVul> devres_add(dev, client); <S2SV_EndVul> <S2SV_StartVul> return client; <S2SV_EndVul> }","- devres_add(dev, client);
- return client;","#include <linux/auxiliary_bus.h> #include <linux/module.h> #include <linux/of.h> #include <linux/platform_device.h> #include <linux/rpmsg.h> #include <linux/slab.h> #include <linux/soc/qcom/pdr.h> #include <linux/soc/qcom/pmic_glink.h> #include <linux/spinlock.h> enum { PMIC_GLINK_CLIENT_BATT = 0, PMIC_GLINK_CLIENT_ALTMODE, PMIC_GLINK_CLIENT_UCSI, };"
610----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49921/bad/dcn31_hwseq.c----dcn31_init_hw,"void dcn31_init_hw(struct dc *dc) { struct abm **abms = dc->res_pool->multiple_abms; struct dce_hwseq *hws = dc->hwseq; struct dc_bios *dcb = dc->ctx->dc_bios; struct resource_pool *res_pool = dc->res_pool; uint32_t backlight = MAX_BACKLIGHT_LEVEL; uint32_t user_level = MAX_BACKLIGHT_LEVEL; int i; if (dc->clk_mgr && dc->clk_mgr->funcs->init_clocks) dc->clk_mgr->funcs->init_clocks(dc->clk_mgr); if (!dcb->funcs->is_accelerated_mode(dcb)) { hws->funcs.bios_golden_init(dc); if (hws->funcs.disable_vga) hws->funcs.disable_vga(dc->hwseq); } if (res_pool->dccg->funcs->dccg_init) res_pool->dccg->funcs->dccg_init(res_pool->dccg); enable_memory_low_power(dc); if (dc->ctx->dc_bios->fw_info_valid) { res_pool->ref_clocks.xtalin_clock_inKhz = dc->ctx->dc_bios->fw_info.pll_info.crystal_frequency; if (res_pool->hubbub) { (res_pool->dccg->funcs->get_dccg_ref_freq)(res_pool->dccg, dc->ctx->dc_bios->fw_info.pll_info.crystal_frequency, &res_pool->ref_clocks.dccg_ref_clock_inKhz); (res_pool->hubbub->funcs->get_dchub_ref_freq)(res_pool->hubbub, res_pool->ref_clocks.dccg_ref_clock_inKhz, &res_pool->ref_clocks.dchub_ref_clock_inKhz); } else { res_pool->ref_clocks.dccg_ref_clock_inKhz = res_pool->ref_clocks.xtalin_clock_inKhz; res_pool->ref_clocks.dchub_ref_clock_inKhz = res_pool->ref_clocks.xtalin_clock_inKhz; } } else ASSERT_CRITICAL(false); for (i = 0; i < dc->link_count; i++) { struct dc_link *link = dc->links[i]; if (link->ep_type != DISPLAY_ENDPOINT_PHY) continue; link->link_enc->funcs->hw_init(link->link_enc); if (link->link_enc->funcs->is_dig_enabled && link->link_enc->funcs->is_dig_enabled(link->link_enc)) { link->link_status.link_active = true; if (link->link_enc->funcs->fec_is_active && link->link_enc->funcs->fec_is_active(link->link_enc)) link->fec_state = dc_link_fec_enabled; } } dc->link_srv->blank_all_dp_displays(dc); if (hws->funcs.enable_power_gating_plane) hws->funcs.enable_power_gating_plane(dc->hwseq, true); if (dcb->funcs->is_accelerated_mode(dcb) || !dc->config.seamless_boot_edp_requested) { if (!dc->caps.seamless_odm) { for (i = 0; i < dc->res_pool->timing_generator_count; i++) { struct timing_generator *tg = dc->res_pool->timing_generators[i]; uint32_t num_opps, opp_id_src0, opp_id_src1; num_opps = 1; if (tg) { if (tg->funcs->is_tg_enabled(tg) && tg->funcs->get_optc_source) { tg->funcs->get_optc_source(tg, &num_opps, &opp_id_src0, &opp_id_src1); } } if (num_opps > 1) { dc->link_srv->blank_all_edp_displays(dc); break; } } } hws->funcs.init_pipes(dc, dc->current_state); if (dc->res_pool->hubbub->funcs->allow_self_refresh_control) dc->res_pool->hubbub->funcs->allow_self_refresh_control(dc->res_pool->hubbub, !dc->res_pool->hubbub->ctx->dc->debug.disable_stutter); } for (i = 0; i < res_pool->audio_count; i++) { struct audio *audio = res_pool->audios[i]; audio->funcs->hw_init(audio); } for (i = 0; i < dc->link_count; i++) { struct dc_link *link = dc->links[i]; if (link->panel_cntl) { backlight = link->panel_cntl->funcs->hw_init(link->panel_cntl); user_level = link->panel_cntl->stored_backlight_registers.USER_LEVEL; } } for (i = 0; i < dc->res_pool->pipe_count; i++) { if (abms[i] != NULL) abms[i]->funcs->abm_init(abms[i], backlight, user_level); } REG_WRITE(DIO_MEM_PWR_CTRL, 0); if (dc->debug.enable_mem_low_power.bits.i2c) REG_UPDATE(DIO_MEM_PWR_CTRL, I2C_LIGHT_SLEEP_FORCE, 1); if (hws->funcs.setup_hpo_hw_control) hws->funcs.setup_hpo_hw_control(hws, false); if (!dc->debug.disable_clock_gate) { REG_WRITE(DCCG_GATE_DISABLE_CNTL, 0); REG_WRITE(DCCG_GATE_DISABLE_CNTL2, 0); REG_UPDATE(DCFCLK_CNTL, DCFCLK_GATE_DIS, 0); } if (!dcb->funcs->is_accelerated_mode(dcb) && dc->res_pool->hubbub->funcs->init_watermarks) dc->res_pool->hubbub->funcs->init_watermarks(dc->res_pool->hubbub); <S2SV_StartVul> if (dc->clk_mgr->funcs->notify_wm_ranges) <S2SV_EndVul> dc->clk_mgr->funcs->notify_wm_ranges(dc->clk_mgr); <S2SV_StartVul> if (dc->clk_mgr->funcs->set_hard_max_memclk && !dc->clk_mgr->dc_mode_softmax_enabled) <S2SV_EndVul> dc->clk_mgr->funcs->set_hard_max_memclk(dc->clk_mgr); if (dc->res_pool->hubbub->funcs->force_pstate_change_control) dc->res_pool->hubbub->funcs->force_pstate_change_control( dc->res_pool->hubbub, false, false); #if defined(CONFIG_DRM_AMD_DC_FP) if (dc->res_pool->hubbub->funcs->init_crb) dc->res_pool->hubbub->funcs->init_crb(dc->res_pool->hubbub); #endif dc_dmub_srv_query_caps_cmd(dc->ctx->dmub_srv); dc->caps.dmub_caps.psr = dc->ctx->dmub_srv->dmub->feature_caps.psr; dc->caps.dmub_caps.mclk_sw = dc->ctx->dmub_srv->dmub->feature_caps.fw_assisted_mclk_switch_ver; }","- if (dc->clk_mgr->funcs->notify_wm_ranges)
- if (dc->clk_mgr->funcs->set_hard_max_memclk && !dc->clk_mgr->dc_mode_softmax_enabled)
+ if (dc->clk_mgr && dc->clk_mgr->funcs->notify_wm_ranges)
+ if (dc->clk_mgr && dc->clk_mgr->funcs->set_hard_max_memclk && !dc->clk_mgr->dc_mode_softmax_enabled)","void dcn31_init_hw(struct dc *dc) { struct abm **abms = dc->res_pool->multiple_abms; struct dce_hwseq *hws = dc->hwseq; struct dc_bios *dcb = dc->ctx->dc_bios; struct resource_pool *res_pool = dc->res_pool; uint32_t backlight = MAX_BACKLIGHT_LEVEL; uint32_t user_level = MAX_BACKLIGHT_LEVEL; int i; if (dc->clk_mgr && dc->clk_mgr->funcs->init_clocks) dc->clk_mgr->funcs->init_clocks(dc->clk_mgr); if (!dcb->funcs->is_accelerated_mode(dcb)) { hws->funcs.bios_golden_init(dc); if (hws->funcs.disable_vga) hws->funcs.disable_vga(dc->hwseq); } if (res_pool->dccg->funcs->dccg_init) res_pool->dccg->funcs->dccg_init(res_pool->dccg); enable_memory_low_power(dc); if (dc->ctx->dc_bios->fw_info_valid) { res_pool->ref_clocks.xtalin_clock_inKhz = dc->ctx->dc_bios->fw_info.pll_info.crystal_frequency; if (res_pool->hubbub) { (res_pool->dccg->funcs->get_dccg_ref_freq)(res_pool->dccg, dc->ctx->dc_bios->fw_info.pll_info.crystal_frequency, &res_pool->ref_clocks.dccg_ref_clock_inKhz); (res_pool->hubbub->funcs->get_dchub_ref_freq)(res_pool->hubbub, res_pool->ref_clocks.dccg_ref_clock_inKhz, &res_pool->ref_clocks.dchub_ref_clock_inKhz); } else { res_pool->ref_clocks.dccg_ref_clock_inKhz = res_pool->ref_clocks.xtalin_clock_inKhz; res_pool->ref_clocks.dchub_ref_clock_inKhz = res_pool->ref_clocks.xtalin_clock_inKhz; } } else ASSERT_CRITICAL(false); for (i = 0; i < dc->link_count; i++) { struct dc_link *link = dc->links[i]; if (link->ep_type != DISPLAY_ENDPOINT_PHY) continue; link->link_enc->funcs->hw_init(link->link_enc); if (link->link_enc->funcs->is_dig_enabled && link->link_enc->funcs->is_dig_enabled(link->link_enc)) { link->link_status.link_active = true; if (link->link_enc->funcs->fec_is_active && link->link_enc->funcs->fec_is_active(link->link_enc)) link->fec_state = dc_link_fec_enabled; } } dc->link_srv->blank_all_dp_displays(dc); if (hws->funcs.enable_power_gating_plane) hws->funcs.enable_power_gating_plane(dc->hwseq, true); if (dcb->funcs->is_accelerated_mode(dcb) || !dc->config.seamless_boot_edp_requested) { if (!dc->caps.seamless_odm) { for (i = 0; i < dc->res_pool->timing_generator_count; i++) { struct timing_generator *tg = dc->res_pool->timing_generators[i]; uint32_t num_opps, opp_id_src0, opp_id_src1; num_opps = 1; if (tg) { if (tg->funcs->is_tg_enabled(tg) && tg->funcs->get_optc_source) { tg->funcs->get_optc_source(tg, &num_opps, &opp_id_src0, &opp_id_src1); } } if (num_opps > 1) { dc->link_srv->blank_all_edp_displays(dc); break; } } } hws->funcs.init_pipes(dc, dc->current_state); if (dc->res_pool->hubbub->funcs->allow_self_refresh_control) dc->res_pool->hubbub->funcs->allow_self_refresh_control(dc->res_pool->hubbub, !dc->res_pool->hubbub->ctx->dc->debug.disable_stutter); } for (i = 0; i < res_pool->audio_count; i++) { struct audio *audio = res_pool->audios[i]; audio->funcs->hw_init(audio); } for (i = 0; i < dc->link_count; i++) { struct dc_link *link = dc->links[i]; if (link->panel_cntl) { backlight = link->panel_cntl->funcs->hw_init(link->panel_cntl); user_level = link->panel_cntl->stored_backlight_registers.USER_LEVEL; } } for (i = 0; i < dc->res_pool->pipe_count; i++) { if (abms[i] != NULL) abms[i]->funcs->abm_init(abms[i], backlight, user_level); } REG_WRITE(DIO_MEM_PWR_CTRL, 0); if (dc->debug.enable_mem_low_power.bits.i2c) REG_UPDATE(DIO_MEM_PWR_CTRL, I2C_LIGHT_SLEEP_FORCE, 1); if (hws->funcs.setup_hpo_hw_control) hws->funcs.setup_hpo_hw_control(hws, false); if (!dc->debug.disable_clock_gate) { REG_WRITE(DCCG_GATE_DISABLE_CNTL, 0); REG_WRITE(DCCG_GATE_DISABLE_CNTL2, 0); REG_UPDATE(DCFCLK_CNTL, DCFCLK_GATE_DIS, 0); } if (!dcb->funcs->is_accelerated_mode(dcb) && dc->res_pool->hubbub->funcs->init_watermarks) dc->res_pool->hubbub->funcs->init_watermarks(dc->res_pool->hubbub); if (dc->clk_mgr && dc->clk_mgr->funcs->notify_wm_ranges) dc->clk_mgr->funcs->notify_wm_ranges(dc->clk_mgr); if (dc->clk_mgr && dc->clk_mgr->funcs->set_hard_max_memclk && !dc->clk_mgr->dc_mode_softmax_enabled) dc->clk_mgr->funcs->set_hard_max_memclk(dc->clk_mgr); if (dc->res_pool->hubbub->funcs->force_pstate_change_control) dc->res_pool->hubbub->funcs->force_pstate_change_control( dc->res_pool->hubbub, false, false); #if defined(CONFIG_DRM_AMD_DC_FP) if (dc->res_pool->hubbub->funcs->init_crb) dc->res_pool->hubbub->funcs->init_crb(dc->res_pool->hubbub); #endif dc_dmub_srv_query_caps_cmd(dc->ctx->dmub_srv); dc->caps.dmub_caps.psr = dc->ctx->dmub_srv->dmub->feature_caps.psr; dc->caps.dmub_caps.mclk_sw = dc->ctx->dmub_srv->dmub->feature_caps.fw_assisted_mclk_switch_ver; }"
317----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46784/bad/mana_en.c----mana_destroy_txq,"static void mana_destroy_txq(struct mana_port_context *apc) { struct napi_struct *napi; int i; if (!apc->tx_qp) return; for (i = 0; i < apc->num_queues; i++) { napi = &apc->tx_qp[i].tx_cq.napi; <S2SV_StartVul> napi_synchronize(napi); <S2SV_EndVul> <S2SV_StartVul> napi_disable(napi); <S2SV_EndVul> <S2SV_StartVul> netif_napi_del(napi); <S2SV_EndVul> mana_destroy_wq_obj(apc, GDMA_SQ, apc->tx_qp[i].tx_object); mana_deinit_cq(apc, &apc->tx_qp[i].tx_cq); mana_deinit_txq(apc, &apc->tx_qp[i].txq); } kfree(apc->tx_qp); apc->tx_qp = NULL; }","- napi_synchronize(napi);
- napi_disable(napi);
- netif_napi_del(napi);
+ if (apc->tx_qp[i].txq.napi_initialized) {
+ napi_synchronize(napi);
+ napi_disable(napi);
+ netif_napi_del(napi);
+ apc->tx_qp[i].txq.napi_initialized = false;
+ }","static void mana_destroy_txq(struct mana_port_context *apc) { struct napi_struct *napi; int i; if (!apc->tx_qp) return; for (i = 0; i < apc->num_queues; i++) { napi = &apc->tx_qp[i].tx_cq.napi; if (apc->tx_qp[i].txq.napi_initialized) { napi_synchronize(napi); napi_disable(napi); netif_napi_del(napi); apc->tx_qp[i].txq.napi_initialized = false; } mana_destroy_wq_obj(apc, GDMA_SQ, apc->tx_qp[i].tx_object); mana_deinit_cq(apc, &apc->tx_qp[i].tx_cq); mana_deinit_txq(apc, &apc->tx_qp[i].txq); } kfree(apc->tx_qp); apc->tx_qp = NULL; }"
1115----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53090/bad/rxrpc.c----afs_wake_up_async_call,"static void afs_wake_up_async_call(struct sock *sk, struct rxrpc_call *rxcall, unsigned long call_user_ID) { struct afs_call *call = (struct afs_call *)call_user_ID; int r; trace_afs_notify_call(rxcall, call); call->need_attention = true; if (__refcount_inc_not_zero(&call->ref, &r)) { trace_afs_call(call->debug_id, afs_call_trace_wake, r + 1, atomic_read(&call->net->nr_outstanding_calls), __builtin_return_address(0)); if (!queue_work(afs_async_calls, &call->async_work)) <S2SV_StartVul> afs_put_call(call); <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul>","- afs_put_call(call);
- }
- }
+ afs_deferred_put_call(call);
+ }
+ }","static void afs_wake_up_async_call(struct sock *sk, struct rxrpc_call *rxcall, unsigned long call_user_ID) { struct afs_call *call = (struct afs_call *)call_user_ID; int r; trace_afs_notify_call(rxcall, call); call->need_attention = true; if (__refcount_inc_not_zero(&call->ref, &r)) { trace_afs_call(call->debug_id, afs_call_trace_wake, r + 1, atomic_read(&call->net->nr_outstanding_calls), __builtin_return_address(0)); if (!queue_work(afs_async_calls, &call->async_work)) afs_deferred_put_call(call); } }"
515----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47750/bad/hns_roce_hw_v2.c----__hns_roce_hw_v2_uninit_instance,"static void __hns_roce_hw_v2_uninit_instance(struct hnae3_handle *handle, bool reset) { struct hns_roce_dev *hr_dev = handle->priv; if (!hr_dev) return; handle->priv = NULL; hr_dev->state = HNS_ROCE_DEVICE_STATE_UNINIT; hns_roce_handle_device_err(hr_dev); <S2SV_StartVul> if (hr_dev->pci_dev->revision == PCI_REVISION_ID_HIP08) <S2SV_EndVul> <S2SV_StartVul> free_mr_exit(hr_dev); <S2SV_EndVul> hns_roce_exit(hr_dev); kfree(hr_dev->priv); ib_dealloc_device(&hr_dev->ib_dev); }","- if (hr_dev->pci_dev->revision == PCI_REVISION_ID_HIP08)
- free_mr_exit(hr_dev);","static void __hns_roce_hw_v2_uninit_instance(struct hnae3_handle *handle, bool reset) { struct hns_roce_dev *hr_dev = handle->priv; if (!hr_dev) return; handle->priv = NULL; hr_dev->state = HNS_ROCE_DEVICE_STATE_UNINIT; hns_roce_handle_device_err(hr_dev); hns_roce_exit(hr_dev); kfree(hr_dev->priv); ib_dealloc_device(&hr_dev->ib_dev); }"
335----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46793/bad/cht_bsw_rt5645.c----snd_cht_mc_probe,"static int snd_cht_mc_probe(struct platform_device *pdev) { struct snd_soc_card *card = snd_soc_cards[0].soc_card; struct snd_soc_acpi_mach *mach; const char *platform_name; struct cht_mc_private *drv; struct acpi_device *adev; struct device *codec_dev; bool sof_parent; bool found = false; bool is_bytcr = false; int dai_index = 0; int ret_val = 0; int i; const char *mclk_name; drv = devm_kzalloc(&pdev->dev, sizeof(*drv), GFP_KERNEL); if (!drv) return -ENOMEM; mach = pdev->dev.platform_data; for (i = 0; i < ARRAY_SIZE(snd_soc_cards); i++) { if (acpi_dev_found(snd_soc_cards[i].codec_id) && (!strncmp(snd_soc_cards[i].codec_id, mach->id, 8))) { dev_dbg(&pdev->dev, ""found codec %s\n"", snd_soc_cards[i].codec_id); card = snd_soc_cards[i].soc_card; drv->acpi_card = &snd_soc_cards[i]; found = true; break; } } if (!found) { dev_err(&pdev->dev, ""No matching HID found in supported list\n""); return -ENODEV; } card->dev = &pdev->dev; for (i = 0; i < ARRAY_SIZE(cht_dailink); i++) <S2SV_StartVul> if (cht_dailink[i].codecs->name && <S2SV_EndVul> !strcmp(cht_dailink[i].codecs->name, ""i2c-10EC5645:00"")) { dai_index = i; break; } adev = acpi_dev_get_first_match_dev(mach->id, NULL, -1); if (adev) { snprintf(cht_rt5645_codec_name, sizeof(cht_rt5645_codec_name), ""i2c-%s"", acpi_dev_name(adev)); cht_dailink[dai_index].codecs->name = cht_rt5645_codec_name; } codec_dev = acpi_get_first_physical_node(adev); acpi_dev_put(adev); if (!codec_dev) return -EPROBE_DEFER; snd_soc_card_chtrt5645.components = rt5645_components(codec_dev); snd_soc_card_chtrt5650.components = rt5645_components(codec_dev); if (soc_intel_is_byt()) { if (mach->mach_params.acpi_ipc_irq_index == 0) is_bytcr = true; } if (is_bytcr) { struct acpi_chan_package chan_package = { 0 }; struct acpi_buffer format = {sizeof(""NN""), ""NN""}; struct acpi_buffer state = {0, NULL}; struct snd_soc_acpi_package_context pkg_ctx; bool pkg_found = false; state.length = sizeof(chan_package); state.pointer = &chan_package; pkg_ctx.name = ""CHAN""; pkg_ctx.length = 2; pkg_ctx.format = &format; pkg_ctx.state = &state; pkg_ctx.data_valid = false; pkg_found = snd_soc_acpi_find_package_from_hid(mach->id, &pkg_ctx); if (pkg_found) { if (chan_package.aif_value == 1) { dev_info(&pdev->dev, ""BIOS Routing: AIF1 connected\n""); cht_rt5645_quirk |= CHT_RT5645_SSP0_AIF1; } else if (chan_package.aif_value == 2) { dev_info(&pdev->dev, ""BIOS Routing: AIF2 connected\n""); cht_rt5645_quirk |= CHT_RT5645_SSP0_AIF2; } else { dev_info(&pdev->dev, ""BIOS Routing isn't valid, ignored\n""); pkg_found = false; } } if (!pkg_found) { cht_rt5645_quirk |= CHT_RT5645_SSP0_AIF2; } } dmi_check_system(cht_rt5645_quirk_table); log_quirks(&pdev->dev); if ((cht_rt5645_quirk & CHT_RT5645_SSP2_AIF2) || (cht_rt5645_quirk & CHT_RT5645_SSP0_AIF2)) cht_dailink[dai_index].codecs->dai_name = ""rt5645-aif2""; if ((cht_rt5645_quirk & CHT_RT5645_SSP0_AIF1) || (cht_rt5645_quirk & CHT_RT5645_SSP0_AIF2)) cht_dailink[dai_index].cpus->dai_name = ""ssp0-port""; platform_name = mach->mach_params.platform; ret_val = snd_soc_fixup_dai_links_platform_name(card, platform_name); if (ret_val) return ret_val; if (cht_rt5645_quirk & CHT_RT5645_PMC_PLT_CLK_0) mclk_name = ""pmc_plt_clk_0""; else mclk_name = ""pmc_plt_clk_3""; drv->mclk = devm_clk_get(&pdev->dev, mclk_name); if (IS_ERR(drv->mclk)) { dev_err(&pdev->dev, ""Failed to get MCLK from %s: %ld\n"", mclk_name, PTR_ERR(drv->mclk)); return PTR_ERR(drv->mclk); } snd_soc_card_set_drvdata(card, drv); sof_parent = snd_soc_acpi_sof_parent(&pdev->dev); if (sof_parent) { snd_soc_card_chtrt5645.name = SOF_CARD_RT5645_NAME; snd_soc_card_chtrt5645.driver_name = SOF_DRIVER_NAME; snd_soc_card_chtrt5650.name = SOF_CARD_RT5650_NAME; snd_soc_card_chtrt5650.driver_name = SOF_DRIVER_NAME; } else { snd_soc_card_chtrt5645.name = CARD_RT5645_NAME; snd_soc_card_chtrt5645.driver_name = DRIVER_NAME; snd_soc_card_chtrt5650.name = CARD_RT5650_NAME; snd_soc_card_chtrt5650.driver_name = DRIVER_NAME; } if (sof_parent) pdev->dev.driver->pm = &snd_soc_pm_ops; ret_val = devm_snd_soc_register_card(&pdev->dev, card); if (ret_val) { dev_err(&pdev->dev, ""snd_soc_register_card failed %d\n"", ret_val); return ret_val; } platform_set_drvdata(pdev, card); return ret_val; }","- if (cht_dailink[i].codecs->name &&
+ if (cht_dailink[i].num_codecs &&","static int snd_cht_mc_probe(struct platform_device *pdev) { struct snd_soc_card *card = snd_soc_cards[0].soc_card; struct snd_soc_acpi_mach *mach; const char *platform_name; struct cht_mc_private *drv; struct acpi_device *adev; struct device *codec_dev; bool sof_parent; bool found = false; bool is_bytcr = false; int dai_index = 0; int ret_val = 0; int i; const char *mclk_name; drv = devm_kzalloc(&pdev->dev, sizeof(*drv), GFP_KERNEL); if (!drv) return -ENOMEM; mach = pdev->dev.platform_data; for (i = 0; i < ARRAY_SIZE(snd_soc_cards); i++) { if (acpi_dev_found(snd_soc_cards[i].codec_id) && (!strncmp(snd_soc_cards[i].codec_id, mach->id, 8))) { dev_dbg(&pdev->dev, ""found codec %s\n"", snd_soc_cards[i].codec_id); card = snd_soc_cards[i].soc_card; drv->acpi_card = &snd_soc_cards[i]; found = true; break; } } if (!found) { dev_err(&pdev->dev, ""No matching HID found in supported list\n""); return -ENODEV; } card->dev = &pdev->dev; for (i = 0; i < ARRAY_SIZE(cht_dailink); i++) if (cht_dailink[i].num_codecs && !strcmp(cht_dailink[i].codecs->name, ""i2c-10EC5645:00"")) { dai_index = i; break; } adev = acpi_dev_get_first_match_dev(mach->id, NULL, -1); if (adev) { snprintf(cht_rt5645_codec_name, sizeof(cht_rt5645_codec_name), ""i2c-%s"", acpi_dev_name(adev)); cht_dailink[dai_index].codecs->name = cht_rt5645_codec_name; } codec_dev = acpi_get_first_physical_node(adev); acpi_dev_put(adev); if (!codec_dev) return -EPROBE_DEFER; snd_soc_card_chtrt5645.components = rt5645_components(codec_dev); snd_soc_card_chtrt5650.components = rt5645_components(codec_dev); if (soc_intel_is_byt()) { if (mach->mach_params.acpi_ipc_irq_index == 0) is_bytcr = true; } if (is_bytcr) { struct acpi_chan_package chan_package = { 0 }; struct acpi_buffer format = {sizeof(""NN""), ""NN""}; struct acpi_buffer state = {0, NULL}; struct snd_soc_acpi_package_context pkg_ctx; bool pkg_found = false; state.length = sizeof(chan_package); state.pointer = &chan_package; pkg_ctx.name = ""CHAN""; pkg_ctx.length = 2; pkg_ctx.format = &format; pkg_ctx.state = &state; pkg_ctx.data_valid = false; pkg_found = snd_soc_acpi_find_package_from_hid(mach->id, &pkg_ctx); if (pkg_found) { if (chan_package.aif_value == 1) { dev_info(&pdev->dev, ""BIOS Routing: AIF1 connected\n""); cht_rt5645_quirk |= CHT_RT5645_SSP0_AIF1; } else if (chan_package.aif_value == 2) { dev_info(&pdev->dev, ""BIOS Routing: AIF2 connected\n""); cht_rt5645_quirk |= CHT_RT5645_SSP0_AIF2; } else { dev_info(&pdev->dev, ""BIOS Routing isn't valid, ignored\n""); pkg_found = false; } } if (!pkg_found) { cht_rt5645_quirk |= CHT_RT5645_SSP0_AIF2; } } dmi_check_system(cht_rt5645_quirk_table); log_quirks(&pdev->dev); if ((cht_rt5645_quirk & CHT_RT5645_SSP2_AIF2) || (cht_rt5645_quirk & CHT_RT5645_SSP0_AIF2)) cht_dailink[dai_index].codecs->dai_name = ""rt5645-aif2""; if ((cht_rt5645_quirk & CHT_RT5645_SSP0_AIF1) || (cht_rt5645_quirk & CHT_RT5645_SSP0_AIF2)) cht_dailink[dai_index].cpus->dai_name = ""ssp0-port""; platform_name = mach->mach_params.platform; ret_val = snd_soc_fixup_dai_links_platform_name(card, platform_name); if (ret_val) return ret_val; if (cht_rt5645_quirk & CHT_RT5645_PMC_PLT_CLK_0) mclk_name = ""pmc_plt_clk_0""; else mclk_name = ""pmc_plt_clk_3""; drv->mclk = devm_clk_get(&pdev->dev, mclk_name); if (IS_ERR(drv->mclk)) { dev_err(&pdev->dev, ""Failed to get MCLK from %s: %ld\n"", mclk_name, PTR_ERR(drv->mclk)); return PTR_ERR(drv->mclk); } snd_soc_card_set_drvdata(card, drv); sof_parent = snd_soc_acpi_sof_parent(&pdev->dev); if (sof_parent) { snd_soc_card_chtrt5645.name = SOF_CARD_RT5645_NAME; snd_soc_card_chtrt5645.driver_name = SOF_DRIVER_NAME; snd_soc_card_chtrt5650.name = SOF_CARD_RT5650_NAME; snd_soc_card_chtrt5650.driver_name = SOF_DRIVER_NAME; } else { snd_soc_card_chtrt5645.name = CARD_RT5645_NAME; snd_soc_card_chtrt5645.driver_name = DRIVER_NAME; snd_soc_card_chtrt5650.name = CARD_RT5650_NAME; snd_soc_card_chtrt5650.driver_name = DRIVER_NAME; } if (sof_parent) pdev->dev.driver->pm = &snd_soc_pm_ops; ret_val = devm_snd_soc_register_card(&pdev->dev, card); if (ret_val) { dev_err(&pdev->dev, ""snd_soc_register_card failed %d\n"", ret_val); return ret_val; } platform_set_drvdata(pdev, card); return ret_val; }"
360----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46821/bad/navi10_ppt.c----navi10_force_clk_levels,"static int navi10_force_clk_levels(struct smu_context *smu, enum smu_clk_type clk_type, uint32_t mask) { int ret = 0; uint32_t soft_min_level = 0, soft_max_level = 0, min_freq = 0, max_freq = 0; soft_min_level = mask ? (ffs(mask) - 1) : 0; soft_max_level = mask ? (fls(mask) - 1) : 0; switch (clk_type) { case SMU_GFXCLK: case SMU_SCLK: case SMU_SOCCLK: case SMU_MCLK: case SMU_UCLK: case SMU_FCLK: <S2SV_StartVul> if (navi10_is_support_fine_grained_dpm(smu, clk_type)) { <S2SV_EndVul> soft_max_level = (soft_max_level >= 1 ? 1 : 0); soft_min_level = (soft_min_level >= 1 ? 1 : 0); } ret = smu_v11_0_get_dpm_freq_by_index(smu, clk_type, soft_min_level, &min_freq); if (ret) return 0; ret = smu_v11_0_get_dpm_freq_by_index(smu, clk_type, soft_max_level, &max_freq); if (ret) return 0; ret = smu_v11_0_set_soft_freq_limited_range(smu, clk_type, min_freq, max_freq); if (ret) return 0; break; case SMU_DCEFCLK: dev_info(smu->adev->dev, ""Setting DCEFCLK min/max dpm level is not supported!\n""); break; default: break; } return 0; }","- if (navi10_is_support_fine_grained_dpm(smu, clk_type)) {
+ ret = navi10_is_support_fine_grained_dpm(smu, clk_type);
+ if (ret < 0)
+ return ret;
+ if (ret) {","static int navi10_force_clk_levels(struct smu_context *smu, enum smu_clk_type clk_type, uint32_t mask) { int ret = 0; uint32_t soft_min_level = 0, soft_max_level = 0, min_freq = 0, max_freq = 0; soft_min_level = mask ? (ffs(mask) - 1) : 0; soft_max_level = mask ? (fls(mask) - 1) : 0; switch (clk_type) { case SMU_GFXCLK: case SMU_SCLK: case SMU_SOCCLK: case SMU_MCLK: case SMU_UCLK: case SMU_FCLK: ret = navi10_is_support_fine_grained_dpm(smu, clk_type); if (ret < 0) return ret; if (ret) { soft_max_level = (soft_max_level >= 1 ? 1 : 0); soft_min_level = (soft_min_level >= 1 ? 1 : 0); } ret = smu_v11_0_get_dpm_freq_by_index(smu, clk_type, soft_min_level, &min_freq); if (ret) return 0; ret = smu_v11_0_get_dpm_freq_by_index(smu, clk_type, soft_max_level, &max_freq); if (ret) return 0; ret = smu_v11_0_set_soft_freq_limited_range(smu, clk_type, min_freq, max_freq); if (ret) return 0; break; case SMU_DCEFCLK: dev_info(smu->adev->dev, ""Setting DCEFCLK min/max dpm level is not supported!\n""); break; default: break; } return 0; }"
855----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50114/bad/vgic-init.c----__kvm_vgic_vcpu_destroy,static void __kvm_vgic_vcpu_destroy(struct kvm_vcpu *vcpu) { struct vgic_cpu *vgic_cpu = &vcpu->arch.vgic_cpu; vgic_flush_pending_lpis(vcpu); INIT_LIST_HEAD(&vgic_cpu->ap_list_head); kfree(vgic_cpu->private_irqs); vgic_cpu->private_irqs = NULL; <S2SV_StartVul> if (vcpu->kvm->arch.vgic.vgic_model == KVM_DEV_TYPE_ARM_VGIC_V3) <S2SV_EndVul> vgic_cpu->rd_iodev.base_addr = VGIC_ADDR_UNDEF; },"- if (vcpu->kvm->arch.vgic.vgic_model == KVM_DEV_TYPE_ARM_VGIC_V3)
+ if (vcpu->kvm->arch.vgic.vgic_model == KVM_DEV_TYPE_ARM_VGIC_V3) {
+ if (kvm_get_vcpu_by_id(vcpu->kvm, vcpu->vcpu_id) != vcpu)
+ vgic_unregister_redist_iodev(vcpu);
+ }
+ }","static void __kvm_vgic_vcpu_destroy(struct kvm_vcpu *vcpu) { struct vgic_cpu *vgic_cpu = &vcpu->arch.vgic_cpu; vgic_flush_pending_lpis(vcpu); INIT_LIST_HEAD(&vgic_cpu->ap_list_head); kfree(vgic_cpu->private_irqs); vgic_cpu->private_irqs = NULL; if (vcpu->kvm->arch.vgic.vgic_model == KVM_DEV_TYPE_ARM_VGIC_V3) { if (kvm_get_vcpu_by_id(vcpu->kvm, vcpu->vcpu_id) != vcpu) vgic_unregister_redist_iodev(vcpu); vgic_cpu->rd_iodev.base_addr = VGIC_ADDR_UNDEF; } }"
745----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50022/bad/device.c----dax_set_mapping,"static void dax_set_mapping(struct vm_fault *vmf, pfn_t pfn, unsigned long fault_size) { unsigned long i, nr_pages = fault_size / PAGE_SIZE; struct file *filp = vmf->vma->vm_file; struct dev_dax *dev_dax = filp->private_data; pgoff_t pgoff; if (dev_dax->pgmap->vmemmap_shift) nr_pages = 1; pgoff = linear_page_index(vmf->vma, <S2SV_StartVul> ALIGN(vmf->address, fault_size)); <S2SV_EndVul> for (i = 0; i < nr_pages; i++) { struct page *page = pfn_to_page(pfn_t_to_pfn(pfn) + i); page = compound_head(page); if (page->mapping) continue; page->mapping = filp->f_mapping; page->index = pgoff + i; } }","- ALIGN(vmf->address, fault_size));
+ ALIGN_DOWN(vmf->address, fault_size));","static void dax_set_mapping(struct vm_fault *vmf, pfn_t pfn, unsigned long fault_size) { unsigned long i, nr_pages = fault_size / PAGE_SIZE; struct file *filp = vmf->vma->vm_file; struct dev_dax *dev_dax = filp->private_data; pgoff_t pgoff; if (dev_dax->pgmap->vmemmap_shift) nr_pages = 1; pgoff = linear_page_index(vmf->vma, ALIGN_DOWN(vmf->address, fault_size)); for (i = 0; i < nr_pages; i++) { struct page *page = pfn_to_page(pfn_t_to_pfn(pfn) + i); page = compound_head(page); if (page->mapping) continue; page->mapping = filp->f_mapping; page->index = pgoff + i; } }"
221----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46701/bad/libfs.c----offset_dir_llseek,"static loff_t offset_dir_llseek(struct file *file, loff_t offset, int whence) { switch (whence) { case SEEK_CUR: offset += file->f_pos; fallthrough; case SEEK_SET: if (offset >= 0) break; fallthrough; default: return -EINVAL; } <S2SV_StartVul> file->private_data = NULL; <S2SV_EndVul> return vfs_setpos(file, offset, LONG_MAX); }","- file->private_data = NULL;
+ if (!offset)
+ file->private_data = (void *)ctx->next_offset;
+ }","static loff_t offset_dir_llseek(struct file *file, loff_t offset, int whence) { struct inode *inode = file->f_inode; struct offset_ctx *ctx = inode->i_op->get_offset_ctx(inode); switch (whence) { case SEEK_CUR: offset += file->f_pos; fallthrough; case SEEK_SET: if (offset >= 0) break; fallthrough; default: return -EINVAL; } if (!offset) file->private_data = (void *)ctx->next_offset; return vfs_setpos(file, offset, LONG_MAX); }"
155----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-45012/bad/firmware.c----nvkm_firmware_dtor,"nvkm_firmware_dtor(struct nvkm_firmware *fw) { struct nvkm_memory *memory = &fw->mem.memory; if (!fw->img) return; switch (fw->func->type) { case NVKM_FIRMWARE_IMG_RAM: kfree(fw->img); break; case NVKM_FIRMWARE_IMG_DMA: nvkm_memory_unref(&memory); <S2SV_StartVul> dma_free_coherent(fw->device->dev, sg_dma_len(&fw->mem.sgl), fw->img, fw->phys); <S2SV_EndVul> break; default: WARN_ON(1); break; } fw->img = NULL; }","- dma_free_coherent(fw->device->dev, sg_dma_len(&fw->mem.sgl), fw->img, fw->phys);
+ dma_free_noncoherent(fw->device->dev, sg_dma_len(&fw->mem.sgl),
+ fw->img, fw->phys, DMA_TO_DEVICE);","nvkm_firmware_dtor(struct nvkm_firmware *fw) { struct nvkm_memory *memory = &fw->mem.memory; if (!fw->img) return; switch (fw->func->type) { case NVKM_FIRMWARE_IMG_RAM: kfree(fw->img); break; case NVKM_FIRMWARE_IMG_DMA: nvkm_memory_unref(&memory); dma_free_noncoherent(fw->device->dev, sg_dma_len(&fw->mem.sgl), fw->img, fw->phys, DMA_TO_DEVICE); break; default: WARN_ON(1); break; } fw->img = NULL; }"
1255----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56538/bad/zynqmp_kms.c----zynqmp_dpsub_drm_cleanup,void zynqmp_dpsub_drm_cleanup(struct zynqmp_dpsub *dpsub) { struct drm_device *drm = &dpsub->drm->dev; <S2SV_StartVul> drm_dev_unregister(drm); <S2SV_EndVul> drm_atomic_helper_shutdown(drm); drm_encoder_cleanup(&dpsub->drm->encoder); drm_kms_helper_poll_fini(drm); },"- drm_dev_unregister(drm);
+ drm_dev_unplug(drm);",void zynqmp_dpsub_drm_cleanup(struct zynqmp_dpsub *dpsub) { struct drm_device *drm = &dpsub->drm->dev; drm_dev_unplug(drm); drm_atomic_helper_shutdown(drm); drm_encoder_cleanup(&dpsub->drm->encoder); drm_kms_helper_poll_fini(drm); }
200----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46683/bad/xe_preempt_fence.c----xe_preempt_fence_arm,"xe_preempt_fence_arm(struct xe_preempt_fence *pfence, struct xe_exec_queue *q, u64 context, u32 seqno) { list_del_init(&pfence->link); pfence->q = xe_exec_queue_get(q); dma_fence_init(&pfence->base, &preempt_fence_ops, <S2SV_StartVul> &q->lr.lock, context, seqno); <S2SV_EndVul> return &pfence->base; }","- &q->lr.lock, context, seqno);
+ spin_lock_init(&pfence->lock);
+ &pfence->lock, context, seqno);","xe_preempt_fence_arm(struct xe_preempt_fence *pfence, struct xe_exec_queue *q, u64 context, u32 seqno) { list_del_init(&pfence->link); pfence->q = xe_exec_queue_get(q); spin_lock_init(&pfence->lock); dma_fence_init(&pfence->base, &preempt_fence_ops, &pfence->lock, context, seqno); return &pfence->base; }"
848----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50106/bad/nfs4state.c----nfsd4_free_stateid,"nfsd4_free_stateid(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate, union nfsd4_op_u *u) { struct nfsd4_free_stateid *free_stateid = &u->free_stateid; stateid_t *stateid = &free_stateid->fr_stateid; struct nfs4_stid *s; struct nfs4_delegation *dp; struct nfs4_client *cl = cstate->clp; __be32 ret = nfserr_bad_stateid; spin_lock(&cl->cl_lock); s = find_stateid_locked(cl, stateid); if (!s || s->sc_status & SC_STATUS_CLOSED) goto out_unlock; if (s->sc_status & SC_STATUS_ADMIN_REVOKED) { nfsd4_drop_revoked_stid(s); ret = nfs_ok; goto out; } spin_lock(&s->sc_lock); switch (s->sc_type) { case SC_TYPE_DELEG: if (s->sc_status & SC_STATUS_REVOKED) { s->sc_status |= SC_STATUS_CLOSED; spin_unlock(&s->sc_lock); dp = delegstateid(s); <S2SV_StartVul> list_del_init(&dp->dl_recall_lru); <S2SV_EndVul> spin_unlock(&cl->cl_lock); nfs4_put_stid(s); ret = nfs_ok; goto out; } ret = nfserr_locks_held; break; case SC_TYPE_OPEN: ret = check_stateid_generation(stateid, &s->sc_stateid, 1); if (ret) break; ret = nfserr_locks_held; break; case SC_TYPE_LOCK: spin_unlock(&s->sc_lock); refcount_inc(&s->sc_count); spin_unlock(&cl->cl_lock); ret = nfsd4_free_lock_stateid(stateid, s); goto out; } spin_unlock(&s->sc_lock); out_unlock: spin_unlock(&cl->cl_lock); out: return ret; }","- list_del_init(&dp->dl_recall_lru);
+ if (s->sc_status & SC_STATUS_FREEABLE)
+ list_del_init(&dp->dl_recall_lru);
+ s->sc_status |= SC_STATUS_FREED;","nfsd4_free_stateid(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate, union nfsd4_op_u *u) { struct nfsd4_free_stateid *free_stateid = &u->free_stateid; stateid_t *stateid = &free_stateid->fr_stateid; struct nfs4_stid *s; struct nfs4_delegation *dp; struct nfs4_client *cl = cstate->clp; __be32 ret = nfserr_bad_stateid; spin_lock(&cl->cl_lock); s = find_stateid_locked(cl, stateid); if (!s || s->sc_status & SC_STATUS_CLOSED) goto out_unlock; if (s->sc_status & SC_STATUS_ADMIN_REVOKED) { nfsd4_drop_revoked_stid(s); ret = nfs_ok; goto out; } spin_lock(&s->sc_lock); switch (s->sc_type) { case SC_TYPE_DELEG: if (s->sc_status & SC_STATUS_REVOKED) { s->sc_status |= SC_STATUS_CLOSED; spin_unlock(&s->sc_lock); dp = delegstateid(s); if (s->sc_status & SC_STATUS_FREEABLE) list_del_init(&dp->dl_recall_lru); s->sc_status |= SC_STATUS_FREED; spin_unlock(&cl->cl_lock); nfs4_put_stid(s); ret = nfs_ok; goto out; } ret = nfserr_locks_held; break; case SC_TYPE_OPEN: ret = check_stateid_generation(stateid, &s->sc_stateid, 1); if (ret) break; ret = nfserr_locks_held; break; case SC_TYPE_LOCK: spin_unlock(&s->sc_lock); refcount_inc(&s->sc_count); spin_unlock(&cl->cl_lock); ret = nfsd4_free_lock_stateid(stateid, s); goto out; } spin_unlock(&s->sc_lock); out_unlock: spin_unlock(&cl->cl_lock); out: return ret; }"
910----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50155/bad/dev.c----nsim_dev_traps_init,"static int nsim_dev_traps_init(struct devlink *devlink) { size_t policers_count = ARRAY_SIZE(nsim_trap_policers_arr); struct nsim_dev *nsim_dev = devlink_priv(devlink); struct nsim_trap_data *nsim_trap_data; int err; nsim_trap_data = kzalloc(sizeof(*nsim_trap_data), GFP_KERNEL); if (!nsim_trap_data) return -ENOMEM; nsim_trap_data->trap_items_arr = kcalloc(ARRAY_SIZE(nsim_traps_arr), sizeof(struct nsim_trap_item), GFP_KERNEL); if (!nsim_trap_data->trap_items_arr) { err = -ENOMEM; goto err_trap_data_free; } nsim_trap_data->trap_policers_cnt_arr = kcalloc(policers_count, sizeof(u64), GFP_KERNEL); if (!nsim_trap_data->trap_policers_cnt_arr) { err = -ENOMEM; goto err_trap_items_free; } spin_lock_init(&nsim_trap_data->trap_lock); nsim_trap_data->nsim_dev = nsim_dev; nsim_dev->trap_data = nsim_trap_data; err = devl_trap_policers_register(devlink, nsim_trap_policers_arr, policers_count); if (err) goto err_trap_policers_cnt_free; err = devl_trap_groups_register(devlink, nsim_trap_groups_arr, ARRAY_SIZE(nsim_trap_groups_arr)); if (err) goto err_trap_policers_unregister; err = devl_traps_register(devlink, nsim_traps_arr, ARRAY_SIZE(nsim_traps_arr), NULL); if (err) goto err_trap_groups_unregister; INIT_DELAYED_WORK(&nsim_dev->trap_data->trap_report_dw, nsim_dev_trap_report_work); <S2SV_StartVul> schedule_delayed_work(&nsim_dev->trap_data->trap_report_dw, <S2SV_EndVul> <S2SV_StartVul> msecs_to_jiffies(NSIM_TRAP_REPORT_INTERVAL_MS)); <S2SV_EndVul> return 0; err_trap_groups_unregister: devl_trap_groups_unregister(devlink, nsim_trap_groups_arr, ARRAY_SIZE(nsim_trap_groups_arr)); err_trap_policers_unregister: devl_trap_policers_unregister(devlink, nsim_trap_policers_arr, ARRAY_SIZE(nsim_trap_policers_arr)); err_trap_policers_cnt_free: kfree(nsim_trap_data->trap_policers_cnt_arr); err_trap_items_free: kfree(nsim_trap_data->trap_items_arr); err_trap_data_free: kfree(nsim_trap_data); return err; }","- schedule_delayed_work(&nsim_dev->trap_data->trap_report_dw,
- msecs_to_jiffies(NSIM_TRAP_REPORT_INTERVAL_MS));
+ queue_delayed_work(system_unbound_wq,
+ &nsim_dev->trap_data->trap_report_dw,
+ msecs_to_jiffies(NSIM_TRAP_REPORT_INTERVAL_MS));","static int nsim_dev_traps_init(struct devlink *devlink) { size_t policers_count = ARRAY_SIZE(nsim_trap_policers_arr); struct nsim_dev *nsim_dev = devlink_priv(devlink); struct nsim_trap_data *nsim_trap_data; int err; nsim_trap_data = kzalloc(sizeof(*nsim_trap_data), GFP_KERNEL); if (!nsim_trap_data) return -ENOMEM; nsim_trap_data->trap_items_arr = kcalloc(ARRAY_SIZE(nsim_traps_arr), sizeof(struct nsim_trap_item), GFP_KERNEL); if (!nsim_trap_data->trap_items_arr) { err = -ENOMEM; goto err_trap_data_free; } nsim_trap_data->trap_policers_cnt_arr = kcalloc(policers_count, sizeof(u64), GFP_KERNEL); if (!nsim_trap_data->trap_policers_cnt_arr) { err = -ENOMEM; goto err_trap_items_free; } spin_lock_init(&nsim_trap_data->trap_lock); nsim_trap_data->nsim_dev = nsim_dev; nsim_dev->trap_data = nsim_trap_data; err = devl_trap_policers_register(devlink, nsim_trap_policers_arr, policers_count); if (err) goto err_trap_policers_cnt_free; err = devl_trap_groups_register(devlink, nsim_trap_groups_arr, ARRAY_SIZE(nsim_trap_groups_arr)); if (err) goto err_trap_policers_unregister; err = devl_traps_register(devlink, nsim_traps_arr, ARRAY_SIZE(nsim_traps_arr), NULL); if (err) goto err_trap_groups_unregister; INIT_DELAYED_WORK(&nsim_dev->trap_data->trap_report_dw, nsim_dev_trap_report_work); queue_delayed_work(system_unbound_wq, &nsim_dev->trap_data->trap_report_dw, msecs_to_jiffies(NSIM_TRAP_REPORT_INTERVAL_MS)); return 0; err_trap_groups_unregister: devl_trap_groups_unregister(devlink, nsim_trap_groups_arr, ARRAY_SIZE(nsim_trap_groups_arr)); err_trap_policers_unregister: devl_trap_policers_unregister(devlink, nsim_trap_policers_arr, ARRAY_SIZE(nsim_trap_policers_arr)); err_trap_policers_cnt_free: kfree(nsim_trap_data->trap_policers_cnt_arr); err_trap_items_free: kfree(nsim_trap_data->trap_items_arr); err_trap_data_free: kfree(nsim_trap_data); return err; }"
1523----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2025-21681/bad/actions.c----do_output,"static void do_output(struct datapath *dp, struct sk_buff *skb, int out_port, struct sw_flow_key *key) { struct vport *vport = ovs_vport_rcu(dp, out_port); <S2SV_StartVul> if (likely(vport && netif_carrier_ok(vport->dev))) { <S2SV_EndVul> u16 mru = OVS_CB(skb)->mru; u32 cutlen = OVS_CB(skb)->cutlen; if (unlikely(cutlen > 0)) { if (skb->len - cutlen > ovs_mac_header_len(key)) pskb_trim(skb, skb->len - cutlen); else pskb_trim(skb, ovs_mac_header_len(key)); } skb->pkt_type = PACKET_OUTGOING; if (likely(!mru || (skb->len <= mru + vport->dev->hard_header_len))) { ovs_vport_send(vport, skb, ovs_key_mac_proto(key)); } else if (mru <= vport->dev->mtu) { struct net *net = read_pnet(&dp->net); ovs_fragment(net, vport, skb, mru, key); } else { kfree_skb_reason(skb, SKB_DROP_REASON_PKT_TOO_BIG); } } else { kfree_skb_reason(skb, SKB_DROP_REASON_DEV_READY); } }","- if (likely(vport && netif_carrier_ok(vport->dev))) {
+ if (likely(vport &&
+ netif_running(vport->dev) &&
+ netif_carrier_ok(vport->dev))) {","static void do_output(struct datapath *dp, struct sk_buff *skb, int out_port, struct sw_flow_key *key) { struct vport *vport = ovs_vport_rcu(dp, out_port); if (likely(vport && netif_running(vport->dev) && netif_carrier_ok(vport->dev))) { u16 mru = OVS_CB(skb)->mru; u32 cutlen = OVS_CB(skb)->cutlen; if (unlikely(cutlen > 0)) { if (skb->len - cutlen > ovs_mac_header_len(key)) pskb_trim(skb, skb->len - cutlen); else pskb_trim(skb, ovs_mac_header_len(key)); } skb->pkt_type = PACKET_OUTGOING; if (likely(!mru || (skb->len <= mru + vport->dev->hard_header_len))) { ovs_vport_send(vport, skb, ovs_key_mac_proto(key)); } else if (mru <= vport->dev->mtu) { struct net *net = read_pnet(&dp->net); ovs_fragment(net, vport, skb, mru, key); } else { kfree_skb_reason(skb, SKB_DROP_REASON_PKT_TOO_BIG); } } else { kfree_skb_reason(skb, SKB_DROP_REASON_DEV_READY); } }"
99----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44963/bad/extent-tree.c----walk_up_proc,"static noinline int walk_up_proc(struct btrfs_trans_handle *trans, struct btrfs_root *root, struct btrfs_path *path, struct walk_control *wc) { struct btrfs_fs_info *fs_info = root->fs_info; <S2SV_StartVul> int ret; <S2SV_EndVul> int level = wc->level; struct extent_buffer *eb = path->nodes[level]; u64 parent = 0; if (wc->stage == UPDATE_BACKREF) { ASSERT(wc->shared_level >= level); if (level < wc->shared_level) goto out; ret = find_next_key(path, level + 1, &wc->update_progress); if (ret > 0) wc->update_ref = 0; wc->stage = DROP_REFERENCE; wc->shared_level = -1; path->slots[level] = 0; if (!path->locks[level]) { ASSERT(level > 0); btrfs_tree_lock(eb); path->locks[level] = BTRFS_WRITE_LOCK; ret = btrfs_lookup_extent_info(trans, fs_info, eb->start, level, 1, &wc->refs[level], &wc->flags[level], NULL); if (ret < 0) { btrfs_tree_unlock_rw(eb, path->locks[level]); path->locks[level] = 0; return ret; } if (unlikely(wc->refs[level] == 0)) { btrfs_tree_unlock_rw(eb, path->locks[level]); btrfs_err(fs_info, ""bytenr %llu has 0 references, expect > 0"", eb->start); return -EUCLEAN; } if (wc->refs[level] == 1) { btrfs_tree_unlock_rw(eb, path->locks[level]); path->locks[level] = 0; return 1; } } } ASSERT(path->locks[level] || wc->refs[level] == 1); if (wc->refs[level] == 1) { if (level == 0) { if (wc->flags[level] & BTRFS_BLOCK_FLAG_FULL_BACKREF) ret = btrfs_dec_ref(trans, root, eb, 1); else ret = btrfs_dec_ref(trans, root, eb, 0); if (ret) { btrfs_abort_transaction(trans, ret); return ret; } if (is_fstree(btrfs_root_id(root))) { ret = btrfs_qgroup_trace_leaf_items(trans, eb); if (ret) { btrfs_err_rl(fs_info, ""error %d accounting leaf items, quota is out of sync, rescan required"", ret); } } } if (!path->locks[level]) { btrfs_tree_lock(eb); path->locks[level] = BTRFS_WRITE_LOCK; } btrfs_clear_buffer_dirty(trans, eb); } if (eb == root->node) { if (wc->flags[level] & BTRFS_BLOCK_FLAG_FULL_BACKREF) parent = eb->start; else if (btrfs_root_id(root) != btrfs_header_owner(eb)) goto owner_mismatch; } else { if (wc->flags[level + 1] & BTRFS_BLOCK_FLAG_FULL_BACKREF) parent = path->nodes[level + 1]->start; else if (btrfs_root_id(root) != btrfs_header_owner(path->nodes[level + 1])) goto owner_mismatch; } <S2SV_StartVul> btrfs_free_tree_block(trans, btrfs_root_id(root), eb, parent, <S2SV_EndVul> <S2SV_StartVul> wc->refs[level] == 1); <S2SV_EndVul> out: wc->refs[level] = 0; wc->flags[level] = 0; <S2SV_StartVul> return 0; <S2SV_EndVul> owner_mismatch: btrfs_err_rl(fs_info, ""unexpected tree owner, have %llu expect %llu"", btrfs_header_owner(eb), btrfs_root_id(root)); return -EUCLEAN; }","- int ret;
- btrfs_free_tree_block(trans, btrfs_root_id(root), eb, parent,
- wc->refs[level] == 1);
- return 0;
+ int ret = 0;
+ ret = btrfs_free_tree_block(trans, btrfs_root_id(root), eb, parent,
+ wc->refs[level] == 1);
+ if (ret < 0)
+ btrfs_abort_transaction(trans, ret);
+ return ret;","static noinline int walk_up_proc(struct btrfs_trans_handle *trans, struct btrfs_root *root, struct btrfs_path *path, struct walk_control *wc) { struct btrfs_fs_info *fs_info = root->fs_info; int ret = 0; int level = wc->level; struct extent_buffer *eb = path->nodes[level]; u64 parent = 0; if (wc->stage == UPDATE_BACKREF) { ASSERT(wc->shared_level >= level); if (level < wc->shared_level) goto out; ret = find_next_key(path, level + 1, &wc->update_progress); if (ret > 0) wc->update_ref = 0; wc->stage = DROP_REFERENCE; wc->shared_level = -1; path->slots[level] = 0; if (!path->locks[level]) { ASSERT(level > 0); btrfs_tree_lock(eb); path->locks[level] = BTRFS_WRITE_LOCK; ret = btrfs_lookup_extent_info(trans, fs_info, eb->start, level, 1, &wc->refs[level], &wc->flags[level], NULL); if (ret < 0) { btrfs_tree_unlock_rw(eb, path->locks[level]); path->locks[level] = 0; return ret; } if (unlikely(wc->refs[level] == 0)) { btrfs_tree_unlock_rw(eb, path->locks[level]); btrfs_err(fs_info, ""bytenr %llu has 0 references, expect > 0"", eb->start); return -EUCLEAN; } if (wc->refs[level] == 1) { btrfs_tree_unlock_rw(eb, path->locks[level]); path->locks[level] = 0; return 1; } } } ASSERT(path->locks[level] || wc->refs[level] == 1); if (wc->refs[level] == 1) { if (level == 0) { if (wc->flags[level] & BTRFS_BLOCK_FLAG_FULL_BACKREF) ret = btrfs_dec_ref(trans, root, eb, 1); else ret = btrfs_dec_ref(trans, root, eb, 0); if (ret) { btrfs_abort_transaction(trans, ret); return ret; } if (is_fstree(btrfs_root_id(root))) { ret = btrfs_qgroup_trace_leaf_items(trans, eb); if (ret) { btrfs_err_rl(fs_info, ""error %d accounting leaf items, quota is out of sync, rescan required"", ret); } } } if (!path->locks[level]) { btrfs_tree_lock(eb); path->locks[level] = BTRFS_WRITE_LOCK; } btrfs_clear_buffer_dirty(trans, eb); } if (eb == root->node) { if (wc->flags[level] & BTRFS_BLOCK_FLAG_FULL_BACKREF) parent = eb->start; else if (btrfs_root_id(root) != btrfs_header_owner(eb)) goto owner_mismatch; } else { if (wc->flags[level + 1] & BTRFS_BLOCK_FLAG_FULL_BACKREF) parent = path->nodes[level + 1]->start; else if (btrfs_root_id(root) != btrfs_header_owner(path->nodes[level + 1])) goto owner_mismatch; } ret = btrfs_free_tree_block(trans, btrfs_root_id(root), eb, parent, wc->refs[level] == 1); if (ret < 0) btrfs_abort_transaction(trans, ret); out: wc->refs[level] = 0; wc->flags[level] = 0; return ret; owner_mismatch: btrfs_err_rl(fs_info, ""unexpected tree owner, have %llu expect %llu"", btrfs_header_owner(eb), btrfs_root_id(root)); return -EUCLEAN; }"
611----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49921/bad/dcn35_hwseq.c----dcn35_calc_blocks_to_gate,"void dcn35_calc_blocks_to_gate(struct dc *dc, struct dc_state *context, struct pg_block_update *update_state) { bool hpo_frl_stream_enc_acquired = false; bool hpo_dp_stream_enc_acquired = false; int i = 0, j = 0; int edp_num = 0; struct dc_link *edp_links[MAX_NUM_EDP] = { NULL }; memset(update_state, 0, sizeof(struct pg_block_update)); for (i = 0; i < dc->res_pool->hpo_dp_stream_enc_count; i++) { if (context->res_ctx.is_hpo_dp_stream_enc_acquired[i] && dc->res_pool->hpo_dp_stream_enc[i]) { hpo_dp_stream_enc_acquired = true; break; } } if (!hpo_frl_stream_enc_acquired && !hpo_dp_stream_enc_acquired) update_state->pg_res_update[PG_HPO] = true; if (hpo_frl_stream_enc_acquired) update_state->pg_pipe_res_update[PG_HDMISTREAM][0] = true; update_state->pg_res_update[PG_DWB] = true; for (i = 0; i < dc->res_pool->pipe_count; i++) { struct pipe_ctx *pipe_ctx = &context->res_ctx.pipe_ctx[i]; for (j = 0; j < PG_HW_PIPE_RESOURCES_NUM_ELEMENT; j++) update_state->pg_pipe_res_update[j][i] = true; if (!pipe_ctx) continue; if (pipe_ctx->plane_res.hubp) update_state->pg_pipe_res_update[PG_HUBP][pipe_ctx->plane_res.hubp->inst] = false; <S2SV_StartVul> if (pipe_ctx->plane_res.dpp) <S2SV_EndVul> update_state->pg_pipe_res_update[PG_DPP][pipe_ctx->plane_res.hubp->inst] = false; if (pipe_ctx->plane_res.dpp || pipe_ctx->stream_res.opp) update_state->pg_pipe_res_update[PG_MPCC][pipe_ctx->plane_res.mpcc_inst] = false; if (pipe_ctx->stream_res.dsc) update_state->pg_pipe_res_update[PG_DSC][pipe_ctx->stream_res.dsc->inst] = false; if (pipe_ctx->stream_res.opp) update_state->pg_pipe_res_update[PG_OPP][pipe_ctx->stream_res.opp->inst] = false; if (pipe_ctx->stream_res.hpo_dp_stream_enc) update_state->pg_pipe_res_update[PG_DPSTREAM][pipe_ctx->stream_res.hpo_dp_stream_enc->inst] = false; } for (i = 0; i < dc->link_count; i++) { update_state->pg_pipe_res_update[PG_PHYSYMCLK][dc->links[i]->link_enc_hw_inst] = true; if (dc->links[i]->type != dc_connection_none) update_state->pg_pipe_res_update[PG_PHYSYMCLK][dc->links[i]->link_enc_hw_inst] = false; } for (i = 0; i < dc->res_pool->timing_generator_count; i++) { struct timing_generator *tg = dc->res_pool->timing_generators[i]; if (tg && tg->funcs->is_tg_enabled(tg)) { update_state->pg_pipe_res_update[PG_OPTC][i] = false; break; } } dc_get_edp_links(dc, edp_links, &edp_num); if (edp_num == 0 || ((!edp_links[0] || !edp_links[0]->edp_sink_present) && (!edp_links[1] || !edp_links[1]->edp_sink_present))) { update_state->pg_pipe_res_update[PG_OPTC][0] = false; } if (dc->caps.sequential_ono) { for (i = dc->res_pool->pipe_count - 1; i >= 0; i--) { if (!update_state->pg_pipe_res_update[PG_HUBP][i] && !update_state->pg_pipe_res_update[PG_DPP][i]) { for (j = i - 1; j >= 0; j--) { update_state->pg_pipe_res_update[PG_HUBP][j] = false; update_state->pg_pipe_res_update[PG_DPP][j] = false; } break; } } } }","- if (pipe_ctx->plane_res.dpp)
+ if (pipe_ctx->plane_res.dpp && pipe_ctx->plane_res.hubp)","void dcn35_calc_blocks_to_gate(struct dc *dc, struct dc_state *context, struct pg_block_update *update_state) { bool hpo_frl_stream_enc_acquired = false; bool hpo_dp_stream_enc_acquired = false; int i = 0, j = 0; int edp_num = 0; struct dc_link *edp_links[MAX_NUM_EDP] = { NULL }; memset(update_state, 0, sizeof(struct pg_block_update)); for (i = 0; i < dc->res_pool->hpo_dp_stream_enc_count; i++) { if (context->res_ctx.is_hpo_dp_stream_enc_acquired[i] && dc->res_pool->hpo_dp_stream_enc[i]) { hpo_dp_stream_enc_acquired = true; break; } } if (!hpo_frl_stream_enc_acquired && !hpo_dp_stream_enc_acquired) update_state->pg_res_update[PG_HPO] = true; if (hpo_frl_stream_enc_acquired) update_state->pg_pipe_res_update[PG_HDMISTREAM][0] = true; update_state->pg_res_update[PG_DWB] = true; for (i = 0; i < dc->res_pool->pipe_count; i++) { struct pipe_ctx *pipe_ctx = &context->res_ctx.pipe_ctx[i]; for (j = 0; j < PG_HW_PIPE_RESOURCES_NUM_ELEMENT; j++) update_state->pg_pipe_res_update[j][i] = true; if (!pipe_ctx) continue; if (pipe_ctx->plane_res.hubp) update_state->pg_pipe_res_update[PG_HUBP][pipe_ctx->plane_res.hubp->inst] = false; if (pipe_ctx->plane_res.dpp && pipe_ctx->plane_res.hubp) update_state->pg_pipe_res_update[PG_DPP][pipe_ctx->plane_res.hubp->inst] = false; if (pipe_ctx->plane_res.dpp || pipe_ctx->stream_res.opp) update_state->pg_pipe_res_update[PG_MPCC][pipe_ctx->plane_res.mpcc_inst] = false; if (pipe_ctx->stream_res.dsc) update_state->pg_pipe_res_update[PG_DSC][pipe_ctx->stream_res.dsc->inst] = false; if (pipe_ctx->stream_res.opp) update_state->pg_pipe_res_update[PG_OPP][pipe_ctx->stream_res.opp->inst] = false; if (pipe_ctx->stream_res.hpo_dp_stream_enc) update_state->pg_pipe_res_update[PG_DPSTREAM][pipe_ctx->stream_res.hpo_dp_stream_enc->inst] = false; } for (i = 0; i < dc->link_count; i++) { update_state->pg_pipe_res_update[PG_PHYSYMCLK][dc->links[i]->link_enc_hw_inst] = true; if (dc->links[i]->type != dc_connection_none) update_state->pg_pipe_res_update[PG_PHYSYMCLK][dc->links[i]->link_enc_hw_inst] = false; } for (i = 0; i < dc->res_pool->timing_generator_count; i++) { struct timing_generator *tg = dc->res_pool->timing_generators[i]; if (tg && tg->funcs->is_tg_enabled(tg)) { update_state->pg_pipe_res_update[PG_OPTC][i] = false; break; } } dc_get_edp_links(dc, edp_links, &edp_num); if (edp_num == 0 || ((!edp_links[0] || !edp_links[0]->edp_sink_present) && (!edp_links[1] || !edp_links[1]->edp_sink_present))) { update_state->pg_pipe_res_update[PG_OPTC][0] = false; } if (dc->caps.sequential_ono) { for (i = dc->res_pool->pipe_count - 1; i >= 0; i--) { if (!update_state->pg_pipe_res_update[PG_HUBP][i] && !update_state->pg_pipe_res_update[PG_DPP][i]) { for (j = i - 1; j >= 0; j--) { update_state->pg_pipe_res_update[PG_HUBP][j] = false; update_state->pg_pipe_res_update[PG_DPP][j] = false; } break; } } } }"
39----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-43907/bad/smu8_hwmgr.c----smu8_apply_state_adjust_rules,"static int smu8_apply_state_adjust_rules(struct pp_hwmgr *hwmgr, struct pp_power_state *prequest_ps, const struct pp_power_state *pcurrent_ps) { <S2SV_StartVul> struct smu8_power_state *smu8_ps = <S2SV_EndVul> <S2SV_StartVul> cast_smu8_power_state(&prequest_ps->hardware); <S2SV_EndVul> <S2SV_StartVul> const struct smu8_power_state *smu8_current_ps = <S2SV_EndVul> <S2SV_StartVul> cast_const_smu8_power_state(&pcurrent_ps->hardware); <S2SV_EndVul> struct smu8_hwmgr *data = hwmgr->backend; struct PP_Clocks clocks = {0, 0, 0, 0}; bool force_high; smu8_ps->need_dfs_bypass = true; data->battery_state = (PP_StateUILabel_Battery == prequest_ps->classification.ui_label); clocks.memoryClock = hwmgr->display_config->min_mem_set_clock != 0 ? hwmgr->display_config->min_mem_set_clock : data->sys_info.nbp_memory_clock[1]; if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_StablePState)) clocks.memoryClock = hwmgr->dyn_state.max_clock_voltage_on_ac.mclk; force_high = (clocks.memoryClock > data->sys_info.nbp_memory_clock[SMU8_NUM_NBPMEMORYCLOCK - 1]) || (hwmgr->display_config->num_display >= 3); smu8_ps->action = smu8_current_ps->action; if (hwmgr->request_dpm_level == AMD_DPM_FORCED_LEVEL_PROFILE_PEAK) smu8_nbdpm_pstate_enable_disable(hwmgr, false, false); else if (hwmgr->request_dpm_level == AMD_DPM_FORCED_LEVEL_PROFILE_STANDARD) smu8_nbdpm_pstate_enable_disable(hwmgr, false, true); else if (!force_high && (smu8_ps->action == FORCE_HIGH)) smu8_ps->action = CANCEL_FORCE_HIGH; else if (force_high && (smu8_ps->action != FORCE_HIGH)) smu8_ps->action = FORCE_HIGH; else smu8_ps->action = DO_NOTHING; return 0; }","- struct smu8_power_state *smu8_ps =
- cast_smu8_power_state(&prequest_ps->hardware);
- const struct smu8_power_state *smu8_current_ps =
- cast_const_smu8_power_state(&pcurrent_ps->hardware);
+ struct smu8_power_state *smu8_ps;
+ const struct smu8_power_state *smu8_current_ps;
+ smu8_ps = cast_smu8_power_state(&prequest_ps->hardware);
+ smu8_current_ps = cast_const_smu8_power_state(&pcurrent_ps->hardware);
+ if (!smu8_ps || !smu8_current_ps)
+ return -EINVAL;","static int smu8_apply_state_adjust_rules(struct pp_hwmgr *hwmgr, struct pp_power_state *prequest_ps, const struct pp_power_state *pcurrent_ps) { struct smu8_power_state *smu8_ps; const struct smu8_power_state *smu8_current_ps; struct smu8_hwmgr *data = hwmgr->backend; struct PP_Clocks clocks = {0, 0, 0, 0}; bool force_high; smu8_ps = cast_smu8_power_state(&prequest_ps->hardware); smu8_current_ps = cast_const_smu8_power_state(&pcurrent_ps->hardware); if (!smu8_ps || !smu8_current_ps) return -EINVAL; smu8_ps->need_dfs_bypass = true; data->battery_state = (PP_StateUILabel_Battery == prequest_ps->classification.ui_label); clocks.memoryClock = hwmgr->display_config->min_mem_set_clock != 0 ? hwmgr->display_config->min_mem_set_clock : data->sys_info.nbp_memory_clock[1]; if (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_StablePState)) clocks.memoryClock = hwmgr->dyn_state.max_clock_voltage_on_ac.mclk; force_high = (clocks.memoryClock > data->sys_info.nbp_memory_clock[SMU8_NUM_NBPMEMORYCLOCK - 1]) || (hwmgr->display_config->num_display >= 3); smu8_ps->action = smu8_current_ps->action; if (hwmgr->request_dpm_level == AMD_DPM_FORCED_LEVEL_PROFILE_PEAK) smu8_nbdpm_pstate_enable_disable(hwmgr, false, false); else if (hwmgr->request_dpm_level == AMD_DPM_FORCED_LEVEL_PROFILE_STANDARD) smu8_nbdpm_pstate_enable_disable(hwmgr, false, true); else if (!force_high && (smu8_ps->action == FORCE_HIGH)) smu8_ps->action = CANCEL_FORCE_HIGH; else if (force_high && (smu8_ps->action != FORCE_HIGH)) smu8_ps->action = FORCE_HIGH; else smu8_ps->action = DO_NOTHING; return 0; }"
1447----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56772/bad/debugfs.c----kunit_debugfs_create_suite,"void kunit_debugfs_create_suite(struct kunit_suite *suite) { struct kunit_case *test_case; struct string_stream *stream; if (suite->log) return; stream = alloc_string_stream(GFP_KERNEL); if (IS_ERR_OR_NULL(stream)) return; string_stream_set_append_newlines(stream, true); suite->log = stream; kunit_suite_for_each_test_case(suite, test_case) { stream = alloc_string_stream(GFP_KERNEL); if (IS_ERR_OR_NULL(stream)) goto err; string_stream_set_append_newlines(stream, true); test_case->log = stream; } suite->debugfs = debugfs_create_dir(suite->name, debugfs_rootdir); debugfs_create_file(KUNIT_DEBUGFS_RESULTS, S_IFREG | 0444, suite->debugfs, suite, &debugfs_results_fops); if (!suite->is_init) { debugfs_create_file(KUNIT_DEBUGFS_RUN, S_IFREG | 0644, suite->debugfs, suite, &debugfs_run_fops); } return; err: string_stream_destroy(suite->log); <S2SV_StartVul> kunit_suite_for_each_test_case(suite, test_case) <S2SV_EndVul> string_stream_destroy(test_case->log); }","- kunit_suite_for_each_test_case(suite, test_case)
+ suite->log = NULL;
+ kunit_suite_for_each_test_case(suite, test_case) {
+ test_case->log = NULL;
+ }
+ }","void kunit_debugfs_create_suite(struct kunit_suite *suite) { struct kunit_case *test_case; struct string_stream *stream; if (suite->log) return; stream = alloc_string_stream(GFP_KERNEL); if (IS_ERR_OR_NULL(stream)) return; string_stream_set_append_newlines(stream, true); suite->log = stream; kunit_suite_for_each_test_case(suite, test_case) { stream = alloc_string_stream(GFP_KERNEL); if (IS_ERR_OR_NULL(stream)) goto err; string_stream_set_append_newlines(stream, true); test_case->log = stream; } suite->debugfs = debugfs_create_dir(suite->name, debugfs_rootdir); debugfs_create_file(KUNIT_DEBUGFS_RESULTS, S_IFREG | 0444, suite->debugfs, suite, &debugfs_results_fops); if (!suite->is_init) { debugfs_create_file(KUNIT_DEBUGFS_RUN, S_IFREG | 0644, suite->debugfs, suite, &debugfs_run_fops); } return; err: string_stream_destroy(suite->log); suite->log = NULL; kunit_suite_for_each_test_case(suite, test_case) { string_stream_destroy(test_case->log); test_case->log = NULL; } }"
545----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49864/bad/local_object.c----rxrpc_open_socket,"static int rxrpc_open_socket(struct rxrpc_local *local, struct net *net) { struct udp_tunnel_sock_cfg tuncfg = {NULL}; struct sockaddr_rxrpc *srx = &local->srx; struct udp_port_cfg udp_conf = {0}; struct task_struct *io_thread; struct sock *usk; int ret; _enter(""%p{%d,%d}"", local, srx->transport_type, srx->transport.family); udp_conf.family = srx->transport.family; udp_conf.use_udp_checksums = true; if (udp_conf.family == AF_INET) { udp_conf.local_ip = srx->transport.sin.sin_addr; udp_conf.local_udp_port = srx->transport.sin.sin_port; #if IS_ENABLED(CONFIG_AF_RXRPC_IPV6) } else { udp_conf.local_ip6 = srx->transport.sin6.sin6_addr; udp_conf.local_udp_port = srx->transport.sin6.sin6_port; udp_conf.use_udp6_tx_checksums = true; udp_conf.use_udp6_rx_checksums = true; #endif } ret = udp_sock_create(net, &udp_conf, &local->socket); if (ret < 0) { _leave("" = %d [socket]"", ret); return ret; } tuncfg.encap_type = UDP_ENCAP_RXRPC; tuncfg.encap_rcv = rxrpc_encap_rcv; tuncfg.encap_err_rcv = rxrpc_encap_err_rcv; tuncfg.sk_user_data = local; setup_udp_tunnel_sock(net, local->socket, &tuncfg); usk = local->socket->sk; usk->sk_error_report = rxrpc_error_report; switch (srx->transport.family) { case AF_INET6: ip6_sock_set_recverr(usk); fallthrough; case AF_INET: ip_sock_set_recverr(usk); rxrpc_local_dont_fragment(local, true); sock_enable_timestamps(usk); break; default: BUG(); } io_thread = kthread_run(rxrpc_io_thread, local, ""krxrpcio/%u"", ntohs(udp_conf.local_udp_port)); if (IS_ERR(io_thread)) { ret = PTR_ERR(io_thread); goto error_sock; } wait_for_completion(&local->io_thread_ready); <S2SV_StartVul> local->io_thread = io_thread; <S2SV_EndVul> _leave("" = 0""); return 0; error_sock: kernel_sock_shutdown(local->socket, SHUT_RDWR); local->socket->sk->sk_user_data = NULL; sock_release(local->socket); local->socket = NULL; return ret; }","- local->io_thread = io_thread;
+ WRITE_ONCE(local->io_thread, io_thread);","static int rxrpc_open_socket(struct rxrpc_local *local, struct net *net) { struct udp_tunnel_sock_cfg tuncfg = {NULL}; struct sockaddr_rxrpc *srx = &local->srx; struct udp_port_cfg udp_conf = {0}; struct task_struct *io_thread; struct sock *usk; int ret; _enter(""%p{%d,%d}"", local, srx->transport_type, srx->transport.family); udp_conf.family = srx->transport.family; udp_conf.use_udp_checksums = true; if (udp_conf.family == AF_INET) { udp_conf.local_ip = srx->transport.sin.sin_addr; udp_conf.local_udp_port = srx->transport.sin.sin_port; #if IS_ENABLED(CONFIG_AF_RXRPC_IPV6) } else { udp_conf.local_ip6 = srx->transport.sin6.sin6_addr; udp_conf.local_udp_port = srx->transport.sin6.sin6_port; udp_conf.use_udp6_tx_checksums = true; udp_conf.use_udp6_rx_checksums = true; #endif } ret = udp_sock_create(net, &udp_conf, &local->socket); if (ret < 0) { _leave("" = %d [socket]"", ret); return ret; } tuncfg.encap_type = UDP_ENCAP_RXRPC; tuncfg.encap_rcv = rxrpc_encap_rcv; tuncfg.encap_err_rcv = rxrpc_encap_err_rcv; tuncfg.sk_user_data = local; setup_udp_tunnel_sock(net, local->socket, &tuncfg); usk = local->socket->sk; usk->sk_error_report = rxrpc_error_report; switch (srx->transport.family) { case AF_INET6: ip6_sock_set_recverr(usk); fallthrough; case AF_INET: ip_sock_set_recverr(usk); rxrpc_local_dont_fragment(local, true); sock_enable_timestamps(usk); break; default: BUG(); } io_thread = kthread_run(rxrpc_io_thread, local, ""krxrpcio/%u"", ntohs(udp_conf.local_udp_port)); if (IS_ERR(io_thread)) { ret = PTR_ERR(io_thread); goto error_sock; } wait_for_completion(&local->io_thread_ready); WRITE_ONCE(local->io_thread, io_thread); _leave("" = 0""); return 0; error_sock: kernel_sock_shutdown(local->socket, SHUT_RDWR); local->socket->sk->sk_user_data = NULL; sock_release(local->socket); local->socket = NULL; return ret; }"
605----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49921/bad/dce110_clk_mgr.c----dce11_pplib_apply_display_requirements,"void dce11_pplib_apply_display_requirements( struct dc *dc, struct dc_state *context) { struct dm_pp_display_configuration *pp_display_cfg = &context->pp_display_cfg; int memory_type_multiplier = MEMORY_TYPE_MULTIPLIER_CZ; if (dc->bw_vbios && dc->bw_vbios->memory_type == bw_def_hbm) memory_type_multiplier = MEMORY_TYPE_HBM; pp_display_cfg->all_displays_in_sync = context->bw_ctx.bw.dce.all_displays_in_sync; pp_display_cfg->nb_pstate_switch_disable = context->bw_ctx.bw.dce.nbp_state_change_enable == false; pp_display_cfg->cpu_cc6_disable = context->bw_ctx.bw.dce.cpuc_state_change_enable == false; pp_display_cfg->cpu_pstate_disable = context->bw_ctx.bw.dce.cpup_state_change_enable == false; pp_display_cfg->cpu_pstate_separation_time = context->bw_ctx.bw.dce.blackout_recovery_time_us; <S2SV_StartVul> if ((dc->ctx->asic_id.chip_family == FAMILY_AI) && <S2SV_EndVul> ASICREV_IS_VEGA20_P(dc->ctx->asic_id.hw_internal_rev) && (context->stream_count >= 2)) { pp_display_cfg->min_memory_clock_khz = max(pp_display_cfg->min_memory_clock_khz, (uint32_t) div64_s64( div64_s64(dc->bw_vbios->high_yclk.value, memory_type_multiplier), 10000)); } else { pp_display_cfg->min_memory_clock_khz = context->bw_ctx.bw.dce.yclk_khz / memory_type_multiplier; } pp_display_cfg->min_engine_clock_khz = determine_sclk_from_bounding_box( dc, context->bw_ctx.bw.dce.sclk_khz); pp_display_cfg->min_dcfclock_khz = (context->stream_count > 4) ? pp_display_cfg->min_engine_clock_khz : 0; pp_display_cfg->min_engine_clock_deep_sleep_khz = context->bw_ctx.bw.dce.sclk_deep_sleep_khz; pp_display_cfg->avail_mclk_switch_time_us = dce110_get_min_vblank_time_us(context); pp_display_cfg->avail_mclk_switch_time_in_disp_active_us = 0; pp_display_cfg->disp_clk_khz = dc->clk_mgr->clks.dispclk_khz; dce110_fill_display_configs(context, pp_display_cfg); if (pp_display_cfg->display_count == 1) { const struct dc_crtc_timing *timing = &context->streams[0]->timing; pp_display_cfg->crtc_index = pp_display_cfg->disp_configs[0].pipe_idx; pp_display_cfg->line_time_in_us = timing->h_total * 10000 / timing->pix_clk_100hz; } if (memcmp(&dc->current_state->pp_display_cfg, pp_display_cfg, sizeof(*pp_display_cfg)) != 0) dm_pp_apply_display_requirements(dc->ctx, pp_display_cfg); }","- if ((dc->ctx->asic_id.chip_family == FAMILY_AI) &&
+ if (dc->bw_vbios && (dc->ctx->asic_id.chip_family == FAMILY_AI) &&","void dce11_pplib_apply_display_requirements( struct dc *dc, struct dc_state *context) { struct dm_pp_display_configuration *pp_display_cfg = &context->pp_display_cfg; int memory_type_multiplier = MEMORY_TYPE_MULTIPLIER_CZ; if (dc->bw_vbios && dc->bw_vbios->memory_type == bw_def_hbm) memory_type_multiplier = MEMORY_TYPE_HBM; pp_display_cfg->all_displays_in_sync = context->bw_ctx.bw.dce.all_displays_in_sync; pp_display_cfg->nb_pstate_switch_disable = context->bw_ctx.bw.dce.nbp_state_change_enable == false; pp_display_cfg->cpu_cc6_disable = context->bw_ctx.bw.dce.cpuc_state_change_enable == false; pp_display_cfg->cpu_pstate_disable = context->bw_ctx.bw.dce.cpup_state_change_enable == false; pp_display_cfg->cpu_pstate_separation_time = context->bw_ctx.bw.dce.blackout_recovery_time_us; if (dc->bw_vbios && (dc->ctx->asic_id.chip_family == FAMILY_AI) && ASICREV_IS_VEGA20_P(dc->ctx->asic_id.hw_internal_rev) && (context->stream_count >= 2)) { pp_display_cfg->min_memory_clock_khz = max(pp_display_cfg->min_memory_clock_khz, (uint32_t) div64_s64( div64_s64(dc->bw_vbios->high_yclk.value, memory_type_multiplier), 10000)); } else { pp_display_cfg->min_memory_clock_khz = context->bw_ctx.bw.dce.yclk_khz / memory_type_multiplier; } pp_display_cfg->min_engine_clock_khz = determine_sclk_from_bounding_box( dc, context->bw_ctx.bw.dce.sclk_khz); pp_display_cfg->min_dcfclock_khz = (context->stream_count > 4) ? pp_display_cfg->min_engine_clock_khz : 0; pp_display_cfg->min_engine_clock_deep_sleep_khz = context->bw_ctx.bw.dce.sclk_deep_sleep_khz; pp_display_cfg->avail_mclk_switch_time_us = dce110_get_min_vblank_time_us(context); pp_display_cfg->avail_mclk_switch_time_in_disp_active_us = 0; pp_display_cfg->disp_clk_khz = dc->clk_mgr->clks.dispclk_khz; dce110_fill_display_configs(context, pp_display_cfg); if (pp_display_cfg->display_count == 1) { const struct dc_crtc_timing *timing = &context->streams[0]->timing; pp_display_cfg->crtc_index = pp_display_cfg->disp_configs[0].pipe_idx; pp_display_cfg->line_time_in_us = timing->h_total * 10000 / timing->pix_clk_100hz; } if (memcmp(&dc->current_state->pp_display_cfg, pp_display_cfg, sizeof(*pp_display_cfg)) != 0) dm_pp_apply_display_requirements(dc->ctx, pp_display_cfg); }"
1073----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53057/bad/sch_api.c----qdisc_tree_reduce_backlog,"void qdisc_tree_reduce_backlog(struct Qdisc *sch, int n, int len) { bool qdisc_is_offloaded = sch->flags & TCQ_F_OFFLOADED; const struct Qdisc_class_ops *cops; unsigned long cl; u32 parentid; bool notify; int drops; if (n == 0 && len == 0) return; drops = max_t(int, n, 0); rcu_read_lock(); while ((parentid = sch->parent)) { <S2SV_StartVul> if (TC_H_MAJ(parentid) == TC_H_MAJ(TC_H_INGRESS)) <S2SV_EndVul> break; if (sch->flags & TCQ_F_NOPARENT) break; notify = !sch->q.qlen && !WARN_ON_ONCE(!n && !qdisc_is_offloaded); sch = qdisc_lookup(qdisc_dev(sch), TC_H_MAJ(parentid)); if (sch == NULL) { WARN_ON_ONCE(parentid != TC_H_ROOT); break; } cops = sch->ops->cl_ops; if (notify && cops->qlen_notify) { cl = cops->find(sch, parentid); cops->qlen_notify(sch, cl); } sch->q.qlen -= n; sch->qstats.backlog -= len; __qdisc_qstats_drop(sch, drops); } rcu_read_unlock(); }","- if (TC_H_MAJ(parentid) == TC_H_MAJ(TC_H_INGRESS))
+ if (parentid == TC_H_ROOT)","void qdisc_tree_reduce_backlog(struct Qdisc *sch, int n, int len) { bool qdisc_is_offloaded = sch->flags & TCQ_F_OFFLOADED; const struct Qdisc_class_ops *cops; unsigned long cl; u32 parentid; bool notify; int drops; if (n == 0 && len == 0) return; drops = max_t(int, n, 0); rcu_read_lock(); while ((parentid = sch->parent)) { if (parentid == TC_H_ROOT) break; if (sch->flags & TCQ_F_NOPARENT) break; notify = !sch->q.qlen && !WARN_ON_ONCE(!n && !qdisc_is_offloaded); sch = qdisc_lookup(qdisc_dev(sch), TC_H_MAJ(parentid)); if (sch == NULL) { WARN_ON_ONCE(parentid != TC_H_ROOT); break; } cops = sch->ops->cl_ops; if (notify && cops->qlen_notify) { cl = cops->find(sch, parentid); cops->qlen_notify(sch, cl); } sch->q.qlen -= n; sch->qstats.backlog -= len; __qdisc_qstats_drop(sch, drops); } rcu_read_unlock(); }"
97----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44963/bad/ctree.c----balance_level,"static noinline int balance_level(struct btrfs_trans_handle *trans, struct btrfs_root *root, struct btrfs_path *path, int level) { struct btrfs_fs_info *fs_info = root->fs_info; struct extent_buffer *right = NULL; struct extent_buffer *mid; struct extent_buffer *left = NULL; struct extent_buffer *parent = NULL; int ret = 0; int wret; int pslot; int orig_slot = path->slots[level]; u64 orig_ptr; ASSERT(level > 0); mid = path->nodes[level]; WARN_ON(path->locks[level] != BTRFS_WRITE_LOCK); WARN_ON(btrfs_header_generation(mid) != trans->transid); orig_ptr = btrfs_node_blockptr(mid, orig_slot); if (level < BTRFS_MAX_LEVEL - 1) { parent = path->nodes[level + 1]; pslot = path->slots[level + 1]; } if (!parent) { struct extent_buffer *child; if (btrfs_header_nritems(mid) != 1) return 0; child = btrfs_read_node_slot(mid, 0); if (IS_ERR(child)) { ret = PTR_ERR(child); goto out; } btrfs_tree_lock(child); ret = btrfs_cow_block(trans, root, child, mid, 0, &child, BTRFS_NESTING_COW); if (ret) { btrfs_tree_unlock(child); free_extent_buffer(child); goto out; } ret = btrfs_tree_mod_log_insert_root(root->node, child, true); if (ret < 0) { btrfs_tree_unlock(child); free_extent_buffer(child); btrfs_abort_transaction(trans, ret); goto out; } rcu_assign_pointer(root->node, child); add_root_to_dirty_list(root); btrfs_tree_unlock(child); path->locks[level] = 0; path->nodes[level] = NULL; btrfs_clear_buffer_dirty(trans, mid); btrfs_tree_unlock(mid); free_extent_buffer(mid); root_sub_used_bytes(root); <S2SV_StartVul> btrfs_free_tree_block(trans, btrfs_root_id(root), mid, 0, 1); <S2SV_EndVul> free_extent_buffer_stale(mid); <S2SV_StartVul> return 0; <S2SV_EndVul> } if (btrfs_header_nritems(mid) > BTRFS_NODEPTRS_PER_BLOCK(fs_info) / 4) return 0; if (pslot) { left = btrfs_read_node_slot(parent, pslot - 1); if (IS_ERR(left)) { ret = PTR_ERR(left); left = NULL; goto out; } btrfs_tree_lock_nested(left, BTRFS_NESTING_LEFT); wret = btrfs_cow_block(trans, root, left, parent, pslot - 1, &left, BTRFS_NESTING_LEFT_COW); if (wret) { ret = wret; goto out; } } if (pslot + 1 < btrfs_header_nritems(parent)) { right = btrfs_read_node_slot(parent, pslot + 1); if (IS_ERR(right)) { ret = PTR_ERR(right); right = NULL; goto out; } btrfs_tree_lock_nested(right, BTRFS_NESTING_RIGHT); wret = btrfs_cow_block(trans, root, right, parent, pslot + 1, &right, BTRFS_NESTING_RIGHT_COW); if (wret) { ret = wret; goto out; } } if (left) { orig_slot += btrfs_header_nritems(left); wret = push_node_left(trans, left, mid, 1); if (wret < 0) ret = wret; } if (right) { wret = push_node_left(trans, mid, right, 1); if (wret < 0 && wret != -ENOSPC) ret = wret; if (btrfs_header_nritems(right) == 0) { btrfs_clear_buffer_dirty(trans, right); btrfs_tree_unlock(right); ret = btrfs_del_ptr(trans, root, path, level + 1, pslot + 1); if (ret < 0) { free_extent_buffer_stale(right); right = NULL; goto out; } root_sub_used_bytes(root); <S2SV_StartVul> btrfs_free_tree_block(trans, btrfs_root_id(root), right, <S2SV_EndVul> <S2SV_StartVul> 0, 1); <S2SV_EndVul> free_extent_buffer_stale(right); right = NULL; } else { struct btrfs_disk_key right_key; btrfs_node_key(right, &right_key, 0); ret = btrfs_tree_mod_log_insert_key(parent, pslot + 1, BTRFS_MOD_LOG_KEY_REPLACE); if (ret < 0) { btrfs_abort_transaction(trans, ret); goto out; } btrfs_set_node_key(parent, &right_key, pslot + 1); btrfs_mark_buffer_dirty(trans, parent); } } if (btrfs_header_nritems(mid) == 1) { if (unlikely(!left)) { btrfs_crit(fs_info, ""missing left child when middle child only has 1 item, parent bytenr %llu level %d mid bytenr %llu root %llu"", parent->start, btrfs_header_level(parent), mid->start, btrfs_root_id(root)); ret = -EUCLEAN; btrfs_abort_transaction(trans, ret); goto out; } wret = balance_node_right(trans, mid, left); if (wret < 0) { ret = wret; goto out; } if (wret == 1) { wret = push_node_left(trans, left, mid, 1); if (wret < 0) ret = wret; } BUG_ON(wret == 1); } if (btrfs_header_nritems(mid) == 0) { btrfs_clear_buffer_dirty(trans, mid); btrfs_tree_unlock(mid); ret = btrfs_del_ptr(trans, root, path, level + 1, pslot); if (ret < 0) { free_extent_buffer_stale(mid); mid = NULL; goto out; } root_sub_used_bytes(root); <S2SV_StartVul> btrfs_free_tree_block(trans, btrfs_root_id(root), mid, 0, 1); <S2SV_EndVul> free_extent_buffer_stale(mid); mid = NULL; } else { struct btrfs_disk_key mid_key; btrfs_node_key(mid, &mid_key, 0); ret = btrfs_tree_mod_log_insert_key(parent, pslot, BTRFS_MOD_LOG_KEY_REPLACE); if (ret < 0) { btrfs_abort_transaction(trans, ret); goto out; } btrfs_set_node_key(parent, &mid_key, pslot); btrfs_mark_buffer_dirty(trans, parent); } if (left) { if (btrfs_header_nritems(left) > orig_slot) { atomic_inc(&left->refs); path->nodes[level] = left; path->slots[level + 1] -= 1; path->slots[level] = orig_slot; if (mid) { btrfs_tree_unlock(mid); free_extent_buffer(mid); } } else { orig_slot -= btrfs_header_nritems(left); path->slots[level] = orig_slot; } } if (orig_ptr != btrfs_node_blockptr(path->nodes[level], path->slots[level])) BUG(); out: if (right) { btrfs_tree_unlock(right); free_extent_buffer(right); } if (left) { if (path->nodes[level] != left) btrfs_tree_unlock(left); free_extent_buffer(left); } return ret; }","- btrfs_free_tree_block(trans, btrfs_root_id(root), mid, 0, 1);
- return 0;
- btrfs_free_tree_block(trans, btrfs_root_id(root), right,
- 0, 1);
- btrfs_free_tree_block(trans, btrfs_root_id(root), mid, 0, 1);
+ ret = btrfs_free_tree_block(trans, btrfs_root_id(root), mid, 0, 1);
+ if (ret < 0) {
+ btrfs_abort_transaction(trans, ret);
+ goto out;
+ }
+ }
+ }
+ ret = btrfs_free_tree_block(trans, btrfs_root_id(root),
+ right, 0, 1);
+ if (ret < 0) {
+ btrfs_abort_transaction(trans, ret);
+ goto out;
+ }
+ }
+ ret = btrfs_free_tree_block(trans, btrfs_root_id(root), mid, 0, 1);
+ if (ret < 0) {
+ btrfs_abort_transaction(trans, ret);
+ goto out;
+ }","static noinline int balance_level(struct btrfs_trans_handle *trans, struct btrfs_root *root, struct btrfs_path *path, int level) { struct btrfs_fs_info *fs_info = root->fs_info; struct extent_buffer *right = NULL; struct extent_buffer *mid; struct extent_buffer *left = NULL; struct extent_buffer *parent = NULL; int ret = 0; int wret; int pslot; int orig_slot = path->slots[level]; u64 orig_ptr; ASSERT(level > 0); mid = path->nodes[level]; WARN_ON(path->locks[level] != BTRFS_WRITE_LOCK); WARN_ON(btrfs_header_generation(mid) != trans->transid); orig_ptr = btrfs_node_blockptr(mid, orig_slot); if (level < BTRFS_MAX_LEVEL - 1) { parent = path->nodes[level + 1]; pslot = path->slots[level + 1]; } if (!parent) { struct extent_buffer *child; if (btrfs_header_nritems(mid) != 1) return 0; child = btrfs_read_node_slot(mid, 0); if (IS_ERR(child)) { ret = PTR_ERR(child); goto out; } btrfs_tree_lock(child); ret = btrfs_cow_block(trans, root, child, mid, 0, &child, BTRFS_NESTING_COW); if (ret) { btrfs_tree_unlock(child); free_extent_buffer(child); goto out; } ret = btrfs_tree_mod_log_insert_root(root->node, child, true); if (ret < 0) { btrfs_tree_unlock(child); free_extent_buffer(child); btrfs_abort_transaction(trans, ret); goto out; } rcu_assign_pointer(root->node, child); add_root_to_dirty_list(root); btrfs_tree_unlock(child); path->locks[level] = 0; path->nodes[level] = NULL; btrfs_clear_buffer_dirty(trans, mid); btrfs_tree_unlock(mid); free_extent_buffer(mid); root_sub_used_bytes(root); ret = btrfs_free_tree_block(trans, btrfs_root_id(root), mid, 0, 1); free_extent_buffer_stale(mid); if (ret < 0) { btrfs_abort_transaction(trans, ret); goto out; } return 0; } if (btrfs_header_nritems(mid) > BTRFS_NODEPTRS_PER_BLOCK(fs_info) / 4) return 0; if (pslot) { left = btrfs_read_node_slot(parent, pslot - 1); if (IS_ERR(left)) { ret = PTR_ERR(left); left = NULL; goto out; } btrfs_tree_lock_nested(left, BTRFS_NESTING_LEFT); wret = btrfs_cow_block(trans, root, left, parent, pslot - 1, &left, BTRFS_NESTING_LEFT_COW); if (wret) { ret = wret; goto out; } } if (pslot + 1 < btrfs_header_nritems(parent)) { right = btrfs_read_node_slot(parent, pslot + 1); if (IS_ERR(right)) { ret = PTR_ERR(right); right = NULL; goto out; } btrfs_tree_lock_nested(right, BTRFS_NESTING_RIGHT); wret = btrfs_cow_block(trans, root, right, parent, pslot + 1, &right, BTRFS_NESTING_RIGHT_COW); if (wret) { ret = wret; goto out; } } if (left) { orig_slot += btrfs_header_nritems(left); wret = push_node_left(trans, left, mid, 1); if (wret < 0) ret = wret; } if (right) { wret = push_node_left(trans, mid, right, 1); if (wret < 0 && wret != -ENOSPC) ret = wret; if (btrfs_header_nritems(right) == 0) { btrfs_clear_buffer_dirty(trans, right); btrfs_tree_unlock(right); ret = btrfs_del_ptr(trans, root, path, level + 1, pslot + 1); if (ret < 0) { free_extent_buffer_stale(right); right = NULL; goto out; } root_sub_used_bytes(root); ret = btrfs_free_tree_block(trans, btrfs_root_id(root), right, 0, 1); free_extent_buffer_stale(right); right = NULL; if (ret < 0) { btrfs_abort_transaction(trans, ret); goto out; } } else { struct btrfs_disk_key right_key; btrfs_node_key(right, &right_key, 0); ret = btrfs_tree_mod_log_insert_key(parent, pslot + 1, BTRFS_MOD_LOG_KEY_REPLACE); if (ret < 0) { btrfs_abort_transaction(trans, ret); goto out; } btrfs_set_node_key(parent, &right_key, pslot + 1); btrfs_mark_buffer_dirty(trans, parent); } } if (btrfs_header_nritems(mid) == 1) { if (unlikely(!left)) { btrfs_crit(fs_info, ""missing left child when middle child only has 1 item, parent bytenr %llu level %d mid bytenr %llu root %llu"", parent->start, btrfs_header_level(parent), mid->start, btrfs_root_id(root)); ret = -EUCLEAN; btrfs_abort_transaction(trans, ret); goto out; } wret = balance_node_right(trans, mid, left); if (wret < 0) { ret = wret; goto out; } if (wret == 1) { wret = push_node_left(trans, left, mid, 1); if (wret < 0) ret = wret; } BUG_ON(wret == 1); } if (btrfs_header_nritems(mid) == 0) { btrfs_clear_buffer_dirty(trans, mid); btrfs_tree_unlock(mid); ret = btrfs_del_ptr(trans, root, path, level + 1, pslot); if (ret < 0) { free_extent_buffer_stale(mid); mid = NULL; goto out; } root_sub_used_bytes(root); ret = btrfs_free_tree_block(trans, btrfs_root_id(root), mid, 0, 1); free_extent_buffer_stale(mid); mid = NULL; if (ret < 0) { btrfs_abort_transaction(trans, ret); goto out; } } else { struct btrfs_disk_key mid_key; btrfs_node_key(mid, &mid_key, 0); ret = btrfs_tree_mod_log_insert_key(parent, pslot, BTRFS_MOD_LOG_KEY_REPLACE); if (ret < 0) { btrfs_abort_transaction(trans, ret); goto out; } btrfs_set_node_key(parent, &mid_key, pslot); btrfs_mark_buffer_dirty(trans, parent); } if (left) { if (btrfs_header_nritems(left) > orig_slot) { atomic_inc(&left->refs); path->nodes[level] = left; path->slots[level + 1] -= 1; path->slots[level] = orig_slot; if (mid) { btrfs_tree_unlock(mid); free_extent_buffer(mid); } } else { orig_slot -= btrfs_header_nritems(left); path->slots[level] = orig_slot; } } if (orig_ptr != btrfs_node_blockptr(path->nodes[level], path->slots[level])) BUG(); out: if (right) { btrfs_tree_unlock(right); free_extent_buffer(right); } if (left) { if (path->nodes[level] != left) btrfs_tree_unlock(left); free_extent_buffer(left); } return ret; }"
473----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47714/bad/mcu.c----mt7996_mcu_sta_bfer_tlv,"mt7996_mcu_sta_bfer_tlv(struct mt7996_dev *dev, struct sk_buff *skb, struct ieee80211_vif *vif, struct ieee80211_sta *sta) { struct mt7996_vif *mvif = (struct mt7996_vif *)vif->drv_priv; struct mt7996_phy *phy = mvif->phy; <S2SV_StartVul> int tx_ant = hweight8(phy->mt76->chainmask) - 1; <S2SV_EndVul> struct sta_rec_bf *bf; struct tlv *tlv; static const u8 matrix[4][4] = { {0, 0, 0, 0}, {1, 1, 0, 0}, {2, 4, 4, 0}, {3, 5, 6, 0} }; bool ebf; if (!(sta->deflink.ht_cap.ht_supported || sta->deflink.he_cap.has_he)) return; ebf = mt7996_is_ebf_supported(phy, vif, sta, false); if (!ebf && !dev->ibf) return; tlv = mt76_connac_mcu_add_tlv(skb, STA_REC_BF, sizeof(*bf)); bf = (struct sta_rec_bf *)tlv; if (sta->deflink.eht_cap.has_eht && ebf) mt7996_mcu_sta_bfer_eht(sta, vif, phy, bf); else if (sta->deflink.he_cap.has_he && ebf) mt7996_mcu_sta_bfer_he(sta, vif, phy, bf); else if (sta->deflink.vht_cap.vht_supported) mt7996_mcu_sta_bfer_vht(sta, phy, bf, ebf); else if (sta->deflink.ht_cap.ht_supported) mt7996_mcu_sta_bfer_ht(sta, phy, bf); else return; bf->bf_cap = ebf ? ebf : dev->ibf << 1; bf->bw = sta->deflink.bandwidth; bf->ibf_dbw = sta->deflink.bandwidth; bf->ibf_nrow = tx_ant; if (!ebf && sta->deflink.bandwidth <= IEEE80211_STA_RX_BW_40 && !bf->ncol) bf->ibf_timeout = 0x48; else bf->ibf_timeout = 0x18; if (ebf && bf->nrow != tx_ant) bf->mem_20m = matrix[tx_ant][bf->ncol]; else bf->mem_20m = matrix[bf->nrow][bf->ncol]; switch (sta->deflink.bandwidth) { case IEEE80211_STA_RX_BW_160: case IEEE80211_STA_RX_BW_80: bf->mem_total = bf->mem_20m * 2; break; case IEEE80211_STA_RX_BW_40: bf->mem_total = bf->mem_20m; break; case IEEE80211_STA_RX_BW_20: default: break; } }","- int tx_ant = hweight8(phy->mt76->chainmask) - 1;
+ int tx_ant = hweight16(phy->mt76->chainmask) - 1;","mt7996_mcu_sta_bfer_tlv(struct mt7996_dev *dev, struct sk_buff *skb, struct ieee80211_vif *vif, struct ieee80211_sta *sta) { struct mt7996_vif *mvif = (struct mt7996_vif *)vif->drv_priv; struct mt7996_phy *phy = mvif->phy; int tx_ant = hweight16(phy->mt76->chainmask) - 1; struct sta_rec_bf *bf; struct tlv *tlv; static const u8 matrix[4][4] = { {0, 0, 0, 0}, {1, 1, 0, 0}, {2, 4, 4, 0}, {3, 5, 6, 0} }; bool ebf; if (!(sta->deflink.ht_cap.ht_supported || sta->deflink.he_cap.has_he)) return; ebf = mt7996_is_ebf_supported(phy, vif, sta, false); if (!ebf && !dev->ibf) return; tlv = mt76_connac_mcu_add_tlv(skb, STA_REC_BF, sizeof(*bf)); bf = (struct sta_rec_bf *)tlv; if (sta->deflink.eht_cap.has_eht && ebf) mt7996_mcu_sta_bfer_eht(sta, vif, phy, bf); else if (sta->deflink.he_cap.has_he && ebf) mt7996_mcu_sta_bfer_he(sta, vif, phy, bf); else if (sta->deflink.vht_cap.vht_supported) mt7996_mcu_sta_bfer_vht(sta, phy, bf, ebf); else if (sta->deflink.ht_cap.ht_supported) mt7996_mcu_sta_bfer_ht(sta, phy, bf); else return; bf->bf_cap = ebf ? ebf : dev->ibf << 1; bf->bw = sta->deflink.bandwidth; bf->ibf_dbw = sta->deflink.bandwidth; bf->ibf_nrow = tx_ant; if (!ebf && sta->deflink.bandwidth <= IEEE80211_STA_RX_BW_40 && !bf->ncol) bf->ibf_timeout = 0x48; else bf->ibf_timeout = 0x18; if (ebf && bf->nrow != tx_ant) bf->mem_20m = matrix[tx_ant][bf->ncol]; else bf->mem_20m = matrix[bf->nrow][bf->ncol]; switch (sta->deflink.bandwidth) { case IEEE80211_STA_RX_BW_160: case IEEE80211_STA_RX_BW_80: bf->mem_total = bf->mem_20m * 2; break; case IEEE80211_STA_RX_BW_40: bf->mem_total = bf->mem_20m; break; case IEEE80211_STA_RX_BW_20: default: break; } }"
826----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50093/bad/processor_thermal_device_pci.c----proc_thermal_pci_remove,"static void proc_thermal_pci_remove(struct pci_dev *pdev) { struct proc_thermal_device *proc_priv = pci_get_drvdata(pdev); struct proc_thermal_pci *pci_info = proc_priv->priv_data; cancel_delayed_work_sync(&pci_info->work); proc_thermal_mmio_write(pci_info, PROC_THERMAL_MMIO_THRES_0, 0); proc_thermal_mmio_write(pci_info, PROC_THERMAL_MMIO_INT_ENABLE_0, 0); if (msi_irq) proc_thermal_free_msi(pdev, pci_info); thermal_zone_device_unregister(pci_info->tzone); proc_thermal_mmio_remove(pdev, pci_info->proc_priv); if (!pci_info->no_legacy) proc_thermal_remove(proc_priv); <S2SV_StartVul> pci_disable_device(pdev); <S2SV_EndVul> }",- pci_disable_device(pdev);,"static void proc_thermal_pci_remove(struct pci_dev *pdev) { struct proc_thermal_device *proc_priv = pci_get_drvdata(pdev); struct proc_thermal_pci *pci_info = proc_priv->priv_data; cancel_delayed_work_sync(&pci_info->work); proc_thermal_mmio_write(pci_info, PROC_THERMAL_MMIO_THRES_0, 0); proc_thermal_mmio_write(pci_info, PROC_THERMAL_MMIO_INT_ENABLE_0, 0); if (msi_irq) proc_thermal_free_msi(pdev, pci_info); thermal_zone_device_unregister(pci_info->tzone); proc_thermal_mmio_remove(pdev, pci_info->proc_priv); if (!pci_info->no_legacy) proc_thermal_remove(proc_priv); }"
466----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47707/bad/route.c----rt6_uncached_list_flush_dev,"static void rt6_uncached_list_flush_dev(struct net *net, struct net_device *dev) { struct net_device *loopback_dev = net->loopback_dev; int cpu; if (dev == loopback_dev) return; for_each_possible_cpu(cpu) { struct uncached_list *ul = per_cpu_ptr(&rt6_uncached_list, cpu); struct rt6_info *rt; spin_lock_bh(&ul->lock); list_for_each_entry(rt, &ul->head, rt6i_uncached) { struct inet6_dev *rt_idev = rt->rt6i_idev; struct net_device *rt_dev = rt->dst.dev; <S2SV_StartVul> if (rt_idev->dev == dev) { <S2SV_EndVul> rt->rt6i_idev = in6_dev_get(loopback_dev); in6_dev_put(rt_idev); } if (rt_dev == dev) { rt->dst.dev = blackhole_netdev; dev_hold(rt->dst.dev); dev_put(rt_dev); } } spin_unlock_bh(&ul->lock); } }","- if (rt_idev->dev == dev) {
+ if (rt_idev && rt_idev->dev == dev) {","static void rt6_uncached_list_flush_dev(struct net *net, struct net_device *dev) { struct net_device *loopback_dev = net->loopback_dev; int cpu; if (dev == loopback_dev) return; for_each_possible_cpu(cpu) { struct uncached_list *ul = per_cpu_ptr(&rt6_uncached_list, cpu); struct rt6_info *rt; spin_lock_bh(&ul->lock); list_for_each_entry(rt, &ul->head, rt6i_uncached) { struct inet6_dev *rt_idev = rt->rt6i_idev; struct net_device *rt_dev = rt->dst.dev; if (rt_idev && rt_idev->dev == dev) { rt->rt6i_idev = in6_dev_get(loopback_dev); in6_dev_put(rt_idev); } if (rt_dev == dev) { rt->dst.dev = blackhole_netdev; dev_hold(rt->dst.dev); dev_put(rt_dev); } } spin_unlock_bh(&ul->lock); } }"
542----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49863/bad/scsi.c----vhost_scsi_get_req,"vhost_scsi_get_req(struct vhost_virtqueue *vq, struct vhost_scsi_ctx *vc, struct vhost_scsi_tpg **tpgp) { int ret = -EIO; if (unlikely(!copy_from_iter_full(vc->req, vc->req_size, &vc->out_iter))) { vq_err(vq, ""Faulted on copy_from_iter_full\n""); } else if (unlikely(*vc->lunp != 1)) { vq_err(vq, ""Illegal virtio-scsi lun: %u\n"", *vc->lunp); <S2SV_StartVul> } else { <S2SV_EndVul> <S2SV_StartVul> struct vhost_scsi_tpg **vs_tpg, *tpg; <S2SV_EndVul> vs_tpg = vhost_vq_get_backend(vq); <S2SV_StartVul> tpg = READ_ONCE(vs_tpg[*vc->target]); <S2SV_EndVul> <S2SV_StartVul> if (unlikely(!tpg)) { <S2SV_EndVul> <S2SV_StartVul> vq_err(vq, ""Target 0x%x does not exist\n"", *vc->target); <S2SV_EndVul> <S2SV_StartVul> } else { <S2SV_EndVul> <S2SV_StartVul> if (tpgp) <S2SV_EndVul> <S2SV_StartVul> *tpgp = tpg; <S2SV_EndVul> <S2SV_StartVul> ret = 0; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> return ret; <S2SV_StartVul> } <S2SV_EndVul>","- } else {
- struct vhost_scsi_tpg **vs_tpg, *tpg;
- tpg = READ_ONCE(vs_tpg[*vc->target]);
- if (unlikely(!tpg)) {
- vq_err(vq, ""Target 0x%x does not exist\n"", *vc->target);
- } else {
- if (tpgp)
- ret = 0;
- }
- }
- }
+ struct vhost_scsi_tpg **vs_tpg, *tpg = NULL;
+ if (vc->target) {
+ vs_tpg = vhost_vq_get_backend(vq);
+ tpg = READ_ONCE(vs_tpg[*vc->target]);
+ if (unlikely(!tpg)) {
+ vq_err(vq, ""Target 0x%x does not exist\n"", *vc->target);
+ goto out;
+ }
+ }
+ if (tpgp)
+ ret = 0;
+ }
+ out:
+ }","vhost_scsi_get_req(struct vhost_virtqueue *vq, struct vhost_scsi_ctx *vc, struct vhost_scsi_tpg **tpgp) { int ret = -EIO; if (unlikely(!copy_from_iter_full(vc->req, vc->req_size, &vc->out_iter))) { vq_err(vq, ""Faulted on copy_from_iter_full\n""); } else if (unlikely(*vc->lunp != 1)) { vq_err(vq, ""Illegal virtio-scsi lun: %u\n"", *vc->lunp); } else { struct vhost_scsi_tpg **vs_tpg, *tpg = NULL; if (vc->target) { vs_tpg = vhost_vq_get_backend(vq); tpg = READ_ONCE(vs_tpg[*vc->target]); if (unlikely(!tpg)) { vq_err(vq, ""Target 0x%x does not exist\n"", *vc->target); goto out; } } if (tpgp) *tpgp = tpg; ret = 0; } out: return ret; }"
190----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46678/bad/bond_main.c----bond_ipsec_del_sa,"static void bond_ipsec_del_sa(struct xfrm_state *xs) { struct net_device *bond_dev = xs->xso.dev; struct net_device *real_dev; struct bond_ipsec *ipsec; struct bonding *bond; struct slave *slave; if (!bond_dev) return; rcu_read_lock(); bond = netdev_priv(bond_dev); <S2SV_StartVul> slave = rcu_dereference(bond->curr_active_slave); <S2SV_EndVul> <S2SV_StartVul> if (!slave) <S2SV_EndVul> <S2SV_StartVul> goto out; <S2SV_EndVul> if (!xs->xso.real_dev) <S2SV_StartVul> goto out; <S2SV_EndVul> <S2SV_StartVul> real_dev = slave->dev; <S2SV_EndVul> WARN_ON(xs->xso.real_dev != real_dev); if (!real_dev->xfrmdev_ops || !real_dev->xfrmdev_ops->xdo_dev_state_delete || netif_is_bond_master(real_dev)) { slave_warn(bond_dev, real_dev, ""%s: no slave xdo_dev_state_delete\n"", __func__); goto out; } real_dev->xfrmdev_ops->xdo_dev_state_delete(xs); out: <S2SV_StartVul> spin_lock_bh(&bond->ipsec_lock); <S2SV_EndVul> list_for_each_entry(ipsec, &bond->ipsec_list, list) { if (ipsec->xs == xs) { list_del(&ipsec->list); kfree(ipsec); break; <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> spin_unlock_bh(&bond->ipsec_lock); <S2SV_EndVul> <S2SV_StartVul> rcu_read_unlock(); <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul>","- slave = rcu_dereference(bond->curr_active_slave);
- if (!slave)
- goto out;
- goto out;
- real_dev = slave->dev;
- spin_lock_bh(&bond->ipsec_lock);
- }
- }
- spin_unlock_bh(&bond->ipsec_lock);
- rcu_read_unlock();
- }
+ netdevice_tracker tracker;
+ real_dev = slave ? slave->dev : NULL;
+ netdev_hold(real_dev, &tracker, GFP_ATOMIC);
+ rcu_read_unlock();
+ goto out;
+ goto out;
+ out:
+ netdev_put(real_dev, &tracker);
+ mutex_lock(&bond->ipsec_lock);
+ mutex_unlock(&bond->ipsec_lock);","static void bond_ipsec_del_sa(struct xfrm_state *xs) { struct net_device *bond_dev = xs->xso.dev; struct net_device *real_dev; netdevice_tracker tracker; struct bond_ipsec *ipsec; struct bonding *bond; struct slave *slave; if (!bond_dev) return; rcu_read_lock(); bond = netdev_priv(bond_dev); slave = rcu_dereference(bond->curr_active_slave); real_dev = slave ? slave->dev : NULL; netdev_hold(real_dev, &tracker, GFP_ATOMIC); rcu_read_unlock(); if (!slave) goto out; if (!xs->xso.real_dev) goto out; WARN_ON(xs->xso.real_dev != real_dev); if (!real_dev->xfrmdev_ops || !real_dev->xfrmdev_ops->xdo_dev_state_delete || netif_is_bond_master(real_dev)) { slave_warn(bond_dev, real_dev, ""%s: no slave xdo_dev_state_delete\n"", __func__); goto out; } real_dev->xfrmdev_ops->xdo_dev_state_delete(xs); out: netdev_put(real_dev, &tracker); mutex_lock(&bond->ipsec_lock); list_for_each_entry(ipsec, &bond->ipsec_list, list) { if (ipsec->xs == xs) { list_del(&ipsec->list); kfree(ipsec); break; } } mutex_unlock(&bond->ipsec_lock); }"
895----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50144/bad/xe_gt_tlb_invalidation.c----xe_gt_tlb_invalidation_ggtt,"int xe_gt_tlb_invalidation_ggtt(struct xe_gt *gt) { struct xe_device *xe = gt_to_xe(gt); if (xe_guc_ct_enabled(&gt->uc.guc.ct) && gt->uc.guc.submission_state.enabled) { struct xe_gt_tlb_invalidation_fence fence; int ret; xe_gt_tlb_invalidation_fence_init(gt, &fence, true); ret = xe_gt_tlb_invalidation_guc(gt, &fence); <S2SV_StartVul> if (ret < 0) { <S2SV_EndVul> <S2SV_StartVul> xe_gt_tlb_invalidation_fence_fini(&fence); <S2SV_EndVul> return ret; <S2SV_StartVul> } <S2SV_EndVul> xe_gt_tlb_invalidation_fence_wait(&fence); } else if (xe_device_uc_enabled(xe) && !xe_device_wedged(xe)) { if (IS_SRIOV_VF(xe)) return 0; xe_gt_WARN_ON(gt, xe_force_wake_get(gt_to_fw(gt), XE_FW_GT)); if (xe->info.platform == XE_PVC || GRAPHICS_VER(xe) >= 20) { xe_mmio_write32(gt, PVC_GUC_TLB_INV_DESC1, PVC_GUC_TLB_INV_DESC1_INVALIDATE); xe_mmio_write32(gt, PVC_GUC_TLB_INV_DESC0, PVC_GUC_TLB_INV_DESC0_VALID); } else { xe_mmio_write32(gt, GUC_TLB_INV_CR, GUC_TLB_INV_CR_INVALIDATE); } xe_force_wake_put(gt_to_fw(gt), XE_FW_GT); } return 0; }","- if (ret < 0) {
- xe_gt_tlb_invalidation_fence_fini(&fence);
- }
+ if (ret)","int xe_gt_tlb_invalidation_ggtt(struct xe_gt *gt) { struct xe_device *xe = gt_to_xe(gt); if (xe_guc_ct_enabled(&gt->uc.guc.ct) && gt->uc.guc.submission_state.enabled) { struct xe_gt_tlb_invalidation_fence fence; int ret; xe_gt_tlb_invalidation_fence_init(gt, &fence, true); ret = xe_gt_tlb_invalidation_guc(gt, &fence); if (ret) return ret; xe_gt_tlb_invalidation_fence_wait(&fence); } else if (xe_device_uc_enabled(xe) && !xe_device_wedged(xe)) { if (IS_SRIOV_VF(xe)) return 0; xe_gt_WARN_ON(gt, xe_force_wake_get(gt_to_fw(gt), XE_FW_GT)); if (xe->info.platform == XE_PVC || GRAPHICS_VER(xe) >= 20) { xe_mmio_write32(gt, PVC_GUC_TLB_INV_DESC1, PVC_GUC_TLB_INV_DESC1_INVALIDATE); xe_mmio_write32(gt, PVC_GUC_TLB_INV_DESC0, PVC_GUC_TLB_INV_DESC0_VALID); } else { xe_mmio_write32(gt, GUC_TLB_INV_CR, GUC_TLB_INV_CR_INVALIDATE); } xe_force_wake_put(gt_to_fw(gt), XE_FW_GT); } return 0; }"
808----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50083/bad/tcp_output.c----tcp_can_coalesce_send_queue_head,"static bool tcp_can_coalesce_send_queue_head(struct sock *sk, int len) { struct sk_buff *skb, *next; skb = tcp_send_head(sk); tcp_for_write_queue_from_safe(skb, next, sk) { if (len <= skb->len) break; <S2SV_StartVul> if (unlikely(TCP_SKB_CB(skb)->eor) || <S2SV_EndVul> <S2SV_StartVul> tcp_has_tx_tstamp(skb) || <S2SV_EndVul> <S2SV_StartVul> !skb_pure_zcopy_same(skb, next)) <S2SV_EndVul> return false; len -= skb->len; } return true; }","- if (unlikely(TCP_SKB_CB(skb)->eor) ||
- tcp_has_tx_tstamp(skb) ||
- !skb_pure_zcopy_same(skb, next))
+ if (tcp_has_tx_tstamp(skb) || !tcp_skb_can_collapse(skb, next))","static bool tcp_can_coalesce_send_queue_head(struct sock *sk, int len) { struct sk_buff *skb, *next; skb = tcp_send_head(sk); tcp_for_write_queue_from_safe(skb, next, sk) { if (len <= skb->len) break; if (tcp_has_tx_tstamp(skb) || !tcp_skb_can_collapse(skb, next)) return false; len -= skb->len; } return true; }"
942----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50189/bad/amd_sfh_client.c----amd_sfh_hid_client_init,"int amd_sfh_hid_client_init(struct amd_mp2_dev *privdata) { struct amd_input_data *in_data = &privdata->in_data; struct amdtp_cl_data *cl_data = privdata->cl_data; struct amd_mp2_sensor_info info; struct device *dev; u32 feature_report_size; u32 input_report_size; int rc, i, status; u8 cl_idx; dev = &privdata->pdev->dev; cl_data->num_hid_devices = amd_mp2_get_sensor_num(privdata, &cl_data->sensor_idx[0]); if (cl_data->num_hid_devices == 0) return -ENODEV; INIT_DELAYED_WORK(&cl_data->work, amd_sfh_work); INIT_DELAYED_WORK(&cl_data->work_buffer, amd_sfh_work_buffer); INIT_LIST_HEAD(&req_list.list); cl_data->in_data = in_data; <S2SV_StartVul> for (i = 0; i < cl_data->num_hid_devices; i++) { <S2SV_EndVul> <S2SV_StartVul> in_data->sensor_virt_addr[i] = dma_alloc_coherent(dev, sizeof(int) * 8, <S2SV_EndVul> <S2SV_StartVul> &cl_data->sensor_dma_addr[i], <S2SV_EndVul> <S2SV_StartVul> GFP_KERNEL); <S2SV_EndVul> if (!in_data->sensor_virt_addr[i]) { rc = -ENOMEM; goto cleanup; } cl_data->sensor_sts[i] = SENSOR_DISABLED; cl_data->sensor_requested_cnt[i] = 0; cl_data->cur_hid_dev = i; cl_idx = cl_data->sensor_idx[i]; cl_data->report_descr_sz[i] = get_descr_sz(cl_idx, descr_size); if (!cl_data->report_descr_sz[i]) { rc = -EINVAL; goto cleanup; } feature_report_size = get_descr_sz(cl_idx, feature_size); if (!feature_report_size) { rc = -EINVAL; goto cleanup; } input_report_size = get_descr_sz(cl_idx, input_size); if (!input_report_size) { rc = -EINVAL; goto cleanup; } cl_data->feature_report[i] = devm_kzalloc(dev, feature_report_size, GFP_KERNEL); if (!cl_data->feature_report[i]) { rc = -ENOMEM; goto cleanup; } in_data->input_report[i] = devm_kzalloc(dev, input_report_size, GFP_KERNEL); if (!in_data->input_report[i]) { rc = -ENOMEM; goto cleanup; } info.period = AMD_SFH_IDLE_LOOP; info.sensor_idx = cl_idx; info.dma_address = cl_data->sensor_dma_addr[i]; cl_data->report_descr[i] = devm_kzalloc(dev, cl_data->report_descr_sz[i], GFP_KERNEL); if (!cl_data->report_descr[i]) { rc = -ENOMEM; goto cleanup; } rc = get_report_descriptor(cl_idx, cl_data->report_descr[i]); if (rc) return rc; privdata->mp2_ops->start(privdata, info); status = amd_sfh_wait_for_response (privdata, cl_data->sensor_idx[i], SENSOR_ENABLED); if (status == SENSOR_ENABLED) { cl_data->sensor_sts[i] = SENSOR_ENABLED; rc = amdtp_hid_probe(cl_data->cur_hid_dev, cl_data); if (rc) { privdata->mp2_ops->stop(privdata, cl_data->sensor_idx[i]); status = amd_sfh_wait_for_response (privdata, cl_data->sensor_idx[i], SENSOR_DISABLED); if (status != SENSOR_ENABLED) cl_data->sensor_sts[i] = SENSOR_DISABLED; dev_dbg(dev, ""sid 0x%x status 0x%x\n"", cl_data->sensor_idx[i], cl_data->sensor_sts[i]); goto cleanup; } } dev_dbg(dev, ""sid 0x%x status 0x%x\n"", cl_data->sensor_idx[i], cl_data->sensor_sts[i]); } if (privdata->mp2_ops->discovery_status && privdata->mp2_ops->discovery_status(privdata) == 0) { amd_sfh_hid_client_deinit(privdata); for (i = 0; i < cl_data->num_hid_devices; i++) { devm_kfree(dev, cl_data->feature_report[i]); devm_kfree(dev, in_data->input_report[i]); devm_kfree(dev, cl_data->report_descr[i]); } dev_warn(dev, ""Failed to discover, sensors not enabled\n""); return -EOPNOTSUPP; } schedule_delayed_work(&cl_data->work_buffer, msecs_to_jiffies(AMD_SFH_IDLE_LOOP)); return 0; cleanup: for (i = 0; i < cl_data->num_hid_devices; i++) { if (in_data->sensor_virt_addr[i]) { dma_free_coherent(&privdata->pdev->dev, 8 * sizeof(int), in_data->sensor_virt_addr[i], cl_data->sensor_dma_addr[i]); } devm_kfree(dev, cl_data->feature_report[i]); devm_kfree(dev, in_data->input_report[i]); devm_kfree(dev, cl_data->report_descr[i]); } return rc; }","- for (i = 0; i < cl_data->num_hid_devices; i++) {
- in_data->sensor_virt_addr[i] = dma_alloc_coherent(dev, sizeof(int) * 8,
- &cl_data->sensor_dma_addr[i],
- GFP_KERNEL);
+ in_data->sensor_virt_addr[i] = dmam_alloc_coherent(dev, sizeof(int) * 8,
+ &cl_data->sensor_dma_addr[i],
+ GFP_KERNEL);","int amd_sfh_hid_client_init(struct amd_mp2_dev *privdata) { struct amd_input_data *in_data = &privdata->in_data; struct amdtp_cl_data *cl_data = privdata->cl_data; struct amd_mp2_sensor_info info; struct device *dev; u32 feature_report_size; u32 input_report_size; int rc, i, status; u8 cl_idx; dev = &privdata->pdev->dev; cl_data->num_hid_devices = amd_mp2_get_sensor_num(privdata, &cl_data->sensor_idx[0]); if (cl_data->num_hid_devices == 0) return -ENODEV; INIT_DELAYED_WORK(&cl_data->work, amd_sfh_work); INIT_DELAYED_WORK(&cl_data->work_buffer, amd_sfh_work_buffer); INIT_LIST_HEAD(&req_list.list); cl_data->in_data = in_data; for (i = 0; i < cl_data->num_hid_devices; i++) { in_data->sensor_virt_addr[i] = dmam_alloc_coherent(dev, sizeof(int) * 8, &cl_data->sensor_dma_addr[i], GFP_KERNEL); if (!in_data->sensor_virt_addr[i]) { rc = -ENOMEM; goto cleanup; } cl_data->sensor_sts[i] = SENSOR_DISABLED; cl_data->sensor_requested_cnt[i] = 0; cl_data->cur_hid_dev = i; cl_idx = cl_data->sensor_idx[i]; cl_data->report_descr_sz[i] = get_descr_sz(cl_idx, descr_size); if (!cl_data->report_descr_sz[i]) { rc = -EINVAL; goto cleanup; } feature_report_size = get_descr_sz(cl_idx, feature_size); if (!feature_report_size) { rc = -EINVAL; goto cleanup; } input_report_size = get_descr_sz(cl_idx, input_size); if (!input_report_size) { rc = -EINVAL; goto cleanup; } cl_data->feature_report[i] = devm_kzalloc(dev, feature_report_size, GFP_KERNEL); if (!cl_data->feature_report[i]) { rc = -ENOMEM; goto cleanup; } in_data->input_report[i] = devm_kzalloc(dev, input_report_size, GFP_KERNEL); if (!in_data->input_report[i]) { rc = -ENOMEM; goto cleanup; } info.period = AMD_SFH_IDLE_LOOP; info.sensor_idx = cl_idx; info.dma_address = cl_data->sensor_dma_addr[i]; cl_data->report_descr[i] = devm_kzalloc(dev, cl_data->report_descr_sz[i], GFP_KERNEL); if (!cl_data->report_descr[i]) { rc = -ENOMEM; goto cleanup; } rc = get_report_descriptor(cl_idx, cl_data->report_descr[i]); if (rc) return rc; privdata->mp2_ops->start(privdata, info); status = amd_sfh_wait_for_response (privdata, cl_data->sensor_idx[i], SENSOR_ENABLED); if (status == SENSOR_ENABLED) { cl_data->sensor_sts[i] = SENSOR_ENABLED; rc = amdtp_hid_probe(cl_data->cur_hid_dev, cl_data); if (rc) { privdata->mp2_ops->stop(privdata, cl_data->sensor_idx[i]); status = amd_sfh_wait_for_response (privdata, cl_data->sensor_idx[i], SENSOR_DISABLED); if (status != SENSOR_ENABLED) cl_data->sensor_sts[i] = SENSOR_DISABLED; dev_dbg(dev, ""sid 0x%x status 0x%x\n"", cl_data->sensor_idx[i], cl_data->sensor_sts[i]); goto cleanup; } } dev_dbg(dev, ""sid 0x%x status 0x%x\n"", cl_data->sensor_idx[i], cl_data->sensor_sts[i]); } if (privdata->mp2_ops->discovery_status && privdata->mp2_ops->discovery_status(privdata) == 0) { amd_sfh_hid_client_deinit(privdata); for (i = 0; i < cl_data->num_hid_devices; i++) { devm_kfree(dev, cl_data->feature_report[i]); devm_kfree(dev, in_data->input_report[i]); devm_kfree(dev, cl_data->report_descr[i]); } dev_warn(dev, ""Failed to discover, sensors not enabled\n""); return -EOPNOTSUPP; } schedule_delayed_work(&cl_data->work_buffer, msecs_to_jiffies(AMD_SFH_IDLE_LOOP)); return 0; cleanup: for (i = 0; i < cl_data->num_hid_devices; i++) { if (in_data->sensor_virt_addr[i]) { dma_free_coherent(&privdata->pdev->dev, 8 * sizeof(int), in_data->sensor_virt_addr[i], cl_data->sensor_dma_addr[i]); } devm_kfree(dev, cl_data->feature_report[i]); devm_kfree(dev, in_data->input_report[i]); devm_kfree(dev, cl_data->report_descr[i]); } return rc; }"
14----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-43891/bad/trace_events_trigger.c----*trigger_start,"static void *trigger_start(struct seq_file *m, loff_t *pos) { struct trace_event_file *event_file; mutex_lock(&event_mutex); <S2SV_StartVul> event_file = event_file_data(m->private); <S2SV_EndVul> if (unlikely(!event_file)) return ERR_PTR(-ENODEV); if (list_empty(&event_file->triggers) || !check_user_trigger(event_file)) return *pos == 0 ? SHOW_AVAILABLE_TRIGGERS : NULL; return seq_list_start(&event_file->triggers, *pos); }","- event_file = event_file_data(m->private);
+ event_file = event_file_file(m->private);","static void *trigger_start(struct seq_file *m, loff_t *pos) { struct trace_event_file *event_file; mutex_lock(&event_mutex); event_file = event_file_file(m->private); if (unlikely(!event_file)) return ERR_PTR(-ENODEV); if (list_empty(&event_file->triggers) || !check_user_trigger(event_file)) return *pos == 0 ? SHOW_AVAILABLE_TRIGGERS : NULL; return seq_list_start(&event_file->triggers, *pos); }"
540----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49860/bad/device_sysfs.c----acpi_device_setup_files,"int acpi_device_setup_files(struct acpi_device *dev) { struct acpi_buffer buffer = {ACPI_ALLOCATE_BUFFER, NULL}; acpi_status status; int result = 0; if (dev->handle) { result = device_create_file(&dev->dev, &dev_attr_path); if (result) goto end; } if (!list_empty(&dev->pnp.ids)) { result = device_create_file(&dev->dev, &dev_attr_hid); if (result) goto end; result = device_create_file(&dev->dev, &dev_attr_modalias); if (result) goto end; } if (acpi_has_method(dev->handle, ""_STR"")) { <S2SV_StartVul> status = acpi_evaluate_object(dev->handle, ""_STR"", <S2SV_EndVul> <S2SV_StartVul> NULL, &buffer); <S2SV_EndVul> if (ACPI_FAILURE(status)) buffer.pointer = NULL; dev->pnp.str_obj = buffer.pointer; result = device_create_file(&dev->dev, &dev_attr_description); if (result) goto end; } if (dev->pnp.type.bus_address) result = device_create_file(&dev->dev, &dev_attr_adr); if (dev->pnp.unique_id) result = device_create_file(&dev->dev, &dev_attr_uid); if (acpi_has_method(dev->handle, ""_SUN"")) { result = device_create_file(&dev->dev, &dev_attr_sun); if (result) goto end; } if (acpi_has_method(dev->handle, ""_HRV"")) { result = device_create_file(&dev->dev, &dev_attr_hrv); if (result) goto end; } if (acpi_has_method(dev->handle, ""_STA"")) { result = device_create_file(&dev->dev, &dev_attr_status); if (result) goto end; } if (acpi_has_method(dev->handle, ""_EJ0"")) { result = device_create_file(&dev->dev, &dev_attr_eject); if (result) return result; } if (dev->flags.power_manageable) { result = device_create_file(&dev->dev, &dev_attr_power_state); if (result) return result; if (dev->power.flags.power_resources) result = device_create_file(&dev->dev, &dev_attr_real_power_state); } acpi_expose_nondev_subnodes(&dev->dev.kobj, &dev->data); end: return result; }","- status = acpi_evaluate_object(dev->handle, ""_STR"",
- NULL, &buffer);
+ status = acpi_evaluate_object_typed(dev->handle, ""_STR"",
+ NULL, &buffer,
+ ACPI_TYPE_BUFFER);","int acpi_device_setup_files(struct acpi_device *dev) { struct acpi_buffer buffer = {ACPI_ALLOCATE_BUFFER, NULL}; acpi_status status; int result = 0; if (dev->handle) { result = device_create_file(&dev->dev, &dev_attr_path); if (result) goto end; } if (!list_empty(&dev->pnp.ids)) { result = device_create_file(&dev->dev, &dev_attr_hid); if (result) goto end; result = device_create_file(&dev->dev, &dev_attr_modalias); if (result) goto end; } if (acpi_has_method(dev->handle, ""_STR"")) { status = acpi_evaluate_object_typed(dev->handle, ""_STR"", NULL, &buffer, ACPI_TYPE_BUFFER); if (ACPI_FAILURE(status)) buffer.pointer = NULL; dev->pnp.str_obj = buffer.pointer; result = device_create_file(&dev->dev, &dev_attr_description); if (result) goto end; } if (dev->pnp.type.bus_address) result = device_create_file(&dev->dev, &dev_attr_adr); if (dev->pnp.unique_id) result = device_create_file(&dev->dev, &dev_attr_uid); if (acpi_has_method(dev->handle, ""_SUN"")) { result = device_create_file(&dev->dev, &dev_attr_sun); if (result) goto end; } if (acpi_has_method(dev->handle, ""_HRV"")) { result = device_create_file(&dev->dev, &dev_attr_hrv); if (result) goto end; } if (acpi_has_method(dev->handle, ""_STA"")) { result = device_create_file(&dev->dev, &dev_attr_status); if (result) goto end; } if (acpi_has_method(dev->handle, ""_EJ0"")) { result = device_create_file(&dev->dev, &dev_attr_eject); if (result) return result; } if (dev->flags.power_manageable) { result = device_create_file(&dev->dev, &dev_attr_power_state); if (result) return result; if (dev->power.flags.power_resources) result = device_create_file(&dev->dev, &dev_attr_real_power_state); } acpi_expose_nondev_subnodes(&dev->dev.kobj, &dev->data); end: return result; }"
995----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50232/bad/ad7124.c----ad7124_write_raw,"static int ad7124_write_raw(struct iio_dev *indio_dev, struct iio_chan_spec const *chan, int val, int val2, long info) { struct ad7124_state *st = iio_priv(indio_dev); unsigned int res, gain, full_scale, vref; int ret = 0; mutex_lock(&st->cfgs_lock); switch (info) { case IIO_CHAN_INFO_SAMP_FREQ: <S2SV_StartVul> if (val2 != 0) { <S2SV_EndVul> ret = -EINVAL; break; } ad7124_set_channel_odr(st, chan->address, val); break; case IIO_CHAN_INFO_SCALE: if (val != 0) { ret = -EINVAL; break; } if (st->channels[chan->address].cfg.bipolar) full_scale = 1 << (chan->scan_type.realbits - 1); else full_scale = 1 << chan->scan_type.realbits; vref = st->channels[chan->address].cfg.vref_mv * 1000000LL; res = DIV_ROUND_CLOSEST(vref, full_scale); gain = DIV_ROUND_CLOSEST(res, val2); res = ad7124_find_closest_match(ad7124_gain, ARRAY_SIZE(ad7124_gain), gain); if (st->channels[chan->address].cfg.pga_bits != res) st->channels[chan->address].cfg.live = false; st->channels[chan->address].cfg.pga_bits = res; break; case IIO_CHAN_INFO_LOW_PASS_FILTER_3DB_FREQUENCY: if (val2 != 0) { ret = -EINVAL; break; } ad7124_set_3db_filter_freq(st, chan->address, val); break; default: ret = -EINVAL; } mutex_unlock(&st->cfgs_lock); return ret; }","- if (val2 != 0) {
+ if (val2 != 0 || val == 0) {","static int ad7124_write_raw(struct iio_dev *indio_dev, struct iio_chan_spec const *chan, int val, int val2, long info) { struct ad7124_state *st = iio_priv(indio_dev); unsigned int res, gain, full_scale, vref; int ret = 0; mutex_lock(&st->cfgs_lock); switch (info) { case IIO_CHAN_INFO_SAMP_FREQ: if (val2 != 0 || val == 0) { ret = -EINVAL; break; } ad7124_set_channel_odr(st, chan->address, val); break; case IIO_CHAN_INFO_SCALE: if (val != 0) { ret = -EINVAL; break; } if (st->channels[chan->address].cfg.bipolar) full_scale = 1 << (chan->scan_type.realbits - 1); else full_scale = 1 << chan->scan_type.realbits; vref = st->channels[chan->address].cfg.vref_mv * 1000000LL; res = DIV_ROUND_CLOSEST(vref, full_scale); gain = DIV_ROUND_CLOSEST(res, val2); res = ad7124_find_closest_match(ad7124_gain, ARRAY_SIZE(ad7124_gain), gain); if (st->channels[chan->address].cfg.pga_bits != res) st->channels[chan->address].cfg.live = false; st->channels[chan->address].cfg.pga_bits = res; break; case IIO_CHAN_INFO_LOW_PASS_FILTER_3DB_FREQUENCY: if (val2 != 0) { ret = -EINVAL; break; } ad7124_set_3db_filter_freq(st, chan->address, val); break; default: ret = -EINVAL; } mutex_unlock(&st->cfgs_lock); return ret; }"
1006----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50244/bad/frecord.c----ni_clear,"void ni_clear(struct ntfs_inode *ni) { struct rb_node *node; <S2SV_StartVul> if (!ni->vfs_inode.i_nlink && ni->mi.mrec && is_rec_inuse(ni->mi.mrec)) <S2SV_EndVul> ni_delete_all(ni); al_destroy(ni); for (node = rb_first(&ni->mi_tree); node;) { struct rb_node *next = rb_next(node); struct mft_inode *mi = rb_entry(node, struct mft_inode, node); rb_erase(node, &ni->mi_tree); mi_put(mi); node = next; } if (ni->ni_flags & NI_FLAG_DIR) indx_clear(&ni->dir); else { run_close(&ni->file.run); #ifdef CONFIG_NTFS3_LZX_XPRESS if (ni->file.offs_page) { put_page(ni->file.offs_page); ni->file.offs_page = NULL; } #endif } mi_clear(&ni->mi); }","- if (!ni->vfs_inode.i_nlink && ni->mi.mrec && is_rec_inuse(ni->mi.mrec))
+ if (!ni->vfs_inode.i_nlink && ni->mi.mrec &&
+ is_rec_inuse(ni->mi.mrec) &&
+ !(ni->mi.sbi->flags & NTFS_FLAGS_LOG_REPLAYING))","void ni_clear(struct ntfs_inode *ni) { struct rb_node *node; if (!ni->vfs_inode.i_nlink && ni->mi.mrec && is_rec_inuse(ni->mi.mrec) && !(ni->mi.sbi->flags & NTFS_FLAGS_LOG_REPLAYING)) ni_delete_all(ni); al_destroy(ni); for (node = rb_first(&ni->mi_tree); node;) { struct rb_node *next = rb_next(node); struct mft_inode *mi = rb_entry(node, struct mft_inode, node); rb_erase(node, &ni->mi_tree); mi_put(mi); node = next; } if (ni->ni_flags & NI_FLAG_DIR) indx_clear(&ni->dir); else { run_close(&ni->file.run); #ifdef CONFIG_NTFS3_LZX_XPRESS if (ni->file.offs_page) { put_page(ni->file.offs_page); ni->file.offs_page = NULL; } #endif } mi_clear(&ni->mi); }"
1159----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53134/bad/imx93-blk-ctrl.c----imx93_blk_ctrl_remove,static void imx93_blk_ctrl_remove(struct platform_device *pdev) { struct imx93_blk_ctrl *bc = dev_get_drvdata(&pdev->dev); int i; of_genpd_del_provider(pdev->dev.of_node); <S2SV_StartVul> for (i = 0; bc->onecell_data.num_domains; i++) { <S2SV_EndVul> struct imx93_blk_ctrl_domain *domain = &bc->domains[i]; pm_genpd_remove(&domain->genpd); } },"- for (i = 0; bc->onecell_data.num_domains; i++) {
+ pm_runtime_disable(&pdev->dev);
+ for (i = 0; i < bc->onecell_data.num_domains; i++) {",static void imx93_blk_ctrl_remove(struct platform_device *pdev) { struct imx93_blk_ctrl *bc = dev_get_drvdata(&pdev->dev); int i; of_genpd_del_provider(pdev->dev.of_node); pm_runtime_disable(&pdev->dev); for (i = 0; i < bc->onecell_data.num_domains; i++) { struct imx93_blk_ctrl_domain *domain = &bc->domains[i]; pm_genpd_remove(&domain->genpd); } }
449----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47689/bad/super.c----f2fs_handle_critical_error,"void f2fs_handle_critical_error(struct f2fs_sb_info *sbi, unsigned char reason, bool irq_context) { struct super_block *sb = sbi->sb; bool shutdown = reason == STOP_CP_REASON_SHUTDOWN; bool continue_fs = !shutdown && F2FS_OPTION(sbi).errors == MOUNT_ERRORS_CONTINUE; set_ckpt_flags(sbi, CP_ERROR_FLAG); if (!f2fs_hw_is_readonly(sbi)) { save_stop_reason(sbi, reason); if (irq_context && !shutdown) schedule_work(&sbi->s_error_work); else f2fs_record_stop_reason(sbi); } if (F2FS_OPTION(sbi).errors == MOUNT_ERRORS_PANIC && !shutdown && !system_going_down() && !is_sbi_flag_set(sbi, SBI_IS_SHUTDOWN)) panic(""F2FS-fs (device %s): panic forced after error\n"", sb->s_id); if (shutdown) set_sbi_flag(sbi, SBI_IS_SHUTDOWN); if (continue_fs || f2fs_readonly(sb) || shutdown) { f2fs_warn(sbi, ""Stopped filesystem due to reason: %d"", reason); return; } f2fs_warn(sbi, ""Remounting filesystem read-only""); <S2SV_StartVul> smp_wmb(); <S2SV_EndVul> <S2SV_StartVul> sb->s_flags |= SB_RDONLY; <S2SV_EndVul> }","- smp_wmb();
- sb->s_flags |= SB_RDONLY;","void f2fs_handle_critical_error(struct f2fs_sb_info *sbi, unsigned char reason, bool irq_context) { struct super_block *sb = sbi->sb; bool shutdown = reason == STOP_CP_REASON_SHUTDOWN; bool continue_fs = !shutdown && F2FS_OPTION(sbi).errors == MOUNT_ERRORS_CONTINUE; set_ckpt_flags(sbi, CP_ERROR_FLAG); if (!f2fs_hw_is_readonly(sbi)) { save_stop_reason(sbi, reason); if (irq_context && !shutdown) schedule_work(&sbi->s_error_work); else f2fs_record_stop_reason(sbi); } if (F2FS_OPTION(sbi).errors == MOUNT_ERRORS_PANIC && !shutdown && !system_going_down() && !is_sbi_flag_set(sbi, SBI_IS_SHUTDOWN)) panic(""F2FS-fs (device %s): panic forced after error\n"", sb->s_id); if (shutdown) set_sbi_flag(sbi, SBI_IS_SHUTDOWN); if (continue_fs || f2fs_readonly(sb) || shutdown) { f2fs_warn(sbi, ""Stopped filesystem due to reason: %d"", reason); return; } f2fs_warn(sbi, ""Remounting filesystem read-only""); }"
291----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46766/bad/ice_base.c----ice_free_q_vector,"static void ice_free_q_vector(struct ice_vsi *vsi, int v_idx) { struct ice_q_vector *q_vector; struct ice_pf *pf = vsi->back; struct ice_tx_ring *tx_ring; struct ice_rx_ring *rx_ring; struct device *dev; dev = ice_pf_to_dev(pf); if (!vsi->q_vectors[v_idx]) { dev_dbg(dev, ""Queue vector at index %d not found\n"", v_idx); return; } q_vector = vsi->q_vectors[v_idx]; <S2SV_StartVul> ice_for_each_tx_ring(tx_ring, q_vector->tx) { <S2SV_EndVul> <S2SV_StartVul> ice_queue_set_napi(vsi, tx_ring->q_index, NETDEV_QUEUE_TYPE_TX, <S2SV_EndVul> <S2SV_StartVul> NULL); <S2SV_EndVul> tx_ring->q_vector = NULL; <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> ice_for_each_rx_ring(rx_ring, q_vector->rx) { <S2SV_EndVul> <S2SV_StartVul> ice_queue_set_napi(vsi, rx_ring->q_index, NETDEV_QUEUE_TYPE_RX, <S2SV_EndVul> <S2SV_StartVul> NULL); <S2SV_EndVul> rx_ring->q_vector = NULL; <S2SV_StartVul> } <S2SV_EndVul> if (vsi->netdev) netif_napi_del(&q_vector->napi); if (q_vector->irq.index < 0) goto free_q_vector; if (vsi->type == ICE_VSI_CTRL && vsi->vf && ice_get_vf_ctrl_vsi(pf, vsi)) goto free_q_vector; ice_free_irq(pf, q_vector->irq); free_q_vector: kfree(q_vector); vsi->q_vectors[v_idx] = NULL; }","- ice_for_each_tx_ring(tx_ring, q_vector->tx) {
- ice_queue_set_napi(vsi, tx_ring->q_index, NETDEV_QUEUE_TYPE_TX,
- NULL);
- }
- ice_for_each_rx_ring(rx_ring, q_vector->rx) {
- ice_queue_set_napi(vsi, rx_ring->q_index, NETDEV_QUEUE_TYPE_RX,
- NULL);
- }
+ ice_for_each_tx_ring(tx_ring, vsi->q_vectors[v_idx]->tx)
+ ice_for_each_rx_ring(rx_ring, vsi->q_vectors[v_idx]->rx)","static void ice_free_q_vector(struct ice_vsi *vsi, int v_idx) { struct ice_q_vector *q_vector; struct ice_pf *pf = vsi->back; struct ice_tx_ring *tx_ring; struct ice_rx_ring *rx_ring; struct device *dev; dev = ice_pf_to_dev(pf); if (!vsi->q_vectors[v_idx]) { dev_dbg(dev, ""Queue vector at index %d not found\n"", v_idx); return; } q_vector = vsi->q_vectors[v_idx]; ice_for_each_tx_ring(tx_ring, vsi->q_vectors[v_idx]->tx) tx_ring->q_vector = NULL; ice_for_each_rx_ring(rx_ring, vsi->q_vectors[v_idx]->rx) rx_ring->q_vector = NULL; if (vsi->netdev) netif_napi_del(&q_vector->napi); if (q_vector->irq.index < 0) goto free_q_vector; if (vsi->type == ICE_VSI_CTRL && vsi->vf && ice_get_vf_ctrl_vsi(pf, vsi)) goto free_q_vector; ice_free_irq(pf, q_vector->irq); free_q_vector: kfree(q_vector); vsi->q_vectors[v_idx] = NULL; }"
1502----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2025-21631/bad/bfq-iosched.c----*bfq_waker_bfqq,static struct bfq_queue *bfq_waker_bfqq(struct bfq_queue *bfqq) { struct bfq_queue *new_bfqq = bfqq->new_bfqq; struct bfq_queue *waker_bfqq = bfqq->waker_bfqq; if (!waker_bfqq) return NULL; while (new_bfqq) { if (new_bfqq == waker_bfqq) { if (bfqq_process_refs(waker_bfqq) == 1) return NULL; <S2SV_StartVul> break; <S2SV_EndVul> } new_bfqq = new_bfqq->new_bfqq; } return waker_bfqq; },"- break;
+ return NULL;
+ return waker_bfqq;
+ if (bfqq_process_refs(waker_bfqq) == 0)
+ return NULL;
+ return waker_bfqq;",static struct bfq_queue *bfq_waker_bfqq(struct bfq_queue *bfqq) { struct bfq_queue *new_bfqq = bfqq->new_bfqq; struct bfq_queue *waker_bfqq = bfqq->waker_bfqq; if (!waker_bfqq) return NULL; while (new_bfqq) { if (new_bfqq == waker_bfqq) { if (bfqq_process_refs(waker_bfqq) == 1) return NULL; return waker_bfqq; } new_bfqq = new_bfqq->new_bfqq; } if (bfqq_process_refs(waker_bfqq) == 0) return NULL; return waker_bfqq; }
1349----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56670/bad/u_serial.c----gs_start_io,"static int gs_start_io(struct gs_port *port) { struct list_head *head = &port->read_pool; struct usb_ep *ep; int status; unsigned started; if (!port->port_usb || !port->port.tty) return -EIO; ep = port->port_usb->out; status = gs_alloc_requests(ep, head, gs_read_complete, &port->read_allocated); if (status) return status; status = gs_alloc_requests(port->port_usb->in, &port->write_pool, gs_write_complete, &port->write_allocated); if (status) { gs_free_requests(ep, head, &port->read_allocated); return status; } port->n_read = 0; started = gs_start_rx(port); if (started) { gs_start_tx(port); tty_wakeup(port->port.tty); } else { <S2SV_StartVul> gs_free_requests(ep, head, &port->read_allocated); <S2SV_EndVul> <S2SV_StartVul> gs_free_requests(port->port_usb->in, &port->write_pool, <S2SV_EndVul> <S2SV_StartVul> &port->write_allocated); <S2SV_EndVul> status = -EIO; } return status; }","- gs_free_requests(ep, head, &port->read_allocated);
- gs_free_requests(port->port_usb->in, &port->write_pool,
- &port->write_allocated);
+ if (port->port_usb) {
+ gs_free_requests(ep, head, &port->read_allocated);
+ gs_free_requests(port->port_usb->in, &port->write_pool,
+ &port->write_allocated);
+ }
+ }","static int gs_start_io(struct gs_port *port) { struct list_head *head = &port->read_pool; struct usb_ep *ep; int status; unsigned started; if (!port->port_usb || !port->port.tty) return -EIO; ep = port->port_usb->out; status = gs_alloc_requests(ep, head, gs_read_complete, &port->read_allocated); if (status) return status; status = gs_alloc_requests(port->port_usb->in, &port->write_pool, gs_write_complete, &port->write_allocated); if (status) { gs_free_requests(ep, head, &port->read_allocated); return status; } port->n_read = 0; started = gs_start_rx(port); if (started) { gs_start_tx(port); tty_wakeup(port->port.tty); } else { if (port->port_usb) { gs_free_requests(ep, head, &port->read_allocated); gs_free_requests(port->port_usb->in, &port->write_pool, &port->write_allocated); } status = -EIO; } return status; }"
1018----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50256/bad/nf_reject_ipv6.c----nf_send_reset6,"void nf_send_reset6(struct net *net, struct sock *sk, struct sk_buff *oldskb, int hook) { <S2SV_StartVul> struct sk_buff *nskb; <S2SV_EndVul> <S2SV_StartVul> struct tcphdr _otcph; <S2SV_EndVul> <S2SV_StartVul> const struct tcphdr *otcph; <S2SV_EndVul> <S2SV_StartVul> unsigned int otcplen, hh_len; <S2SV_EndVul> const struct ipv6hdr *oip6h = ipv6_hdr(oldskb); struct dst_entry *dst = NULL; struct flowi6 fl6; if ((!(ipv6_addr_type(&oip6h->saddr) & IPV6_ADDR_UNICAST)) || (!(ipv6_addr_type(&oip6h->daddr) & IPV6_ADDR_UNICAST))) { pr_debug(""addr is not unicast.\n""); return; } otcph = nf_reject_ip6_tcphdr_get(oldskb, &_otcph, &otcplen, hook); if (!otcph) return; memset(&fl6, 0, sizeof(fl6)); fl6.flowi6_proto = IPPROTO_TCP; fl6.saddr = oip6h->daddr; fl6.daddr = oip6h->saddr; fl6.fl6_sport = otcph->dest; fl6.fl6_dport = otcph->source; if (hook == NF_INET_PRE_ROUTING || hook == NF_INET_INGRESS) { nf_ip6_route(net, &dst, flowi6_to_flowi(&fl6), false); if (!dst) return; skb_dst_set(oldskb, dst); } fl6.flowi6_oif = l3mdev_master_ifindex(skb_dst(oldskb)->dev); fl6.flowi6_mark = IP6_REPLY_MARK(net, oldskb->mark); security_skb_classify_flow(oldskb, flowi6_to_flowi_common(&fl6)); dst = ip6_route_output(net, NULL, &fl6); if (dst->error) { dst_release(dst); return; } dst = xfrm_lookup(net, dst, flowi6_to_flowi(&fl6), NULL, 0); if (IS_ERR(dst)) return; <S2SV_StartVul> hh_len = (dst->dev->hard_header_len + 15)&~15; <S2SV_EndVul> <S2SV_StartVul> nskb = alloc_skb(hh_len + 15 + dst->header_len + sizeof(struct ipv6hdr) <S2SV_EndVul> <S2SV_StartVul> + sizeof(struct tcphdr) + dst->trailer_len, <S2SV_EndVul> GFP_ATOMIC); if (!nskb) { net_dbg_ratelimited(""cannot alloc skb\n""); dst_release(dst); return; } skb_dst_set(nskb, dst); nskb->mark = fl6.flowi6_mark; <S2SV_StartVul> skb_reserve(nskb, hh_len + dst->header_len); <S2SV_EndVul> nf_reject_ip6hdr_put(nskb, oldskb, IPPROTO_TCP, ip6_dst_hoplimit(dst)); nf_reject_ip6_tcphdr_put(nskb, oldskb, otcph, otcplen); nf_ct_attach(nskb, oldskb); nf_ct_set_closing(skb_nfct(oldskb)); #if IS_ENABLED(CONFIG_BRIDGE_NETFILTER) if (nf_bridge_info_exists(oldskb)) { struct ethhdr *oeth = eth_hdr(oldskb); struct ipv6hdr *ip6h = ipv6_hdr(nskb); struct net_device *br_indev; br_indev = nf_bridge_get_physindev(oldskb, net); if (!br_indev) { kfree_skb(nskb); return; } nskb->dev = br_indev; nskb->protocol = htons(ETH_P_IPV6); ip6h->payload_len = htons(sizeof(struct tcphdr)); if (dev_hard_header(nskb, nskb->dev, ntohs(nskb->protocol), oeth->h_source, oeth->h_dest, nskb->len) < 0) { kfree_skb(nskb); return; } dev_queue_xmit(nskb); } else #endif ip6_local_out(net, sk, nskb); }","- struct sk_buff *nskb;
- struct tcphdr _otcph;
- const struct tcphdr *otcph;
- unsigned int otcplen, hh_len;
- hh_len = (dst->dev->hard_header_len + 15)&~15;
- nskb = alloc_skb(hh_len + 15 + dst->header_len + sizeof(struct ipv6hdr)
- + sizeof(struct tcphdr) + dst->trailer_len,
- skb_reserve(nskb, hh_len + dst->header_len);
+ const struct tcphdr *otcph;
+ struct sk_buff *nskb;
+ struct tcphdr _otcph;
+ unsigned int otcplen;
+ sizeof(struct tcphdr) + dst->trailer_len,
+ skb_reserve(nskb, LL_MAX_HEADER);","void nf_send_reset6(struct net *net, struct sock *sk, struct sk_buff *oldskb, int hook) { const struct ipv6hdr *oip6h = ipv6_hdr(oldskb); struct dst_entry *dst = NULL; const struct tcphdr *otcph; struct sk_buff *nskb; struct tcphdr _otcph; unsigned int otcplen; struct flowi6 fl6; if ((!(ipv6_addr_type(&oip6h->saddr) & IPV6_ADDR_UNICAST)) || (!(ipv6_addr_type(&oip6h->daddr) & IPV6_ADDR_UNICAST))) { pr_debug(""addr is not unicast.\n""); return; } otcph = nf_reject_ip6_tcphdr_get(oldskb, &_otcph, &otcplen, hook); if (!otcph) return; memset(&fl6, 0, sizeof(fl6)); fl6.flowi6_proto = IPPROTO_TCP; fl6.saddr = oip6h->daddr; fl6.daddr = oip6h->saddr; fl6.fl6_sport = otcph->dest; fl6.fl6_dport = otcph->source; if (hook == NF_INET_PRE_ROUTING || hook == NF_INET_INGRESS) { nf_ip6_route(net, &dst, flowi6_to_flowi(&fl6), false); if (!dst) return; skb_dst_set(oldskb, dst); } fl6.flowi6_oif = l3mdev_master_ifindex(skb_dst(oldskb)->dev); fl6.flowi6_mark = IP6_REPLY_MARK(net, oldskb->mark); security_skb_classify_flow(oldskb, flowi6_to_flowi_common(&fl6)); dst = ip6_route_output(net, NULL, &fl6); if (dst->error) { dst_release(dst); return; } dst = xfrm_lookup(net, dst, flowi6_to_flowi(&fl6), NULL, 0); if (IS_ERR(dst)) return; nskb = alloc_skb(LL_MAX_HEADER + sizeof(struct ipv6hdr) + sizeof(struct tcphdr) + dst->trailer_len, GFP_ATOMIC); if (!nskb) { net_dbg_ratelimited(""cannot alloc skb\n""); dst_release(dst); return; } skb_dst_set(nskb, dst); nskb->mark = fl6.flowi6_mark; skb_reserve(nskb, LL_MAX_HEADER); nf_reject_ip6hdr_put(nskb, oldskb, IPPROTO_TCP, ip6_dst_hoplimit(dst)); nf_reject_ip6_tcphdr_put(nskb, oldskb, otcph, otcplen); nf_ct_attach(nskb, oldskb); nf_ct_set_closing(skb_nfct(oldskb)); #if IS_ENABLED(CONFIG_BRIDGE_NETFILTER) if (nf_bridge_info_exists(oldskb)) { struct ethhdr *oeth = eth_hdr(oldskb); struct ipv6hdr *ip6h = ipv6_hdr(nskb); struct net_device *br_indev; br_indev = nf_bridge_get_physindev(oldskb, net); if (!br_indev) { kfree_skb(nskb); return; } nskb->dev = br_indev; nskb->protocol = htons(ETH_P_IPV6); ip6h->payload_len = htons(sizeof(struct tcphdr)); if (dev_hard_header(nskb, nskb->dev, ntohs(nskb->protocol), oeth->h_source, oeth->h_dest, nskb->len) < 0) { kfree_skb(nskb); return; } dev_queue_xmit(nskb); } else #endif ip6_local_out(net, sk, nskb); }"
585----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49906/bad/dcn20_hwseq.c----dcn20_program_pipe,"static void dcn20_program_pipe( struct dc *dc, struct pipe_ctx *pipe_ctx, struct dc_state *context) { struct dce_hwseq *hws = dc->hwseq; if (resource_is_pipe_type(pipe_ctx, OTG_MASTER)) { if (pipe_ctx->update_flags.bits.enable || pipe_ctx->update_flags.bits.odm || pipe_ctx->stream->update_flags.bits.abm_level) hws->funcs.blank_pixel_data(dc, pipe_ctx, !pipe_ctx->plane_state || !pipe_ctx->plane_state->visible); } if (pipe_ctx->update_flags.bits.global_sync && !pipe_ctx->top_pipe && !pipe_ctx->prev_odm_pipe) { pipe_ctx->stream_res.tg->funcs->program_global_sync( pipe_ctx->stream_res.tg, calculate_vready_offset_for_group(pipe_ctx), pipe_ctx->pipe_dlg_param.vstartup_start, pipe_ctx->pipe_dlg_param.vupdate_offset, pipe_ctx->pipe_dlg_param.vupdate_width); if (dc_state_get_pipe_subvp_type(context, pipe_ctx) != SUBVP_PHANTOM) pipe_ctx->stream_res.tg->funcs->wait_for_state(pipe_ctx->stream_res.tg, CRTC_STATE_VACTIVE); pipe_ctx->stream_res.tg->funcs->set_vtg_params( pipe_ctx->stream_res.tg, &pipe_ctx->stream->timing, true); if (hws->funcs.setup_vupdate_interrupt) hws->funcs.setup_vupdate_interrupt(dc, pipe_ctx); } if (pipe_ctx->update_flags.bits.odm) hws->funcs.update_odm(dc, context, pipe_ctx); if (pipe_ctx->update_flags.bits.enable) { if (hws->funcs.enable_plane) hws->funcs.enable_plane(dc, pipe_ctx, context); else dcn20_enable_plane(dc, pipe_ctx, context); if (dc->res_pool->hubbub->funcs->force_wm_propagate_to_pipes) dc->res_pool->hubbub->funcs->force_wm_propagate_to_pipes(dc->res_pool->hubbub); } if (pipe_ctx->update_flags.bits.det_size) { if (dc->res_pool->hubbub->funcs->program_det_size) dc->res_pool->hubbub->funcs->program_det_size( dc->res_pool->hubbub, pipe_ctx->plane_res.hubp->inst, pipe_ctx->det_buffer_size_kb); if (dc->res_pool->hubbub->funcs->program_det_segments) dc->res_pool->hubbub->funcs->program_det_segments( dc->res_pool->hubbub, pipe_ctx->plane_res.hubp->inst, pipe_ctx->hubp_regs.det_size); } if (pipe_ctx->update_flags.raw || (pipe_ctx->plane_state && pipe_ctx->plane_state->update_flags.raw) || pipe_ctx->stream->update_flags.raw) dcn20_update_dchubp_dpp(dc, pipe_ctx, context); if (pipe_ctx->update_flags.bits.enable || (pipe_ctx->plane_state && pipe_ctx->plane_state->update_flags.bits.hdr_mult)) hws->funcs.set_hdr_multiplier(pipe_ctx); if (hws->funcs.populate_mcm_luts) { if (pipe_ctx->plane_state) { hws->funcs.populate_mcm_luts(dc, pipe_ctx, pipe_ctx->plane_state->mcm_luts, pipe_ctx->plane_state->lut_bank_a); pipe_ctx->plane_state->lut_bank_a = !pipe_ctx->plane_state->lut_bank_a; } } <S2SV_StartVul> if (pipe_ctx->update_flags.bits.enable || <S2SV_EndVul> <S2SV_StartVul> (pipe_ctx->plane_state && <S2SV_EndVul> pipe_ctx->plane_state->update_flags.bits.in_transfer_func_change) || <S2SV_StartVul> (pipe_ctx->plane_state && <S2SV_EndVul> pipe_ctx->plane_state->update_flags.bits.gamma_change) || <S2SV_StartVul> (pipe_ctx->plane_state && <S2SV_EndVul> <S2SV_StartVul> pipe_ctx->plane_state->update_flags.bits.lut_3d)) <S2SV_EndVul> hws->funcs.set_input_transfer_func(dc, pipe_ctx, pipe_ctx->plane_state); if (pipe_ctx->update_flags.bits.enable || pipe_ctx->update_flags.bits.plane_changed || pipe_ctx->stream->update_flags.bits.out_tf || (pipe_ctx->plane_state && pipe_ctx->plane_state->update_flags.bits.output_tf_change)) hws->funcs.set_output_transfer_func(dc, pipe_ctx, pipe_ctx->stream); if (pipe_ctx->update_flags.bits.enable || pipe_ctx->update_flags.bits.opp_changed) { pipe_ctx->stream_res.opp->funcs->opp_set_dyn_expansion( pipe_ctx->stream_res.opp, COLOR_SPACE_YCBCR601, pipe_ctx->stream->timing.display_color_depth, pipe_ctx->stream->signal); pipe_ctx->stream_res.opp->funcs->opp_program_fmt( pipe_ctx->stream_res.opp, &pipe_ctx->stream->bit_depth_params, &pipe_ctx->stream->clamping); } if ((pipe_ctx->plane_state && pipe_ctx->plane_state->visible)) { if (pipe_ctx->stream_res.abm) { dc->hwss.set_pipe(pipe_ctx); pipe_ctx->stream_res.abm->funcs->set_abm_level(pipe_ctx->stream_res.abm, pipe_ctx->stream->abm_level); } } if (pipe_ctx->update_flags.bits.test_pattern_changed) { struct output_pixel_processor *odm_opp = pipe_ctx->stream_res.opp; struct bit_depth_reduction_params params; memset(&params, 0, sizeof(params)); odm_opp->funcs->opp_program_bit_depth_reduction(odm_opp, &params); dc->hwss.set_disp_pattern_generator(dc, pipe_ctx, pipe_ctx->stream_res.test_pattern_params.test_pattern, pipe_ctx->stream_res.test_pattern_params.color_space, pipe_ctx->stream_res.test_pattern_params.color_depth, NULL, pipe_ctx->stream_res.test_pattern_params.width, pipe_ctx->stream_res.test_pattern_params.height, pipe_ctx->stream_res.test_pattern_params.offset); } }","- if (pipe_ctx->update_flags.bits.enable ||
- (pipe_ctx->plane_state &&
- (pipe_ctx->plane_state &&
- (pipe_ctx->plane_state &&
- pipe_ctx->plane_state->update_flags.bits.lut_3d))
+ hws->funcs.set_hdr_multiplier(pipe_ctx);
+ if ((pipe_ctx->plane_state && pipe_ctx->plane_state->update_flags.bits.hdr_mult) ||
+ pipe_ctx->update_flags.bits.enable)
+ hws->funcs.set_hdr_multiplier(pipe_ctx);
+ if ((pipe_ctx->plane_state &&
+ pipe_ctx->plane_state->update_flags.bits.lut_3d) ||
+ pipe_ctx->update_flags.bits.enable)","static void dcn20_program_pipe( struct dc *dc, struct pipe_ctx *pipe_ctx, struct dc_state *context) { struct dce_hwseq *hws = dc->hwseq; if (resource_is_pipe_type(pipe_ctx, OTG_MASTER)) { if (pipe_ctx->update_flags.bits.enable || pipe_ctx->update_flags.bits.odm || pipe_ctx->stream->update_flags.bits.abm_level) hws->funcs.blank_pixel_data(dc, pipe_ctx, !pipe_ctx->plane_state || !pipe_ctx->plane_state->visible); } if (pipe_ctx->update_flags.bits.global_sync && !pipe_ctx->top_pipe && !pipe_ctx->prev_odm_pipe) { pipe_ctx->stream_res.tg->funcs->program_global_sync( pipe_ctx->stream_res.tg, calculate_vready_offset_for_group(pipe_ctx), pipe_ctx->pipe_dlg_param.vstartup_start, pipe_ctx->pipe_dlg_param.vupdate_offset, pipe_ctx->pipe_dlg_param.vupdate_width); if (dc_state_get_pipe_subvp_type(context, pipe_ctx) != SUBVP_PHANTOM) pipe_ctx->stream_res.tg->funcs->wait_for_state(pipe_ctx->stream_res.tg, CRTC_STATE_VACTIVE); pipe_ctx->stream_res.tg->funcs->set_vtg_params( pipe_ctx->stream_res.tg, &pipe_ctx->stream->timing, true); if (hws->funcs.setup_vupdate_interrupt) hws->funcs.setup_vupdate_interrupt(dc, pipe_ctx); } if (pipe_ctx->update_flags.bits.odm) hws->funcs.update_odm(dc, context, pipe_ctx); if (pipe_ctx->update_flags.bits.enable) { if (hws->funcs.enable_plane) hws->funcs.enable_plane(dc, pipe_ctx, context); else dcn20_enable_plane(dc, pipe_ctx, context); if (dc->res_pool->hubbub->funcs->force_wm_propagate_to_pipes) dc->res_pool->hubbub->funcs->force_wm_propagate_to_pipes(dc->res_pool->hubbub); } if (pipe_ctx->update_flags.bits.det_size) { if (dc->res_pool->hubbub->funcs->program_det_size) dc->res_pool->hubbub->funcs->program_det_size( dc->res_pool->hubbub, pipe_ctx->plane_res.hubp->inst, pipe_ctx->det_buffer_size_kb); if (dc->res_pool->hubbub->funcs->program_det_segments) dc->res_pool->hubbub->funcs->program_det_segments( dc->res_pool->hubbub, pipe_ctx->plane_res.hubp->inst, pipe_ctx->hubp_regs.det_size); } if (pipe_ctx->update_flags.raw || (pipe_ctx->plane_state && pipe_ctx->plane_state->update_flags.raw) || pipe_ctx->stream->update_flags.raw) dcn20_update_dchubp_dpp(dc, pipe_ctx, context); if (pipe_ctx->update_flags.bits.enable || (pipe_ctx->plane_state && pipe_ctx->plane_state->update_flags.bits.hdr_mult)) hws->funcs.set_hdr_multiplier(pipe_ctx); if ((pipe_ctx->plane_state && pipe_ctx->plane_state->update_flags.bits.hdr_mult) || pipe_ctx->update_flags.bits.enable) hws->funcs.set_hdr_multiplier(pipe_ctx); if (hws->funcs.populate_mcm_luts) { if (pipe_ctx->plane_state) { hws->funcs.populate_mcm_luts(dc, pipe_ctx, pipe_ctx->plane_state->mcm_luts, pipe_ctx->plane_state->lut_bank_a); pipe_ctx->plane_state->lut_bank_a = !pipe_ctx->plane_state->lut_bank_a; } } if ((pipe_ctx->plane_state && pipe_ctx->plane_state->update_flags.bits.in_transfer_func_change) || (pipe_ctx->plane_state && pipe_ctx->plane_state->update_flags.bits.gamma_change) || (pipe_ctx->plane_state && pipe_ctx->plane_state->update_flags.bits.lut_3d) || pipe_ctx->update_flags.bits.enable) hws->funcs.set_input_transfer_func(dc, pipe_ctx, pipe_ctx->plane_state); if (pipe_ctx->update_flags.bits.enable || pipe_ctx->update_flags.bits.plane_changed || pipe_ctx->stream->update_flags.bits.out_tf || (pipe_ctx->plane_state && pipe_ctx->plane_state->update_flags.bits.output_tf_change)) hws->funcs.set_output_transfer_func(dc, pipe_ctx, pipe_ctx->stream); if (pipe_ctx->update_flags.bits.enable || pipe_ctx->update_flags.bits.opp_changed) { pipe_ctx->stream_res.opp->funcs->opp_set_dyn_expansion( pipe_ctx->stream_res.opp, COLOR_SPACE_YCBCR601, pipe_ctx->stream->timing.display_color_depth, pipe_ctx->stream->signal); pipe_ctx->stream_res.opp->funcs->opp_program_fmt( pipe_ctx->stream_res.opp, &pipe_ctx->stream->bit_depth_params, &pipe_ctx->stream->clamping); } if ((pipe_ctx->plane_state && pipe_ctx->plane_state->visible)) { if (pipe_ctx->stream_res.abm) { dc->hwss.set_pipe(pipe_ctx); pipe_ctx->stream_res.abm->funcs->set_abm_level(pipe_ctx->stream_res.abm, pipe_ctx->stream->abm_level); } } if (pipe_ctx->update_flags.bits.test_pattern_changed) { struct output_pixel_processor *odm_opp = pipe_ctx->stream_res.opp; struct bit_depth_reduction_params params; memset(&params, 0, sizeof(params)); odm_opp->funcs->opp_program_bit_depth_reduction(odm_opp, &params); dc->hwss.set_disp_pattern_generator(dc, pipe_ctx, pipe_ctx->stream_res.test_pattern_params.test_pattern, pipe_ctx->stream_res.test_pattern_params.color_space, pipe_ctx->stream_res.test_pattern_params.color_depth, NULL, pipe_ctx->stream_res.test_pattern_params.width, pipe_ctx->stream_res.test_pattern_params.height, pipe_ctx->stream_res.test_pattern_params.offset); } }"
796----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50074/bad/procfs.c----do_hardware_base_addr,"static int do_hardware_base_addr(const struct ctl_table *table, int write, void *result, size_t *lenp, loff_t *ppos) { struct parport *port = (struct parport *)table->extra1; char buffer[64]; int len = 0; if (*ppos) { *lenp = 0; return 0; } if (write) return -EACCES; <S2SV_StartVul> len += snprintf (buffer, sizeof(buffer), ""%lu\t%lu\n"", port->base, port->base_hi); <S2SV_EndVul> if (len > *lenp) len = *lenp; else *lenp = len; *ppos += len; memcpy(result, buffer, len); return 0; }","- len += snprintf (buffer, sizeof(buffer), ""%lu\t%lu\n"", port->base, port->base_hi);
+ len += scnprintf (buffer, sizeof(buffer), ""%lu\t%lu\n"", port->base, port->base_hi);","static int do_hardware_base_addr(const struct ctl_table *table, int write, void *result, size_t *lenp, loff_t *ppos) { struct parport *port = (struct parport *)table->extra1; char buffer[64]; int len = 0; if (*ppos) { *lenp = 0; return 0; } if (write) return -EACCES; len += scnprintf (buffer, sizeof(buffer), ""%lu\t%lu\n"", port->base, port->base_hi); if (len > *lenp) len = *lenp; else *lenp = len; *ppos += len; memcpy(result, buffer, len); return 0; }"
105----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44964/bad/idpf_lib.c----idpf_vport_open,"static int idpf_vport_open(struct idpf_vport *vport, bool alloc_res) { struct idpf_netdev_priv *np = netdev_priv(vport->netdev); struct idpf_adapter *adapter = vport->adapter; struct idpf_vport_config *vport_config; int err; if (np->state != __IDPF_VPORT_DOWN) return -EBUSY; netif_carrier_off(vport->netdev); <S2SV_StartVul> if (alloc_res) { <S2SV_EndVul> <S2SV_StartVul> err = idpf_vport_queues_alloc(vport); <S2SV_EndVul> <S2SV_StartVul> if (err) <S2SV_EndVul> <S2SV_StartVul> return err; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> err = idpf_vport_intr_alloc(vport); if (err) { dev_err(&adapter->pdev->dev, ""Failed to allocate interrupts for vport %u: %d\n"", vport->vport_id, err); goto queues_rel; } err = idpf_vport_queue_ids_init(vport); if (err) { dev_err(&adapter->pdev->dev, ""Failed to initialize queue ids for vport %u: %d\n"", vport->vport_id, err); goto intr_rel; } err = idpf_vport_intr_init(vport); if (err) { dev_err(&adapter->pdev->dev, ""Failed to initialize interrupts for vport %u: %d\n"", vport->vport_id, err); goto intr_rel; } err = idpf_rx_bufs_init_all(vport); if (err) { dev_err(&adapter->pdev->dev, ""Failed to initialize RX buffers for vport %u: %d\n"", vport->vport_id, err); goto intr_rel; } err = idpf_queue_reg_init(vport); if (err) { dev_err(&adapter->pdev->dev, ""Failed to initialize queue registers for vport %u: %d\n"", vport->vport_id, err); goto intr_rel; } idpf_rx_init_buf_tail(vport); idpf_vport_intr_ena(vport); err = idpf_send_config_queues_msg(vport); if (err) { dev_err(&adapter->pdev->dev, ""Failed to configure queues for vport %u, %d\n"", vport->vport_id, err); goto intr_deinit; } err = idpf_send_map_unmap_queue_vector_msg(vport, true); if (err) { dev_err(&adapter->pdev->dev, ""Failed to map queue vectors for vport %u: %d\n"", vport->vport_id, err); goto intr_deinit; } err = idpf_send_enable_queues_msg(vport); if (err) { dev_err(&adapter->pdev->dev, ""Failed to enable queues for vport %u: %d\n"", vport->vport_id, err); goto unmap_queue_vectors; } err = idpf_send_enable_vport_msg(vport); if (err) { dev_err(&adapter->pdev->dev, ""Failed to enable vport %u: %d\n"", vport->vport_id, err); err = -EAGAIN; goto disable_queues; } idpf_restore_features(vport); vport_config = adapter->vport_config[vport->idx]; if (vport_config->user_config.rss_data.rss_lut) err = idpf_config_rss(vport); else err = idpf_init_rss(vport); if (err) { dev_err(&adapter->pdev->dev, ""Failed to initialize RSS for vport %u: %d\n"", vport->vport_id, err); goto disable_vport; } err = idpf_up_complete(vport); if (err) { dev_err(&adapter->pdev->dev, ""Failed to complete interface up for vport %u: %d\n"", vport->vport_id, err); goto deinit_rss; } return 0; deinit_rss: idpf_deinit_rss(vport); disable_vport: idpf_send_disable_vport_msg(vport); disable_queues: idpf_send_disable_queues_msg(vport); unmap_queue_vectors: idpf_send_map_unmap_queue_vector_msg(vport, false); intr_deinit: idpf_vport_intr_deinit(vport); intr_rel: idpf_vport_intr_rel(vport); queues_rel: idpf_vport_queues_rel(vport); return err; }","- if (alloc_res) {
- err = idpf_vport_queues_alloc(vport);
- if (err)
- return err;
- }
+ static int idpf_init_vector_stack(struct idpf_adapter *adapter)
+ {","#include ""idpf.h"" #include ""idpf_virtchnl.h"" static const struct net_device_ops idpf_netdev_ops; static int idpf_init_vector_stack(struct idpf_adapter *adapter) { struct idpf_vector_lifo *stack; u16 min_vec; u32 i; mutex_lock(&adapter->vector_lock); min_vec = adapter->num_msix_entries - adapter->num_avail_msix; stack = &adapter->vector_stack; stack->size = adapter->num_msix_entries; stack->base = min_vec; stack->top = min_vec; stack->vec_idx = kcalloc(stack->size, sizeof(u16), GFP_KERNEL); if (!stack->vec_idx) { mutex_unlock(&adapter->vector_lock); return -ENOMEM; } for (i = 0; i < stack->size; i++) stack->vec_idx[i] = i; mutex_unlock(&adapter->vector_lock); return 0; }"
1188----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53179/bad/smb2transport.c----smb2_calc_signature,"smb2_calc_signature(struct smb_rqst *rqst, struct TCP_Server_Info *server, bool allocate_crypto) { int rc; unsigned char smb2_signature[SMB2_HMACSHA256_SIZE]; unsigned char *sigptr = smb2_signature; struct kvec *iov = rqst->rq_iov; struct smb2_hdr *shdr = (struct smb2_hdr *)iov[0].iov_base; <S2SV_StartVul> struct cifs_ses *ses; <S2SV_EndVul> struct shash_desc *shash = NULL; struct smb_rqst drqst; <S2SV_StartVul> ses = smb2_find_smb_ses(server, le64_to_cpu(shdr->SessionId)); <S2SV_EndVul> <S2SV_StartVul> if (unlikely(!ses)) { <S2SV_EndVul> <S2SV_StartVul> cifs_server_dbg(FYI, ""%s: Could not find session\n"", __func__); <S2SV_EndVul> <S2SV_StartVul> return -ENOENT; <S2SV_EndVul> } memset(smb2_signature, 0x0, SMB2_HMACSHA256_SIZE); memset(shdr->Signature, 0x0, SMB2_SIGNATURE_SIZE); if (allocate_crypto) { rc = cifs_alloc_hash(""hmac(sha256)"", &shash); if (rc) { cifs_server_dbg(VFS, ""%s: sha256 alloc failed\n"", __func__); goto out; } } else { shash = server->secmech.hmacsha256; } <S2SV_StartVul> rc = crypto_shash_setkey(shash->tfm, ses->auth_key.response, <S2SV_EndVul> <S2SV_StartVul> SMB2_NTLMV2_SESSKEY_SIZE); <S2SV_EndVul> if (rc) { cifs_server_dbg(VFS, ""%s: Could not update with response\n"", __func__); goto out; } rc = crypto_shash_init(shash); if (rc) { cifs_server_dbg(VFS, ""%s: Could not init sha256"", __func__); goto out; } drqst = *rqst; if (drqst.rq_nvec >= 2 && iov[0].iov_len == 4) { rc = crypto_shash_update(shash, iov[0].iov_base, iov[0].iov_len); if (rc) { cifs_server_dbg(VFS, ""%s: Could not update with payload\n"", __func__); goto out; } drqst.rq_iov++; drqst.rq_nvec--; } rc = __cifs_calc_signature(&drqst, server, sigptr, shash); if (!rc) memcpy(shdr->Signature, sigptr, SMB2_SIGNATURE_SIZE); out: if (allocate_crypto) cifs_free_hash(&shash); <S2SV_StartVul> if (ses) <S2SV_EndVul> <S2SV_StartVul> cifs_put_smb_ses(ses); <S2SV_EndVul> return rc; }","- struct cifs_ses *ses;
- ses = smb2_find_smb_ses(server, le64_to_cpu(shdr->SessionId));
- if (unlikely(!ses)) {
- cifs_server_dbg(FYI, ""%s: Could not find session\n"", __func__);
- return -ENOENT;
- rc = crypto_shash_setkey(shash->tfm, ses->auth_key.response,
- SMB2_NTLMV2_SESSKEY_SIZE);
- if (ses)
- cifs_put_smb_ses(ses);
+ __u64 sid = le64_to_cpu(shdr->SessionId);
+ u8 key[SMB2_NTLMV2_SESSKEY_SIZE];
+ rc = smb2_get_sign_key(server, sid, key);
+ if (unlikely(rc)) {
+ cifs_server_dbg(FYI, ""%s: [sesid=0x%llx] couldn't find signing key: %d\n"",
+ __func__, sid, rc);
+ return rc;
+ }
+ }
+ rc = crypto_shash_setkey(shash->tfm, key, sizeof(key));
+ return rc;
+ }","smb2_calc_signature(struct smb_rqst *rqst, struct TCP_Server_Info *server, bool allocate_crypto) { int rc; unsigned char smb2_signature[SMB2_HMACSHA256_SIZE]; unsigned char *sigptr = smb2_signature; struct kvec *iov = rqst->rq_iov; struct smb2_hdr *shdr = (struct smb2_hdr *)iov[0].iov_base; struct shash_desc *shash = NULL; struct smb_rqst drqst; __u64 sid = le64_to_cpu(shdr->SessionId); u8 key[SMB2_NTLMV2_SESSKEY_SIZE]; rc = smb2_get_sign_key(server, sid, key); if (unlikely(rc)) { cifs_server_dbg(FYI, ""%s: [sesid=0x%llx] couldn't find signing key: %d\n"", __func__, sid, rc); return rc; } memset(smb2_signature, 0x0, SMB2_HMACSHA256_SIZE); memset(shdr->Signature, 0x0, SMB2_SIGNATURE_SIZE); if (allocate_crypto) { rc = cifs_alloc_hash(""hmac(sha256)"", &shash); if (rc) { cifs_server_dbg(VFS, ""%s: sha256 alloc failed\n"", __func__); goto out; } } else { shash = server->secmech.hmacsha256; } rc = crypto_shash_setkey(shash->tfm, key, sizeof(key)); if (rc) { cifs_server_dbg(VFS, ""%s: Could not update with response\n"", __func__); goto out; } rc = crypto_shash_init(shash); if (rc) { cifs_server_dbg(VFS, ""%s: Could not init sha256"", __func__); goto out; } drqst = *rqst; if (drqst.rq_nvec >= 2 && iov[0].iov_len == 4) { rc = crypto_shash_update(shash, iov[0].iov_base, iov[0].iov_len); if (rc) { cifs_server_dbg(VFS, ""%s: Could not update with payload\n"", __func__); goto out; } drqst.rq_iov++; drqst.rq_nvec--; } rc = __cifs_calc_signature(&drqst, server, sigptr, shash); if (!rc) memcpy(shdr->Signature, sigptr, SMB2_SIGNATURE_SIZE); out: if (allocate_crypto) cifs_free_hash(&shash); return rc; }"
875----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50131/bad/trace_probe.c----traceprobe_parse_event_name,"int traceprobe_parse_event_name(const char **pevent, const char **pgroup, char *buf, int offset) { const char *slash, *event = *pevent; int len; slash = strchr(event, '/'); if (!slash) slash = strchr(event, '.'); if (slash) { if (slash == event) { trace_probe_log_err(offset, NO_GROUP_NAME); return -EINVAL; } if (slash - event + 1 > MAX_EVENT_NAME_LEN) { trace_probe_log_err(offset, GROUP_TOO_LONG); return -EINVAL; } strscpy(buf, event, slash - event + 1); if (!is_good_system_name(buf)) { trace_probe_log_err(offset, BAD_GROUP_NAME); return -EINVAL; } *pgroup = buf; *pevent = slash + 1; offset += slash - event + 1; event = *pevent; } len = strlen(event); if (len == 0) { if (slash) { *pevent = NULL; return 0; } trace_probe_log_err(offset, NO_EVENT_NAME); return -EINVAL; <S2SV_StartVul> } else if (len > MAX_EVENT_NAME_LEN) { <S2SV_EndVul> trace_probe_log_err(offset, EVENT_TOO_LONG); return -EINVAL; } if (!is_good_name(event)) { trace_probe_log_err(offset, BAD_EVENT_NAME); return -EINVAL; } return 0; }","- } else if (len > MAX_EVENT_NAME_LEN) {
+ } else if (len >= MAX_EVENT_NAME_LEN) {","int traceprobe_parse_event_name(const char **pevent, const char **pgroup, char *buf, int offset) { const char *slash, *event = *pevent; int len; slash = strchr(event, '/'); if (!slash) slash = strchr(event, '.'); if (slash) { if (slash == event) { trace_probe_log_err(offset, NO_GROUP_NAME); return -EINVAL; } if (slash - event + 1 > MAX_EVENT_NAME_LEN) { trace_probe_log_err(offset, GROUP_TOO_LONG); return -EINVAL; } strscpy(buf, event, slash - event + 1); if (!is_good_system_name(buf)) { trace_probe_log_err(offset, BAD_GROUP_NAME); return -EINVAL; } *pgroup = buf; *pevent = slash + 1; offset += slash - event + 1; event = *pevent; } len = strlen(event); if (len == 0) { if (slash) { *pevent = NULL; return 0; } trace_probe_log_err(offset, NO_EVENT_NAME); return -EINVAL; } else if (len >= MAX_EVENT_NAME_LEN) { trace_probe_log_err(offset, EVENT_TOO_LONG); return -EINVAL; } if (!is_good_name(event)) { trace_probe_log_err(offset, BAD_EVENT_NAME); return -EINVAL; } return 0; }"
629----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49935/bad/acpi_pad.c----exit_round_robin,"static void exit_round_robin(unsigned int tsk_index) { struct cpumask *pad_busy_cpus = to_cpumask(pad_busy_cpus_bits); <S2SV_StartVul> cpumask_clear_cpu(tsk_in_cpu[tsk_index], pad_busy_cpus); <S2SV_EndVul> <S2SV_StartVul> tsk_in_cpu[tsk_index] = -1; <S2SV_EndVul> }","- cpumask_clear_cpu(tsk_in_cpu[tsk_index], pad_busy_cpus);
- tsk_in_cpu[tsk_index] = -1;
+ if (tsk_in_cpu[tsk_index] != -1) {
+ cpumask_clear_cpu(tsk_in_cpu[tsk_index], pad_busy_cpus);
+ tsk_in_cpu[tsk_index] = -1;
+ }
+ }","static void exit_round_robin(unsigned int tsk_index) { struct cpumask *pad_busy_cpus = to_cpumask(pad_busy_cpus_bits); if (tsk_in_cpu[tsk_index] != -1) { cpumask_clear_cpu(tsk_in_cpu[tsk_index], pad_busy_cpus); tsk_in_cpu[tsk_index] = -1; } }"
1352----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56673/bad/init.c----free_pmd_table,"static void __meminit free_pmd_table(pmd_t *pmd_start, pud_t *pud) { struct page *page = pud_page(*pud); struct ptdesc *ptdesc = page_ptdesc(page); pmd_t *pmd; int i; for (i = 0; i < PTRS_PER_PMD; i++) { pmd = pmd_start + i; if (!pmd_none(*pmd)) return; } <S2SV_StartVul> pagetable_pmd_dtor(ptdesc); <S2SV_EndVul> if (PageReserved(page)) free_reserved_page(page); else pagetable_free(ptdesc); pud_clear(pud); }","- pagetable_pmd_dtor(ptdesc);
+ #include <linux/swiotlb.h>
+ #include <linux/sizes.h>","#include <linux/init.h> #include <linux/mm.h> #include <linux/memblock.h> #include <linux/initrd.h> #include <linux/swap.h> #include <linux/swiotlb.h> #include <linux/sizes.h> #include <linux/of_fdt.h> #include <linux/of_reserved_mem.h> #include <linux/libfdt.h> #include <linux/set_memory.h> #include <linux/dma-map-ops.h> #include <linux/crash_dump.h> #include <linux/hugetlb.h> #ifdef CONFIG_RELOCATABLE #include <linux/elf.h> #endif #include <linux/kfence.h> #include <linux/execmem.h> #include <asm/fixmap.h> #include <asm/io.h> #include <asm/kasan.h> #include <asm/numa.h> #include <asm/pgtable.h> #include <asm/sections.h> #include <asm/soc.h> #include <asm/tlbflush.h> #include ""../kernel/head.h"" u64 new_vmalloc[NR_CPUS / sizeof(u64) + 1]; struct kernel_mapping kernel_map __ro_after_init; EXPORT_SYMBOL(kernel_map); #ifdef CONFIG_XIP_KERNEL #define kernel_map (*(struct kernel_mapping *)XIP_FIXUP(&kernel_map)) #endif #ifdef CONFIG_64BIT u64 satp_mode __ro_after_init = !IS_ENABLED(CONFIG_XIP_KERNEL) ? SATP_MODE_57 : SATP_MODE_39; #else u64 satp_mode __ro_after_init = SATP_MODE_32; #endif EXPORT_SYMBOL(satp_mode); #ifdef CONFIG_64BIT bool pgtable_l4_enabled __ro_after_init = !IS_ENABLED(CONFIG_XIP_KERNEL); bool pgtable_l5_enabled __ro_after_init = !IS_ENABLED(CONFIG_XIP_KERNEL); EXPORT_SYMBOL(pgtable_l4_enabled); EXPORT_SYMBOL(pgtable_l5_enabled); #endif phys_addr_t phys_ram_base __ro_after_init; EXPORT_SYMBOL(phys_ram_base); unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)] __page_aligned_bss; EXPORT_SYMBOL(empty_zero_page); extern char _start[]; void *_dtb_early_va __initdata; uintptr_t _dtb_early_pa __initdata; phys_addr_t dma32_phys_limit __initdata; static void __init zone_sizes_init(void) { unsigned long max_zone_pfns[MAX_NR_ZONES] = { 0, }; #ifdef CONFIG_ZONE_DMA32 max_zone_pfns[ZONE_DMA32] = PFN_DOWN(dma32_phys_limit); #endif max_zone_pfns[ZONE_NORMAL] = max_low_pfn; free_area_init(max_zone_pfns); }"
1154----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53130/bad/mdt.c----nilfs_mdt_create_block,"static int nilfs_mdt_create_block(struct inode *inode, unsigned long block, struct buffer_head **out_bh, void (*init_block)(struct inode *, struct buffer_head *, void *)) { struct super_block *sb = inode->i_sb; struct nilfs_transaction_info ti; struct buffer_head *bh; int err; nilfs_transaction_begin(sb, &ti, 0); err = -ENOMEM; bh = nilfs_grab_buffer(inode, inode->i_mapping, block, 0); if (unlikely(!bh)) goto failed_unlock; err = -EEXIST; if (buffer_uptodate(bh)) goto failed_bh; wait_on_buffer(bh); if (buffer_uptodate(bh)) goto failed_bh; <S2SV_StartVul> bh->b_bdev = sb->s_bdev; <S2SV_EndVul> err = nilfs_mdt_insert_new_block(inode, block, bh, init_block); if (likely(!err)) { get_bh(bh); *out_bh = bh; } failed_bh: unlock_page(bh->b_page); put_page(bh->b_page); brelse(bh); failed_unlock: if (likely(!err)) err = nilfs_transaction_commit(sb); else nilfs_transaction_abort(sb); return err; }",- bh->b_bdev = sb->s_bdev;,"static int nilfs_mdt_create_block(struct inode *inode, unsigned long block, struct buffer_head **out_bh, void (*init_block)(struct inode *, struct buffer_head *, void *)) { struct super_block *sb = inode->i_sb; struct nilfs_transaction_info ti; struct buffer_head *bh; int err; nilfs_transaction_begin(sb, &ti, 0); err = -ENOMEM; bh = nilfs_grab_buffer(inode, inode->i_mapping, block, 0); if (unlikely(!bh)) goto failed_unlock; err = -EEXIST; if (buffer_uptodate(bh)) goto failed_bh; wait_on_buffer(bh); if (buffer_uptodate(bh)) goto failed_bh; err = nilfs_mdt_insert_new_block(inode, block, bh, init_block); if (likely(!err)) { get_bh(bh); *out_bh = bh; } failed_bh: unlock_page(bh->b_page); put_page(bh->b_page); brelse(bh); failed_unlock: if (likely(!err)) err = nilfs_transaction_commit(sb); else nilfs_transaction_abort(sb); return err; }"
103----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44964/bad/idpf_lib.c----idpf_initiate_soft_reset,"int idpf_initiate_soft_reset(struct idpf_vport *vport, enum idpf_vport_reset_cause reset_cause) { struct idpf_netdev_priv *np = netdev_priv(vport->netdev); enum idpf_vport_state current_state = np->state; struct idpf_adapter *adapter = vport->adapter; struct idpf_vport *new_vport; int err; new_vport = kzalloc(sizeof(*vport), GFP_KERNEL); if (!new_vport) return -ENOMEM; memcpy(new_vport, vport, offsetof(struct idpf_vport, link_speed_mbps)); switch (reset_cause) { case IDPF_SR_Q_CHANGE: err = idpf_vport_adjust_qs(new_vport); if (err) goto free_vport; break; case IDPF_SR_Q_DESC_CHANGE: idpf_vport_calc_num_q_desc(new_vport); break; case IDPF_SR_MTU_CHANGE: case IDPF_SR_RSC_CHANGE: break; default: dev_err(&adapter->pdev->dev, ""Unhandled soft reset cause\n""); err = -EINVAL; goto free_vport; <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> err = idpf_vport_queues_alloc(new_vport); <S2SV_EndVul> <S2SV_StartVul> if (err) <S2SV_EndVul> <S2SV_StartVul> goto free_vport; <S2SV_EndVul> if (current_state <= __IDPF_VPORT_DOWN) { idpf_send_delete_queues_msg(vport); } else { set_bit(IDPF_VPORT_DEL_QUEUES, vport->flags); idpf_vport_stop(vport); } idpf_deinit_rss(vport); err = idpf_send_add_queues_msg(vport, new_vport->num_txq, new_vport->num_complq, new_vport->num_rxq, new_vport->num_bufq); if (err) goto err_reset; memcpy(vport, new_vport, offsetof(struct idpf_vport, link_speed_mbps)); if (reset_cause == IDPF_SR_Q_CHANGE) idpf_vport_alloc_vec_indexes(vport); err = idpf_set_real_num_queues(vport); <S2SV_StartVul> if (err) <S2SV_EndVul> <S2SV_StartVul> goto err_reset; <S2SV_EndVul> if (current_state == __IDPF_VPORT_UP) <S2SV_StartVul> err = idpf_vport_open(vport, false); <S2SV_EndVul> kfree(new_vport); <S2SV_StartVul> return err; <S2SV_EndVul> err_reset: <S2SV_StartVul> idpf_vport_queues_rel(new_vport); <S2SV_EndVul> free_vport: kfree(new_vport); return err; }","- }
- err = idpf_vport_queues_alloc(new_vport);
- if (err)
- goto free_vport;
- if (err)
- goto err_reset;
- err = idpf_vport_open(vport, false);
- return err;
- idpf_vport_queues_rel(new_vport);
+ if (err)
+ goto err_open;
+ if (current_state == __IDPF_VPORT_UP)
+ err = idpf_vport_open(vport);
+ return err;
+ idpf_send_add_queues_msg(vport, vport->num_txq, vport->num_complq,
+ vport->num_rxq, vport->num_bufq);
+ err_open:
+ if (current_state == __IDPF_VPORT_UP)
+ idpf_vport_open(vport);","int idpf_initiate_soft_reset(struct idpf_vport *vport, enum idpf_vport_reset_cause reset_cause) { struct idpf_netdev_priv *np = netdev_priv(vport->netdev); enum idpf_vport_state current_state = np->state; struct idpf_adapter *adapter = vport->adapter; struct idpf_vport *new_vport; int err; new_vport = kzalloc(sizeof(*vport), GFP_KERNEL); if (!new_vport) return -ENOMEM; memcpy(new_vport, vport, offsetof(struct idpf_vport, link_speed_mbps)); switch (reset_cause) { case IDPF_SR_Q_CHANGE: err = idpf_vport_adjust_qs(new_vport); if (err) goto free_vport; break; case IDPF_SR_Q_DESC_CHANGE: idpf_vport_calc_num_q_desc(new_vport); break; case IDPF_SR_MTU_CHANGE: case IDPF_SR_RSC_CHANGE: break; default: dev_err(&adapter->pdev->dev, ""Unhandled soft reset cause\n""); err = -EINVAL; goto free_vport; } if (current_state <= __IDPF_VPORT_DOWN) { idpf_send_delete_queues_msg(vport); } else { set_bit(IDPF_VPORT_DEL_QUEUES, vport->flags); idpf_vport_stop(vport); } idpf_deinit_rss(vport); err = idpf_send_add_queues_msg(vport, new_vport->num_txq, new_vport->num_complq, new_vport->num_rxq, new_vport->num_bufq); if (err) goto err_reset; memcpy(vport, new_vport, offsetof(struct idpf_vport, link_speed_mbps)); if (reset_cause == IDPF_SR_Q_CHANGE) idpf_vport_alloc_vec_indexes(vport); err = idpf_set_real_num_queues(vport); if (err) goto err_open; if (current_state == __IDPF_VPORT_UP) err = idpf_vport_open(vport); kfree(new_vport); return err; err_reset: idpf_send_add_queues_msg(vport, vport->num_txq, vport->num_complq, vport->num_rxq, vport->num_bufq); err_open: if (current_state == __IDPF_VPORT_UP) idpf_vport_open(vport); free_vport: kfree(new_vport); return err; }"
405----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46866/bad/xe_drm_client.c----show_meminfo,"static void show_meminfo(struct drm_printer *p, struct drm_file *file) { struct drm_memory_stats stats[TTM_NUM_MEM_TYPES] = {}; struct xe_file *xef = file->driver_priv; struct ttm_device *bdev = &xef->xe->ttm; struct ttm_resource_manager *man; struct xe_drm_client *client; struct drm_gem_object *obj; struct xe_bo *bo; LLIST_HEAD(deferred); unsigned int id; u32 mem_type; client = xef->client; spin_lock(&file->table_lock); idr_for_each_entry(&file->object_idr, obj, id) { struct xe_bo *bo = gem_to_xe_bo(obj); <S2SV_StartVul> bo_meminfo(bo, stats); <S2SV_EndVul> } spin_unlock(&file->table_lock); spin_lock(&client->bos_lock); list_for_each_entry(bo, &client->bos_list, client_link) { if (!kref_get_unless_zero(&bo->ttm.base.refcount)) continue; <S2SV_StartVul> bo_meminfo(bo, stats); <S2SV_EndVul> xe_bo_put_deferred(bo, &deferred); } spin_unlock(&client->bos_lock); xe_bo_put_commit(&deferred); for (mem_type = XE_PL_SYSTEM; mem_type < TTM_NUM_MEM_TYPES; ++mem_type) { if (!xe_mem_type_to_name[mem_type]) continue; man = ttm_manager_type(bdev, mem_type); if (man) { drm_print_memory_stats(p, &stats[mem_type], DRM_GEM_OBJECT_RESIDENT | (mem_type != XE_PL_SYSTEM ? 0 : DRM_GEM_OBJECT_PURGEABLE), xe_mem_type_to_name[mem_type]); } } }","- bo_meminfo(bo, stats);
- bo_meminfo(bo, stats);
+ if (dma_resv_trylock(bo->ttm.base.resv)) {
+ bo_meminfo(bo, stats);
+ xe_bo_unlock(bo);
+ } else {
+ xe_bo_get(bo);
+ spin_unlock(&file->table_lock);
+ xe_bo_lock(bo, false);
+ bo_meminfo(bo, stats);
+ xe_bo_unlock(bo);
+ xe_bo_put(bo);
+ spin_lock(&file->table_lock);
+ }
+ }
+ spin_unlock(&file->table_lock);
+ if (dma_resv_trylock(bo->ttm.base.resv)) {
+ bo_meminfo(bo, stats);
+ xe_bo_unlock(bo);
+ } else {
+ spin_unlock(&client->bos_lock);
+ xe_bo_lock(bo, false);
+ bo_meminfo(bo, stats);
+ xe_bo_unlock(bo);
+ spin_lock(&client->bos_lock);
+ xe_assert(xef->xe, !list_empty(&bo->client_link));
+ }
+ }
+ spin_unlock(&client->bos_lock);","static void show_meminfo(struct drm_printer *p, struct drm_file *file) { struct drm_memory_stats stats[TTM_NUM_MEM_TYPES] = {}; struct xe_file *xef = file->driver_priv; struct ttm_device *bdev = &xef->xe->ttm; struct ttm_resource_manager *man; struct xe_drm_client *client; struct drm_gem_object *obj; struct xe_bo *bo; LLIST_HEAD(deferred); unsigned int id; u32 mem_type; client = xef->client; spin_lock(&file->table_lock); idr_for_each_entry(&file->object_idr, obj, id) { struct xe_bo *bo = gem_to_xe_bo(obj); if (dma_resv_trylock(bo->ttm.base.resv)) { bo_meminfo(bo, stats); xe_bo_unlock(bo); } else { xe_bo_get(bo); spin_unlock(&file->table_lock); xe_bo_lock(bo, false); bo_meminfo(bo, stats); xe_bo_unlock(bo); xe_bo_put(bo); spin_lock(&file->table_lock); } } spin_unlock(&file->table_lock); spin_lock(&client->bos_lock); list_for_each_entry(bo, &client->bos_list, client_link) { if (!kref_get_unless_zero(&bo->ttm.base.refcount)) continue; if (dma_resv_trylock(bo->ttm.base.resv)) { bo_meminfo(bo, stats); xe_bo_unlock(bo); } else { spin_unlock(&client->bos_lock); xe_bo_lock(bo, false); bo_meminfo(bo, stats); xe_bo_unlock(bo); spin_lock(&client->bos_lock); xe_assert(xef->xe, !list_empty(&bo->client_link)); } xe_bo_put_deferred(bo, &deferred); } spin_unlock(&client->bos_lock); xe_bo_put_commit(&deferred); for (mem_type = XE_PL_SYSTEM; mem_type < TTM_NUM_MEM_TYPES; ++mem_type) { if (!xe_mem_type_to_name[mem_type]) continue; man = ttm_manager_type(bdev, mem_type); if (man) { drm_print_memory_stats(p, &stats[mem_type], DRM_GEM_OBJECT_RESIDENT | (mem_type != XE_PL_SYSTEM ? 0 : DRM_GEM_OBJECT_PURGEABLE), xe_mem_type_to_name[mem_type]); } } }"
1041----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50282/bad/amdgpu_debugfs.c----amdgpu_debugfs_regs_smc_read,"static ssize_t amdgpu_debugfs_regs_smc_read(struct file *f, char __user *buf, size_t size, loff_t *pos) { struct amdgpu_device *adev = file_inode(f)->i_private; ssize_t result = 0; int r; if (!adev->smc_rreg) return -EOPNOTSUPP; <S2SV_StartVul> if (size & 0x3 || *pos & 0x3) <S2SV_EndVul> return -EINVAL; while (size) { uint32_t value; value = RREG32_SMC(*pos); r = put_user(value, (uint32_t *)buf); if (r) return r; result += 4; buf += 4; *pos += 4; size -= 4; } return result; }","- if (size & 0x3 || *pos & 0x3)
+ if (size > 4096 || size & 0x3 || *pos & 0x3)","static ssize_t amdgpu_debugfs_regs_smc_read(struct file *f, char __user *buf, size_t size, loff_t *pos) { struct amdgpu_device *adev = file_inode(f)->i_private; ssize_t result = 0; int r; if (!adev->smc_rreg) return -EOPNOTSUPP; if (size > 4096 || size & 0x3 || *pos & 0x3) return -EINVAL; while (size) { uint32_t value; value = RREG32_SMC(*pos); r = put_user(value, (uint32_t *)buf); if (r) return r; result += 4; buf += 4; *pos += 4; size -= 4; } return result; }"
25----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-43899/bad/dcn20_resource.c----dcn20_get_dcc_compression_cap,"bool dcn20_get_dcc_compression_cap(const struct dc *dc, const struct dc_dcc_surface_param *input, struct dc_surface_dcc_cap *output) { <S2SV_StartVul> return dc->res_pool->hubbub->funcs->get_dcc_compression_cap( <S2SV_EndVul> <S2SV_StartVul> dc->res_pool->hubbub, <S2SV_EndVul> <S2SV_StartVul> input, <S2SV_EndVul> <S2SV_StartVul> output); <S2SV_EndVul> }","- return dc->res_pool->hubbub->funcs->get_dcc_compression_cap(
- dc->res_pool->hubbub,
- input,
- output);
+ if (dc->res_pool->hubbub->funcs->get_dcc_compression_cap)
+ return dc->res_pool->hubbub->funcs->get_dcc_compression_cap(
+ dc->res_pool->hubbub, input, output);
+ return false;","bool dcn20_get_dcc_compression_cap(const struct dc *dc, const struct dc_dcc_surface_param *input, struct dc_surface_dcc_cap *output) { if (dc->res_pool->hubbub->funcs->get_dcc_compression_cap) return dc->res_pool->hubbub->funcs->get_dcc_compression_cap( dc->res_pool->hubbub, input, output); return false; }"
1069----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53053/bad/ufshcd.c----ufshcd_update_rtc,"static void ufshcd_update_rtc(struct ufs_hba *hba) { struct timespec64 ts64; int err; u32 val; ktime_get_real_ts64(&ts64); if (ts64.tv_sec < hba->dev_info.rtc_time_baseline) { dev_warn_once(hba->dev, ""%s: Current time precedes previous setting!\n"", __func__); return; } val = ts64.tv_sec - hba->dev_info.rtc_time_baseline; if (ufshcd_rpm_get_if_active(hba) <= 0) return; err = ufshcd_query_attr(hba, UPIU_QUERY_OPCODE_WRITE_ATTR, QUERY_ATTR_IDN_SECONDS_PASSED, 0, 0, &val); <S2SV_StartVul> ufshcd_rpm_put_sync(hba); <S2SV_EndVul> if (err) dev_err(hba->dev, ""%s: Failed to update rtc %d\n"", __func__, err); else if (hba->dev_info.rtc_type == UFS_RTC_RELATIVE) hba->dev_info.rtc_time_baseline = ts64.tv_sec; }","- ufshcd_rpm_put_sync(hba);
+ ufshcd_rpm_put(hba);","static void ufshcd_update_rtc(struct ufs_hba *hba) { struct timespec64 ts64; int err; u32 val; ktime_get_real_ts64(&ts64); if (ts64.tv_sec < hba->dev_info.rtc_time_baseline) { dev_warn_once(hba->dev, ""%s: Current time precedes previous setting!\n"", __func__); return; } val = ts64.tv_sec - hba->dev_info.rtc_time_baseline; if (ufshcd_rpm_get_if_active(hba) <= 0) return; err = ufshcd_query_attr(hba, UPIU_QUERY_OPCODE_WRITE_ATTR, QUERY_ATTR_IDN_SECONDS_PASSED, 0, 0, &val); ufshcd_rpm_put(hba); if (err) dev_err(hba->dev, ""%s: Failed to update rtc %d\n"", __func__, err); else if (hba->dev_info.rtc_type == UFS_RTC_RELATIVE) hba->dev_info.rtc_time_baseline = ts64.tv_sec; }"
370----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46834/bad/channels.c----ethnl_set_channels,"ethnl_set_channels(struct ethnl_req_info *req_info, struct genl_info *info) { unsigned int from_channel, old_total, i; bool mod = false, mod_combined = false; struct net_device *dev = req_info->dev; struct ethtool_channels channels = {}; struct nlattr **tb = info->attrs; u32 err_attr, max_rxfh_in_use; u64 max_rxnfc_in_use; int ret; dev->ethtool_ops->get_channels(dev, &channels); old_total = channels.combined_count + max(channels.rx_count, channels.tx_count); ethnl_update_u32(&channels.rx_count, tb[ETHTOOL_A_CHANNELS_RX_COUNT], &mod); ethnl_update_u32(&channels.tx_count, tb[ETHTOOL_A_CHANNELS_TX_COUNT], &mod); ethnl_update_u32(&channels.other_count, tb[ETHTOOL_A_CHANNELS_OTHER_COUNT], &mod); ethnl_update_u32(&channels.combined_count, tb[ETHTOOL_A_CHANNELS_COMBINED_COUNT], &mod_combined); mod |= mod_combined; if (!mod) return 0; if (channels.rx_count > channels.max_rx) err_attr = ETHTOOL_A_CHANNELS_RX_COUNT; else if (channels.tx_count > channels.max_tx) err_attr = ETHTOOL_A_CHANNELS_TX_COUNT; else if (channels.other_count > channels.max_other) err_attr = ETHTOOL_A_CHANNELS_OTHER_COUNT; else if (channels.combined_count > channels.max_combined) err_attr = ETHTOOL_A_CHANNELS_COMBINED_COUNT; else err_attr = 0; if (err_attr) { NL_SET_ERR_MSG_ATTR(info->extack, tb[err_attr], ""requested channel count exceeds maximum""); return -EINVAL; } if (!channels.combined_count && !channels.rx_count) err_attr = ETHTOOL_A_CHANNELS_RX_COUNT; else if (!channels.combined_count && !channels.tx_count) err_attr = ETHTOOL_A_CHANNELS_TX_COUNT; else err_attr = 0; if (err_attr) { if (mod_combined) err_attr = ETHTOOL_A_CHANNELS_COMBINED_COUNT; NL_SET_ERR_MSG_ATTR(info->extack, tb[err_attr], ""requested channel counts would result in no RX or TX channel being configured""); return -EINVAL; } if (ethtool_get_max_rxnfc_channel(dev, &max_rxnfc_in_use)) max_rxnfc_in_use = 0; if (!netif_is_rxfh_configured(dev) || ethtool_get_max_rxfh_channel(dev, &max_rxfh_in_use)) max_rxfh_in_use = 0; if (channels.combined_count + channels.rx_count <= max_rxfh_in_use) { GENL_SET_ERR_MSG(info, ""requested channel counts are too low for existing indirection table settings""); return -EINVAL; } if (channels.combined_count + channels.rx_count <= max_rxnfc_in_use) { GENL_SET_ERR_MSG(info, ""requested channel counts are too low for existing ntuple filter settings""); <S2SV_StartVul> return -EINVAL; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> from_channel = channels.combined_count + <S2SV_EndVul> min(channels.rx_count, channels.tx_count); for (i = from_channel; i < old_total; i++) if (xsk_get_pool_from_qid(dev, i)) { GENL_SET_ERR_MSG(info, ""requested channel counts are too low for existing zerocopy AF_XDP sockets""); return -EINVAL; } ret = dev->ethtool_ops->set_channels(dev, &channels); return ret < 0 ? ret : 1; }","- return -EINVAL;
- }
- from_channel = channels.combined_count +
+ from_channel = channels.combined_count +","ethnl_set_channels(struct ethnl_req_info *req_info, struct genl_info *info) { unsigned int from_channel, old_total, i; bool mod = false, mod_combined = false; struct net_device *dev = req_info->dev; struct ethtool_channels channels = {}; struct nlattr **tb = info->attrs; u32 err_attr, max_rxfh_in_use; u64 max_rxnfc_in_use; int ret; dev->ethtool_ops->get_channels(dev, &channels); old_total = channels.combined_count + max(channels.rx_count, channels.tx_count); ethnl_update_u32(&channels.rx_count, tb[ETHTOOL_A_CHANNELS_RX_COUNT], &mod); ethnl_update_u32(&channels.tx_count, tb[ETHTOOL_A_CHANNELS_TX_COUNT], &mod); ethnl_update_u32(&channels.other_count, tb[ETHTOOL_A_CHANNELS_OTHER_COUNT], &mod); ethnl_update_u32(&channels.combined_count, tb[ETHTOOL_A_CHANNELS_COMBINED_COUNT], &mod_combined); mod |= mod_combined; if (!mod) return 0; if (channels.rx_count > channels.max_rx) err_attr = ETHTOOL_A_CHANNELS_RX_COUNT; else if (channels.tx_count > channels.max_tx) err_attr = ETHTOOL_A_CHANNELS_TX_COUNT; else if (channels.other_count > channels.max_other) err_attr = ETHTOOL_A_CHANNELS_OTHER_COUNT; else if (channels.combined_count > channels.max_combined) err_attr = ETHTOOL_A_CHANNELS_COMBINED_COUNT; else err_attr = 0; if (err_attr) { NL_SET_ERR_MSG_ATTR(info->extack, tb[err_attr], ""requested channel count exceeds maximum""); return -EINVAL; } if (!channels.combined_count && !channels.rx_count) err_attr = ETHTOOL_A_CHANNELS_RX_COUNT; else if (!channels.combined_count && !channels.tx_count) err_attr = ETHTOOL_A_CHANNELS_TX_COUNT; else err_attr = 0; if (err_attr) { if (mod_combined) err_attr = ETHTOOL_A_CHANNELS_COMBINED_COUNT; NL_SET_ERR_MSG_ATTR(info->extack, tb[err_attr], ""requested channel counts would result in no RX or TX channel being configured""); return -EINVAL; } if (ethtool_get_max_rxnfc_channel(dev, &max_rxnfc_in_use)) max_rxnfc_in_use = 0; max_rxfh_in_use = ethtool_get_max_rxfh_channel(dev); if (channels.combined_count + channels.rx_count <= max_rxfh_in_use) { GENL_SET_ERR_MSG_FMT(info, ""requested channel counts are too low for existing indirection table (%d)"", max_rxfh_in_use); return -EINVAL; } if (channels.combined_count + channels.rx_count <= max_rxnfc_in_use) { GENL_SET_ERR_MSG(info, ""requested channel counts are too low for existing ntuple filter settings""); return -EINVAL; } from_channel = channels.combined_count + min(channels.rx_count, channels.tx_count); for (i = from_channel; i < old_total; i++) if (xsk_get_pool_from_qid(dev, i)) { GENL_SET_ERR_MSG(info, ""requested channel counts are too low for existing zerocopy AF_XDP sockets""); return -EINVAL; } ret = dev->ethtool_ops->set_channels(dev, &channels); return ret < 0 ? ret : 1; }"
104----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44964/bad/idpf_lib.c----idpf_init_task,"void idpf_init_task(struct work_struct *work) { struct idpf_vport_config *vport_config; struct idpf_vport_max_q max_q; struct idpf_adapter *adapter; struct idpf_netdev_priv *np; struct idpf_vport *vport; u16 num_default_vports; struct pci_dev *pdev; bool default_vport; int index, err; adapter = container_of(work, struct idpf_adapter, init_task.work); num_default_vports = idpf_get_default_vports(adapter); if (adapter->num_alloc_vports < num_default_vports) default_vport = true; else default_vport = false; err = idpf_vport_alloc_max_qs(adapter, &max_q); if (err) goto unwind_vports; err = idpf_send_create_vport_msg(adapter, &max_q); if (err) { idpf_vport_dealloc_max_qs(adapter, &max_q); goto unwind_vports; } pdev = adapter->pdev; vport = idpf_vport_alloc(adapter, &max_q); if (!vport) { err = -EFAULT; dev_err(&pdev->dev, ""failed to allocate vport: %d\n"", err); idpf_vport_dealloc_max_qs(adapter, &max_q); goto unwind_vports; } index = vport->idx; vport_config = adapter->vport_config[index]; init_waitqueue_head(&vport->sw_marker_wq); spin_lock_init(&vport_config->mac_filter_list_lock); INIT_LIST_HEAD(&vport_config->user_config.mac_filter_list); err = idpf_check_supported_desc_ids(vport); if (err) { dev_err(&pdev->dev, ""failed to get required descriptor ids\n""); goto cfg_netdev_err; } if (idpf_cfg_netdev(vport)) goto cfg_netdev_err; err = idpf_send_get_rx_ptype_msg(vport); if (err) goto handle_err; np = netdev_priv(vport->netdev); np->state = __IDPF_VPORT_DOWN; if (test_and_clear_bit(IDPF_VPORT_UP_REQUESTED, vport_config->flags)) <S2SV_StartVul> idpf_vport_open(vport, true); <S2SV_EndVul> if (adapter->num_alloc_vports < num_default_vports) { queue_delayed_work(adapter->init_wq, &adapter->init_task, msecs_to_jiffies(5 * (adapter->pdev->devfn & 0x07))); return; } for (index = 0; index < adapter->max_vports; index++) { if (adapter->netdevs[index] && !test_bit(IDPF_VPORT_REG_NETDEV, adapter->vport_config[index]->flags)) { register_netdev(adapter->netdevs[index]); set_bit(IDPF_VPORT_REG_NETDEV, adapter->vport_config[index]->flags); } } clear_bit(IDPF_HR_RESET_IN_PROG, adapter->flags); queue_delayed_work(adapter->stats_wq, &adapter->stats_task, msecs_to_jiffies(10 * (pdev->devfn & 0x07))); return; handle_err: idpf_decfg_netdev(vport); cfg_netdev_err: idpf_vport_rel(vport); adapter->vports[index] = NULL; unwind_vports: if (default_vport) { for (index = 0; index < adapter->max_vports; index++) { if (adapter->vports[index]) idpf_vport_dealloc(adapter->vports[index]); } } clear_bit(IDPF_HR_RESET_IN_PROG, adapter->flags); }","- idpf_vport_open(vport, true);
+ idpf_vport_open(vport);","void idpf_init_task(struct work_struct *work) { struct idpf_vport_config *vport_config; struct idpf_vport_max_q max_q; struct idpf_adapter *adapter; struct idpf_netdev_priv *np; struct idpf_vport *vport; u16 num_default_vports; struct pci_dev *pdev; bool default_vport; int index, err; adapter = container_of(work, struct idpf_adapter, init_task.work); num_default_vports = idpf_get_default_vports(adapter); if (adapter->num_alloc_vports < num_default_vports) default_vport = true; else default_vport = false; err = idpf_vport_alloc_max_qs(adapter, &max_q); if (err) goto unwind_vports; err = idpf_send_create_vport_msg(adapter, &max_q); if (err) { idpf_vport_dealloc_max_qs(adapter, &max_q); goto unwind_vports; } pdev = adapter->pdev; vport = idpf_vport_alloc(adapter, &max_q); if (!vport) { err = -EFAULT; dev_err(&pdev->dev, ""failed to allocate vport: %d\n"", err); idpf_vport_dealloc_max_qs(adapter, &max_q); goto unwind_vports; } index = vport->idx; vport_config = adapter->vport_config[index]; init_waitqueue_head(&vport->sw_marker_wq); spin_lock_init(&vport_config->mac_filter_list_lock); INIT_LIST_HEAD(&vport_config->user_config.mac_filter_list); err = idpf_check_supported_desc_ids(vport); if (err) { dev_err(&pdev->dev, ""failed to get required descriptor ids\n""); goto cfg_netdev_err; } if (idpf_cfg_netdev(vport)) goto cfg_netdev_err; err = idpf_send_get_rx_ptype_msg(vport); if (err) goto handle_err; np = netdev_priv(vport->netdev); np->state = __IDPF_VPORT_DOWN; if (test_and_clear_bit(IDPF_VPORT_UP_REQUESTED, vport_config->flags)) idpf_vport_open(vport); if (adapter->num_alloc_vports < num_default_vports) { queue_delayed_work(adapter->init_wq, &adapter->init_task, msecs_to_jiffies(5 * (adapter->pdev->devfn & 0x07))); return; } for (index = 0; index < adapter->max_vports; index++) { if (adapter->netdevs[index] && !test_bit(IDPF_VPORT_REG_NETDEV, adapter->vport_config[index]->flags)) { register_netdev(adapter->netdevs[index]); set_bit(IDPF_VPORT_REG_NETDEV, adapter->vport_config[index]->flags); } } clear_bit(IDPF_HR_RESET_IN_PROG, adapter->flags); queue_delayed_work(adapter->stats_wq, &adapter->stats_task, msecs_to_jiffies(10 * (pdev->devfn & 0x07))); return; handle_err: idpf_decfg_netdev(vport); cfg_netdev_err: idpf_vport_rel(vport); adapter->vports[index] = NULL; unwind_vports: if (default_vport) { for (index = 0; index < adapter->max_vports; index++) { if (adapter->vports[index]) idpf_vport_dealloc(adapter->vports[index]); } } clear_bit(IDPF_HR_RESET_IN_PROG, adapter->flags); }"
512----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47748/bad/vdpa.c----vhost_vdpa_setup_vq_irq,"static void vhost_vdpa_setup_vq_irq(struct vhost_vdpa *v, u16 qid) { struct vhost_virtqueue *vq = &v->vqs[qid]; const struct vdpa_config_ops *ops = v->vdpa->config; struct vdpa_device *vdpa = v->vdpa; int ret, irq; if (!ops->get_vq_irq) return; irq = ops->get_vq_irq(vdpa, qid); if (irq < 0) return; <S2SV_StartVul> irq_bypass_unregister_producer(&vq->call_ctx.producer); <S2SV_EndVul> if (!vq->call_ctx.ctx) return; <S2SV_StartVul> vq->call_ctx.producer.token = vq->call_ctx.ctx; <S2SV_EndVul> vq->call_ctx.producer.irq = irq; ret = irq_bypass_register_producer(&vq->call_ctx.producer); if (unlikely(ret)) dev_info(&v->dev, ""vq %u, irq bypass producer (token %p) registration fails, ret = %d\n"", qid, vq->call_ctx.producer.token, ret); }","- irq_bypass_unregister_producer(&vq->call_ctx.producer);
- vq->call_ctx.producer.token = vq->call_ctx.ctx;","static void vhost_vdpa_setup_vq_irq(struct vhost_vdpa *v, u16 qid) { struct vhost_virtqueue *vq = &v->vqs[qid]; const struct vdpa_config_ops *ops = v->vdpa->config; struct vdpa_device *vdpa = v->vdpa; int ret, irq; if (!ops->get_vq_irq) return; irq = ops->get_vq_irq(vdpa, qid); if (irq < 0) return; if (!vq->call_ctx.ctx) return; vq->call_ctx.producer.irq = irq; ret = irq_bypass_register_producer(&vq->call_ctx.producer); if (unlikely(ret)) dev_info(&v->dev, ""vq %u, irq bypass producer (token %p) registration fails, ret = %d\n"", qid, vq->call_ctx.producer.token, ret); }"
1538----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2025-21689/bad/quatech2.c----qt2_process_read_urb,"static void qt2_process_read_urb(struct urb *urb) { struct usb_serial *serial; struct qt2_serial_private *serial_priv; struct usb_serial_port *port; bool escapeflag; unsigned char *ch; int i; unsigned char newport; int len = urb->actual_length; if (!len) return; ch = urb->transfer_buffer; serial = urb->context; serial_priv = usb_get_serial_data(serial); port = serial->port[serial_priv->current_port]; for (i = 0; i < urb->actual_length; i++) { ch = (unsigned char *)urb->transfer_buffer + i; if ((i <= (len - 3)) && (*ch == QT2_CONTROL_BYTE) && (*(ch + 1) == QT2_CONTROL_BYTE)) { escapeflag = false; switch (*(ch + 2)) { case QT2_LINE_STATUS: case QT2_MODEM_STATUS: if (i > (len - 4)) { dev_warn(&port->dev, ""%s - status message too short\n"", __func__); break; } qt2_process_status(port, ch + 2); i += 3; escapeflag = true; break; case QT2_XMIT_HOLD: if (i > (len - 5)) { dev_warn(&port->dev, ""%s - xmit_empty message too short\n"", __func__); break; } i += 4; escapeflag = true; break; case QT2_CHANGE_PORT: if (i > (len - 4)) { dev_warn(&port->dev, ""%s - change_port message too short\n"", __func__); break; } tty_flip_buffer_push(&port->port); newport = *(ch + 3); <S2SV_StartVul> if (newport > serial->num_ports) { <S2SV_EndVul> dev_err(&port->dev, ""%s - port change to invalid port: %i\n"", __func__, newport); break; } serial_priv->current_port = newport; port = serial->port[serial_priv->current_port]; i += 3; escapeflag = true; break; case QT2_REC_FLUSH: case QT2_XMIT_FLUSH: i += 2; escapeflag = true; break; case QT2_CONTROL_ESCAPE: tty_insert_flip_string(&port->port, ch, 2); i += 2; escapeflag = true; break; default: dev_warn(&port->dev, ""%s - unsupported command %i\n"", __func__, *(ch + 2)); break; } if (escapeflag) continue; } tty_insert_flip_char(&port->port, *ch, TTY_NORMAL); } tty_flip_buffer_push(&port->port); }","- if (newport > serial->num_ports) {
+ if (newport >= serial->num_ports) {","static void qt2_process_read_urb(struct urb *urb) { struct usb_serial *serial; struct qt2_serial_private *serial_priv; struct usb_serial_port *port; bool escapeflag; unsigned char *ch; int i; unsigned char newport; int len = urb->actual_length; if (!len) return; ch = urb->transfer_buffer; serial = urb->context; serial_priv = usb_get_serial_data(serial); port = serial->port[serial_priv->current_port]; for (i = 0; i < urb->actual_length; i++) { ch = (unsigned char *)urb->transfer_buffer + i; if ((i <= (len - 3)) && (*ch == QT2_CONTROL_BYTE) && (*(ch + 1) == QT2_CONTROL_BYTE)) { escapeflag = false; switch (*(ch + 2)) { case QT2_LINE_STATUS: case QT2_MODEM_STATUS: if (i > (len - 4)) { dev_warn(&port->dev, ""%s - status message too short\n"", __func__); break; } qt2_process_status(port, ch + 2); i += 3; escapeflag = true; break; case QT2_XMIT_HOLD: if (i > (len - 5)) { dev_warn(&port->dev, ""%s - xmit_empty message too short\n"", __func__); break; } i += 4; escapeflag = true; break; case QT2_CHANGE_PORT: if (i > (len - 4)) { dev_warn(&port->dev, ""%s - change_port message too short\n"", __func__); break; } tty_flip_buffer_push(&port->port); newport = *(ch + 3); if (newport >= serial->num_ports) { dev_err(&port->dev, ""%s - port change to invalid port: %i\n"", __func__, newport); break; } serial_priv->current_port = newport; port = serial->port[serial_priv->current_port]; i += 3; escapeflag = true; break; case QT2_REC_FLUSH: case QT2_XMIT_FLUSH: i += 2; escapeflag = true; break; case QT2_CONTROL_ESCAPE: tty_insert_flip_string(&port->port, ch, 2); i += 2; escapeflag = true; break; default: dev_warn(&port->dev, ""%s - unsupported command %i\n"", __func__, *(ch + 2)); break; } if (escapeflag) continue; } tty_insert_flip_char(&port->port, *ch, TTY_NORMAL); } tty_flip_buffer_push(&port->port); }"
264----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46744/bad/inode.c----squashfs_read_inode,"int squashfs_read_inode(struct inode *inode, long long ino) { struct super_block *sb = inode->i_sb; struct squashfs_sb_info *msblk = sb->s_fs_info; u64 block = SQUASHFS_INODE_BLK(ino) + msblk->inode_table; int err, type, offset = SQUASHFS_INODE_OFFSET(ino); union squashfs_inode squashfs_ino; struct squashfs_base_inode *sqshb_ino = &squashfs_ino.base; int xattr_id = SQUASHFS_INVALID_XATTR; TRACE(""Entered squashfs_read_inode\n""); err = squashfs_read_metadata(sb, sqshb_ino, &block, &offset, sizeof(*sqshb_ino)); if (err < 0) goto failed_read; err = squashfs_new_inode(sb, inode, sqshb_ino); if (err) goto failed_read; block = SQUASHFS_INODE_BLK(ino) + msblk->inode_table; offset = SQUASHFS_INODE_OFFSET(ino); type = le16_to_cpu(sqshb_ino->inode_type); switch (type) { case SQUASHFS_REG_TYPE: { unsigned int frag_offset, frag; int frag_size; u64 frag_blk; struct squashfs_reg_inode *sqsh_ino = &squashfs_ino.reg; err = squashfs_read_metadata(sb, sqsh_ino, &block, &offset, sizeof(*sqsh_ino)); if (err < 0) goto failed_read; frag = le32_to_cpu(sqsh_ino->fragment); if (frag != SQUASHFS_INVALID_FRAG) { frag_offset = le32_to_cpu(sqsh_ino->offset); frag_size = squashfs_frag_lookup(sb, frag, &frag_blk); if (frag_size < 0) { err = frag_size; goto failed_read; } } else { frag_blk = SQUASHFS_INVALID_BLK; frag_size = 0; frag_offset = 0; } set_nlink(inode, 1); inode->i_size = le32_to_cpu(sqsh_ino->file_size); inode->i_fop = &generic_ro_fops; inode->i_mode |= S_IFREG; inode->i_blocks = ((inode->i_size - 1) >> 9) + 1; squashfs_i(inode)->fragment_block = frag_blk; squashfs_i(inode)->fragment_size = frag_size; squashfs_i(inode)->fragment_offset = frag_offset; squashfs_i(inode)->start = le32_to_cpu(sqsh_ino->start_block); squashfs_i(inode)->block_list_start = block; squashfs_i(inode)->offset = offset; inode->i_data.a_ops = &squashfs_aops; TRACE(""File inode %x:%x, start_block %llx, block_list_start "" ""%llx, offset %x\n"", SQUASHFS_INODE_BLK(ino), offset, squashfs_i(inode)->start, block, offset); break; } case SQUASHFS_LREG_TYPE: { unsigned int frag_offset, frag; int frag_size; u64 frag_blk; struct squashfs_lreg_inode *sqsh_ino = &squashfs_ino.lreg; err = squashfs_read_metadata(sb, sqsh_ino, &block, &offset, sizeof(*sqsh_ino)); if (err < 0) goto failed_read; frag = le32_to_cpu(sqsh_ino->fragment); if (frag != SQUASHFS_INVALID_FRAG) { frag_offset = le32_to_cpu(sqsh_ino->offset); frag_size = squashfs_frag_lookup(sb, frag, &frag_blk); if (frag_size < 0) { err = frag_size; goto failed_read; } } else { frag_blk = SQUASHFS_INVALID_BLK; frag_size = 0; frag_offset = 0; } xattr_id = le32_to_cpu(sqsh_ino->xattr); set_nlink(inode, le32_to_cpu(sqsh_ino->nlink)); inode->i_size = le64_to_cpu(sqsh_ino->file_size); inode->i_op = &squashfs_inode_ops; inode->i_fop = &generic_ro_fops; inode->i_mode |= S_IFREG; inode->i_blocks = (inode->i_size - le64_to_cpu(sqsh_ino->sparse) + 511) >> 9; squashfs_i(inode)->fragment_block = frag_blk; squashfs_i(inode)->fragment_size = frag_size; squashfs_i(inode)->fragment_offset = frag_offset; squashfs_i(inode)->start = le64_to_cpu(sqsh_ino->start_block); squashfs_i(inode)->block_list_start = block; squashfs_i(inode)->offset = offset; inode->i_data.a_ops = &squashfs_aops; TRACE(""File inode %x:%x, start_block %llx, block_list_start "" ""%llx, offset %x\n"", SQUASHFS_INODE_BLK(ino), offset, squashfs_i(inode)->start, block, offset); break; } case SQUASHFS_DIR_TYPE: { struct squashfs_dir_inode *sqsh_ino = &squashfs_ino.dir; err = squashfs_read_metadata(sb, sqsh_ino, &block, &offset, sizeof(*sqsh_ino)); if (err < 0) goto failed_read; set_nlink(inode, le32_to_cpu(sqsh_ino->nlink)); inode->i_size = le16_to_cpu(sqsh_ino->file_size); inode->i_op = &squashfs_dir_inode_ops; inode->i_fop = &squashfs_dir_ops; inode->i_mode |= S_IFDIR; squashfs_i(inode)->start = le32_to_cpu(sqsh_ino->start_block); squashfs_i(inode)->offset = le16_to_cpu(sqsh_ino->offset); squashfs_i(inode)->dir_idx_cnt = 0; squashfs_i(inode)->parent = le32_to_cpu(sqsh_ino->parent_inode); TRACE(""Directory inode %x:%x, start_block %llx, offset %x\n"", SQUASHFS_INODE_BLK(ino), offset, squashfs_i(inode)->start, le16_to_cpu(sqsh_ino->offset)); break; } case SQUASHFS_LDIR_TYPE: { struct squashfs_ldir_inode *sqsh_ino = &squashfs_ino.ldir; err = squashfs_read_metadata(sb, sqsh_ino, &block, &offset, sizeof(*sqsh_ino)); if (err < 0) goto failed_read; xattr_id = le32_to_cpu(sqsh_ino->xattr); set_nlink(inode, le32_to_cpu(sqsh_ino->nlink)); inode->i_size = le32_to_cpu(sqsh_ino->file_size); inode->i_op = &squashfs_dir_inode_ops; inode->i_fop = &squashfs_dir_ops; inode->i_mode |= S_IFDIR; squashfs_i(inode)->start = le32_to_cpu(sqsh_ino->start_block); squashfs_i(inode)->offset = le16_to_cpu(sqsh_ino->offset); squashfs_i(inode)->dir_idx_start = block; squashfs_i(inode)->dir_idx_offset = offset; squashfs_i(inode)->dir_idx_cnt = le16_to_cpu(sqsh_ino->i_count); squashfs_i(inode)->parent = le32_to_cpu(sqsh_ino->parent_inode); TRACE(""Long directory inode %x:%x, start_block %llx, offset "" ""%x\n"", SQUASHFS_INODE_BLK(ino), offset, squashfs_i(inode)->start, le16_to_cpu(sqsh_ino->offset)); break; } case SQUASHFS_SYMLINK_TYPE: case SQUASHFS_LSYMLINK_TYPE: { struct squashfs_symlink_inode *sqsh_ino = &squashfs_ino.symlink; err = squashfs_read_metadata(sb, sqsh_ino, &block, &offset, sizeof(*sqsh_ino)); if (err < 0) goto failed_read; <S2SV_StartVul> set_nlink(inode, le32_to_cpu(sqsh_ino->nlink)); <S2SV_EndVul> inode->i_size = le32_to_cpu(sqsh_ino->symlink_size); inode->i_op = &squashfs_symlink_inode_ops; inode_nohighmem(inode); inode->i_data.a_ops = &squashfs_symlink_aops; inode->i_mode |= S_IFLNK; squashfs_i(inode)->start = block; squashfs_i(inode)->offset = offset; if (type == SQUASHFS_LSYMLINK_TYPE) { __le32 xattr; err = squashfs_read_metadata(sb, NULL, &block, &offset, inode->i_size); if (err < 0) goto failed_read; err = squashfs_read_metadata(sb, &xattr, &block, &offset, sizeof(xattr)); if (err < 0) goto failed_read; xattr_id = le32_to_cpu(xattr); } TRACE(""Symbolic link inode %x:%x, start_block %llx, offset "" ""%x\n"", SQUASHFS_INODE_BLK(ino), offset, block, offset); break; } case SQUASHFS_BLKDEV_TYPE: case SQUASHFS_CHRDEV_TYPE: { struct squashfs_dev_inode *sqsh_ino = &squashfs_ino.dev; unsigned int rdev; err = squashfs_read_metadata(sb, sqsh_ino, &block, &offset, sizeof(*sqsh_ino)); if (err < 0) goto failed_read; if (type == SQUASHFS_CHRDEV_TYPE) inode->i_mode |= S_IFCHR; else inode->i_mode |= S_IFBLK; set_nlink(inode, le32_to_cpu(sqsh_ino->nlink)); rdev = le32_to_cpu(sqsh_ino->rdev); init_special_inode(inode, inode->i_mode, new_decode_dev(rdev)); TRACE(""Device inode %x:%x, rdev %x\n"", SQUASHFS_INODE_BLK(ino), offset, rdev); break; } case SQUASHFS_LBLKDEV_TYPE: case SQUASHFS_LCHRDEV_TYPE: { struct squashfs_ldev_inode *sqsh_ino = &squashfs_ino.ldev; unsigned int rdev; err = squashfs_read_metadata(sb, sqsh_ino, &block, &offset, sizeof(*sqsh_ino)); if (err < 0) goto failed_read; if (type == SQUASHFS_LCHRDEV_TYPE) inode->i_mode |= S_IFCHR; else inode->i_mode |= S_IFBLK; xattr_id = le32_to_cpu(sqsh_ino->xattr); inode->i_op = &squashfs_inode_ops; set_nlink(inode, le32_to_cpu(sqsh_ino->nlink)); rdev = le32_to_cpu(sqsh_ino->rdev); init_special_inode(inode, inode->i_mode, new_decode_dev(rdev)); TRACE(""Device inode %x:%x, rdev %x\n"", SQUASHFS_INODE_BLK(ino), offset, rdev); break; } case SQUASHFS_FIFO_TYPE: case SQUASHFS_SOCKET_TYPE: { struct squashfs_ipc_inode *sqsh_ino = &squashfs_ino.ipc; err = squashfs_read_metadata(sb, sqsh_ino, &block, &offset, sizeof(*sqsh_ino)); if (err < 0) goto failed_read; if (type == SQUASHFS_FIFO_TYPE) inode->i_mode |= S_IFIFO; else inode->i_mode |= S_IFSOCK; set_nlink(inode, le32_to_cpu(sqsh_ino->nlink)); init_special_inode(inode, inode->i_mode, 0); break; } case SQUASHFS_LFIFO_TYPE: case SQUASHFS_LSOCKET_TYPE: { struct squashfs_lipc_inode *sqsh_ino = &squashfs_ino.lipc; err = squashfs_read_metadata(sb, sqsh_ino, &block, &offset, sizeof(*sqsh_ino)); if (err < 0) goto failed_read; if (type == SQUASHFS_LFIFO_TYPE) inode->i_mode |= S_IFIFO; else inode->i_mode |= S_IFSOCK; xattr_id = le32_to_cpu(sqsh_ino->xattr); inode->i_op = &squashfs_inode_ops; set_nlink(inode, le32_to_cpu(sqsh_ino->nlink)); init_special_inode(inode, inode->i_mode, 0); break; } default: ERROR(""Unknown inode type %d in squashfs_iget!\n"", type); return -EINVAL; } if (xattr_id != SQUASHFS_INVALID_XATTR && msblk->xattr_id_table) { err = squashfs_xattr_lookup(sb, xattr_id, &squashfs_i(inode)->xattr_count, &squashfs_i(inode)->xattr_size, &squashfs_i(inode)->xattr); if (err < 0) goto failed_read; inode->i_blocks += ((squashfs_i(inode)->xattr_size - 1) >> 9) + 1; } else squashfs_i(inode)->xattr_count = 0; return 0; failed_read: ERROR(""Unable to read inode 0x%llx\n"", ino); return err; }","- set_nlink(inode, le32_to_cpu(sqsh_ino->nlink));
+ if (inode->i_size > PAGE_SIZE) {
+ ERROR(""Corrupted symlink\n"");
+ return -EINVAL;
+ }
+ set_nlink(inode, le32_to_cpu(sqsh_ino->nlink));","int squashfs_read_inode(struct inode *inode, long long ino) { struct super_block *sb = inode->i_sb; struct squashfs_sb_info *msblk = sb->s_fs_info; u64 block = SQUASHFS_INODE_BLK(ino) + msblk->inode_table; int err, type, offset = SQUASHFS_INODE_OFFSET(ino); union squashfs_inode squashfs_ino; struct squashfs_base_inode *sqshb_ino = &squashfs_ino.base; int xattr_id = SQUASHFS_INVALID_XATTR; TRACE(""Entered squashfs_read_inode\n""); err = squashfs_read_metadata(sb, sqshb_ino, &block, &offset, sizeof(*sqshb_ino)); if (err < 0) goto failed_read; err = squashfs_new_inode(sb, inode, sqshb_ino); if (err) goto failed_read; block = SQUASHFS_INODE_BLK(ino) + msblk->inode_table; offset = SQUASHFS_INODE_OFFSET(ino); type = le16_to_cpu(sqshb_ino->inode_type); switch (type) { case SQUASHFS_REG_TYPE: { unsigned int frag_offset, frag; int frag_size; u64 frag_blk; struct squashfs_reg_inode *sqsh_ino = &squashfs_ino.reg; err = squashfs_read_metadata(sb, sqsh_ino, &block, &offset, sizeof(*sqsh_ino)); if (err < 0) goto failed_read; frag = le32_to_cpu(sqsh_ino->fragment); if (frag != SQUASHFS_INVALID_FRAG) { frag_offset = le32_to_cpu(sqsh_ino->offset); frag_size = squashfs_frag_lookup(sb, frag, &frag_blk); if (frag_size < 0) { err = frag_size; goto failed_read; } } else { frag_blk = SQUASHFS_INVALID_BLK; frag_size = 0; frag_offset = 0; } set_nlink(inode, 1); inode->i_size = le32_to_cpu(sqsh_ino->file_size); inode->i_fop = &generic_ro_fops; inode->i_mode |= S_IFREG; inode->i_blocks = ((inode->i_size - 1) >> 9) + 1; squashfs_i(inode)->fragment_block = frag_blk; squashfs_i(inode)->fragment_size = frag_size; squashfs_i(inode)->fragment_offset = frag_offset; squashfs_i(inode)->start = le32_to_cpu(sqsh_ino->start_block); squashfs_i(inode)->block_list_start = block; squashfs_i(inode)->offset = offset; inode->i_data.a_ops = &squashfs_aops; TRACE(""File inode %x:%x, start_block %llx, block_list_start "" ""%llx, offset %x\n"", SQUASHFS_INODE_BLK(ino), offset, squashfs_i(inode)->start, block, offset); break; } case SQUASHFS_LREG_TYPE: { unsigned int frag_offset, frag; int frag_size; u64 frag_blk; struct squashfs_lreg_inode *sqsh_ino = &squashfs_ino.lreg; err = squashfs_read_metadata(sb, sqsh_ino, &block, &offset, sizeof(*sqsh_ino)); if (err < 0) goto failed_read; frag = le32_to_cpu(sqsh_ino->fragment); if (frag != SQUASHFS_INVALID_FRAG) { frag_offset = le32_to_cpu(sqsh_ino->offset); frag_size = squashfs_frag_lookup(sb, frag, &frag_blk); if (frag_size < 0) { err = frag_size; goto failed_read; } } else { frag_blk = SQUASHFS_INVALID_BLK; frag_size = 0; frag_offset = 0; } xattr_id = le32_to_cpu(sqsh_ino->xattr); set_nlink(inode, le32_to_cpu(sqsh_ino->nlink)); inode->i_size = le64_to_cpu(sqsh_ino->file_size); inode->i_op = &squashfs_inode_ops; inode->i_fop = &generic_ro_fops; inode->i_mode |= S_IFREG; inode->i_blocks = (inode->i_size - le64_to_cpu(sqsh_ino->sparse) + 511) >> 9; squashfs_i(inode)->fragment_block = frag_blk; squashfs_i(inode)->fragment_size = frag_size; squashfs_i(inode)->fragment_offset = frag_offset; squashfs_i(inode)->start = le64_to_cpu(sqsh_ino->start_block); squashfs_i(inode)->block_list_start = block; squashfs_i(inode)->offset = offset; inode->i_data.a_ops = &squashfs_aops; TRACE(""File inode %x:%x, start_block %llx, block_list_start "" ""%llx, offset %x\n"", SQUASHFS_INODE_BLK(ino), offset, squashfs_i(inode)->start, block, offset); break; } case SQUASHFS_DIR_TYPE: { struct squashfs_dir_inode *sqsh_ino = &squashfs_ino.dir; err = squashfs_read_metadata(sb, sqsh_ino, &block, &offset, sizeof(*sqsh_ino)); if (err < 0) goto failed_read; set_nlink(inode, le32_to_cpu(sqsh_ino->nlink)); inode->i_size = le16_to_cpu(sqsh_ino->file_size); inode->i_op = &squashfs_dir_inode_ops; inode->i_fop = &squashfs_dir_ops; inode->i_mode |= S_IFDIR; squashfs_i(inode)->start = le32_to_cpu(sqsh_ino->start_block); squashfs_i(inode)->offset = le16_to_cpu(sqsh_ino->offset); squashfs_i(inode)->dir_idx_cnt = 0; squashfs_i(inode)->parent = le32_to_cpu(sqsh_ino->parent_inode); TRACE(""Directory inode %x:%x, start_block %llx, offset %x\n"", SQUASHFS_INODE_BLK(ino), offset, squashfs_i(inode)->start, le16_to_cpu(sqsh_ino->offset)); break; } case SQUASHFS_LDIR_TYPE: { struct squashfs_ldir_inode *sqsh_ino = &squashfs_ino.ldir; err = squashfs_read_metadata(sb, sqsh_ino, &block, &offset, sizeof(*sqsh_ino)); if (err < 0) goto failed_read; xattr_id = le32_to_cpu(sqsh_ino->xattr); set_nlink(inode, le32_to_cpu(sqsh_ino->nlink)); inode->i_size = le32_to_cpu(sqsh_ino->file_size); inode->i_op = &squashfs_dir_inode_ops; inode->i_fop = &squashfs_dir_ops; inode->i_mode |= S_IFDIR; squashfs_i(inode)->start = le32_to_cpu(sqsh_ino->start_block); squashfs_i(inode)->offset = le16_to_cpu(sqsh_ino->offset); squashfs_i(inode)->dir_idx_start = block; squashfs_i(inode)->dir_idx_offset = offset; squashfs_i(inode)->dir_idx_cnt = le16_to_cpu(sqsh_ino->i_count); squashfs_i(inode)->parent = le32_to_cpu(sqsh_ino->parent_inode); TRACE(""Long directory inode %x:%x, start_block %llx, offset "" ""%x\n"", SQUASHFS_INODE_BLK(ino), offset, squashfs_i(inode)->start, le16_to_cpu(sqsh_ino->offset)); break; } case SQUASHFS_SYMLINK_TYPE: case SQUASHFS_LSYMLINK_TYPE: { struct squashfs_symlink_inode *sqsh_ino = &squashfs_ino.symlink; err = squashfs_read_metadata(sb, sqsh_ino, &block, &offset, sizeof(*sqsh_ino)); if (err < 0) goto failed_read; inode->i_size = le32_to_cpu(sqsh_ino->symlink_size); if (inode->i_size > PAGE_SIZE) { ERROR(""Corrupted symlink\n""); return -EINVAL; } set_nlink(inode, le32_to_cpu(sqsh_ino->nlink)); inode->i_op = &squashfs_symlink_inode_ops; inode_nohighmem(inode); inode->i_data.a_ops = &squashfs_symlink_aops; inode->i_mode |= S_IFLNK; squashfs_i(inode)->start = block; squashfs_i(inode)->offset = offset; if (type == SQUASHFS_LSYMLINK_TYPE) { __le32 xattr; err = squashfs_read_metadata(sb, NULL, &block, &offset, inode->i_size); if (err < 0) goto failed_read; err = squashfs_read_metadata(sb, &xattr, &block, &offset, sizeof(xattr)); if (err < 0) goto failed_read; xattr_id = le32_to_cpu(xattr); } TRACE(""Symbolic link inode %x:%x, start_block %llx, offset "" ""%x\n"", SQUASHFS_INODE_BLK(ino), offset, block, offset); break; } case SQUASHFS_BLKDEV_TYPE: case SQUASHFS_CHRDEV_TYPE: { struct squashfs_dev_inode *sqsh_ino = &squashfs_ino.dev; unsigned int rdev; err = squashfs_read_metadata(sb, sqsh_ino, &block, &offset, sizeof(*sqsh_ino)); if (err < 0) goto failed_read; if (type == SQUASHFS_CHRDEV_TYPE) inode->i_mode |= S_IFCHR; else inode->i_mode |= S_IFBLK; set_nlink(inode, le32_to_cpu(sqsh_ino->nlink)); rdev = le32_to_cpu(sqsh_ino->rdev); init_special_inode(inode, inode->i_mode, new_decode_dev(rdev)); TRACE(""Device inode %x:%x, rdev %x\n"", SQUASHFS_INODE_BLK(ino), offset, rdev); break; } case SQUASHFS_LBLKDEV_TYPE: case SQUASHFS_LCHRDEV_TYPE: { struct squashfs_ldev_inode *sqsh_ino = &squashfs_ino.ldev; unsigned int rdev; err = squashfs_read_metadata(sb, sqsh_ino, &block, &offset, sizeof(*sqsh_ino)); if (err < 0) goto failed_read; if (type == SQUASHFS_LCHRDEV_TYPE) inode->i_mode |= S_IFCHR; else inode->i_mode |= S_IFBLK; xattr_id = le32_to_cpu(sqsh_ino->xattr); inode->i_op = &squashfs_inode_ops; set_nlink(inode, le32_to_cpu(sqsh_ino->nlink)); rdev = le32_to_cpu(sqsh_ino->rdev); init_special_inode(inode, inode->i_mode, new_decode_dev(rdev)); TRACE(""Device inode %x:%x, rdev %x\n"", SQUASHFS_INODE_BLK(ino), offset, rdev); break; } case SQUASHFS_FIFO_TYPE: case SQUASHFS_SOCKET_TYPE: { struct squashfs_ipc_inode *sqsh_ino = &squashfs_ino.ipc; err = squashfs_read_metadata(sb, sqsh_ino, &block, &offset, sizeof(*sqsh_ino)); if (err < 0) goto failed_read; if (type == SQUASHFS_FIFO_TYPE) inode->i_mode |= S_IFIFO; else inode->i_mode |= S_IFSOCK; set_nlink(inode, le32_to_cpu(sqsh_ino->nlink)); init_special_inode(inode, inode->i_mode, 0); break; } case SQUASHFS_LFIFO_TYPE: case SQUASHFS_LSOCKET_TYPE: { struct squashfs_lipc_inode *sqsh_ino = &squashfs_ino.lipc; err = squashfs_read_metadata(sb, sqsh_ino, &block, &offset, sizeof(*sqsh_ino)); if (err < 0) goto failed_read; if (type == SQUASHFS_LFIFO_TYPE) inode->i_mode |= S_IFIFO; else inode->i_mode |= S_IFSOCK; xattr_id = le32_to_cpu(sqsh_ino->xattr); inode->i_op = &squashfs_inode_ops; set_nlink(inode, le32_to_cpu(sqsh_ino->nlink)); init_special_inode(inode, inode->i_mode, 0); break; } default: ERROR(""Unknown inode type %d in squashfs_iget!\n"", type); return -EINVAL; } if (xattr_id != SQUASHFS_INVALID_XATTR && msblk->xattr_id_table) { err = squashfs_xattr_lookup(sb, xattr_id, &squashfs_i(inode)->xattr_count, &squashfs_i(inode)->xattr_size, &squashfs_i(inode)->xattr); if (err < 0) goto failed_read; inode->i_blocks += ((squashfs_i(inode)->xattr_size - 1) >> 9) + 1; } else squashfs_i(inode)->xattr_count = 0; return 0; failed_read: ERROR(""Unable to read inode 0x%llx\n"", ino); return err; }"
432----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47670/bad/xattr.c----ocfs2_xattr_ibody_get,"static int ocfs2_xattr_ibody_get(struct inode *inode, int name_index, const char *name, void *buffer, size_t buffer_size, struct ocfs2_xattr_search *xs) { struct ocfs2_inode_info *oi = OCFS2_I(inode); struct ocfs2_dinode *di = (struct ocfs2_dinode *)xs->inode_bh->b_data; struct ocfs2_xattr_value_root *xv; size_t size; int ret = 0; if (!(oi->ip_dyn_features & OCFS2_INLINE_XATTR_FL)) return -ENODATA; xs->end = (void *)di + inode->i_sb->s_blocksize; xs->header = (struct ocfs2_xattr_header *) (xs->end - le16_to_cpu(di->i_xattr_inline_size)); xs->base = (void *)xs->header; xs->here = xs->header->xh_entries; <S2SV_StartVul> ret = ocfs2_xattr_find_entry(name_index, name, xs); <S2SV_EndVul> if (ret) return ret; size = le64_to_cpu(xs->here->xe_value_size); if (buffer) { if (size > buffer_size) return -ERANGE; if (ocfs2_xattr_is_local(xs->here)) { memcpy(buffer, (void *)xs->base + le16_to_cpu(xs->here->xe_name_offset) + OCFS2_XATTR_SIZE(xs->here->xe_name_len), size); } else { xv = (struct ocfs2_xattr_value_root *) (xs->base + le16_to_cpu( xs->here->xe_name_offset) + OCFS2_XATTR_SIZE(xs->here->xe_name_len)); ret = ocfs2_xattr_get_value_outside(inode, xv, buffer, size); if (ret < 0) { mlog_errno(ret); return ret; } } } return size; }","- ret = ocfs2_xattr_find_entry(name_index, name, xs);
+ ret = ocfs2_xattr_find_entry(inode, name_index, name, xs);","static int ocfs2_xattr_ibody_get(struct inode *inode, int name_index, const char *name, void *buffer, size_t buffer_size, struct ocfs2_xattr_search *xs) { struct ocfs2_inode_info *oi = OCFS2_I(inode); struct ocfs2_dinode *di = (struct ocfs2_dinode *)xs->inode_bh->b_data; struct ocfs2_xattr_value_root *xv; size_t size; int ret = 0; if (!(oi->ip_dyn_features & OCFS2_INLINE_XATTR_FL)) return -ENODATA; xs->end = (void *)di + inode->i_sb->s_blocksize; xs->header = (struct ocfs2_xattr_header *) (xs->end - le16_to_cpu(di->i_xattr_inline_size)); xs->base = (void *)xs->header; xs->here = xs->header->xh_entries; ret = ocfs2_xattr_find_entry(inode, name_index, name, xs); if (ret) return ret; size = le64_to_cpu(xs->here->xe_value_size); if (buffer) { if (size > buffer_size) return -ERANGE; if (ocfs2_xattr_is_local(xs->here)) { memcpy(buffer, (void *)xs->base + le16_to_cpu(xs->here->xe_name_offset) + OCFS2_XATTR_SIZE(xs->here->xe_name_len), size); } else { xv = (struct ocfs2_xattr_value_root *) (xs->base + le16_to_cpu( xs->here->xe_name_offset) + OCFS2_XATTR_SIZE(xs->here->xe_name_len)); ret = ocfs2_xattr_get_value_outside(inode, xv, buffer, size); if (ret < 0) { mlog_errno(ret); return ret; } } } return size; }"
1439----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56764/bad/ublk_drv.c----ublk_stop_dev,static void ublk_stop_dev(struct ublk_device *ub) { struct gendisk *disk; mutex_lock(&ub->mutex); if (ub->dev_info.state == UBLK_S_DEV_DEAD) goto unlock; if (ublk_nosrv_dev_should_queue_io(ub)) { if (ub->dev_info.state == UBLK_S_DEV_LIVE) __ublk_quiesce_dev(ub); ublk_unquiesce_dev(ub); } del_gendisk(ub->ub_disk); <S2SV_StartVul> spin_lock(&ub->lock); <S2SV_EndVul> <S2SV_StartVul> disk = ub->ub_disk; <S2SV_EndVul> <S2SV_StartVul> ub->dev_info.state = UBLK_S_DEV_DEAD; <S2SV_EndVul> <S2SV_StartVul> ub->dev_info.ublksrv_pid = -1; <S2SV_EndVul> <S2SV_StartVul> ub->ub_disk = NULL; <S2SV_EndVul> <S2SV_StartVul> spin_unlock(&ub->lock); <S2SV_EndVul> put_disk(disk); unlock: mutex_unlock(&ub->mutex); ublk_cancel_dev(ub); },"- spin_lock(&ub->lock);
- disk = ub->ub_disk;
- ub->dev_info.state = UBLK_S_DEV_DEAD;
- ub->dev_info.ublksrv_pid = -1;
- ub->ub_disk = NULL;
- spin_unlock(&ub->lock);
+ }
+ disk = ublk_detach_disk(ub);",static void ublk_stop_dev(struct ublk_device *ub) { struct gendisk *disk; mutex_lock(&ub->mutex); if (ub->dev_info.state == UBLK_S_DEV_DEAD) goto unlock; if (ublk_nosrv_dev_should_queue_io(ub)) { if (ub->dev_info.state == UBLK_S_DEV_LIVE) __ublk_quiesce_dev(ub); ublk_unquiesce_dev(ub); } del_gendisk(ub->ub_disk); disk = ublk_detach_disk(ub); put_disk(disk); unlock: mutex_unlock(&ub->mutex); ublk_cancel_dev(ub); }
1457----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56787/bad/soc-imx8m.c----imx8_soc_init,"static int __init imx8_soc_init(void) { struct soc_device_attribute *soc_dev_attr; struct soc_device *soc_dev; const struct of_device_id *id; u32 soc_rev = 0; const struct imx8_soc_data *data; int ret; soc_dev_attr = kzalloc(sizeof(*soc_dev_attr), GFP_KERNEL); if (!soc_dev_attr) return -ENOMEM; soc_dev_attr->family = ""Freescale i.MX""; ret = of_property_read_string(of_root, ""model"", &soc_dev_attr->machine); if (ret) goto free_soc; id = of_match_node(imx8_soc_match, of_root); if (!id) { ret = -ENODEV; goto free_soc; } data = id->data; if (data) { soc_dev_attr->soc_id = data->name; <S2SV_StartVul> if (data->soc_revision) <S2SV_EndVul> <S2SV_StartVul> soc_rev = data->soc_revision(); <S2SV_EndVul> } soc_dev_attr->revision = imx8_revision(soc_rev); if (!soc_dev_attr->revision) { ret = -ENOMEM; goto free_soc; } soc_dev_attr->serial_number = kasprintf(GFP_KERNEL, ""%016llX"", soc_uid); if (!soc_dev_attr->serial_number) { ret = -ENOMEM; goto free_rev; } soc_dev = soc_device_register(soc_dev_attr); if (IS_ERR(soc_dev)) { ret = PTR_ERR(soc_dev); goto free_serial_number; } pr_info(""SoC: %s revision %s\n"", soc_dev_attr->soc_id, soc_dev_attr->revision); if (IS_ENABLED(CONFIG_ARM_IMX_CPUFREQ_DT)) platform_device_register_simple(""imx-cpufreq-dt"", -1, NULL, 0); return 0; free_serial_number: kfree(soc_dev_attr->serial_number); free_rev: if (strcmp(soc_dev_attr->revision, ""unknown"")) kfree(soc_dev_attr->revision); free_soc: kfree(soc_dev_attr); return ret; }","- if (data->soc_revision)
- soc_rev = data->soc_revision();
+ static int __init imx8_soc_init(void)
+ {
+ struct platform_device *pdev;
+ int ret;
+ if (!of_match_node(imx8_soc_match, of_root))
+ return 0;
+ ret = platform_driver_register(&imx8m_soc_driver);
+ if (ret) {
+ pr_err(""Failed to register imx8m-soc platform driver: %d\n"", ret);
+ return ret;
+ }
+ pdev = platform_device_register_simple(""imx8m-soc"", -1, NULL, 0);
+ if (IS_ERR(pdev)) {
+ pr_err(""Failed to register imx8m-soc platform device: %ld\n"", PTR_ERR(pdev));
+ platform_driver_unregister(&imx8m_soc_driver);
+ return PTR_ERR(pdev);
+ }
+ return 0;
+ }","static int __init imx8_soc_init(void) { struct platform_device *pdev; int ret; if (!of_match_node(imx8_soc_match, of_root)) return 0; ret = platform_driver_register(&imx8m_soc_driver); if (ret) { pr_err(""Failed to register imx8m-soc platform driver: %d\n"", ret); return ret; } pdev = platform_device_register_simple(""imx8m-soc"", -1, NULL, 0); if (IS_ERR(pdev)) { pr_err(""Failed to register imx8m-soc platform device: %ld\n"", PTR_ERR(pdev)); platform_driver_unregister(&imx8m_soc_driver); return PTR_ERR(pdev); } return 0; }"
959----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50202/bad/namei.c----nilfs_lookup,"nilfs_lookup(struct inode *dir, struct dentry *dentry, unsigned int flags) { struct inode *inode; ino_t ino; if (dentry->d_name.len > NILFS_NAME_LEN) return ERR_PTR(-ENAMETOOLONG); <S2SV_StartVul> ino = nilfs_inode_by_name(dir, &dentry->d_name); <S2SV_EndVul> <S2SV_StartVul> inode = ino ? nilfs_iget(dir->i_sb, NILFS_I(dir)->i_root, ino) : NULL; <S2SV_EndVul> return d_splice_alias(inode, dentry); }","- ino = nilfs_inode_by_name(dir, &dentry->d_name);
- inode = ino ? nilfs_iget(dir->i_sb, NILFS_I(dir)->i_root, ino) : NULL;
+ ino_t ino;
+ int res;
+ res = nilfs_inode_by_name(dir, &dentry->d_name, &ino);
+ if (res) {
+ if (res != -ENOENT)
+ return ERR_PTR(res);
+ inode = NULL;
+ } else {
+ inode = nilfs_iget(dir->i_sb, NILFS_I(dir)->i_root, ino);
+ }
+ }","nilfs_lookup(struct inode *dir, struct dentry *dentry, unsigned int flags) { struct inode *inode; ino_t ino; int res; if (dentry->d_name.len > NILFS_NAME_LEN) return ERR_PTR(-ENAMETOOLONG); res = nilfs_inode_by_name(dir, &dentry->d_name, &ino); if (res) { if (res != -ENOENT) return ERR_PTR(res); inode = NULL; } else { inode = nilfs_iget(dir->i_sb, NILFS_I(dir)->i_root, ino); } return d_splice_alias(inode, dentry); }"
391----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46852/bad/cma_heap.c----cma_heap_vm_fault,static vm_fault_t cma_heap_vm_fault(struct vm_fault *vmf) { struct vm_area_struct *vma = vmf->vma; struct cma_heap_buffer *buffer = vma->vm_private_data; <S2SV_StartVul> if (vmf->pgoff > buffer->pagecount) <S2SV_EndVul> return VM_FAULT_SIGBUS; vmf->page = buffer->pages[vmf->pgoff]; get_page(vmf->page); return 0; },"- if (vmf->pgoff > buffer->pagecount)
+ if (vmf->pgoff >= buffer->pagecount)",static vm_fault_t cma_heap_vm_fault(struct vm_fault *vmf) { struct vm_area_struct *vma = vmf->vma; struct cma_heap_buffer *buffer = vma->vm_private_data; if (vmf->pgoff >= buffer->pagecount) return VM_FAULT_SIGBUS; vmf->page = buffer->pages[vmf->pgoff]; get_page(vmf->page); return 0; }
692----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49985/bad/i2c-stm32f7.c----stm32f7_i2c_runtime_suspend,static int __maybe_unused stm32f7_i2c_runtime_suspend(struct device *dev) { struct stm32f7_i2c_dev *i2c_dev = dev_get_drvdata(dev); if (!stm32f7_i2c_is_slave_registered(i2c_dev)) <S2SV_StartVul> clk_disable_unprepare(i2c_dev->clk); <S2SV_EndVul> return 0; },"- clk_disable_unprepare(i2c_dev->clk);
+ clk_disable(i2c_dev->clk);",static int __maybe_unused stm32f7_i2c_runtime_suspend(struct device *dev) { struct stm32f7_i2c_dev *i2c_dev = dev_get_drvdata(dev); if (!stm32f7_i2c_is_slave_registered(i2c_dev)) clk_disable(i2c_dev->clk); return 0; }
1165----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53139/bad/ipv6.c----sctp_v6_available,"static int sctp_v6_available(union sctp_addr *addr, struct sctp_sock *sp) { const struct in6_addr *in6 = (const struct in6_addr *)&addr->v6.sin6_addr; struct sock *sk = &sp->inet.sk; struct net *net = sock_net(sk); struct net_device *dev = NULL; <S2SV_StartVul> int type; <S2SV_EndVul> type = ipv6_addr_type(in6); if (IPV6_ADDR_ANY == type) return 1; if (type == IPV6_ADDR_MAPPED) { if (sp && ipv6_only_sock(sctp_opt2sk(sp))) return 0; sctp_v6_map_v4(addr); return sctp_get_af_specific(AF_INET)->available(addr, sp); } if (!(type & IPV6_ADDR_UNICAST)) <S2SV_StartVul> return 0; <S2SV_EndVul> <S2SV_StartVul> if (sk->sk_bound_dev_if) { <S2SV_EndVul> <S2SV_StartVul> dev = dev_get_by_index_rcu(net, sk->sk_bound_dev_if); <S2SV_EndVul> if (!dev) <S2SV_StartVul> return 0; <S2SV_EndVul> } <S2SV_StartVul> return ipv6_can_nonlocal_bind(net, &sp->inet) || <S2SV_EndVul> <S2SV_StartVul> ipv6_chk_addr(net, in6, dev, 0); <S2SV_EndVul> }","- int type;
- return 0;
- if (sk->sk_bound_dev_if) {
- dev = dev_get_by_index_rcu(net, sk->sk_bound_dev_if);
- return 0;
- return ipv6_can_nonlocal_bind(net, &sp->inet) ||
- ipv6_chk_addr(net, in6, dev, 0);
+ int type, res, bound_dev_if;
+ rcu_read_lock();
+ bound_dev_if = READ_ONCE(sk->sk_bound_dev_if);
+ if (bound_dev_if) {
+ res = 0;
+ dev = dev_get_by_index_rcu(net, bound_dev_if);
+ goto out;
+ res = ipv6_can_nonlocal_bind(net, &sp->inet) ||
+ ipv6_chk_addr(net, in6, dev, 0);
+ out:
+ rcu_read_unlock();
+ return res;","static int sctp_v6_available(union sctp_addr *addr, struct sctp_sock *sp) { const struct in6_addr *in6 = (const struct in6_addr *)&addr->v6.sin6_addr; struct sock *sk = &sp->inet.sk; struct net *net = sock_net(sk); struct net_device *dev = NULL; int type, res, bound_dev_if; type = ipv6_addr_type(in6); if (IPV6_ADDR_ANY == type) return 1; if (type == IPV6_ADDR_MAPPED) { if (sp && ipv6_only_sock(sctp_opt2sk(sp))) return 0; sctp_v6_map_v4(addr); return sctp_get_af_specific(AF_INET)->available(addr, sp); } if (!(type & IPV6_ADDR_UNICAST)) return 0; rcu_read_lock(); bound_dev_if = READ_ONCE(sk->sk_bound_dev_if); if (bound_dev_if) { res = 0; dev = dev_get_by_index_rcu(net, bound_dev_if); if (!dev) goto out; } res = ipv6_can_nonlocal_bind(net, &sp->inet) || ipv6_chk_addr(net, in6, dev, 0); out: rcu_read_unlock(); return res; }"
561----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49880/bad/resize.c----*alloc_flex_gd,"static struct ext4_new_flex_group_data *alloc_flex_gd(unsigned int flexbg_size, ext4_group_t o_group, ext4_group_t n_group) { ext4_group_t last_group; struct ext4_new_flex_group_data *flex_gd; flex_gd = kmalloc(sizeof(*flex_gd), GFP_NOFS); if (flex_gd == NULL) goto out3; <S2SV_StartVul> if (unlikely(flexbg_size > MAX_RESIZE_BG)) <S2SV_EndVul> <S2SV_StartVul> flex_gd->resize_bg = MAX_RESIZE_BG; <S2SV_EndVul> <S2SV_StartVul> else <S2SV_EndVul> <S2SV_StartVul> flex_gd->resize_bg = flexbg_size; <S2SV_EndVul> last_group = o_group | (flex_gd->resize_bg - 1); if (n_group <= last_group) <S2SV_StartVul> flex_gd->resize_bg = 1 << fls(n_group - o_group + 1); <S2SV_EndVul> else if (n_group - last_group < flex_gd->resize_bg) <S2SV_StartVul> flex_gd->resize_bg = 1 << max(fls(last_group - o_group + 1), <S2SV_EndVul> fls(n_group - last_group)); flex_gd->groups = kmalloc_array(flex_gd->resize_bg, sizeof(struct ext4_new_group_data), GFP_NOFS); if (flex_gd->groups == NULL) goto out2; flex_gd->bg_flags = kmalloc_array(flex_gd->resize_bg, sizeof(__u16), GFP_NOFS); if (flex_gd->bg_flags == NULL) goto out1; return flex_gd; out1: kfree(flex_gd->groups); out2: kfree(flex_gd); out3: return NULL; }","- if (unlikely(flexbg_size > MAX_RESIZE_BG))
- flex_gd->resize_bg = MAX_RESIZE_BG;
- else
- flex_gd->resize_bg = flexbg_size;
- flex_gd->resize_bg = 1 << fls(n_group - o_group + 1);
- flex_gd->resize_bg = 1 << max(fls(last_group - o_group + 1),
+ unsigned int max_resize_bg;
+ max_resize_bg = umin(flexbg_size, MAX_RESIZE_BG);
+ flex_gd->resize_bg = max_resize_bg;
+ flex_gd->resize_bg = 1 << fls(n_group - o_group);
+ flex_gd->resize_bg = 1 << max(fls(last_group - o_group),
+ if (WARN_ON_ONCE(flex_gd->resize_bg > max_resize_bg))
+ flex_gd->resize_bg = max_resize_bg;","static struct ext4_new_flex_group_data *alloc_flex_gd(unsigned int flexbg_size, ext4_group_t o_group, ext4_group_t n_group) { ext4_group_t last_group; unsigned int max_resize_bg; struct ext4_new_flex_group_data *flex_gd; flex_gd = kmalloc(sizeof(*flex_gd), GFP_NOFS); if (flex_gd == NULL) goto out3; max_resize_bg = umin(flexbg_size, MAX_RESIZE_BG); flex_gd->resize_bg = max_resize_bg; last_group = o_group | (flex_gd->resize_bg - 1); if (n_group <= last_group) flex_gd->resize_bg = 1 << fls(n_group - o_group); else if (n_group - last_group < flex_gd->resize_bg) flex_gd->resize_bg = 1 << max(fls(last_group - o_group), fls(n_group - last_group)); if (WARN_ON_ONCE(flex_gd->resize_bg > max_resize_bg)) flex_gd->resize_bg = max_resize_bg; flex_gd->groups = kmalloc_array(flex_gd->resize_bg, sizeof(struct ext4_new_group_data), GFP_NOFS); if (flex_gd->groups == NULL) goto out2; flex_gd->bg_flags = kmalloc_array(flex_gd->resize_bg, sizeof(__u16), GFP_NOFS); if (flex_gd->bg_flags == NULL) goto out1; return flex_gd; out1: kfree(flex_gd->groups); out2: kfree(flex_gd); out3: return NULL; }"
980----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50214/bad/drm_connector_test.c----drm_test_drm_hdmi_compute_mode_clock_rgb_10bpc,"static void drm_test_drm_hdmi_compute_mode_clock_rgb_10bpc(struct kunit *test) { struct drm_connector_init_priv *priv = test->priv; const struct drm_display_mode *mode; unsigned long long rate; struct drm_device *drm = &priv->drm; <S2SV_StartVul> mode = drm_display_mode_from_cea_vic(drm, 16); <S2SV_EndVul> KUNIT_ASSERT_NOT_NULL(test, mode); KUNIT_ASSERT_FALSE(test, mode->flags & DRM_MODE_FLAG_DBLCLK); rate = drm_hdmi_compute_mode_clock(mode, 10, HDMI_COLORSPACE_RGB); KUNIT_ASSERT_GT(test, rate, 0); KUNIT_EXPECT_EQ(test, mode->clock * 1250, rate); }","- mode = drm_display_mode_from_cea_vic(drm, 16);
+ mode = drm_kunit_display_mode_from_cea_vic(test, drm, 16);","static void drm_test_drm_hdmi_compute_mode_clock_rgb_10bpc(struct kunit *test) { struct drm_connector_init_priv *priv = test->priv; const struct drm_display_mode *mode; unsigned long long rate; struct drm_device *drm = &priv->drm; mode = drm_kunit_display_mode_from_cea_vic(test, drm, 16); KUNIT_ASSERT_NOT_NULL(test, mode); KUNIT_ASSERT_FALSE(test, mode->flags & DRM_MODE_FLAG_DBLCLK); rate = drm_hdmi_compute_mode_clock(mode, 10, HDMI_COLORSPACE_RGB); KUNIT_ASSERT_GT(test, rate, 0); KUNIT_EXPECT_EQ(test, mode->clock * 1250, rate); }"
899----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50147/bad/cmd.c----mlx5_cmd_enable,"int mlx5_cmd_enable(struct mlx5_core_dev *dev) { int size = sizeof(struct mlx5_cmd_prot_block); int align = roundup_pow_of_two(size); struct mlx5_cmd *cmd = &dev->cmd; u32 cmd_h, cmd_l; int err; memset(&cmd->vars, 0, sizeof(cmd->vars)); cmd->vars.cmdif_rev = cmdif_rev(dev); if (cmd->vars.cmdif_rev != CMD_IF_REV) { mlx5_core_err(dev, ""Driver cmdif rev(%d) differs from firmware's(%d)\n"", CMD_IF_REV, cmd->vars.cmdif_rev); return -EINVAL; } cmd_l = ioread32be(&dev->iseg->cmdq_addr_l_sz) & 0xff; cmd->vars.log_sz = cmd_l >> 4 & 0xf; cmd->vars.log_stride = cmd_l & 0xf; if (1 << cmd->vars.log_sz > MLX5_MAX_COMMANDS) { mlx5_core_err(dev, ""firmware reports too many outstanding commands %d\n"", 1 << cmd->vars.log_sz); return -EINVAL; } if (cmd->vars.log_sz + cmd->vars.log_stride > MLX5_ADAPTER_PAGE_SHIFT) { mlx5_core_err(dev, ""command queue size overflow\n""); return -EINVAL; } cmd->state = MLX5_CMDIF_STATE_DOWN; cmd->vars.max_reg_cmds = (1 << cmd->vars.log_sz) - 1; <S2SV_StartVul> cmd->vars.bitmask = (1UL << cmd->vars.max_reg_cmds) - 1; <S2SV_EndVul> sema_init(&cmd->vars.sem, cmd->vars.max_reg_cmds); sema_init(&cmd->vars.pages_sem, 1); sema_init(&cmd->vars.throttle_sem, DIV_ROUND_UP(cmd->vars.max_reg_cmds, 2)); cmd->pool = dma_pool_create(""mlx5_cmd"", mlx5_core_dma_dev(dev), size, align, 0); if (!cmd->pool) return -ENOMEM; err = alloc_cmd_page(dev, cmd); if (err) goto err_free_pool; cmd_h = (u32)((u64)(cmd->dma) >> 32); cmd_l = (u32)(cmd->dma); if (cmd_l & 0xfff) { mlx5_core_err(dev, ""invalid command queue address\n""); err = -ENOMEM; goto err_cmd_page; } iowrite32be(cmd_h, &dev->iseg->cmdq_addr_h); iowrite32be(cmd_l, &dev->iseg->cmdq_addr_l_sz); wmb(); mlx5_core_dbg(dev, ""descriptor at dma 0x%llx\n"", (unsigned long long)(cmd->dma)); cmd->mode = CMD_MODE_POLLING; cmd->allowed_opcode = CMD_ALLOWED_OPCODE_ALL; create_msg_cache(dev); create_debugfs_files(dev); return 0; err_cmd_page: free_cmd_page(dev, cmd); err_free_pool: dma_pool_destroy(cmd->pool); return err; }","- cmd->vars.bitmask = (1UL << cmd->vars.max_reg_cmds) - 1;
+ cmd->vars.bitmask = MLX5_CMD_MASK;","int mlx5_cmd_enable(struct mlx5_core_dev *dev) { int size = sizeof(struct mlx5_cmd_prot_block); int align = roundup_pow_of_two(size); struct mlx5_cmd *cmd = &dev->cmd; u32 cmd_h, cmd_l; int err; memset(&cmd->vars, 0, sizeof(cmd->vars)); cmd->vars.cmdif_rev = cmdif_rev(dev); if (cmd->vars.cmdif_rev != CMD_IF_REV) { mlx5_core_err(dev, ""Driver cmdif rev(%d) differs from firmware's(%d)\n"", CMD_IF_REV, cmd->vars.cmdif_rev); return -EINVAL; } cmd_l = ioread32be(&dev->iseg->cmdq_addr_l_sz) & 0xff; cmd->vars.log_sz = cmd_l >> 4 & 0xf; cmd->vars.log_stride = cmd_l & 0xf; if (1 << cmd->vars.log_sz > MLX5_MAX_COMMANDS) { mlx5_core_err(dev, ""firmware reports too many outstanding commands %d\n"", 1 << cmd->vars.log_sz); return -EINVAL; } if (cmd->vars.log_sz + cmd->vars.log_stride > MLX5_ADAPTER_PAGE_SHIFT) { mlx5_core_err(dev, ""command queue size overflow\n""); return -EINVAL; } cmd->state = MLX5_CMDIF_STATE_DOWN; cmd->vars.max_reg_cmds = (1 << cmd->vars.log_sz) - 1; cmd->vars.bitmask = MLX5_CMD_MASK; sema_init(&cmd->vars.sem, cmd->vars.max_reg_cmds); sema_init(&cmd->vars.pages_sem, 1); sema_init(&cmd->vars.throttle_sem, DIV_ROUND_UP(cmd->vars.max_reg_cmds, 2)); cmd->pool = dma_pool_create(""mlx5_cmd"", mlx5_core_dma_dev(dev), size, align, 0); if (!cmd->pool) return -ENOMEM; err = alloc_cmd_page(dev, cmd); if (err) goto err_free_pool; cmd_h = (u32)((u64)(cmd->dma) >> 32); cmd_l = (u32)(cmd->dma); if (cmd_l & 0xfff) { mlx5_core_err(dev, ""invalid command queue address\n""); err = -ENOMEM; goto err_cmd_page; } iowrite32be(cmd_h, &dev->iseg->cmdq_addr_h); iowrite32be(cmd_l, &dev->iseg->cmdq_addr_l_sz); wmb(); mlx5_core_dbg(dev, ""descriptor at dma 0x%llx\n"", (unsigned long long)(cmd->dma)); cmd->mode = CMD_MODE_POLLING; cmd->allowed_opcode = CMD_ALLOWED_OPCODE_ALL; create_msg_cache(dev); create_debugfs_files(dev); return 0; err_cmd_page: free_cmd_page(dev, cmd); err_free_pool: dma_pool_destroy(cmd->pool); return err; }"
65----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44943/bad/gup.c----get_gate_page,"static int get_gate_page(struct mm_struct *mm, unsigned long address, unsigned int gup_flags, struct vm_area_struct **vma, struct page **page) { pgd_t *pgd; p4d_t *p4d; pud_t *pud; pmd_t *pmd; pte_t *pte; pte_t entry; int ret = -EFAULT; if (gup_flags & FOLL_WRITE) return -EFAULT; if (address > TASK_SIZE) pgd = pgd_offset_k(address); else pgd = pgd_offset_gate(mm, address); if (pgd_none(*pgd)) return -EFAULT; p4d = p4d_offset(pgd, address); if (p4d_none(*p4d)) return -EFAULT; pud = pud_offset(p4d, address); if (pud_none(*pud)) return -EFAULT; pmd = pmd_offset(pud, address); if (!pmd_present(*pmd)) return -EFAULT; pte = pte_offset_map(pmd, address); if (!pte) return -EFAULT; entry = ptep_get(pte); if (pte_none(entry)) goto unmap; *vma = get_gate_vma(mm); if (!page) goto out; *page = vm_normal_page(*vma, address, entry); if (!*page) { if ((gup_flags & FOLL_DUMP) || !is_zero_pfn(pte_pfn(entry))) goto unmap; *page = pte_page(entry); <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> ret = try_grab_page(*page, gup_flags); <S2SV_EndVul> if (unlikely(ret)) goto unmap; out: ret = 0; unmap: pte_unmap(pte); return ret; }","- }
- ret = try_grab_page(*page, gup_flags);
+ }
+ ret = try_grab_folio(page_folio(*page), 1, gup_flags);","static int get_gate_page(struct mm_struct *mm, unsigned long address, unsigned int gup_flags, struct vm_area_struct **vma, struct page **page) { pgd_t *pgd; p4d_t *p4d; pud_t *pud; pmd_t *pmd; pte_t *pte; pte_t entry; int ret = -EFAULT; if (gup_flags & FOLL_WRITE) return -EFAULT; if (address > TASK_SIZE) pgd = pgd_offset_k(address); else pgd = pgd_offset_gate(mm, address); if (pgd_none(*pgd)) return -EFAULT; p4d = p4d_offset(pgd, address); if (p4d_none(*p4d)) return -EFAULT; pud = pud_offset(p4d, address); if (pud_none(*pud)) return -EFAULT; pmd = pmd_offset(pud, address); if (!pmd_present(*pmd)) return -EFAULT; pte = pte_offset_map(pmd, address); if (!pte) return -EFAULT; entry = ptep_get(pte); if (pte_none(entry)) goto unmap; *vma = get_gate_vma(mm); if (!page) goto out; *page = vm_normal_page(*vma, address, entry); if (!*page) { if ((gup_flags & FOLL_DUMP) || !is_zero_pfn(pte_pfn(entry))) goto unmap; *page = pte_page(entry); } ret = try_grab_folio(page_folio(*page), 1, gup_flags); if (unlikely(ret)) goto unmap; out: ret = 0; unmap: pte_unmap(pte); return ret; }"
196----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46681/bad/pktgen.c----pktgen_thread_worker,"static int pktgen_thread_worker(void *arg) { struct pktgen_thread *t = arg; struct pktgen_dev *pkt_dev = NULL; int cpu = t->cpu; <S2SV_StartVul> WARN_ON(smp_processor_id() != cpu); <S2SV_EndVul> init_waitqueue_head(&t->queue); complete(&t->start_done); pr_debug(""starting pktgen/%d: pid=%d\n"", cpu, task_pid_nr(current)); set_freezable(); while (!kthread_should_stop()) { pkt_dev = next_to_run(t); if (unlikely(!pkt_dev && t->control == 0)) { if (t->net->pktgen_exiting) break; wait_event_freezable_timeout(t->queue, t->control != 0, HZ / 10); continue; } if (likely(pkt_dev)) { pktgen_xmit(pkt_dev); if (need_resched()) pktgen_resched(pkt_dev); else cpu_relax(); } if (t->control & T_STOP) { pktgen_stop(t); t->control &= ~(T_STOP); } if (t->control & T_RUN) { pktgen_run(t); t->control &= ~(T_RUN); } if (t->control & T_REMDEVALL) { pktgen_rem_all_ifs(t); t->control &= ~(T_REMDEVALL); } if (t->control & T_REMDEV) { pktgen_rem_one_if(t); t->control &= ~(T_REMDEV); } try_to_freeze(); } pr_debug(""%s stopping all device\n"", t->tsk->comm); pktgen_stop(t); pr_debug(""%s removing all device\n"", t->tsk->comm); pktgen_rem_all_ifs(t); pr_debug(""%s removing thread\n"", t->tsk->comm); pktgen_rem_thread(t); return 0; }","- WARN_ON(smp_processor_id() != cpu);
+ WARN_ON_ONCE(smp_processor_id() != cpu);","static int pktgen_thread_worker(void *arg) { struct pktgen_thread *t = arg; struct pktgen_dev *pkt_dev = NULL; int cpu = t->cpu; WARN_ON_ONCE(smp_processor_id() != cpu); init_waitqueue_head(&t->queue); complete(&t->start_done); pr_debug(""starting pktgen/%d: pid=%d\n"", cpu, task_pid_nr(current)); set_freezable(); while (!kthread_should_stop()) { pkt_dev = next_to_run(t); if (unlikely(!pkt_dev && t->control == 0)) { if (t->net->pktgen_exiting) break; wait_event_freezable_timeout(t->queue, t->control != 0, HZ / 10); continue; } if (likely(pkt_dev)) { pktgen_xmit(pkt_dev); if (need_resched()) pktgen_resched(pkt_dev); else cpu_relax(); } if (t->control & T_STOP) { pktgen_stop(t); t->control &= ~(T_STOP); } if (t->control & T_RUN) { pktgen_run(t); t->control &= ~(T_RUN); } if (t->control & T_REMDEVALL) { pktgen_rem_all_ifs(t); t->control &= ~(T_REMDEVALL); } if (t->control & T_REMDEV) { pktgen_rem_one_if(t); t->control &= ~(T_REMDEV); } try_to_freeze(); } pr_debug(""%s stopping all device\n"", t->tsk->comm); pktgen_stop(t); pr_debug(""%s removing all device\n"", t->tsk->comm); pktgen_rem_all_ifs(t); pr_debug(""%s removing thread\n"", t->tsk->comm); pktgen_rem_thread(t); return 0; }"
161----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-45016/bad/sch_netem.c----netem_enqueue,"static int netem_enqueue(struct sk_buff *skb, struct Qdisc *sch, struct sk_buff **to_free) { struct netem_sched_data *q = qdisc_priv(sch); struct netem_skb_cb *cb; <S2SV_StartVul> struct sk_buff *skb2; <S2SV_EndVul> struct sk_buff *segs = NULL; unsigned int prev_len = qdisc_pkt_len(skb); int count = 1; <S2SV_StartVul> int rc = NET_XMIT_SUCCESS; <S2SV_EndVul> <S2SV_StartVul> int rc_drop = NET_XMIT_DROP; <S2SV_EndVul> skb->prev = NULL; if (q->duplicate && q->duplicate >= get_crandom(&q->dup_cor, &q->prng)) ++count; if (loss_event(q)) { if (q->ecn && INET_ECN_set_ce(skb)) qdisc_qstats_drop(sch); else --count; } if (count == 0) { qdisc_qstats_drop(sch); __qdisc_drop(skb, to_free); return NET_XMIT_SUCCESS | __NET_XMIT_BYPASS; } if (q->latency || q->jitter || q->rate) skb_orphan_partial(skb); <S2SV_StartVul> if (count > 1 && (skb2 = skb_clone(skb, GFP_ATOMIC)) != NULL) { <S2SV_EndVul> <S2SV_StartVul> struct Qdisc *rootq = qdisc_root_bh(sch); <S2SV_EndVul> u32 dupsave = q->duplicate; <S2SV_StartVul> q->duplicate = 0; <S2SV_EndVul> <S2SV_StartVul> rootq->enqueue(skb2, rootq, to_free); <S2SV_EndVul> <S2SV_StartVul> q->duplicate = dupsave; <S2SV_EndVul> <S2SV_StartVul> rc_drop = NET_XMIT_SUCCESS; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> if (q->corrupt && q->corrupt >= get_crandom(&q->corrupt_cor, &q->prng)) { if (skb_is_gso(skb)) { skb = netem_segment(skb, sch, to_free); if (!skb) <S2SV_StartVul> return rc_drop; <S2SV_EndVul> segs = skb->next; skb_mark_not_on_list(skb); qdisc_skb_cb(skb)->pkt_len = skb->len; } skb = skb_unshare(skb, GFP_ATOMIC); if (unlikely(!skb)) { qdisc_qstats_drop(sch); goto finish_segs; } if (skb->ip_summed == CHECKSUM_PARTIAL && skb_checksum_help(skb)) { qdisc_drop(skb, sch, to_free); skb = NULL; goto finish_segs; } skb->data[get_random_u32_below(skb_headlen(skb))] ^= 1<<get_random_u32_below(8); } if (unlikely(sch->q.qlen >= sch->limit)) { skb->next = segs; qdisc_drop_all(skb, sch, to_free); <S2SV_StartVul> return rc_drop; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> qdisc_qstats_backlog_inc(sch, skb); cb = netem_skb_cb(skb); if (q->gap == 0 || q->counter < q->gap - 1 || q->reorder < get_crandom(&q->reorder_cor, &q->prng)) { u64 now; s64 delay; delay = tabledist(q->latency, q->jitter, &q->delay_cor, &q->prng, q->delay_dist); now = ktime_get_ns(); if (q->rate) { struct netem_skb_cb *last = NULL; if (sch->q.tail) last = netem_skb_cb(sch->q.tail); if (q->t_root.rb_node) { struct sk_buff *t_skb; struct netem_skb_cb *t_last; t_skb = skb_rb_last(&q->t_root); t_last = netem_skb_cb(t_skb); if (!last || t_last->time_to_send > last->time_to_send) last = t_last; } if (q->t_tail) { struct netem_skb_cb *t_last = netem_skb_cb(q->t_tail); if (!last || t_last->time_to_send > last->time_to_send) last = t_last; } if (last) { delay -= last->time_to_send - now; delay = max_t(s64, 0, delay); now = last->time_to_send; } delay += packet_time_ns(qdisc_pkt_len(skb), q); } cb->time_to_send = now + delay; ++q->counter; tfifo_enqueue(skb, sch); } else { cb->time_to_send = ktime_get_ns(); q->counter = 0; __qdisc_enqueue_head(skb, &sch->q); sch->qstats.requeues++; } finish_segs: if (segs) { unsigned int len, last_len; <S2SV_StartVul> int nb; <S2SV_EndVul> len = skb ? skb->len : 0; nb = skb ? 1 : 0; while (segs) { skb2 = segs->next; skb_mark_not_on_list(segs); qdisc_skb_cb(segs)->pkt_len = segs->len; last_len = segs->len; rc = qdisc_enqueue(segs, sch, to_free); if (rc != NET_XMIT_SUCCESS) { if (net_xmit_drop_count(rc)) qdisc_qstats_drop(sch); } else { nb++; len += last_len; } segs = skb2; } qdisc_tree_reduce_backlog(sch, -(nb - 1), -(len - prev_len)); } else if (!skb) { return NET_XMIT_DROP; } return NET_XMIT_SUCCESS; }","- struct sk_buff *skb2;
- int rc = NET_XMIT_SUCCESS;
- int rc_drop = NET_XMIT_DROP;
- if (count > 1 && (skb2 = skb_clone(skb, GFP_ATOMIC)) != NULL) {
- struct Qdisc *rootq = qdisc_root_bh(sch);
- q->duplicate = 0;
- rootq->enqueue(skb2, rootq, to_free);
- q->duplicate = dupsave;
- rc_drop = NET_XMIT_SUCCESS;
- }
- return rc_drop;
- return rc_drop;
- }
- int nb;
+ struct sk_buff *skb2 = NULL;
+ if (count > 1)
+ skb2 = skb_clone(skb, GFP_ATOMIC);
+ goto finish_segs;
+ if (skb2)
+ __qdisc_drop(skb2, to_free);
+ return NET_XMIT_DROP;
+ }
+ if (skb2) {
+ struct Qdisc *rootq = qdisc_root_bh(sch);
+ q->duplicate = 0;
+ rootq->enqueue(skb2, rootq, to_free);
+ q->duplicate = dupsave;
+ skb2 = NULL;
+ }
+ if (skb2)
+ __qdisc_drop(skb2, to_free);
+ int rc, nb;","static int netem_enqueue(struct sk_buff *skb, struct Qdisc *sch, struct sk_buff **to_free) { struct netem_sched_data *q = qdisc_priv(sch); struct netem_skb_cb *cb; struct sk_buff *skb2 = NULL; struct sk_buff *segs = NULL; unsigned int prev_len = qdisc_pkt_len(skb); int count = 1; skb->prev = NULL; if (q->duplicate && q->duplicate >= get_crandom(&q->dup_cor, &q->prng)) ++count; if (loss_event(q)) { if (q->ecn && INET_ECN_set_ce(skb)) qdisc_qstats_drop(sch); else --count; } if (count == 0) { qdisc_qstats_drop(sch); __qdisc_drop(skb, to_free); return NET_XMIT_SUCCESS | __NET_XMIT_BYPASS; } if (q->latency || q->jitter || q->rate) skb_orphan_partial(skb); if (count > 1) skb2 = skb_clone(skb, GFP_ATOMIC); if (q->corrupt && q->corrupt >= get_crandom(&q->corrupt_cor, &q->prng)) { if (skb_is_gso(skb)) { skb = netem_segment(skb, sch, to_free); if (!skb) goto finish_segs; segs = skb->next; skb_mark_not_on_list(skb); qdisc_skb_cb(skb)->pkt_len = skb->len; } skb = skb_unshare(skb, GFP_ATOMIC); if (unlikely(!skb)) { qdisc_qstats_drop(sch); goto finish_segs; } if (skb->ip_summed == CHECKSUM_PARTIAL && skb_checksum_help(skb)) { qdisc_drop(skb, sch, to_free); skb = NULL; goto finish_segs; } skb->data[get_random_u32_below(skb_headlen(skb))] ^= 1<<get_random_u32_below(8); } if (unlikely(sch->q.qlen >= sch->limit)) { skb->next = segs; qdisc_drop_all(skb, sch, to_free); if (skb2) __qdisc_drop(skb2, to_free); return NET_XMIT_DROP; } if (skb2) { struct Qdisc *rootq = qdisc_root_bh(sch); u32 dupsave = q->duplicate; q->duplicate = 0; rootq->enqueue(skb2, rootq, to_free); q->duplicate = dupsave; skb2 = NULL; } qdisc_qstats_backlog_inc(sch, skb); cb = netem_skb_cb(skb); if (q->gap == 0 || q->counter < q->gap - 1 || q->reorder < get_crandom(&q->reorder_cor, &q->prng)) { u64 now; s64 delay; delay = tabledist(q->latency, q->jitter, &q->delay_cor, &q->prng, q->delay_dist); now = ktime_get_ns(); if (q->rate) { struct netem_skb_cb *last = NULL; if (sch->q.tail) last = netem_skb_cb(sch->q.tail); if (q->t_root.rb_node) { struct sk_buff *t_skb; struct netem_skb_cb *t_last; t_skb = skb_rb_last(&q->t_root); t_last = netem_skb_cb(t_skb); if (!last || t_last->time_to_send > last->time_to_send) last = t_last; } if (q->t_tail) { struct netem_skb_cb *t_last = netem_skb_cb(q->t_tail); if (!last || t_last->time_to_send > last->time_to_send) last = t_last; } if (last) { delay -= last->time_to_send - now; delay = max_t(s64, 0, delay); now = last->time_to_send; } delay += packet_time_ns(qdisc_pkt_len(skb), q); } cb->time_to_send = now + delay; ++q->counter; tfifo_enqueue(skb, sch); } else { cb->time_to_send = ktime_get_ns(); q->counter = 0; __qdisc_enqueue_head(skb, &sch->q); sch->qstats.requeues++; } finish_segs: if (skb2) __qdisc_drop(skb2, to_free); if (segs) { unsigned int len, last_len; int rc, nb; len = skb ? skb->len : 0; nb = skb ? 1 : 0; while (segs) { skb2 = segs->next; skb_mark_not_on_list(segs); qdisc_skb_cb(segs)->pkt_len = segs->len; last_len = segs->len; rc = qdisc_enqueue(segs, sch, to_free); if (rc != NET_XMIT_SUCCESS) { if (net_xmit_drop_count(rc)) qdisc_qstats_drop(sch); } else { nb++; len += last_len; } segs = skb2; } qdisc_tree_reduce_backlog(sch, -(nb - 1), -(len - prev_len)); } else if (!skb) { return NET_XMIT_DROP; } return NET_XMIT_SUCCESS; }"
447----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47686/bad/clock.c----ep93xx_div_recalc_rate,"static unsigned long ep93xx_div_recalc_rate(struct clk_hw *hw, unsigned long parent_rate) { struct clk_psc *psc = to_clk_psc(hw); u32 val = __raw_readl(psc->reg); u8 index = (val & psc->mask) >> psc->shift; <S2SV_StartVul> if (index > psc->num_div) <S2SV_EndVul> return 0; return DIV_ROUND_UP_ULL(parent_rate, psc->div[index]); }","- if (index > psc->num_div)
+ if (index >= psc->num_div)","static unsigned long ep93xx_div_recalc_rate(struct clk_hw *hw, unsigned long parent_rate) { struct clk_psc *psc = to_clk_psc(hw); u32 val = __raw_readl(psc->reg); u8 index = (val & psc->mask) >> psc->shift; if (index >= psc->num_div) return 0; return DIV_ROUND_UP_ULL(parent_rate, psc->div[index]); }"
287----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46763/bad/fou_core.c----fou_gro_complete,"static int fou_gro_complete(struct sock *sk, struct sk_buff *skb, int nhoff) { const struct net_offload __rcu **offloads; <S2SV_StartVul> u8 proto = fou_from_sock(sk)->protocol; <S2SV_EndVul> const struct net_offload *ops; <S2SV_StartVul> int err = -ENOSYS; <S2SV_EndVul> offloads = NAPI_GRO_CB(skb)->is_ipv6 ? inet6_offloads : inet_offloads; ops = rcu_dereference(offloads[proto]); <S2SV_StartVul> if (WARN_ON(!ops || !ops->callbacks.gro_complete)) <S2SV_EndVul> goto out; err = ops->callbacks.gro_complete(skb, nhoff); skb_set_inner_mac_header(skb, nhoff); out: return err; }","- u8 proto = fou_from_sock(sk)->protocol;
- int err = -ENOSYS;
- if (WARN_ON(!ops || !ops->callbacks.gro_complete))
+ struct fou *fou = fou_from_sock(sk);
+ u8 proto;
+ int err;
+ if (!fou) {
+ err = -ENOENT;
+ goto out;
+ }
+ proto = fou->protocol;
+ if (WARN_ON(!ops || !ops->callbacks.gro_complete)) {
+ err = -ENOSYS;
+ goto out;
+ }","static int fou_gro_complete(struct sock *sk, struct sk_buff *skb, int nhoff) { const struct net_offload __rcu **offloads; struct fou *fou = fou_from_sock(sk); const struct net_offload *ops; u8 proto; int err; if (!fou) { err = -ENOENT; goto out; } proto = fou->protocol; offloads = NAPI_GRO_CB(skb)->is_ipv6 ? inet6_offloads : inet_offloads; ops = rcu_dereference(offloads[proto]); if (WARN_ON(!ops || !ops->callbacks.gro_complete)) { err = -ENOSYS; goto out; } err = ops->callbacks.gro_complete(skb, nhoff); skb_set_inner_mac_header(skb, nhoff); out: return err; }"
1137----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53113/bad/page_alloc.c----__alloc_pages_bulk,"unsigned long __alloc_pages_bulk(gfp_t gfp, int preferred_nid, nodemask_t *nodemask, int nr_pages, struct list_head *page_list, struct page **page_array) { struct page *page; unsigned long __maybe_unused UP_flags; struct zone *zone; struct zoneref *z; struct per_cpu_pages *pcp; struct list_head *pcp_list; struct alloc_context ac; gfp_t alloc_gfp; unsigned int alloc_flags = ALLOC_WMARK_LOW; int nr_populated = 0, nr_account = 0; while (page_array && nr_populated < nr_pages && page_array[nr_populated]) nr_populated++; if (unlikely(nr_pages <= 0)) goto out; if (unlikely(page_array && nr_pages - nr_populated == 0)) goto out; if (memcg_kmem_online() && (gfp & __GFP_ACCOUNT)) goto failed; if (nr_pages - nr_populated == 1) goto failed; #ifdef CONFIG_PAGE_OWNER if (static_branch_unlikely(&page_owner_inited)) goto failed; #endif gfp &= gfp_allowed_mask; alloc_gfp = gfp; if (!prepare_alloc_pages(gfp, 0, preferred_nid, nodemask, &ac, &alloc_gfp, &alloc_flags)) goto out; gfp = alloc_gfp; <S2SV_StartVul> for_each_zone_zonelist_nodemask(zone, z, ac.zonelist, ac.highest_zoneidx, ac.nodemask) { <S2SV_EndVul> unsigned long mark; if (cpusets_enabled() && (alloc_flags & ALLOC_CPUSET) && !__cpuset_zone_allowed(zone, gfp)) { continue; } if (nr_online_nodes > 1 && zone != ac.preferred_zoneref->zone && zone_to_nid(zone) != zone_to_nid(ac.preferred_zoneref->zone)) { goto failed; } mark = wmark_pages(zone, alloc_flags & ALLOC_WMARK_MASK) + nr_pages; if (zone_watermark_fast(zone, 0, mark, zonelist_zone_idx(ac.preferred_zoneref), alloc_flags, gfp)) { break; } } if (unlikely(!zone)) goto failed; pcp_trylock_prepare(UP_flags); pcp = pcp_spin_trylock(zone->per_cpu_pageset); if (!pcp) goto failed_irq; pcp_list = &pcp->lists[order_to_pindex(ac.migratetype, 0)]; while (nr_populated < nr_pages) { if (page_array && page_array[nr_populated]) { nr_populated++; continue; } page = __rmqueue_pcplist(zone, 0, ac.migratetype, alloc_flags, pcp, pcp_list); if (unlikely(!page)) { if (!nr_account) { pcp_spin_unlock(pcp); goto failed_irq; } break; } nr_account++; prep_new_page(page, 0, gfp, 0); if (page_list) list_add(&page->lru, page_list); else page_array[nr_populated] = page; nr_populated++; } pcp_spin_unlock(pcp); pcp_trylock_finish(UP_flags); __count_zid_vm_events(PGALLOC, zone_idx(zone), nr_account); zone_statistics(ac.preferred_zoneref->zone, zone, nr_account); out: return nr_populated; failed_irq: pcp_trylock_finish(UP_flags); failed: page = __alloc_pages(gfp, 0, preferred_nid, nodemask); if (page) { if (page_list) list_add(&page->lru, page_list); else page_array[nr_populated] = page; nr_populated++; } goto out; }","- for_each_zone_zonelist_nodemask(zone, z, ac.zonelist, ac.highest_zoneidx, ac.nodemask) {
+ z = ac.preferred_zoneref;
+ for_next_zone_zonelist_nodemask(zone, z, ac.highest_zoneidx, ac.nodemask) {","unsigned long __alloc_pages_bulk(gfp_t gfp, int preferred_nid, nodemask_t *nodemask, int nr_pages, struct list_head *page_list, struct page **page_array) { struct page *page; unsigned long __maybe_unused UP_flags; struct zone *zone; struct zoneref *z; struct per_cpu_pages *pcp; struct list_head *pcp_list; struct alloc_context ac; gfp_t alloc_gfp; unsigned int alloc_flags = ALLOC_WMARK_LOW; int nr_populated = 0, nr_account = 0; while (page_array && nr_populated < nr_pages && page_array[nr_populated]) nr_populated++; if (unlikely(nr_pages <= 0)) goto out; if (unlikely(page_array && nr_pages - nr_populated == 0)) goto out; if (memcg_kmem_online() && (gfp & __GFP_ACCOUNT)) goto failed; if (nr_pages - nr_populated == 1) goto failed; #ifdef CONFIG_PAGE_OWNER if (static_branch_unlikely(&page_owner_inited)) goto failed; #endif gfp &= gfp_allowed_mask; alloc_gfp = gfp; if (!prepare_alloc_pages(gfp, 0, preferred_nid, nodemask, &ac, &alloc_gfp, &alloc_flags)) goto out; gfp = alloc_gfp; z = ac.preferred_zoneref; for_next_zone_zonelist_nodemask(zone, z, ac.highest_zoneidx, ac.nodemask) { unsigned long mark; if (cpusets_enabled() && (alloc_flags & ALLOC_CPUSET) && !__cpuset_zone_allowed(zone, gfp)) { continue; } if (nr_online_nodes > 1 && zone != ac.preferred_zoneref->zone && zone_to_nid(zone) != zone_to_nid(ac.preferred_zoneref->zone)) { goto failed; } mark = wmark_pages(zone, alloc_flags & ALLOC_WMARK_MASK) + nr_pages; if (zone_watermark_fast(zone, 0, mark, zonelist_zone_idx(ac.preferred_zoneref), alloc_flags, gfp)) { break; } } if (unlikely(!zone)) goto failed; pcp_trylock_prepare(UP_flags); pcp = pcp_spin_trylock(zone->per_cpu_pageset); if (!pcp) goto failed_irq; pcp_list = &pcp->lists[order_to_pindex(ac.migratetype, 0)]; while (nr_populated < nr_pages) { if (page_array && page_array[nr_populated]) { nr_populated++; continue; } page = __rmqueue_pcplist(zone, 0, ac.migratetype, alloc_flags, pcp, pcp_list); if (unlikely(!page)) { if (!nr_account) { pcp_spin_unlock(pcp); goto failed_irq; } break; } nr_account++; prep_new_page(page, 0, gfp, 0); if (page_list) list_add(&page->lru, page_list); else page_array[nr_populated] = page; nr_populated++; } pcp_spin_unlock(pcp); pcp_trylock_finish(UP_flags); __count_zid_vm_events(PGALLOC, zone_idx(zone), nr_account); zone_statistics(ac.preferred_zoneref->zone, zone, nr_account); out: return nr_populated; failed_irq: pcp_trylock_finish(UP_flags); failed: page = __alloc_pages(gfp, 0, preferred_nid, nodemask); if (page) { if (page_list) list_add(&page->lru, page_list); else page_array[nr_populated] = page; nr_populated++; } goto out; }"
401----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46859/bad/panasonic-laptop.c----acpi_pcc_hotkey_add,"static int acpi_pcc_hotkey_add(struct acpi_device *device) { struct backlight_properties props; struct pcc_acpi *pcc; int num_sifr, result; if (!device) return -EINVAL; num_sifr = acpi_pcc_get_sqty(device); <S2SV_StartVul> if (num_sifr < 0 || num_sifr > 255) { <S2SV_EndVul> <S2SV_StartVul> pr_err(""num_sifr out of range""); <S2SV_EndVul> return -ENODEV; } pcc = kzalloc(sizeof(struct pcc_acpi), GFP_KERNEL); if (!pcc) { pr_err(""Couldn't allocate mem for pcc""); return -ENOMEM; } pcc->sinf = kcalloc(num_sifr + 1, sizeof(u32), GFP_KERNEL); if (!pcc->sinf) { result = -ENOMEM; goto out_hotkey; } pcc->device = device; pcc->handle = device->handle; pcc->num_sifr = num_sifr; device->driver_data = pcc; strcpy(acpi_device_name(device), ACPI_PCC_DEVICE_NAME); strcpy(acpi_device_class(device), ACPI_PCC_CLASS); result = acpi_pcc_init_input(pcc); if (result) { pr_err(""Error installing keyinput handler\n""); goto out_sinf; } if (!acpi_pcc_retrieve_biosdata(pcc)) { result = -EIO; pr_err(""Couldn't retrieve BIOS data\n""); goto out_input; } if (acpi_video_get_backlight_type() == acpi_backlight_vendor) { memset(&props, 0, sizeof(struct backlight_properties)); props.type = BACKLIGHT_PLATFORM; props.max_brightness = pcc->sinf[SINF_AC_MAX_BRIGHT]; pcc->backlight = backlight_device_register(""panasonic"", NULL, pcc, &pcc_backlight_ops, &props); if (IS_ERR(pcc->backlight)) { result = PTR_ERR(pcc->backlight); goto out_input; } pcc->backlight->props.brightness = pcc->sinf[SINF_AC_CUR_BRIGHT]; } acpi_pcc_write_sset(pcc, SINF_STICKY_KEY, 0); pcc->sticky_key = 0; <S2SV_StartVul> pcc->eco_mode = pcc->sinf[SINF_ECO_MODE]; <S2SV_EndVul> <S2SV_StartVul> pcc->mute = pcc->sinf[SINF_MUTE]; <S2SV_EndVul> pcc->ac_brightness = pcc->sinf[SINF_AC_CUR_BRIGHT]; pcc->dc_brightness = pcc->sinf[SINF_DC_CUR_BRIGHT]; <S2SV_StartVul> pcc->current_brightness = pcc->sinf[SINF_CUR_BRIGHT]; <S2SV_EndVul> result = sysfs_create_group(&device->dev.kobj, &pcc_attr_group); if (result) goto out_backlight; if (ACPI_SUCCESS(check_optd_present())) { pcc->platform = platform_device_register_simple(""panasonic"", PLATFORM_DEVID_NONE, NULL, 0); if (IS_ERR(pcc->platform)) { result = PTR_ERR(pcc->platform); goto out_backlight; } result = device_create_file(&pcc->platform->dev, &dev_attr_cdpower); pcc_register_optd_notifier(pcc, ""\\_SB.PCI0.EHCI.ERHB.OPTD""); if (result) goto out_platform; } else { pcc->platform = NULL; } i8042_install_filter(panasonic_i8042_filter); return 0; out_platform: platform_device_unregister(pcc->platform); out_backlight: backlight_device_unregister(pcc->backlight); out_input: input_unregister_device(pcc->input_dev); out_sinf: kfree(pcc->sinf); out_hotkey: kfree(pcc); return result; }","- if (num_sifr < 0 || num_sifr > 255) {
- pr_err(""num_sifr out of range"");
- pcc->eco_mode = pcc->sinf[SINF_ECO_MODE];
- pcc->mute = pcc->sinf[SINF_MUTE];
- pcc->current_brightness = pcc->sinf[SINF_CUR_BRIGHT];
+ if (num_sifr <= SINF_DC_CUR_BRIGHT || num_sifr > 255) {
+ pr_err(""num_sifr %d out of range %d - 255\n"", num_sifr, SINF_DC_CUR_BRIGHT + 1);
+ }
+ if (pcc->num_sifr > SINF_MUTE)
+ pcc->mute = pcc->sinf[SINF_MUTE];
+ if (pcc->num_sifr > SINF_ECO_MODE)
+ pcc->eco_mode = pcc->sinf[SINF_ECO_MODE];
+ if (pcc->num_sifr > SINF_CUR_BRIGHT)
+ pcc->current_brightness = pcc->sinf[SINF_CUR_BRIGHT];","static int acpi_pcc_hotkey_add(struct acpi_device *device) { struct backlight_properties props; struct pcc_acpi *pcc; int num_sifr, result; if (!device) return -EINVAL; num_sifr = acpi_pcc_get_sqty(device); if (num_sifr <= SINF_DC_CUR_BRIGHT || num_sifr > 255) { pr_err(""num_sifr %d out of range %d - 255\n"", num_sifr, SINF_DC_CUR_BRIGHT + 1); return -ENODEV; } pcc = kzalloc(sizeof(struct pcc_acpi), GFP_KERNEL); if (!pcc) { pr_err(""Couldn't allocate mem for pcc""); return -ENOMEM; } pcc->sinf = kcalloc(num_sifr + 1, sizeof(u32), GFP_KERNEL); if (!pcc->sinf) { result = -ENOMEM; goto out_hotkey; } pcc->device = device; pcc->handle = device->handle; pcc->num_sifr = num_sifr; device->driver_data = pcc; strcpy(acpi_device_name(device), ACPI_PCC_DEVICE_NAME); strcpy(acpi_device_class(device), ACPI_PCC_CLASS); result = acpi_pcc_init_input(pcc); if (result) { pr_err(""Error installing keyinput handler\n""); goto out_sinf; } if (!acpi_pcc_retrieve_biosdata(pcc)) { result = -EIO; pr_err(""Couldn't retrieve BIOS data\n""); goto out_input; } if (acpi_video_get_backlight_type() == acpi_backlight_vendor) { memset(&props, 0, sizeof(struct backlight_properties)); props.type = BACKLIGHT_PLATFORM; props.max_brightness = pcc->sinf[SINF_AC_MAX_BRIGHT]; pcc->backlight = backlight_device_register(""panasonic"", NULL, pcc, &pcc_backlight_ops, &props); if (IS_ERR(pcc->backlight)) { result = PTR_ERR(pcc->backlight); goto out_input; } pcc->backlight->props.brightness = pcc->sinf[SINF_AC_CUR_BRIGHT]; } acpi_pcc_write_sset(pcc, SINF_STICKY_KEY, 0); pcc->sticky_key = 0; pcc->ac_brightness = pcc->sinf[SINF_AC_CUR_BRIGHT]; pcc->dc_brightness = pcc->sinf[SINF_DC_CUR_BRIGHT]; if (pcc->num_sifr > SINF_MUTE) pcc->mute = pcc->sinf[SINF_MUTE]; if (pcc->num_sifr > SINF_ECO_MODE) pcc->eco_mode = pcc->sinf[SINF_ECO_MODE]; if (pcc->num_sifr > SINF_CUR_BRIGHT) pcc->current_brightness = pcc->sinf[SINF_CUR_BRIGHT]; result = sysfs_create_group(&device->dev.kobj, &pcc_attr_group); if (result) goto out_backlight; if (ACPI_SUCCESS(check_optd_present())) { pcc->platform = platform_device_register_simple(""panasonic"", PLATFORM_DEVID_NONE, NULL, 0); if (IS_ERR(pcc->platform)) { result = PTR_ERR(pcc->platform); goto out_backlight; } result = device_create_file(&pcc->platform->dev, &dev_attr_cdpower); pcc_register_optd_notifier(pcc, ""\\_SB.PCI0.EHCI.ERHB.OPTD""); if (result) goto out_platform; } else { pcc->platform = NULL; } i8042_install_filter(panasonic_i8042_filter); return 0; out_platform: platform_device_unregister(pcc->platform); out_backlight: backlight_device_unregister(pcc->backlight); out_input: input_unregister_device(pcc->input_dev); out_sinf: kfree(pcc->sinf); out_hotkey: kfree(pcc); return result; }"
384----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46846/bad/spi-rockchip.c----rockchip_spi_suspend,static int rockchip_spi_suspend(struct device *dev) { int ret; struct spi_controller *ctlr = dev_get_drvdata(dev); <S2SV_StartVul> struct rockchip_spi *rs = spi_controller_get_devdata(ctlr); <S2SV_EndVul> ret = spi_controller_suspend(ctlr); <S2SV_StartVul> if (ret < 0) <S2SV_EndVul> return ret; <S2SV_StartVul> clk_disable_unprepare(rs->spiclk); <S2SV_EndVul> <S2SV_StartVul> clk_disable_unprepare(rs->apb_pclk); <S2SV_EndVul> pinctrl_pm_select_sleep_state(dev); return 0; },"- struct rockchip_spi *rs = spi_controller_get_devdata(ctlr);
- if (ret < 0)
- clk_disable_unprepare(rs->spiclk);
- clk_disable_unprepare(rs->apb_pclk);
+ return ret;
+ ret = pm_runtime_force_suspend(dev);
+ if (ret < 0) {
+ spi_controller_resume(ctlr);
+ return ret;
+ }",static int rockchip_spi_suspend(struct device *dev) { int ret; struct spi_controller *ctlr = dev_get_drvdata(dev); ret = spi_controller_suspend(ctlr); if (ret < 0) return ret; ret = pm_runtime_force_suspend(dev); if (ret < 0) { spi_controller_resume(ctlr); return ret; } pinctrl_pm_select_sleep_state(dev); return 0; }
698----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49988/bad/oplock.c----smb_break_all_write_oplock,"static void smb_break_all_write_oplock(struct ksmbd_work *work, struct ksmbd_file *fp, int is_trunc) { struct oplock_info *brk_opinfo; brk_opinfo = opinfo_get_list(fp->f_ci); if (!brk_opinfo) return; if (brk_opinfo->level != SMB2_OPLOCK_LEVEL_BATCH && brk_opinfo->level != SMB2_OPLOCK_LEVEL_EXCLUSIVE) { <S2SV_StartVul> opinfo_conn_put(brk_opinfo); <S2SV_EndVul> return; <S2SV_StartVul> } <S2SV_EndVul> brk_opinfo->open_trunc = is_trunc; list_add(&work->interim_entry, &brk_opinfo->interim_list); oplock_break(brk_opinfo, SMB2_OPLOCK_LEVEL_II); <S2SV_StartVul> opinfo_conn_put(brk_opinfo); <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul>","- opinfo_conn_put(brk_opinfo);
- }
- opinfo_conn_put(brk_opinfo);
- }
+ opinfo_put(brk_opinfo);
+ opinfo_put(brk_opinfo);","static void smb_break_all_write_oplock(struct ksmbd_work *work, struct ksmbd_file *fp, int is_trunc) { struct oplock_info *brk_opinfo; brk_opinfo = opinfo_get_list(fp->f_ci); if (!brk_opinfo) return; if (brk_opinfo->level != SMB2_OPLOCK_LEVEL_BATCH && brk_opinfo->level != SMB2_OPLOCK_LEVEL_EXCLUSIVE) { opinfo_put(brk_opinfo); return; } brk_opinfo->open_trunc = is_trunc; list_add(&work->interim_entry, &brk_opinfo->interim_list); oplock_break(brk_opinfo, SMB2_OPLOCK_LEVEL_II); opinfo_put(brk_opinfo); }"
744----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50021/bad/ice_dpll.c----ice_dpll_init_rclk_pins,"ice_dpll_init_rclk_pins(struct ice_pf *pf, struct ice_dpll_pin *pin, int start_idx, const struct dpll_pin_ops *ops) { struct ice_vsi *vsi = ice_get_main_vsi(pf); struct dpll_pin *parent; int ret, i; ret = ice_dpll_get_pins(pf, pin, start_idx, ICE_DPLL_RCLK_NUM_PER_PF, pf->dplls.clock_id); if (ret) return ret; for (i = 0; i < pf->dplls.rclk.num_parents; i++) { parent = pf->dplls.inputs[pf->dplls.rclk.parent_idx[i]].pin; if (!parent) { ret = -ENODEV; goto unregister_pins; } ret = dpll_pin_on_pin_register(parent, pf->dplls.rclk.pin, ops, &pf->dplls.rclk); if (ret) goto unregister_pins; } <S2SV_StartVul> if (WARN_ON((!vsi || !vsi->netdev))) <S2SV_EndVul> <S2SV_StartVul> return -EINVAL; <S2SV_EndVul> dpll_netdev_pin_set(vsi->netdev, pf->dplls.rclk.pin); return 0; unregister_pins: while (i) { parent = pf->dplls.inputs[pf->dplls.rclk.parent_idx[--i]].pin; dpll_pin_on_pin_unregister(parent, pf->dplls.rclk.pin, &ice_dpll_rclk_ops, &pf->dplls.rclk); } ice_dpll_release_pins(pin, ICE_DPLL_RCLK_NUM_PER_PF); return ret; }","- if (WARN_ON((!vsi || !vsi->netdev)))
- return -EINVAL;
+ if (WARN_ON((!vsi || !vsi->netdev)))
+ return -EINVAL;","ice_dpll_init_rclk_pins(struct ice_pf *pf, struct ice_dpll_pin *pin, int start_idx, const struct dpll_pin_ops *ops) { struct ice_vsi *vsi = ice_get_main_vsi(pf); struct dpll_pin *parent; int ret, i; if (WARN_ON((!vsi || !vsi->netdev))) return -EINVAL; ret = ice_dpll_get_pins(pf, pin, start_idx, ICE_DPLL_RCLK_NUM_PER_PF, pf->dplls.clock_id); if (ret) return ret; for (i = 0; i < pf->dplls.rclk.num_parents; i++) { parent = pf->dplls.inputs[pf->dplls.rclk.parent_idx[i]].pin; if (!parent) { ret = -ENODEV; goto unregister_pins; } ret = dpll_pin_on_pin_register(parent, pf->dplls.rclk.pin, ops, &pf->dplls.rclk); if (ret) goto unregister_pins; } dpll_netdev_pin_set(vsi->netdev, pf->dplls.rclk.pin); return 0; unregister_pins: while (i) { parent = pf->dplls.inputs[pf->dplls.rclk.parent_idx[--i]].pin; dpll_pin_on_pin_unregister(parent, pf->dplls.rclk.pin, &ice_dpll_rclk_ops, &pf->dplls.rclk); } ice_dpll_release_pins(pin, ICE_DPLL_RCLK_NUM_PER_PF); return ret; }"
1001----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50237/bad/cfg.c----ieee80211_get_tx_power,"static int ieee80211_get_tx_power(struct wiphy *wiphy, struct wireless_dev *wdev, int *dbm) { struct ieee80211_local *local = wiphy_priv(wiphy); struct ieee80211_sub_if_data *sdata = IEEE80211_WDEV_TO_SUB_IF(wdev); <S2SV_StartVul> if (local->ops->get_txpower) <S2SV_EndVul> return drv_get_txpower(local, sdata, dbm); if (!local->use_chanctx) *dbm = local->hw.conf.power_level; else *dbm = sdata->vif.bss_conf.txpower; if (*dbm == INT_MIN) return -EINVAL; return 0; }","- if (local->ops->get_txpower)
+ if (local->ops->get_txpower &&
+ (sdata->flags & IEEE80211_SDATA_IN_DRIVER))","static int ieee80211_get_tx_power(struct wiphy *wiphy, struct wireless_dev *wdev, int *dbm) { struct ieee80211_local *local = wiphy_priv(wiphy); struct ieee80211_sub_if_data *sdata = IEEE80211_WDEV_TO_SUB_IF(wdev); if (local->ops->get_txpower && (sdata->flags & IEEE80211_SDATA_IN_DRIVER)) return drv_get_txpower(local, sdata, dbm); if (!local->use_chanctx) *dbm = local->hw.conf.power_level; else *dbm = sdata->vif.bss_conf.txpower; if (*dbm == INT_MIN) return -EINVAL; return 0; }"
119----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44978/bad/xe_sched_job.c----xe_sched_job_destroy,"void xe_sched_job_destroy(struct kref *ref) { struct xe_sched_job *job = container_of(ref, struct xe_sched_job, refcount); struct xe_device *xe = job_to_xe(job); xe_sched_job_free_fences(job); <S2SV_StartVul> xe_exec_queue_put(job->q); <S2SV_EndVul> dma_fence_put(job->fence); drm_sched_job_cleanup(&job->drm); job_free(job); xe_pm_runtime_put(xe); }","- xe_exec_queue_put(job->q);
+ struct xe_exec_queue *q = job->q;
+ xe_exec_queue_put(q);","void xe_sched_job_destroy(struct kref *ref) { struct xe_sched_job *job = container_of(ref, struct xe_sched_job, refcount); struct xe_device *xe = job_to_xe(job); struct xe_exec_queue *q = job->q; xe_sched_job_free_fences(job); dma_fence_put(job->fence); drm_sched_job_cleanup(&job->drm); job_free(job); xe_exec_queue_put(q); xe_pm_runtime_put(xe); }"
1213----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53221/bad/super.c----exit_f2fs_fs,static void __exit exit_f2fs_fs(void) { f2fs_destroy_casefold_cache(); f2fs_destroy_compress_cache(); f2fs_destroy_compress_mempool(); f2fs_destroy_bioset(); f2fs_destroy_bio_entry_cache(); f2fs_destroy_iostat_processing(); f2fs_destroy_post_read_processing(); f2fs_destroy_root_stats(); <S2SV_StartVul> unregister_filesystem(&f2fs_fs_type); <S2SV_EndVul> f2fs_exit_shrinker(); f2fs_exit_sysfs(); f2fs_destroy_garbage_collection_cache(); f2fs_destroy_extent_cache(); f2fs_destroy_recovery_cache(); f2fs_destroy_checkpoint_caches(); f2fs_destroy_segment_manager_caches(); f2fs_destroy_node_manager_caches(); destroy_inodecache(); },- unregister_filesystem(&f2fs_fs_type);,static void __exit exit_f2fs_fs(void) { unregister_filesystem(&f2fs_fs_type); f2fs_destroy_casefold_cache(); f2fs_destroy_compress_cache(); f2fs_destroy_compress_mempool(); f2fs_destroy_bioset(); f2fs_destroy_bio_entry_cache(); f2fs_destroy_iostat_processing(); f2fs_destroy_post_read_processing(); f2fs_destroy_root_stats(); f2fs_exit_shrinker(); f2fs_exit_sysfs(); f2fs_destroy_garbage_collection_cache(); f2fs_destroy_extent_cache(); f2fs_destroy_recovery_cache(); f2fs_destroy_checkpoint_caches(); f2fs_destroy_segment_manager_caches(); f2fs_destroy_node_manager_caches(); destroy_inodecache(); }
329----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46793/bad/bytcht_cx2072x.c----snd_byt_cht_cx2072x_probe,"static int snd_byt_cht_cx2072x_probe(struct platform_device *pdev) { struct snd_soc_acpi_mach *mach; struct acpi_device *adev; int dai_index = 0; bool sof_parent; int i, ret; byt_cht_cx2072x_card.dev = &pdev->dev; mach = dev_get_platdata(&pdev->dev); for (i = 0; i < ARRAY_SIZE(byt_cht_cx2072x_dais); i++) { <S2SV_StartVul> if (byt_cht_cx2072x_dais[i].codecs->name && <S2SV_EndVul> !strcmp(byt_cht_cx2072x_dais[i].codecs->name, ""i2c-14F10720:00"")) { dai_index = i; break; } } adev = acpi_dev_get_first_match_dev(mach->id, NULL, -1); if (adev) { snprintf(codec_name, sizeof(codec_name), ""i2c-%s"", acpi_dev_name(adev)); byt_cht_cx2072x_dais[dai_index].codecs->name = codec_name; } acpi_dev_put(adev); ret = snd_soc_fixup_dai_links_platform_name(&byt_cht_cx2072x_card, mach->mach_params.platform); if (ret) return ret; sof_parent = snd_soc_acpi_sof_parent(&pdev->dev); if (sof_parent) { byt_cht_cx2072x_card.name = SOF_CARD_NAME; byt_cht_cx2072x_card.driver_name = SOF_DRIVER_NAME; } else { byt_cht_cx2072x_card.name = CARD_NAME; byt_cht_cx2072x_card.driver_name = DRIVER_NAME; } if (sof_parent) pdev->dev.driver->pm = &snd_soc_pm_ops; return devm_snd_soc_register_card(&pdev->dev, &byt_cht_cx2072x_card); }","- if (byt_cht_cx2072x_dais[i].codecs->name &&
+ if (byt_cht_cx2072x_dais[i].num_codecs &&","static int snd_byt_cht_cx2072x_probe(struct platform_device *pdev) { struct snd_soc_acpi_mach *mach; struct acpi_device *adev; int dai_index = 0; bool sof_parent; int i, ret; byt_cht_cx2072x_card.dev = &pdev->dev; mach = dev_get_platdata(&pdev->dev); for (i = 0; i < ARRAY_SIZE(byt_cht_cx2072x_dais); i++) { if (byt_cht_cx2072x_dais[i].num_codecs && !strcmp(byt_cht_cx2072x_dais[i].codecs->name, ""i2c-14F10720:00"")) { dai_index = i; break; } } adev = acpi_dev_get_first_match_dev(mach->id, NULL, -1); if (adev) { snprintf(codec_name, sizeof(codec_name), ""i2c-%s"", acpi_dev_name(adev)); byt_cht_cx2072x_dais[dai_index].codecs->name = codec_name; } acpi_dev_put(adev); ret = snd_soc_fixup_dai_links_platform_name(&byt_cht_cx2072x_card, mach->mach_params.platform); if (ret) return ret; sof_parent = snd_soc_acpi_sof_parent(&pdev->dev); if (sof_parent) { byt_cht_cx2072x_card.name = SOF_CARD_NAME; byt_cht_cx2072x_card.driver_name = SOF_DRIVER_NAME; } else { byt_cht_cx2072x_card.name = CARD_NAME; byt_cht_cx2072x_card.driver_name = DRIVER_NAME; } if (sof_parent) pdev->dev.driver->pm = &snd_soc_pm_ops; return devm_snd_soc_register_card(&pdev->dev, &byt_cht_cx2072x_card); }"
154----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-45012/bad/firmware.c----nvkm_firmware_ctor,"nvkm_firmware_ctor(const struct nvkm_firmware_func *func, const char *name, struct nvkm_device *device, const void *src, int len, struct nvkm_firmware *fw) { fw->func = func; fw->name = name; fw->device = device; fw->len = len; switch (fw->func->type) { case NVKM_FIRMWARE_IMG_RAM: fw->img = kmemdup(src, fw->len, GFP_KERNEL); break; case NVKM_FIRMWARE_IMG_DMA: { dma_addr_t addr; len = ALIGN(fw->len, PAGE_SIZE); <S2SV_StartVul> fw->img = dma_alloc_coherent(fw->device->dev, len, &addr, GFP_KERNEL); <S2SV_EndVul> if (fw->img) { memcpy(fw->img, src, fw->len); fw->phys = addr; } sg_init_one(&fw->mem.sgl, fw->img, len); sg_dma_address(&fw->mem.sgl) = fw->phys; sg_dma_len(&fw->mem.sgl) = len; } break; default: WARN_ON(1); return -EINVAL; } if (!fw->img) return -ENOMEM; nvkm_memory_ctor(&nvkm_firmware_mem, &fw->mem.memory); return 0; }","- fw->img = dma_alloc_coherent(fw->device->dev, len, &addr, GFP_KERNEL);
+ fw->img = dma_alloc_noncoherent(fw->device->dev,
+ len, &addr,
+ DMA_TO_DEVICE,
+ GFP_KERNEL);","nvkm_firmware_ctor(const struct nvkm_firmware_func *func, const char *name, struct nvkm_device *device, const void *src, int len, struct nvkm_firmware *fw) { fw->func = func; fw->name = name; fw->device = device; fw->len = len; switch (fw->func->type) { case NVKM_FIRMWARE_IMG_RAM: fw->img = kmemdup(src, fw->len, GFP_KERNEL); break; case NVKM_FIRMWARE_IMG_DMA: { dma_addr_t addr; len = ALIGN(fw->len, PAGE_SIZE); fw->img = dma_alloc_noncoherent(fw->device->dev, len, &addr, DMA_TO_DEVICE, GFP_KERNEL); if (fw->img) { memcpy(fw->img, src, fw->len); fw->phys = addr; } sg_init_one(&fw->mem.sgl, fw->img, len); sg_dma_address(&fw->mem.sgl) = fw->phys; sg_dma_len(&fw->mem.sgl) = len; } break; default: WARN_ON(1); return -EINVAL; } if (!fw->img) return -ENOMEM; nvkm_memory_ctor(&nvkm_firmware_mem, &fw->mem.memory); return 0; }"
735----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50010/bad/exec.c----*do_open_execat,"static struct file *do_open_execat(int fd, struct filename *name, int flags) { struct file *file; int err; struct open_flags open_exec_flags = { .open_flag = O_LARGEFILE | O_RDONLY | __FMODE_EXEC, .acc_mode = MAY_EXEC, .intent = LOOKUP_OPEN, .lookup_flags = LOOKUP_FOLLOW, }; if ((flags & ~(AT_SYMLINK_NOFOLLOW | AT_EMPTY_PATH)) != 0) return ERR_PTR(-EINVAL); if (flags & AT_SYMLINK_NOFOLLOW) open_exec_flags.lookup_flags &= ~LOOKUP_FOLLOW; if (flags & AT_EMPTY_PATH) open_exec_flags.lookup_flags |= LOOKUP_EMPTY; file = do_filp_open(fd, name, &open_exec_flags); if (IS_ERR(file)) <S2SV_StartVul> goto out; <S2SV_EndVul> err = -EACCES; <S2SV_StartVul> if (WARN_ON_ONCE(!S_ISREG(file_inode(file)->i_mode) || <S2SV_EndVul> <S2SV_StartVul> path_noexec(&file->f_path))) <S2SV_EndVul> goto exit; err = deny_write_access(file); if (err) goto exit; <S2SV_StartVul> out: <S2SV_EndVul> return file; exit: fput(file); return ERR_PTR(err); }","- goto out;
- if (WARN_ON_ONCE(!S_ISREG(file_inode(file)->i_mode) ||
- path_noexec(&file->f_path)))
- out:
+ return file;
+ if (WARN_ON_ONCE(!S_ISREG(file_inode(file)->i_mode)) ||
+ path_noexec(&file->f_path))
+ return file;","static struct file *do_open_execat(int fd, struct filename *name, int flags) { struct file *file; int err; struct open_flags open_exec_flags = { .open_flag = O_LARGEFILE | O_RDONLY | __FMODE_EXEC, .acc_mode = MAY_EXEC, .intent = LOOKUP_OPEN, .lookup_flags = LOOKUP_FOLLOW, }; if ((flags & ~(AT_SYMLINK_NOFOLLOW | AT_EMPTY_PATH)) != 0) return ERR_PTR(-EINVAL); if (flags & AT_SYMLINK_NOFOLLOW) open_exec_flags.lookup_flags &= ~LOOKUP_FOLLOW; if (flags & AT_EMPTY_PATH) open_exec_flags.lookup_flags |= LOOKUP_EMPTY; file = do_filp_open(fd, name, &open_exec_flags); if (IS_ERR(file)) return file; err = -EACCES; if (WARN_ON_ONCE(!S_ISREG(file_inode(file)->i_mode)) || path_noexec(&file->f_path)) goto exit; err = deny_write_access(file); if (err) goto exit; return file; exit: fput(file); return ERR_PTR(err); }"
1475----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-57890/bad/uverbs_cmd.c----ib_uverbs_post_send,"static int ib_uverbs_post_send(struct uverbs_attr_bundle *attrs) { struct ib_uverbs_post_send cmd; struct ib_uverbs_post_send_resp resp; struct ib_uverbs_send_wr *user_wr; struct ib_send_wr *wr = NULL, *last, *next; const struct ib_send_wr *bad_wr; struct ib_qp *qp; int i, sg_ind; int is_ud; int ret, ret2; size_t next_size; const struct ib_sge __user *sgls; const void __user *wqes; struct uverbs_req_iter iter; ret = uverbs_request_start(attrs, &iter, &cmd, sizeof(cmd)); if (ret) return ret; <S2SV_StartVul> wqes = uverbs_request_next_ptr(&iter, cmd.wqe_size * cmd.wr_count); <S2SV_EndVul> if (IS_ERR(wqes)) return PTR_ERR(wqes); <S2SV_StartVul> sgls = uverbs_request_next_ptr( <S2SV_EndVul> <S2SV_StartVul> &iter, cmd.sge_count * sizeof(struct ib_uverbs_sge)); <S2SV_EndVul> if (IS_ERR(sgls)) return PTR_ERR(sgls); ret = uverbs_request_finish(&iter); if (ret) return ret; user_wr = kmalloc(cmd.wqe_size, GFP_KERNEL); if (!user_wr) return -ENOMEM; qp = uobj_get_obj_read(qp, UVERBS_OBJECT_QP, cmd.qp_handle, attrs); if (!qp) { ret = -EINVAL; goto out; } is_ud = qp->qp_type == IB_QPT_UD; sg_ind = 0; last = NULL; for (i = 0; i < cmd.wr_count; ++i) { if (copy_from_user(user_wr, wqes + i * cmd.wqe_size, cmd.wqe_size)) { ret = -EFAULT; goto out_put; } if (user_wr->num_sge + sg_ind > cmd.sge_count) { ret = -EINVAL; goto out_put; } if (is_ud) { struct ib_ud_wr *ud; if (user_wr->opcode != IB_WR_SEND && user_wr->opcode != IB_WR_SEND_WITH_IMM) { ret = -EINVAL; goto out_put; } next_size = sizeof(*ud); ud = alloc_wr(next_size, user_wr->num_sge); if (!ud) { ret = -ENOMEM; goto out_put; } ud->ah = uobj_get_obj_read(ah, UVERBS_OBJECT_AH, user_wr->wr.ud.ah, attrs); if (!ud->ah) { kfree(ud); ret = -EINVAL; goto out_put; } ud->remote_qpn = user_wr->wr.ud.remote_qpn; ud->remote_qkey = user_wr->wr.ud.remote_qkey; next = &ud->wr; } else if (user_wr->opcode == IB_WR_RDMA_WRITE_WITH_IMM || user_wr->opcode == IB_WR_RDMA_WRITE || user_wr->opcode == IB_WR_RDMA_READ) { struct ib_rdma_wr *rdma; next_size = sizeof(*rdma); rdma = alloc_wr(next_size, user_wr->num_sge); if (!rdma) { ret = -ENOMEM; goto out_put; } rdma->remote_addr = user_wr->wr.rdma.remote_addr; rdma->rkey = user_wr->wr.rdma.rkey; next = &rdma->wr; } else if (user_wr->opcode == IB_WR_ATOMIC_CMP_AND_SWP || user_wr->opcode == IB_WR_ATOMIC_FETCH_AND_ADD) { struct ib_atomic_wr *atomic; next_size = sizeof(*atomic); atomic = alloc_wr(next_size, user_wr->num_sge); if (!atomic) { ret = -ENOMEM; goto out_put; } atomic->remote_addr = user_wr->wr.atomic.remote_addr; atomic->compare_add = user_wr->wr.atomic.compare_add; atomic->swap = user_wr->wr.atomic.swap; atomic->rkey = user_wr->wr.atomic.rkey; next = &atomic->wr; } else if (user_wr->opcode == IB_WR_SEND || user_wr->opcode == IB_WR_SEND_WITH_IMM || user_wr->opcode == IB_WR_SEND_WITH_INV) { next_size = sizeof(*next); next = alloc_wr(next_size, user_wr->num_sge); if (!next) { ret = -ENOMEM; goto out_put; } } else { ret = -EINVAL; goto out_put; } if (user_wr->opcode == IB_WR_SEND_WITH_IMM || user_wr->opcode == IB_WR_RDMA_WRITE_WITH_IMM) { next->ex.imm_data = (__be32 __force) user_wr->ex.imm_data; } else if (user_wr->opcode == IB_WR_SEND_WITH_INV) { next->ex.invalidate_rkey = user_wr->ex.invalidate_rkey; } if (!last) wr = next; else last->next = next; last = next; next->next = NULL; next->wr_id = user_wr->wr_id; next->num_sge = user_wr->num_sge; next->opcode = user_wr->opcode; next->send_flags = user_wr->send_flags; if (next->num_sge) { next->sg_list = (void *) next + ALIGN(next_size, sizeof(struct ib_sge)); if (copy_from_user(next->sg_list, sgls + sg_ind, next->num_sge * sizeof(struct ib_sge))) { ret = -EFAULT; goto out_put; } sg_ind += next->num_sge; } else next->sg_list = NULL; } resp.bad_wr = 0; ret = qp->device->ops.post_send(qp->real_qp, wr, &bad_wr); if (ret) for (next = wr; next; next = next->next) { ++resp.bad_wr; if (next == bad_wr) break; } ret2 = uverbs_response(attrs, &resp, sizeof(resp)); if (ret2) ret = ret2; out_put: rdma_lookup_put_uobject(&qp->uobject->uevent.uobject, UVERBS_LOOKUP_READ); while (wr) { if (is_ud && ud_wr(wr)->ah) uobj_put_obj_read(ud_wr(wr)->ah); next = wr->next; kfree(wr); wr = next; } out: kfree(user_wr); return ret; }","- wqes = uverbs_request_next_ptr(&iter, cmd.wqe_size * cmd.wr_count);
- sgls = uverbs_request_next_ptr(
- &iter, cmd.sge_count * sizeof(struct ib_uverbs_sge));
+ wqes = uverbs_request_next_ptr(&iter, size_mul(cmd.wqe_size,
+ cmd.wr_count));
+ sgls = uverbs_request_next_ptr(&iter,
+ size_mul(cmd.sge_count,
+ sizeof(struct ib_uverbs_sge)));","static int ib_uverbs_post_send(struct uverbs_attr_bundle *attrs) { struct ib_uverbs_post_send cmd; struct ib_uverbs_post_send_resp resp; struct ib_uverbs_send_wr *user_wr; struct ib_send_wr *wr = NULL, *last, *next; const struct ib_send_wr *bad_wr; struct ib_qp *qp; int i, sg_ind; int is_ud; int ret, ret2; size_t next_size; const struct ib_sge __user *sgls; const void __user *wqes; struct uverbs_req_iter iter; ret = uverbs_request_start(attrs, &iter, &cmd, sizeof(cmd)); if (ret) return ret; wqes = uverbs_request_next_ptr(&iter, size_mul(cmd.wqe_size, cmd.wr_count)); if (IS_ERR(wqes)) return PTR_ERR(wqes); sgls = uverbs_request_next_ptr(&iter, size_mul(cmd.sge_count, sizeof(struct ib_uverbs_sge))); if (IS_ERR(sgls)) return PTR_ERR(sgls); ret = uverbs_request_finish(&iter); if (ret) return ret; user_wr = kmalloc(cmd.wqe_size, GFP_KERNEL); if (!user_wr) return -ENOMEM; qp = uobj_get_obj_read(qp, UVERBS_OBJECT_QP, cmd.qp_handle, attrs); if (!qp) { ret = -EINVAL; goto out; } is_ud = qp->qp_type == IB_QPT_UD; sg_ind = 0; last = NULL; for (i = 0; i < cmd.wr_count; ++i) { if (copy_from_user(user_wr, wqes + i * cmd.wqe_size, cmd.wqe_size)) { ret = -EFAULT; goto out_put; } if (user_wr->num_sge + sg_ind > cmd.sge_count) { ret = -EINVAL; goto out_put; } if (is_ud) { struct ib_ud_wr *ud; if (user_wr->opcode != IB_WR_SEND && user_wr->opcode != IB_WR_SEND_WITH_IMM) { ret = -EINVAL; goto out_put; } next_size = sizeof(*ud); ud = alloc_wr(next_size, user_wr->num_sge); if (!ud) { ret = -ENOMEM; goto out_put; } ud->ah = uobj_get_obj_read(ah, UVERBS_OBJECT_AH, user_wr->wr.ud.ah, attrs); if (!ud->ah) { kfree(ud); ret = -EINVAL; goto out_put; } ud->remote_qpn = user_wr->wr.ud.remote_qpn; ud->remote_qkey = user_wr->wr.ud.remote_qkey; next = &ud->wr; } else if (user_wr->opcode == IB_WR_RDMA_WRITE_WITH_IMM || user_wr->opcode == IB_WR_RDMA_WRITE || user_wr->opcode == IB_WR_RDMA_READ) { struct ib_rdma_wr *rdma; next_size = sizeof(*rdma); rdma = alloc_wr(next_size, user_wr->num_sge); if (!rdma) { ret = -ENOMEM; goto out_put; } rdma->remote_addr = user_wr->wr.rdma.remote_addr; rdma->rkey = user_wr->wr.rdma.rkey; next = &rdma->wr; } else if (user_wr->opcode == IB_WR_ATOMIC_CMP_AND_SWP || user_wr->opcode == IB_WR_ATOMIC_FETCH_AND_ADD) { struct ib_atomic_wr *atomic; next_size = sizeof(*atomic); atomic = alloc_wr(next_size, user_wr->num_sge); if (!atomic) { ret = -ENOMEM; goto out_put; } atomic->remote_addr = user_wr->wr.atomic.remote_addr; atomic->compare_add = user_wr->wr.atomic.compare_add; atomic->swap = user_wr->wr.atomic.swap; atomic->rkey = user_wr->wr.atomic.rkey; next = &atomic->wr; } else if (user_wr->opcode == IB_WR_SEND || user_wr->opcode == IB_WR_SEND_WITH_IMM || user_wr->opcode == IB_WR_SEND_WITH_INV) { next_size = sizeof(*next); next = alloc_wr(next_size, user_wr->num_sge); if (!next) { ret = -ENOMEM; goto out_put; } } else { ret = -EINVAL; goto out_put; } if (user_wr->opcode == IB_WR_SEND_WITH_IMM || user_wr->opcode == IB_WR_RDMA_WRITE_WITH_IMM) { next->ex.imm_data = (__be32 __force) user_wr->ex.imm_data; } else if (user_wr->opcode == IB_WR_SEND_WITH_INV) { next->ex.invalidate_rkey = user_wr->ex.invalidate_rkey; } if (!last) wr = next; else last->next = next; last = next; next->next = NULL; next->wr_id = user_wr->wr_id; next->num_sge = user_wr->num_sge; next->opcode = user_wr->opcode; next->send_flags = user_wr->send_flags; if (next->num_sge) { next->sg_list = (void *) next + ALIGN(next_size, sizeof(struct ib_sge)); if (copy_from_user(next->sg_list, sgls + sg_ind, next->num_sge * sizeof(struct ib_sge))) { ret = -EFAULT; goto out_put; } sg_ind += next->num_sge; } else next->sg_list = NULL; } resp.bad_wr = 0; ret = qp->device->ops.post_send(qp->real_qp, wr, &bad_wr); if (ret) for (next = wr; next; next = next->next) { ++resp.bad_wr; if (next == bad_wr) break; } ret2 = uverbs_response(attrs, &resp, sizeof(resp)); if (ret2) ret = ret2; out_put: rdma_lookup_put_uobject(&qp->uobject->uevent.uobject, UVERBS_LOOKUP_READ); while (wr) { if (is_ud && ud_wr(wr)->ah) uobj_put_obj_read(ud_wr(wr)->ah); next = wr->next; kfree(wr); wr = next; } out: kfree(user_wr); return ret; }"
440----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47678/bad/icmp.c----icmpv6_echo_reply,"static enum skb_drop_reason icmpv6_echo_reply(struct sk_buff *skb) { struct net *net = dev_net(skb->dev); struct sock *sk; struct inet6_dev *idev; struct ipv6_pinfo *np; const struct in6_addr *saddr = NULL; struct icmp6hdr *icmph = icmp6_hdr(skb); struct icmp6hdr tmp_hdr; struct flowi6 fl6; struct icmpv6_msg msg; struct dst_entry *dst; struct ipcm6_cookie ipc6; u32 mark = IP6_REPLY_MARK(net, skb->mark); SKB_DR(reason); bool acast; u8 type; if (ipv6_addr_is_multicast(&ipv6_hdr(skb)->daddr) && net->ipv6.sysctl.icmpv6_echo_ignore_multicast) return reason; saddr = &ipv6_hdr(skb)->daddr; acast = ipv6_anycast_destination(skb_dst(skb), saddr); if (acast && net->ipv6.sysctl.icmpv6_echo_ignore_anycast) return reason; if (!ipv6_unicast_destination(skb) && !(net->ipv6.sysctl.anycast_src_echo_reply && acast)) saddr = NULL; if (icmph->icmp6_type == ICMPV6_EXT_ECHO_REQUEST) type = ICMPV6_EXT_ECHO_REPLY; else type = ICMPV6_ECHO_REPLY; memcpy(&tmp_hdr, icmph, sizeof(tmp_hdr)); tmp_hdr.icmp6_type = type; memset(&fl6, 0, sizeof(fl6)); if (net->ipv6.sysctl.flowlabel_reflect & FLOWLABEL_REFLECT_ICMPV6_ECHO_REPLIES) fl6.flowlabel = ip6_flowlabel(ipv6_hdr(skb)); fl6.flowi6_proto = IPPROTO_ICMPV6; fl6.daddr = ipv6_hdr(skb)->saddr; if (saddr) fl6.saddr = *saddr; fl6.flowi6_oif = icmp6_iif(skb); fl6.fl6_icmp_type = type; fl6.flowi6_mark = mark; fl6.flowi6_uid = sock_net_uid(net, NULL); security_skb_classify_flow(skb, flowi6_to_flowi_common(&fl6)); local_bh_disable(); sk = icmpv6_xmit_lock(net); if (!sk) goto out_bh_enable; np = inet6_sk(sk); if (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr)) fl6.flowi6_oif = READ_ONCE(np->mcast_oif); else if (!fl6.flowi6_oif) fl6.flowi6_oif = READ_ONCE(np->ucast_oif); if (ip6_dst_lookup(net, sk, &dst, &fl6)) goto out; dst = xfrm_lookup(net, dst, flowi6_to_flowi(&fl6), sk, 0); if (IS_ERR(dst)) goto out; <S2SV_StartVul> if ((!(skb->dev->flags & IFF_LOOPBACK) && !icmpv6_global_allow(net, ICMPV6_ECHO_REPLY)) || <S2SV_EndVul> <S2SV_StartVul> !icmpv6_xrlim_allow(sk, ICMPV6_ECHO_REPLY, &fl6)) <S2SV_EndVul> goto out_dst_release; idev = __in6_dev_get(skb->dev); msg.skb = skb; msg.offset = 0; msg.type = type; ipcm6_init_sk(&ipc6, sk); ipc6.hlimit = ip6_sk_dst_hoplimit(np, &fl6, dst); ipc6.tclass = ipv6_get_dsfield(ipv6_hdr(skb)); ipc6.sockc.mark = mark; if (icmph->icmp6_type == ICMPV6_EXT_ECHO_REQUEST) if (!icmp_build_probe(skb, (struct icmphdr *)&tmp_hdr)) goto out_dst_release; if (ip6_append_data(sk, icmpv6_getfrag, &msg, skb->len + sizeof(struct icmp6hdr), sizeof(struct icmp6hdr), &ipc6, &fl6, dst_rt6_info(dst), MSG_DONTWAIT)) { __ICMP6_INC_STATS(net, idev, ICMP6_MIB_OUTERRORS); ip6_flush_pending_frames(sk); } else { icmpv6_push_pending_frames(sk, &fl6, &tmp_hdr, skb->len + sizeof(struct icmp6hdr)); reason = SKB_CONSUMED; } out_dst_release: dst_release(dst); out: icmpv6_xmit_unlock(sk); out_bh_enable: local_bh_enable(); return reason; }","- if ((!(skb->dev->flags & IFF_LOOPBACK) && !icmpv6_global_allow(net, ICMPV6_ECHO_REPLY)) ||
- !icmpv6_xrlim_allow(sk, ICMPV6_ECHO_REPLY, &fl6))
+ bool apply_ratelimit = false;
+ if ((!(skb->dev->flags & IFF_LOOPBACK) &&
+ !icmpv6_global_allow(net, ICMPV6_ECHO_REPLY, &apply_ratelimit)) ||
+ !icmpv6_xrlim_allow(sk, ICMPV6_ECHO_REPLY, &fl6, apply_ratelimit))","static enum skb_drop_reason icmpv6_echo_reply(struct sk_buff *skb) { struct net *net = dev_net(skb->dev); struct sock *sk; struct inet6_dev *idev; struct ipv6_pinfo *np; const struct in6_addr *saddr = NULL; struct icmp6hdr *icmph = icmp6_hdr(skb); bool apply_ratelimit = false; struct icmp6hdr tmp_hdr; struct flowi6 fl6; struct icmpv6_msg msg; struct dst_entry *dst; struct ipcm6_cookie ipc6; u32 mark = IP6_REPLY_MARK(net, skb->mark); SKB_DR(reason); bool acast; u8 type; if (ipv6_addr_is_multicast(&ipv6_hdr(skb)->daddr) && net->ipv6.sysctl.icmpv6_echo_ignore_multicast) return reason; saddr = &ipv6_hdr(skb)->daddr; acast = ipv6_anycast_destination(skb_dst(skb), saddr); if (acast && net->ipv6.sysctl.icmpv6_echo_ignore_anycast) return reason; if (!ipv6_unicast_destination(skb) && !(net->ipv6.sysctl.anycast_src_echo_reply && acast)) saddr = NULL; if (icmph->icmp6_type == ICMPV6_EXT_ECHO_REQUEST) type = ICMPV6_EXT_ECHO_REPLY; else type = ICMPV6_ECHO_REPLY; memcpy(&tmp_hdr, icmph, sizeof(tmp_hdr)); tmp_hdr.icmp6_type = type; memset(&fl6, 0, sizeof(fl6)); if (net->ipv6.sysctl.flowlabel_reflect & FLOWLABEL_REFLECT_ICMPV6_ECHO_REPLIES) fl6.flowlabel = ip6_flowlabel(ipv6_hdr(skb)); fl6.flowi6_proto = IPPROTO_ICMPV6; fl6.daddr = ipv6_hdr(skb)->saddr; if (saddr) fl6.saddr = *saddr; fl6.flowi6_oif = icmp6_iif(skb); fl6.fl6_icmp_type = type; fl6.flowi6_mark = mark; fl6.flowi6_uid = sock_net_uid(net, NULL); security_skb_classify_flow(skb, flowi6_to_flowi_common(&fl6)); local_bh_disable(); sk = icmpv6_xmit_lock(net); if (!sk) goto out_bh_enable; np = inet6_sk(sk); if (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr)) fl6.flowi6_oif = READ_ONCE(np->mcast_oif); else if (!fl6.flowi6_oif) fl6.flowi6_oif = READ_ONCE(np->ucast_oif); if (ip6_dst_lookup(net, sk, &dst, &fl6)) goto out; dst = xfrm_lookup(net, dst, flowi6_to_flowi(&fl6), sk, 0); if (IS_ERR(dst)) goto out; if ((!(skb->dev->flags & IFF_LOOPBACK) && !icmpv6_global_allow(net, ICMPV6_ECHO_REPLY, &apply_ratelimit)) || !icmpv6_xrlim_allow(sk, ICMPV6_ECHO_REPLY, &fl6, apply_ratelimit)) goto out_dst_release; idev = __in6_dev_get(skb->dev); msg.skb = skb; msg.offset = 0; msg.type = type; ipcm6_init_sk(&ipc6, sk); ipc6.hlimit = ip6_sk_dst_hoplimit(np, &fl6, dst); ipc6.tclass = ipv6_get_dsfield(ipv6_hdr(skb)); ipc6.sockc.mark = mark; if (icmph->icmp6_type == ICMPV6_EXT_ECHO_REQUEST) if (!icmp_build_probe(skb, (struct icmphdr *)&tmp_hdr)) goto out_dst_release; if (ip6_append_data(sk, icmpv6_getfrag, &msg, skb->len + sizeof(struct icmp6hdr), sizeof(struct icmp6hdr), &ipc6, &fl6, dst_rt6_info(dst), MSG_DONTWAIT)) { __ICMP6_INC_STATS(net, idev, ICMP6_MIB_OUTERRORS); ip6_flush_pending_frames(sk); } else { icmpv6_push_pending_frames(sk, &fl6, &tmp_hdr, skb->len + sizeof(struct icmp6hdr)); reason = SKB_CONSUMED; } out_dst_release: dst_release(dst); out: icmpv6_xmit_unlock(sk); out_bh_enable: local_bh_enable(); return reason; }"
937----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50185/bad/subflow.c----skb_is_fully_mapped,"static bool skb_is_fully_mapped(struct sock *ssk, struct sk_buff *skb) { struct mptcp_subflow_context *subflow = mptcp_subflow_ctx(ssk); unsigned int skb_consumed; skb_consumed = tcp_sk(ssk)->copied_seq - TCP_SKB_CB(skb)->seq; <S2SV_StartVul> if (WARN_ON_ONCE(skb_consumed >= skb->len)) <S2SV_EndVul> return true; return skb->len - skb_consumed <= subflow->map_data_len - mptcp_subflow_get_map_offset(subflow); }","- if (WARN_ON_ONCE(skb_consumed >= skb->len))
+ if (unlikely(skb_consumed >= skb->len)) {
+ DEBUG_NET_WARN_ON_ONCE(1);
+ }","static bool skb_is_fully_mapped(struct sock *ssk, struct sk_buff *skb) { struct mptcp_subflow_context *subflow = mptcp_subflow_ctx(ssk); unsigned int skb_consumed; skb_consumed = tcp_sk(ssk)->copied_seq - TCP_SKB_CB(skb)->seq; if (unlikely(skb_consumed >= skb->len)) { DEBUG_NET_WARN_ON_ONCE(1); return true; } return skb->len - skb_consumed <= subflow->map_data_len - mptcp_subflow_get_map_offset(subflow); }"
700----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49988/bad/oplock.c----smb_break_all_levII_oplock,"void smb_break_all_levII_oplock(struct ksmbd_work *work, struct ksmbd_file *fp, int is_trunc) { struct oplock_info *op, *brk_op; struct ksmbd_inode *ci; struct ksmbd_conn *conn = work->conn; if (!test_share_config_flag(work->tcon->share_conf, KSMBD_SHARE_FLAG_OPLOCKS)) return; ci = fp->f_ci; op = opinfo_get(fp); rcu_read_lock(); list_for_each_entry_rcu(brk_op, &ci->m_op_list, op_entry) { if (brk_op->conn == NULL) continue; if (!atomic_inc_not_zero(&brk_op->refcount)) continue; <S2SV_StartVul> atomic_inc(&brk_op->conn->r_count); <S2SV_EndVul> <S2SV_StartVul> if (ksmbd_conn_releasing(brk_op->conn)) { <S2SV_EndVul> <S2SV_StartVul> atomic_dec(&brk_op->conn->r_count); <S2SV_EndVul> continue; <S2SV_StartVul> } <S2SV_EndVul> rcu_read_unlock(); if (brk_op->is_lease && (brk_op->o_lease->state & (~(SMB2_LEASE_READ_CACHING_LE | SMB2_LEASE_HANDLE_CACHING_LE)))) { ksmbd_debug(OPLOCK, ""unexpected lease state(0x%x)\n"", brk_op->o_lease->state); goto next; } else if (brk_op->level != SMB2_OPLOCK_LEVEL_II) { ksmbd_debug(OPLOCK, ""unexpected oplock(0x%x)\n"", brk_op->level); goto next; } if (brk_op->is_lease && brk_op->o_lease->new_state == SMB2_LEASE_NONE_LE && atomic_read(&brk_op->breaking_cnt)) goto next; if (op && op->is_lease && brk_op->is_lease && !memcmp(conn->ClientGUID, brk_op->conn->ClientGUID, SMB2_CLIENT_GUID_SIZE) && !memcmp(op->o_lease->lease_key, brk_op->o_lease->lease_key, SMB2_LEASE_KEY_SIZE)) goto next; brk_op->open_trunc = is_trunc; oplock_break(brk_op, SMB2_OPLOCK_LEVEL_NONE); next: <S2SV_StartVul> opinfo_conn_put(brk_op); <S2SV_EndVul> rcu_read_lock(); <S2SV_StartVul> } <S2SV_EndVul> rcu_read_unlock(); if (op) opinfo_put(op); }","- atomic_inc(&brk_op->conn->r_count);
- if (ksmbd_conn_releasing(brk_op->conn)) {
- atomic_dec(&brk_op->conn->r_count);
- }
- opinfo_conn_put(brk_op);
- }
+ if (ksmbd_conn_releasing(brk_op->conn))
+ opinfo_put(brk_op);","void smb_break_all_levII_oplock(struct ksmbd_work *work, struct ksmbd_file *fp, int is_trunc) { struct oplock_info *op, *brk_op; struct ksmbd_inode *ci; struct ksmbd_conn *conn = work->conn; if (!test_share_config_flag(work->tcon->share_conf, KSMBD_SHARE_FLAG_OPLOCKS)) return; ci = fp->f_ci; op = opinfo_get(fp); rcu_read_lock(); list_for_each_entry_rcu(brk_op, &ci->m_op_list, op_entry) { if (brk_op->conn == NULL) continue; if (!atomic_inc_not_zero(&brk_op->refcount)) continue; if (ksmbd_conn_releasing(brk_op->conn)) continue; rcu_read_unlock(); if (brk_op->is_lease && (brk_op->o_lease->state & (~(SMB2_LEASE_READ_CACHING_LE | SMB2_LEASE_HANDLE_CACHING_LE)))) { ksmbd_debug(OPLOCK, ""unexpected lease state(0x%x)\n"", brk_op->o_lease->state); goto next; } else if (brk_op->level != SMB2_OPLOCK_LEVEL_II) { ksmbd_debug(OPLOCK, ""unexpected oplock(0x%x)\n"", brk_op->level); goto next; } if (brk_op->is_lease && brk_op->o_lease->new_state == SMB2_LEASE_NONE_LE && atomic_read(&brk_op->breaking_cnt)) goto next; if (op && op->is_lease && brk_op->is_lease && !memcmp(conn->ClientGUID, brk_op->conn->ClientGUID, SMB2_CLIENT_GUID_SIZE) && !memcmp(op->o_lease->lease_key, brk_op->o_lease->lease_key, SMB2_LEASE_KEY_SIZE)) goto next; brk_op->open_trunc = is_trunc; oplock_break(brk_op, SMB2_OPLOCK_LEVEL_NONE); next: opinfo_put(brk_op); rcu_read_lock(); } rcu_read_unlock(); if (op) opinfo_put(op); }"
30----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-43902/bad/amdgpu_dm.c----amdgpu_dm_connector_get_modes,"static int amdgpu_dm_connector_get_modes(struct drm_connector *connector) { struct amdgpu_dm_connector *amdgpu_dm_connector = to_amdgpu_dm_connector(connector); struct drm_encoder *encoder; struct edid *edid = amdgpu_dm_connector->edid; encoder = amdgpu_dm_connector_to_encoder(connector); if (!drm_edid_is_valid(edid)) { amdgpu_dm_connector->num_modes = drm_add_modes_noedid(connector, 640, 480); } else { amdgpu_dm_connector_ddc_get_modes(connector, edid); <S2SV_StartVul> amdgpu_dm_connector_add_common_modes(encoder, connector); <S2SV_EndVul> amdgpu_dm_connector_add_freesync_modes(connector, edid); } amdgpu_dm_fbc_init(connector); return amdgpu_dm_connector->num_modes; }","- amdgpu_dm_connector_add_common_modes(encoder, connector);
+ if (encoder)
+ amdgpu_dm_connector_add_common_modes(encoder, connector);","static int amdgpu_dm_connector_get_modes(struct drm_connector *connector) { struct amdgpu_dm_connector *amdgpu_dm_connector = to_amdgpu_dm_connector(connector); struct drm_encoder *encoder; struct edid *edid = amdgpu_dm_connector->edid; encoder = amdgpu_dm_connector_to_encoder(connector); if (!drm_edid_is_valid(edid)) { amdgpu_dm_connector->num_modes = drm_add_modes_noedid(connector, 640, 480); } else { amdgpu_dm_connector_ddc_get_modes(connector, edid); if (encoder) amdgpu_dm_connector_add_common_modes(encoder, connector); amdgpu_dm_connector_add_freesync_modes(connector, edid); } amdgpu_dm_fbc_init(connector); return amdgpu_dm_connector->num_modes; }"
1529----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2025-21684/bad/gpio-xilinx.c----xgpio_irq_unmask,"static void xgpio_irq_unmask(struct irq_data *irq_data) { unsigned long flags; struct xgpio_instance *chip = irq_data_get_irq_chip_data(irq_data); int irq_offset = irqd_to_hwirq(irq_data); int bit = xgpio_to_bit(chip, irq_offset); u32 old_enable = xgpio_get_value32(chip->enable, bit); u32 mask = BIT(bit / 32), val; gpiochip_enable_irq(&chip->gc, irq_offset); <S2SV_StartVul> spin_lock_irqsave(&chip->gpio_lock, flags); <S2SV_EndVul> __set_bit(bit, chip->enable); if (old_enable == 0) { val = xgpio_readreg(chip->regs + XGPIO_IPISR_OFFSET); val &= mask; xgpio_writereg(chip->regs + XGPIO_IPISR_OFFSET, val); xgpio_read_ch(chip, XGPIO_DATA_OFFSET, bit, chip->last_irq_read); val = xgpio_readreg(chip->regs + XGPIO_IPIER_OFFSET); val |= mask; xgpio_writereg(chip->regs + XGPIO_IPIER_OFFSET, val); } <S2SV_StartVul> spin_unlock_irqrestore(&chip->gpio_lock, flags); <S2SV_EndVul> }","- spin_lock_irqsave(&chip->gpio_lock, flags);
- spin_unlock_irqrestore(&chip->gpio_lock, flags);
+ raw_spin_lock_irqsave(&chip->gpio_lock, flags);
+ raw_spin_unlock_irqrestore(&chip->gpio_lock, flags);","static void xgpio_irq_unmask(struct irq_data *irq_data) { unsigned long flags; struct xgpio_instance *chip = irq_data_get_irq_chip_data(irq_data); int irq_offset = irqd_to_hwirq(irq_data); int bit = xgpio_to_bit(chip, irq_offset); u32 old_enable = xgpio_get_value32(chip->enable, bit); u32 mask = BIT(bit / 32), val; gpiochip_enable_irq(&chip->gc, irq_offset); raw_spin_lock_irqsave(&chip->gpio_lock, flags); __set_bit(bit, chip->enable); if (old_enable == 0) { val = xgpio_readreg(chip->regs + XGPIO_IPISR_OFFSET); val &= mask; xgpio_writereg(chip->regs + XGPIO_IPISR_OFFSET, val); xgpio_read_ch(chip, XGPIO_DATA_OFFSET, bit, chip->last_irq_read); val = xgpio_readreg(chip->regs + XGPIO_IPIER_OFFSET); val |= mask; xgpio_writereg(chip->regs + XGPIO_IPIER_OFFSET, val); } raw_spin_unlock_irqrestore(&chip->gpio_lock, flags); }"
20----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-43896/bad/cs-amp-lib.c----cs_amp_get_efi_variable,"static efi_status_t cs_amp_get_efi_variable(efi_char16_t *name, efi_guid_t *guid, unsigned long *size, void *buf) { u32 attr; KUNIT_STATIC_STUB_REDIRECT(cs_amp_get_efi_variable, name, guid, size, buf); <S2SV_StartVul> if (IS_ENABLED(CONFIG_EFI)) <S2SV_EndVul> return efi.get_variable(name, guid, &attr, size, buf); return EFI_NOT_FOUND; }","- if (IS_ENABLED(CONFIG_EFI))
+ if (efi_rt_services_supported(EFI_RT_SUPPORTED_GET_VARIABLE))","static efi_status_t cs_amp_get_efi_variable(efi_char16_t *name, efi_guid_t *guid, unsigned long *size, void *buf) { u32 attr; KUNIT_STATIC_STUB_REDIRECT(cs_amp_get_efi_variable, name, guid, size, buf); if (efi_rt_services_supported(EFI_RT_SUPPORTED_GET_VARIABLE)) return efi.get_variable(name, guid, &attr, size, buf); return EFI_NOT_FOUND; }"
825----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50092/bad/netconsole.c----send_ext_msg_udp,"static void send_ext_msg_udp(struct netconsole_target *nt, const char *msg, int msg_len) { static char buf[MAX_PRINT_CHUNK]; const char *header, *body; int offset = 0; int header_len, body_len; const char *msg_ready = msg; const char *release; int release_len = 0; int userdata_len = 0; char *userdata = NULL; #ifdef CONFIG_NETCONSOLE_DYNAMIC userdata = nt->userdata_complete; userdata_len = nt->userdata_length; #endif if (nt->release) { release = init_utsname()->release; release_len = strlen(release) + 1; } if (msg_len + release_len + userdata_len <= MAX_PRINT_CHUNK) { if (nt->release) { scnprintf(buf, MAX_PRINT_CHUNK, ""%s,%s"", release, msg); msg_len += release_len; } else { memcpy(buf, msg, msg_len); } if (userdata) msg_len += scnprintf(&buf[msg_len], MAX_PRINT_CHUNK - msg_len, ""%s"", userdata); msg_ready = buf; netpoll_send_udp(&nt->np, msg_ready, msg_len); return; } header = msg; body = memchr(msg, ';', msg_len); if (WARN_ON_ONCE(!body)) return; header_len = body - header; body_len = msg_len - header_len - 1; body++; if (nt->release) scnprintf(buf, MAX_PRINT_CHUNK, ""%s,"", release); memcpy(buf + release_len, header, header_len); header_len += release_len; while (offset < body_len + userdata_len) { int this_header = header_len; int this_offset = 0; int this_chunk = 0; this_header += scnprintf(buf + this_header, sizeof(buf) - this_header, "",ncfrag=%d/%d;"", offset, body_len + userdata_len); if (offset < body_len) { this_chunk = min(body_len - offset, MAX_PRINT_CHUNK - this_header); if (WARN_ON_ONCE(this_chunk <= 0)) return; memcpy(buf + this_header, body + offset, this_chunk); this_offset += this_chunk; } if (offset + this_offset >= body_len && offset + this_offset < userdata_len + body_len) { int sent_userdata = (offset + this_offset) - body_len; int preceding_bytes = this_chunk + this_header; if (WARN_ON_ONCE(sent_userdata < 0)) return; this_chunk = min(userdata_len - sent_userdata, MAX_PRINT_CHUNK - preceding_bytes); <S2SV_StartVul> if (WARN_ON_ONCE(this_chunk <= 0)) <S2SV_EndVul> return; memcpy(buf + this_header + this_offset, userdata + sent_userdata, this_chunk); this_offset += this_chunk; } netpoll_send_udp(&nt->np, buf, this_header + this_offset); offset += this_offset; } }","- if (WARN_ON_ONCE(this_chunk <= 0))
+ if (WARN_ON_ONCE(this_chunk < 0))","static void send_ext_msg_udp(struct netconsole_target *nt, const char *msg, int msg_len) { static char buf[MAX_PRINT_CHUNK]; const char *header, *body; int offset = 0; int header_len, body_len; const char *msg_ready = msg; const char *release; int release_len = 0; int userdata_len = 0; char *userdata = NULL; #ifdef CONFIG_NETCONSOLE_DYNAMIC userdata = nt->userdata_complete; userdata_len = nt->userdata_length; #endif if (nt->release) { release = init_utsname()->release; release_len = strlen(release) + 1; } if (msg_len + release_len + userdata_len <= MAX_PRINT_CHUNK) { if (nt->release) { scnprintf(buf, MAX_PRINT_CHUNK, ""%s,%s"", release, msg); msg_len += release_len; } else { memcpy(buf, msg, msg_len); } if (userdata) msg_len += scnprintf(&buf[msg_len], MAX_PRINT_CHUNK - msg_len, ""%s"", userdata); msg_ready = buf; netpoll_send_udp(&nt->np, msg_ready, msg_len); return; } header = msg; body = memchr(msg, ';', msg_len); if (WARN_ON_ONCE(!body)) return; header_len = body - header; body_len = msg_len - header_len - 1; body++; if (nt->release) scnprintf(buf, MAX_PRINT_CHUNK, ""%s,"", release); memcpy(buf + release_len, header, header_len); header_len += release_len; while (offset < body_len + userdata_len) { int this_header = header_len; int this_offset = 0; int this_chunk = 0; this_header += scnprintf(buf + this_header, sizeof(buf) - this_header, "",ncfrag=%d/%d;"", offset, body_len + userdata_len); if (offset < body_len) { this_chunk = min(body_len - offset, MAX_PRINT_CHUNK - this_header); if (WARN_ON_ONCE(this_chunk <= 0)) return; memcpy(buf + this_header, body + offset, this_chunk); this_offset += this_chunk; } if (offset + this_offset >= body_len && offset + this_offset < userdata_len + body_len) { int sent_userdata = (offset + this_offset) - body_len; int preceding_bytes = this_chunk + this_header; if (WARN_ON_ONCE(sent_userdata < 0)) return; this_chunk = min(userdata_len - sent_userdata, MAX_PRINT_CHUNK - preceding_bytes); if (WARN_ON_ONCE(this_chunk < 0)) return; memcpy(buf + this_header + this_offset, userdata + sent_userdata, this_chunk); this_offset += this_chunk; } netpoll_send_udp(&nt->np, buf, this_header + this_offset); offset += this_offset; } }"
1417----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56745/bad/pci.c----reset_method_store,"static ssize_t reset_method_store(struct device *dev, struct device_attribute *attr, const char *buf, size_t count) { struct pci_dev *pdev = to_pci_dev(dev); <S2SV_StartVul> char *options, *name; <S2SV_EndVul> int m, n; u8 reset_methods[PCI_NUM_RESET_METHODS] = { 0 }; if (sysfs_streq(buf, """")) { pdev->reset_methods[0] = 0; pci_warn(pdev, ""All device reset methods disabled by user""); return count; } if (sysfs_streq(buf, ""default"")) { pci_init_reset_methods(pdev); return count; } options = kstrndup(buf, count, GFP_KERNEL); if (!options) return -ENOMEM; n = 0; <S2SV_StartVul> while ((name = strsep(&options, "" "")) != NULL) { <S2SV_EndVul> if (sysfs_streq(name, """")) continue; name = strim(name); m = reset_method_lookup(name); if (!m) { pci_err(pdev, ""Invalid reset method '%s'"", name); goto error; } if (pci_reset_fn_methods[m].reset_fn(pdev, PCI_RESET_PROBE)) { pci_err(pdev, ""Unsupported reset method '%s'"", name); goto error; } if (n == PCI_NUM_RESET_METHODS - 1) { pci_err(pdev, ""Too many reset methods\n""); goto error; } reset_methods[n++] = m; } reset_methods[n] = 0; if (pci_reset_fn_methods[1].reset_fn(pdev, PCI_RESET_PROBE) == 0 && reset_methods[0] != 1) pci_warn(pdev, ""Device-specific reset disabled/de-prioritized by user""); memcpy(pdev->reset_methods, reset_methods, sizeof(pdev->reset_methods)); kfree(options); return count; error: kfree(options); return -EINVAL; }","- char *options, *name;
- while ((name = strsep(&options, "" "")) != NULL) {
+ char *options, *tmp_options, *name;
+ tmp_options = options;
+ while ((name = strsep(&tmp_options, "" "")) != NULL) {","static ssize_t reset_method_store(struct device *dev, struct device_attribute *attr, const char *buf, size_t count) { struct pci_dev *pdev = to_pci_dev(dev); char *options, *tmp_options, *name; int m, n; u8 reset_methods[PCI_NUM_RESET_METHODS] = { 0 }; if (sysfs_streq(buf, """")) { pdev->reset_methods[0] = 0; pci_warn(pdev, ""All device reset methods disabled by user""); return count; } if (sysfs_streq(buf, ""default"")) { pci_init_reset_methods(pdev); return count; } options = kstrndup(buf, count, GFP_KERNEL); if (!options) return -ENOMEM; n = 0; tmp_options = options; while ((name = strsep(&tmp_options, "" "")) != NULL) { if (sysfs_streq(name, """")) continue; name = strim(name); m = reset_method_lookup(name); if (!m) { pci_err(pdev, ""Invalid reset method '%s'"", name); goto error; } if (pci_reset_fn_methods[m].reset_fn(pdev, PCI_RESET_PROBE)) { pci_err(pdev, ""Unsupported reset method '%s'"", name); goto error; } if (n == PCI_NUM_RESET_METHODS - 1) { pci_err(pdev, ""Too many reset methods\n""); goto error; } reset_methods[n++] = m; } reset_methods[n] = 0; if (pci_reset_fn_methods[1].reset_fn(pdev, PCI_RESET_PROBE) == 0 && reset_methods[0] != 1) pci_warn(pdev, ""Device-specific reset disabled/de-prioritized by user""); memcpy(pdev->reset_methods, reset_methods, sizeof(pdev->reset_methods)); kfree(options); return count; error: kfree(options); return -EINVAL; }"
1344----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56662/bad/core.c----acpi_nfit_ctl,"int acpi_nfit_ctl(struct nvdimm_bus_descriptor *nd_desc, struct nvdimm *nvdimm, unsigned int cmd, void *buf, unsigned int buf_len, int *cmd_rc) { struct acpi_nfit_desc *acpi_desc = to_acpi_desc(nd_desc); struct nfit_mem *nfit_mem = nvdimm_provider_data(nvdimm); union acpi_object in_obj, in_buf, *out_obj; const struct nd_cmd_desc *desc = NULL; struct device *dev = acpi_desc->dev; struct nd_cmd_pkg *call_pkg = NULL; const char *cmd_name, *dimm_name; unsigned long cmd_mask, dsm_mask; u32 offset, fw_status = 0; acpi_handle handle; const guid_t *guid; int func, rc, i; int family = 0; if (cmd_rc) *cmd_rc = -EINVAL; <S2SV_StartVul> if (cmd == ND_CMD_CALL) <S2SV_EndVul> call_pkg = buf; func = cmd_to_func(nfit_mem, cmd, call_pkg, &family); if (func < 0) return func; if (nvdimm) { struct acpi_device *adev = nfit_mem->adev; if (!adev) return -ENOTTY; dimm_name = nvdimm_name(nvdimm); cmd_name = nvdimm_cmd_name(cmd); cmd_mask = nvdimm_cmd_mask(nvdimm); dsm_mask = nfit_mem->dsm_mask; desc = nd_cmd_dimm_desc(cmd); guid = to_nfit_uuid(nfit_mem->family); handle = adev->handle; } else { struct acpi_device *adev = to_acpi_dev(acpi_desc); cmd_name = nvdimm_bus_cmd_name(cmd); cmd_mask = nd_desc->cmd_mask; if (cmd == ND_CMD_CALL && call_pkg->nd_family) { family = call_pkg->nd_family; if (family > NVDIMM_BUS_FAMILY_MAX || !test_bit(family, &nd_desc->bus_family_mask)) return -EINVAL; family = array_index_nospec(family, NVDIMM_BUS_FAMILY_MAX + 1); dsm_mask = acpi_desc->family_dsm_mask[family]; guid = to_nfit_bus_uuid(family); } else { dsm_mask = acpi_desc->bus_dsm_mask; guid = to_nfit_uuid(NFIT_DEV_BUS); } desc = nd_cmd_bus_desc(cmd); handle = adev->handle; dimm_name = ""bus""; } if (!desc || (cmd && (desc->out_num + desc->in_num == 0))) return -ENOTTY; if (cmd == ND_CMD_CALL && (func > NVDIMM_CMD_MAX || !test_bit(func, &dsm_mask))) return -ENOTTY; else if (!test_bit(cmd, &cmd_mask)) return -ENOTTY; in_obj.type = ACPI_TYPE_PACKAGE; in_obj.package.count = 1; in_obj.package.elements = &in_buf; in_buf.type = ACPI_TYPE_BUFFER; in_buf.buffer.pointer = buf; in_buf.buffer.length = 0; for (i = 0; i < desc->in_num; i++) in_buf.buffer.length += nd_cmd_in_size(nvdimm, cmd, desc, i, buf); if (call_pkg) { in_buf.buffer.pointer = (void *) &call_pkg->nd_payload; in_buf.buffer.length = call_pkg->nd_size_in; } dev_dbg(dev, ""%s cmd: %d: family: %d func: %d input length: %d\n"", dimm_name, cmd, family, func, in_buf.buffer.length); if (payload_dumpable(nvdimm, func)) print_hex_dump_debug(""nvdimm in "", DUMP_PREFIX_OFFSET, 4, 4, in_buf.buffer.pointer, min_t(u32, 256, in_buf.buffer.length), true); if (nvdimm && cmd == ND_CMD_GET_CONFIG_SIZE && test_bit(NFIT_MEM_LSR, &nfit_mem->flags)) out_obj = acpi_label_info(handle); else if (nvdimm && cmd == ND_CMD_GET_CONFIG_DATA && test_bit(NFIT_MEM_LSR, &nfit_mem->flags)) { struct nd_cmd_get_config_data_hdr *p = buf; out_obj = acpi_label_read(handle, p->in_offset, p->in_length); } else if (nvdimm && cmd == ND_CMD_SET_CONFIG_DATA && test_bit(NFIT_MEM_LSW, &nfit_mem->flags)) { struct nd_cmd_set_config_hdr *p = buf; out_obj = acpi_label_write(handle, p->in_offset, p->in_length, p->in_buf); } else { u8 revid; if (nvdimm) revid = nfit_dsm_revid(nfit_mem->family, func); else revid = 1; out_obj = acpi_evaluate_dsm(handle, guid, revid, func, &in_obj); } if (!out_obj) { dev_dbg(dev, ""%s _DSM failed cmd: %s\n"", dimm_name, cmd_name); return -EINVAL; } if (out_obj->type != ACPI_TYPE_BUFFER) { dev_dbg(dev, ""%s unexpected output object type cmd: %s type: %d\n"", dimm_name, cmd_name, out_obj->type); rc = -EINVAL; goto out; } dev_dbg(dev, ""%s cmd: %s output length: %d\n"", dimm_name, cmd_name, out_obj->buffer.length); print_hex_dump_debug(cmd_name, DUMP_PREFIX_OFFSET, 4, 4, out_obj->buffer.pointer, min_t(u32, 128, out_obj->buffer.length), true); if (call_pkg) { call_pkg->nd_fw_size = out_obj->buffer.length; memcpy(call_pkg->nd_payload + call_pkg->nd_size_in, out_obj->buffer.pointer, min(call_pkg->nd_fw_size, call_pkg->nd_size_out)); ACPI_FREE(out_obj); if (cmd_rc) *cmd_rc = 0; return 0; } for (i = 0, offset = 0; i < desc->out_num; i++) { u32 out_size = nd_cmd_out_size(nvdimm, cmd, desc, i, buf, (u32 *) out_obj->buffer.pointer, out_obj->buffer.length - offset); if (offset + out_size > out_obj->buffer.length) { dev_dbg(dev, ""%s output object underflow cmd: %s field: %d\n"", dimm_name, cmd_name, i); break; } if (in_buf.buffer.length + offset + out_size > buf_len) { dev_dbg(dev, ""%s output overrun cmd: %s field: %d\n"", dimm_name, cmd_name, i); rc = -ENXIO; goto out; } memcpy(buf + in_buf.buffer.length + offset, out_obj->buffer.pointer + offset, out_size); offset += out_size; } if (i >= 1 && ((!nvdimm && cmd >= ND_CMD_ARS_CAP && cmd <= ND_CMD_CLEAR_ERROR) || (nvdimm && cmd >= ND_CMD_SMART && cmd <= ND_CMD_VENDOR))) fw_status = *(u32 *) out_obj->buffer.pointer; if (offset + in_buf.buffer.length < buf_len) { if (i >= 1) { rc = buf_len - offset - in_buf.buffer.length; if (cmd_rc) *cmd_rc = xlat_status(nvdimm, buf, cmd, fw_status); } else { dev_err(dev, ""%s:%s underrun cmd: %s buf_len: %d out_len: %d\n"", __func__, dimm_name, cmd_name, buf_len, offset); rc = -ENXIO; } } else { rc = 0; if (cmd_rc) *cmd_rc = xlat_status(nvdimm, buf, cmd, fw_status); } out: ACPI_FREE(out_obj); return rc; }","- if (cmd == ND_CMD_CALL)
+ if (cmd == ND_CMD_CALL) {
+ if (!buf || buf_len < sizeof(*call_pkg))
+ return -EINVAL;
+ }","int acpi_nfit_ctl(struct nvdimm_bus_descriptor *nd_desc, struct nvdimm *nvdimm, unsigned int cmd, void *buf, unsigned int buf_len, int *cmd_rc) { struct acpi_nfit_desc *acpi_desc = to_acpi_desc(nd_desc); struct nfit_mem *nfit_mem = nvdimm_provider_data(nvdimm); union acpi_object in_obj, in_buf, *out_obj; const struct nd_cmd_desc *desc = NULL; struct device *dev = acpi_desc->dev; struct nd_cmd_pkg *call_pkg = NULL; const char *cmd_name, *dimm_name; unsigned long cmd_mask, dsm_mask; u32 offset, fw_status = 0; acpi_handle handle; const guid_t *guid; int func, rc, i; int family = 0; if (cmd_rc) *cmd_rc = -EINVAL; if (cmd == ND_CMD_CALL) { if (!buf || buf_len < sizeof(*call_pkg)) return -EINVAL; call_pkg = buf; } func = cmd_to_func(nfit_mem, cmd, call_pkg, &family); if (func < 0) return func; if (nvdimm) { struct acpi_device *adev = nfit_mem->adev; if (!adev) return -ENOTTY; dimm_name = nvdimm_name(nvdimm); cmd_name = nvdimm_cmd_name(cmd); cmd_mask = nvdimm_cmd_mask(nvdimm); dsm_mask = nfit_mem->dsm_mask; desc = nd_cmd_dimm_desc(cmd); guid = to_nfit_uuid(nfit_mem->family); handle = adev->handle; } else { struct acpi_device *adev = to_acpi_dev(acpi_desc); cmd_name = nvdimm_bus_cmd_name(cmd); cmd_mask = nd_desc->cmd_mask; if (cmd == ND_CMD_CALL && call_pkg->nd_family) { family = call_pkg->nd_family; if (family > NVDIMM_BUS_FAMILY_MAX || !test_bit(family, &nd_desc->bus_family_mask)) return -EINVAL; family = array_index_nospec(family, NVDIMM_BUS_FAMILY_MAX + 1); dsm_mask = acpi_desc->family_dsm_mask[family]; guid = to_nfit_bus_uuid(family); } else { dsm_mask = acpi_desc->bus_dsm_mask; guid = to_nfit_uuid(NFIT_DEV_BUS); } desc = nd_cmd_bus_desc(cmd); handle = adev->handle; dimm_name = ""bus""; } if (!desc || (cmd && (desc->out_num + desc->in_num == 0))) return -ENOTTY; if (cmd == ND_CMD_CALL && (func > NVDIMM_CMD_MAX || !test_bit(func, &dsm_mask))) return -ENOTTY; else if (!test_bit(cmd, &cmd_mask)) return -ENOTTY; in_obj.type = ACPI_TYPE_PACKAGE; in_obj.package.count = 1; in_obj.package.elements = &in_buf; in_buf.type = ACPI_TYPE_BUFFER; in_buf.buffer.pointer = buf; in_buf.buffer.length = 0; for (i = 0; i < desc->in_num; i++) in_buf.buffer.length += nd_cmd_in_size(nvdimm, cmd, desc, i, buf); if (call_pkg) { in_buf.buffer.pointer = (void *) &call_pkg->nd_payload; in_buf.buffer.length = call_pkg->nd_size_in; } dev_dbg(dev, ""%s cmd: %d: family: %d func: %d input length: %d\n"", dimm_name, cmd, family, func, in_buf.buffer.length); if (payload_dumpable(nvdimm, func)) print_hex_dump_debug(""nvdimm in "", DUMP_PREFIX_OFFSET, 4, 4, in_buf.buffer.pointer, min_t(u32, 256, in_buf.buffer.length), true); if (nvdimm && cmd == ND_CMD_GET_CONFIG_SIZE && test_bit(NFIT_MEM_LSR, &nfit_mem->flags)) out_obj = acpi_label_info(handle); else if (nvdimm && cmd == ND_CMD_GET_CONFIG_DATA && test_bit(NFIT_MEM_LSR, &nfit_mem->flags)) { struct nd_cmd_get_config_data_hdr *p = buf; out_obj = acpi_label_read(handle, p->in_offset, p->in_length); } else if (nvdimm && cmd == ND_CMD_SET_CONFIG_DATA && test_bit(NFIT_MEM_LSW, &nfit_mem->flags)) { struct nd_cmd_set_config_hdr *p = buf; out_obj = acpi_label_write(handle, p->in_offset, p->in_length, p->in_buf); } else { u8 revid; if (nvdimm) revid = nfit_dsm_revid(nfit_mem->family, func); else revid = 1; out_obj = acpi_evaluate_dsm(handle, guid, revid, func, &in_obj); } if (!out_obj) { dev_dbg(dev, ""%s _DSM failed cmd: %s\n"", dimm_name, cmd_name); return -EINVAL; } if (out_obj->type != ACPI_TYPE_BUFFER) { dev_dbg(dev, ""%s unexpected output object type cmd: %s type: %d\n"", dimm_name, cmd_name, out_obj->type); rc = -EINVAL; goto out; } dev_dbg(dev, ""%s cmd: %s output length: %d\n"", dimm_name, cmd_name, out_obj->buffer.length); print_hex_dump_debug(cmd_name, DUMP_PREFIX_OFFSET, 4, 4, out_obj->buffer.pointer, min_t(u32, 128, out_obj->buffer.length), true); if (call_pkg) { call_pkg->nd_fw_size = out_obj->buffer.length; memcpy(call_pkg->nd_payload + call_pkg->nd_size_in, out_obj->buffer.pointer, min(call_pkg->nd_fw_size, call_pkg->nd_size_out)); ACPI_FREE(out_obj); if (cmd_rc) *cmd_rc = 0; return 0; } for (i = 0, offset = 0; i < desc->out_num; i++) { u32 out_size = nd_cmd_out_size(nvdimm, cmd, desc, i, buf, (u32 *) out_obj->buffer.pointer, out_obj->buffer.length - offset); if (offset + out_size > out_obj->buffer.length) { dev_dbg(dev, ""%s output object underflow cmd: %s field: %d\n"", dimm_name, cmd_name, i); break; } if (in_buf.buffer.length + offset + out_size > buf_len) { dev_dbg(dev, ""%s output overrun cmd: %s field: %d\n"", dimm_name, cmd_name, i); rc = -ENXIO; goto out; } memcpy(buf + in_buf.buffer.length + offset, out_obj->buffer.pointer + offset, out_size); offset += out_size; } if (i >= 1 && ((!nvdimm && cmd >= ND_CMD_ARS_CAP && cmd <= ND_CMD_CLEAR_ERROR) || (nvdimm && cmd >= ND_CMD_SMART && cmd <= ND_CMD_VENDOR))) fw_status = *(u32 *) out_obj->buffer.pointer; if (offset + in_buf.buffer.length < buf_len) { if (i >= 1) { rc = buf_len - offset - in_buf.buffer.length; if (cmd_rc) *cmd_rc = xlat_status(nvdimm, buf, cmd, fw_status); } else { dev_err(dev, ""%s:%s underrun cmd: %s buf_len: %d out_len: %d\n"", __func__, dimm_name, cmd_name, buf_len, offset); rc = -ENXIO; } } else { rc = 0; if (cmd_rc) *cmd_rc = xlat_status(nvdimm, buf, cmd, fw_status); } out: ACPI_FREE(out_obj); return rc; }"
652----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49954/bad/static_call_inline.c----static_call_module_notify,"static int static_call_module_notify(struct notifier_block *nb, unsigned long val, void *data) { struct module *mod = data; int ret = 0; cpus_read_lock(); static_call_lock(); switch (val) { case MODULE_STATE_COMING: ret = static_call_add_module(mod); if (ret) { <S2SV_StartVul> WARN(1, ""Failed to allocate memory for static calls""); <S2SV_EndVul> static_call_del_module(mod); } break; case MODULE_STATE_GOING: static_call_del_module(mod); break; } static_call_unlock(); cpus_read_unlock(); return notifier_from_errno(ret); }","- WARN(1, ""Failed to allocate memory for static calls"");
+ pr_warn(""Failed to allocate memory for static calls\n"");","static int static_call_module_notify(struct notifier_block *nb, unsigned long val, void *data) { struct module *mod = data; int ret = 0; cpus_read_lock(); static_call_lock(); switch (val) { case MODULE_STATE_COMING: ret = static_call_add_module(mod); if (ret) { pr_warn(""Failed to allocate memory for static calls\n""); static_call_del_module(mod); } break; case MODULE_STATE_GOING: static_call_del_module(mod); break; } static_call_unlock(); cpus_read_unlock(); return notifier_from_errno(ret); }"
378----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46840/bad/extent-tree.c----do_walk_down,"static noinline int do_walk_down(struct btrfs_trans_handle *trans, struct btrfs_root *root, struct btrfs_path *path, struct walk_control *wc, int *lookup_info) { struct btrfs_fs_info *fs_info = root->fs_info; u64 bytenr; u64 generation; u64 parent; struct btrfs_key key; struct btrfs_key first_key; struct btrfs_ref ref = { 0 }; struct extent_buffer *next; int level = wc->level; int reada = 0; int ret = 0; bool need_account = false; generation = btrfs_node_ptr_generation(path->nodes[level], path->slots[level]); if (wc->stage == UPDATE_BACKREF && generation <= root->root_key.offset) { *lookup_info = 1; return 1; } bytenr = btrfs_node_blockptr(path->nodes[level], path->slots[level]); btrfs_node_key_to_cpu(path->nodes[level], &first_key, path->slots[level]); next = find_extent_buffer(fs_info, bytenr); if (!next) { next = btrfs_find_create_tree_block(fs_info, bytenr, root->root_key.objectid, level - 1); if (IS_ERR(next)) return PTR_ERR(next); reada = 1; } btrfs_tree_lock(next); ret = btrfs_lookup_extent_info(trans, fs_info, bytenr, level - 1, 1, &wc->refs[level - 1], &wc->flags[level - 1]); if (ret < 0) goto out_unlock; if (unlikely(wc->refs[level - 1] == 0)) { <S2SV_StartVul> btrfs_err(fs_info, ""Missing references.""); <S2SV_EndVul> <S2SV_StartVul> ret = -EIO; <S2SV_EndVul> goto out_unlock; } *lookup_info = 0; if (wc->stage == DROP_REFERENCE) { if (wc->refs[level - 1] > 1) { need_account = true; if (level == 1 && (wc->flags[0] & BTRFS_BLOCK_FLAG_FULL_BACKREF)) goto skip; if (!wc->update_ref || generation <= root->root_key.offset) goto skip; btrfs_node_key_to_cpu(path->nodes[level], &key, path->slots[level]); ret = btrfs_comp_cpu_keys(&key, &wc->update_progress); if (ret < 0) goto skip; wc->stage = UPDATE_BACKREF; wc->shared_level = level - 1; } } else { if (level == 1 && (wc->flags[0] & BTRFS_BLOCK_FLAG_FULL_BACKREF)) goto skip; } if (!btrfs_buffer_uptodate(next, generation, 0)) { btrfs_tree_unlock(next); free_extent_buffer(next); next = NULL; *lookup_info = 1; } if (!next) { if (reada && level == 1) reada_walk_down(trans, root, wc, path); next = read_tree_block(fs_info, bytenr, root->root_key.objectid, generation, level - 1, &first_key); if (IS_ERR(next)) { return PTR_ERR(next); } else if (!extent_buffer_uptodate(next)) { free_extent_buffer(next); return -EIO; } btrfs_tree_lock(next); } level--; ASSERT(level == btrfs_header_level(next)); if (level != btrfs_header_level(next)) { btrfs_err(root->fs_info, ""mismatched level""); ret = -EIO; goto out_unlock; } path->nodes[level] = next; path->slots[level] = 0; path->locks[level] = BTRFS_WRITE_LOCK; wc->level = level; if (wc->level == 1) wc->reada_slot = 0; return 0; skip: wc->refs[level - 1] = 0; wc->flags[level - 1] = 0; if (wc->stage == DROP_REFERENCE) { if (wc->flags[level] & BTRFS_BLOCK_FLAG_FULL_BACKREF) { parent = path->nodes[level]->start; } else { ASSERT(root->root_key.objectid == btrfs_header_owner(path->nodes[level])); if (root->root_key.objectid != btrfs_header_owner(path->nodes[level])) { btrfs_err(root->fs_info, ""mismatched block owner""); ret = -EIO; goto out_unlock; } parent = 0; } if (wc->restarted) { ret = check_ref_exists(trans, root, bytenr, parent, level - 1); if (ret < 0) goto out_unlock; if (ret == 0) goto no_delete; ret = 0; wc->restarted = 0; } if (root->root_key.objectid != BTRFS_TREE_RELOC_OBJECTID && need_account) { ret = btrfs_qgroup_trace_subtree(trans, next, generation, level - 1); if (ret) { btrfs_err_rl(fs_info, ""Error %d accounting shared subtree. Quota is out of sync, rescan required."", ret); } } wc->drop_level = level; find_next_key(path, level, &wc->drop_progress); btrfs_init_generic_ref(&ref, BTRFS_DROP_DELAYED_REF, bytenr, fs_info->nodesize, parent); btrfs_init_tree_ref(&ref, level - 1, root->root_key.objectid, 0, false); ret = btrfs_free_extent(trans, &ref); if (ret) goto out_unlock; } no_delete: *lookup_info = 1; ret = 1; out_unlock: btrfs_tree_unlock(next); free_extent_buffer(next); return ret; }","- btrfs_err(fs_info, ""Missing references."");
- ret = -EIO;
+ btrfs_err(fs_info, ""bytenr %llu has 0 references, expect > 0"",
+ bytenr);
+ ret = -EUCLEAN;
+ }","static noinline int do_walk_down(struct btrfs_trans_handle *trans, struct btrfs_root *root, struct btrfs_path *path, struct walk_control *wc, int *lookup_info) { struct btrfs_fs_info *fs_info = root->fs_info; u64 bytenr; u64 generation; u64 parent; struct btrfs_key key; struct btrfs_key first_key; struct btrfs_ref ref = { 0 }; struct extent_buffer *next; int level = wc->level; int reada = 0; int ret = 0; bool need_account = false; generation = btrfs_node_ptr_generation(path->nodes[level], path->slots[level]); if (wc->stage == UPDATE_BACKREF && generation <= root->root_key.offset) { *lookup_info = 1; return 1; } bytenr = btrfs_node_blockptr(path->nodes[level], path->slots[level]); btrfs_node_key_to_cpu(path->nodes[level], &first_key, path->slots[level]); next = find_extent_buffer(fs_info, bytenr); if (!next) { next = btrfs_find_create_tree_block(fs_info, bytenr, root->root_key.objectid, level - 1); if (IS_ERR(next)) return PTR_ERR(next); reada = 1; } btrfs_tree_lock(next); ret = btrfs_lookup_extent_info(trans, fs_info, bytenr, level - 1, 1, &wc->refs[level - 1], &wc->flags[level - 1]); if (ret < 0) goto out_unlock; if (unlikely(wc->refs[level - 1] == 0)) { btrfs_err(fs_info, ""bytenr %llu has 0 references, expect > 0"", bytenr); ret = -EUCLEAN; goto out_unlock; } *lookup_info = 0; if (wc->stage == DROP_REFERENCE) { if (wc->refs[level - 1] > 1) { need_account = true; if (level == 1 && (wc->flags[0] & BTRFS_BLOCK_FLAG_FULL_BACKREF)) goto skip; if (!wc->update_ref || generation <= root->root_key.offset) goto skip; btrfs_node_key_to_cpu(path->nodes[level], &key, path->slots[level]); ret = btrfs_comp_cpu_keys(&key, &wc->update_progress); if (ret < 0) goto skip; wc->stage = UPDATE_BACKREF; wc->shared_level = level - 1; } } else { if (level == 1 && (wc->flags[0] & BTRFS_BLOCK_FLAG_FULL_BACKREF)) goto skip; } if (!btrfs_buffer_uptodate(next, generation, 0)) { btrfs_tree_unlock(next); free_extent_buffer(next); next = NULL; *lookup_info = 1; } if (!next) { if (reada && level == 1) reada_walk_down(trans, root, wc, path); next = read_tree_block(fs_info, bytenr, root->root_key.objectid, generation, level - 1, &first_key); if (IS_ERR(next)) { return PTR_ERR(next); } else if (!extent_buffer_uptodate(next)) { free_extent_buffer(next); return -EIO; } btrfs_tree_lock(next); } level--; ASSERT(level == btrfs_header_level(next)); if (level != btrfs_header_level(next)) { btrfs_err(root->fs_info, ""mismatched level""); ret = -EIO; goto out_unlock; } path->nodes[level] = next; path->slots[level] = 0; path->locks[level] = BTRFS_WRITE_LOCK; wc->level = level; if (wc->level == 1) wc->reada_slot = 0; return 0; skip: wc->refs[level - 1] = 0; wc->flags[level - 1] = 0; if (wc->stage == DROP_REFERENCE) { if (wc->flags[level] & BTRFS_BLOCK_FLAG_FULL_BACKREF) { parent = path->nodes[level]->start; } else { ASSERT(root->root_key.objectid == btrfs_header_owner(path->nodes[level])); if (root->root_key.objectid != btrfs_header_owner(path->nodes[level])) { btrfs_err(root->fs_info, ""mismatched block owner""); ret = -EIO; goto out_unlock; } parent = 0; } if (wc->restarted) { ret = check_ref_exists(trans, root, bytenr, parent, level - 1); if (ret < 0) goto out_unlock; if (ret == 0) goto no_delete; ret = 0; wc->restarted = 0; } if (root->root_key.objectid != BTRFS_TREE_RELOC_OBJECTID && need_account) { ret = btrfs_qgroup_trace_subtree(trans, next, generation, level - 1); if (ret) { btrfs_err_rl(fs_info, ""Error %d accounting shared subtree. Quota is out of sync, rescan required."", ret); } } wc->drop_level = level; find_next_key(path, level, &wc->drop_progress); btrfs_init_generic_ref(&ref, BTRFS_DROP_DELAYED_REF, bytenr, fs_info->nodesize, parent); btrfs_init_tree_ref(&ref, level - 1, root->root_key.objectid, 0, false); ret = btrfs_free_extent(trans, &ref); if (ret) goto out_unlock; } no_delete: *lookup_info = 1; ret = 1; out_unlock: btrfs_tree_unlock(next); free_extent_buffer(next); return ret; }"
1332----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56651/bad/hi311x.c----hi3110_can_ist,"static irqreturn_t hi3110_can_ist(int irq, void *dev_id) { struct hi3110_priv *priv = dev_id; struct spi_device *spi = priv->spi; struct net_device *net = priv->net; mutex_lock(&priv->hi3110_lock); while (!priv->force_quit) { enum can_state new_state; u8 intf, eflag, statf; while (!(HI3110_STAT_RXFMTY & (statf = hi3110_read(spi, HI3110_READ_STATF)))) { hi3110_hw_rx(spi); } intf = hi3110_read(spi, HI3110_READ_INTF); eflag = hi3110_read(spi, HI3110_READ_ERR); if (eflag & HI3110_ERR_BUSOFF) new_state = CAN_STATE_BUS_OFF; else if (eflag & HI3110_ERR_PASSIVE_MASK) new_state = CAN_STATE_ERROR_PASSIVE; else if (statf & HI3110_STAT_ERRW) new_state = CAN_STATE_ERROR_WARNING; else new_state = CAN_STATE_ERROR_ACTIVE; if (new_state != priv->can.state) { struct can_frame *cf; struct sk_buff *skb; enum can_state rx_state, tx_state; u8 rxerr, txerr; skb = alloc_can_err_skb(net, &cf); if (!skb) break; txerr = hi3110_read(spi, HI3110_READ_TEC); rxerr = hi3110_read(spi, HI3110_READ_REC); tx_state = txerr >= rxerr ? new_state : 0; rx_state = txerr <= rxerr ? new_state : 0; can_change_state(net, cf, tx_state, rx_state); <S2SV_StartVul> netif_rx(skb); <S2SV_EndVul> if (new_state == CAN_STATE_BUS_OFF) { can_bus_off(net); if (priv->can.restart_ms == 0) { priv->force_quit = 1; hi3110_hw_sleep(spi); break; } } else { cf->can_id |= CAN_ERR_CNT; cf->data[6] = txerr; cf->data[7] = rxerr; } } if ((intf & HI3110_INT_BUSERR) && (priv->can.ctrlmode & CAN_CTRLMODE_BERR_REPORTING)) { struct can_frame *cf; struct sk_buff *skb; if (eflag & HI3110_ERR_PROTOCOL_MASK) { skb = alloc_can_err_skb(net, &cf); if (!skb) break; cf->can_id |= CAN_ERR_PROT | CAN_ERR_BUSERROR; priv->can.can_stats.bus_error++; priv->net->stats.rx_errors++; if (eflag & HI3110_ERR_BITERR) cf->data[2] |= CAN_ERR_PROT_BIT; else if (eflag & HI3110_ERR_FRMERR) cf->data[2] |= CAN_ERR_PROT_FORM; else if (eflag & HI3110_ERR_STUFERR) cf->data[2] |= CAN_ERR_PROT_STUFF; else if (eflag & HI3110_ERR_CRCERR) cf->data[3] |= CAN_ERR_PROT_LOC_CRC_SEQ; else if (eflag & HI3110_ERR_ACKERR) cf->data[3] |= CAN_ERR_PROT_LOC_ACK; cf->data[6] = hi3110_read(spi, HI3110_READ_TEC); cf->data[7] = hi3110_read(spi, HI3110_READ_REC); netdev_dbg(priv->net, ""Bus Error\n""); netif_rx(skb); } } if (priv->tx_busy && statf & HI3110_STAT_TXMTY) { net->stats.tx_packets++; net->stats.tx_bytes += can_get_echo_skb(net, 0, NULL); priv->tx_busy = false; netif_wake_queue(net); } if (intf == 0) break; } mutex_unlock(&priv->hi3110_lock); return IRQ_HANDLED; }","- netif_rx(skb);
+ netif_rx(skb);
+ netif_rx(skb);","static irqreturn_t hi3110_can_ist(int irq, void *dev_id) { struct hi3110_priv *priv = dev_id; struct spi_device *spi = priv->spi; struct net_device *net = priv->net; mutex_lock(&priv->hi3110_lock); while (!priv->force_quit) { enum can_state new_state; u8 intf, eflag, statf; while (!(HI3110_STAT_RXFMTY & (statf = hi3110_read(spi, HI3110_READ_STATF)))) { hi3110_hw_rx(spi); } intf = hi3110_read(spi, HI3110_READ_INTF); eflag = hi3110_read(spi, HI3110_READ_ERR); if (eflag & HI3110_ERR_BUSOFF) new_state = CAN_STATE_BUS_OFF; else if (eflag & HI3110_ERR_PASSIVE_MASK) new_state = CAN_STATE_ERROR_PASSIVE; else if (statf & HI3110_STAT_ERRW) new_state = CAN_STATE_ERROR_WARNING; else new_state = CAN_STATE_ERROR_ACTIVE; if (new_state != priv->can.state) { struct can_frame *cf; struct sk_buff *skb; enum can_state rx_state, tx_state; u8 rxerr, txerr; skb = alloc_can_err_skb(net, &cf); if (!skb) break; txerr = hi3110_read(spi, HI3110_READ_TEC); rxerr = hi3110_read(spi, HI3110_READ_REC); tx_state = txerr >= rxerr ? new_state : 0; rx_state = txerr <= rxerr ? new_state : 0; can_change_state(net, cf, tx_state, rx_state); if (new_state == CAN_STATE_BUS_OFF) { netif_rx(skb); can_bus_off(net); if (priv->can.restart_ms == 0) { priv->force_quit = 1; hi3110_hw_sleep(spi); break; } } else { cf->can_id |= CAN_ERR_CNT; cf->data[6] = txerr; cf->data[7] = rxerr; netif_rx(skb); } } if ((intf & HI3110_INT_BUSERR) && (priv->can.ctrlmode & CAN_CTRLMODE_BERR_REPORTING)) { struct can_frame *cf; struct sk_buff *skb; if (eflag & HI3110_ERR_PROTOCOL_MASK) { skb = alloc_can_err_skb(net, &cf); if (!skb) break; cf->can_id |= CAN_ERR_PROT | CAN_ERR_BUSERROR; priv->can.can_stats.bus_error++; priv->net->stats.rx_errors++; if (eflag & HI3110_ERR_BITERR) cf->data[2] |= CAN_ERR_PROT_BIT; else if (eflag & HI3110_ERR_FRMERR) cf->data[2] |= CAN_ERR_PROT_FORM; else if (eflag & HI3110_ERR_STUFERR) cf->data[2] |= CAN_ERR_PROT_STUFF; else if (eflag & HI3110_ERR_CRCERR) cf->data[3] |= CAN_ERR_PROT_LOC_CRC_SEQ; else if (eflag & HI3110_ERR_ACKERR) cf->data[3] |= CAN_ERR_PROT_LOC_ACK; cf->data[6] = hi3110_read(spi, HI3110_READ_TEC); cf->data[7] = hi3110_read(spi, HI3110_READ_REC); netdev_dbg(priv->net, ""Bus Error\n""); netif_rx(skb); } } if (priv->tx_busy && statf & HI3110_STAT_TXMTY) { net->stats.tx_packets++; net->stats.tx_bytes += can_get_echo_skb(net, 0, NULL); priv->tx_busy = false; netif_wake_queue(net); } if (intf == 0) break; } mutex_unlock(&priv->hi3110_lock); return IRQ_HANDLED; }"
1128----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53099/bad/syscall.c----bpf_link_show_fdinfo,"static void bpf_link_show_fdinfo(struct seq_file *m, struct file *filp) { const struct bpf_link *link = filp->private_data; const struct bpf_prog *prog = link->prog; char prog_tag[sizeof(prog->tag) * 2 + 1] = { }; bin2hex(prog_tag, prog->tag, sizeof(prog->tag)); seq_printf(m, <S2SV_StartVul> ""link_type:\t%s\n"" <S2SV_EndVul> <S2SV_StartVul> ""link_id:\t%u\n"" <S2SV_EndVul> ""prog_tag:\t%s\n"" ""prog_id:\t%u\n"", <S2SV_StartVul> bpf_link_type_strs[link->type], <S2SV_EndVul> <S2SV_StartVul> link->id, <S2SV_EndVul> prog_tag, prog->aux->id); if (link->ops->show_fdinfo) link->ops->show_fdinfo(link, m); }","- ""link_type:\t%s\n""
- ""link_id:\t%u\n""
- bpf_link_type_strs[link->type],
- link->id,
+ enum bpf_link_type type = link->type;
+ if (type < ARRAY_SIZE(bpf_link_type_strs) && bpf_link_type_strs[type]) {
+ seq_printf(m, ""link_type:\t%s\n"", bpf_link_type_strs[type]);
+ } else {
+ WARN_ONCE(1, ""missing BPF_LINK_TYPE(...) for link type %u\n"", type);
+ seq_printf(m, ""link_type:\t<%u>\n"", type);
+ }
+ seq_printf(m, ""link_id:\t%u\n"", link->id);","static void bpf_link_show_fdinfo(struct seq_file *m, struct file *filp) { const struct bpf_link *link = filp->private_data; const struct bpf_prog *prog = link->prog; enum bpf_link_type type = link->type; char prog_tag[sizeof(prog->tag) * 2 + 1] = { }; if (type < ARRAY_SIZE(bpf_link_type_strs) && bpf_link_type_strs[type]) { seq_printf(m, ""link_type:\t%s\n"", bpf_link_type_strs[type]); } else { WARN_ONCE(1, ""missing BPF_LINK_TYPE(...) for link type %u\n"", type); seq_printf(m, ""link_type:\t<%u>\n"", type); } seq_printf(m, ""link_id:\t%u\n"", link->id); bin2hex(prog_tag, prog->tag, sizeof(prog->tag)); seq_printf(m, ""prog_tag:\t%s\n"" ""prog_id:\t%u\n"", prog_tag, prog->aux->id); if (link->ops->show_fdinfo) link->ops->show_fdinfo(link, m); }"
1221----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53232/bad/s390-iommu.c----*s390_iommu_probe_device,static struct iommu_device *s390_iommu_probe_device(struct device *dev) { struct zpci_dev *zdev; if (!dev_is_pci(dev)) return ERR_PTR(-ENODEV); zdev = to_zpci_dev(dev); if (zdev->start_dma > zdev->end_dma || zdev->start_dma > ZPCI_TABLE_SIZE_RT - 1) return ERR_PTR(-EINVAL); if (zdev->end_dma > ZPCI_TABLE_SIZE_RT - 1) zdev->end_dma = ZPCI_TABLE_SIZE_RT - 1; if (zdev->tlb_refresh) dev->iommu->shadow_on_flush = 1; <S2SV_StartVul> return &zdev->iommu_dev; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul>,"- return &zdev->iommu_dev;
- }
+ spin_lock_init(&zdev->dom_lock);
+ zdev_s390_domain_update(zdev, &blocking_domain);
+ return &zdev->iommu_dev;
+ }","static struct iommu_device *s390_iommu_probe_device(struct device *dev) { struct zpci_dev *zdev; if (!dev_is_pci(dev)) return ERR_PTR(-ENODEV); zdev = to_zpci_dev(dev); if (zdev->start_dma > zdev->end_dma || zdev->start_dma > ZPCI_TABLE_SIZE_RT - 1) return ERR_PTR(-EINVAL); if (zdev->end_dma > ZPCI_TABLE_SIZE_RT - 1) zdev->end_dma = ZPCI_TABLE_SIZE_RT - 1; if (zdev->tlb_refresh) dev->iommu->shadow_on_flush = 1; spin_lock_init(&zdev->dom_lock); zdev_s390_domain_update(zdev, &blocking_domain); return &zdev->iommu_dev; }"
1387----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56722/bad/hns_roce_hem.c----hns_roce_table_put,"void hns_roce_table_put(struct hns_roce_dev *hr_dev, struct hns_roce_hem_table *table, unsigned long obj) { struct device *dev = hr_dev->dev; unsigned long i; int ret; if (hns_roce_check_whether_mhop(hr_dev, table->type)) { hns_roce_table_mhop_put(hr_dev, table, obj, 1); return; } i = obj / (table->table_chunk_size / table->obj_size); if (!refcount_dec_and_mutex_lock(&table->hem[i]->refcount, &table->mutex)) return; ret = hr_dev->hw->clear_hem(hr_dev, table, obj, HEM_HOP_STEP_DIRECT); if (ret) <S2SV_StartVul> dev_warn(dev, ""failed to clear HEM base address, ret = %d.\n"", <S2SV_EndVul> <S2SV_StartVul> ret); <S2SV_EndVul> hns_roce_free_hem(hr_dev, table->hem[i]); table->hem[i] = NULL; mutex_unlock(&table->mutex); }","- dev_warn(dev, ""failed to clear HEM base address, ret = %d.\n"",
- ret);
+ dev_warn_ratelimited(dev, ""failed to clear HEM base address, ret = %d.\n"",
+ ret);","void hns_roce_table_put(struct hns_roce_dev *hr_dev, struct hns_roce_hem_table *table, unsigned long obj) { struct device *dev = hr_dev->dev; unsigned long i; int ret; if (hns_roce_check_whether_mhop(hr_dev, table->type)) { hns_roce_table_mhop_put(hr_dev, table, obj, 1); return; } i = obj / (table->table_chunk_size / table->obj_size); if (!refcount_dec_and_mutex_lock(&table->hem[i]->refcount, &table->mutex)) return; ret = hr_dev->hw->clear_hem(hr_dev, table, obj, HEM_HOP_STEP_DIRECT); if (ret) dev_warn_ratelimited(dev, ""failed to clear HEM base address, ret = %d.\n"", ret); hns_roce_free_hem(hr_dev, table->hem[i]); table->hem[i] = NULL; mutex_unlock(&table->mutex); }"
894----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50144/bad/xe_gt_tlb_invalidation.c----send_tlb_invalidation,"static int send_tlb_invalidation(struct xe_guc *guc, struct xe_gt_tlb_invalidation_fence *fence, u32 *action, int len) { struct xe_gt *gt = guc_to_gt(guc); struct xe_device *xe = gt_to_xe(gt); int seqno; int ret; xe_gt_assert(gt, fence); mutex_lock(&guc->ct.lock); seqno = gt->tlb_invalidation.seqno; fence->seqno = seqno; trace_xe_gt_tlb_invalidation_fence_send(xe, fence); action[1] = seqno; ret = xe_guc_ct_send_locked(&guc->ct, action, len, G2H_LEN_DW_TLB_INVALIDATE, 1); if (!ret) { spin_lock_irq(&gt->tlb_invalidation.pending_lock); if (tlb_invalidation_seqno_past(gt, seqno)) { __invalidation_fence_signal(xe, fence); } else { fence->invalidation_time = ktime_get(); list_add_tail(&fence->link, &gt->tlb_invalidation.pending_fences); if (list_is_singular(&gt->tlb_invalidation.pending_fences)) queue_delayed_work(system_wq, &gt->tlb_invalidation.fence_tdr, tlb_timeout_jiffies(gt)); <S2SV_StartVul> } <S2SV_EndVul> spin_unlock_irq(&gt->tlb_invalidation.pending_lock); <S2SV_StartVul> } else if (ret < 0) { <S2SV_EndVul> __invalidation_fence_signal(xe, fence); <S2SV_StartVul> } <S2SV_EndVul> if (!ret) { gt->tlb_invalidation.seqno = (gt->tlb_invalidation.seqno + 1) % TLB_INVALIDATION_SEQNO_MAX; if (!gt->tlb_invalidation.seqno) gt->tlb_invalidation.seqno = 1; } mutex_unlock(&guc->ct.lock); return ret; }","- }
- } else if (ret < 0) {
- }
+ }
+ } else {
+ }","static int send_tlb_invalidation(struct xe_guc *guc, struct xe_gt_tlb_invalidation_fence *fence, u32 *action, int len) { struct xe_gt *gt = guc_to_gt(guc); struct xe_device *xe = gt_to_xe(gt); int seqno; int ret; xe_gt_assert(gt, fence); mutex_lock(&guc->ct.lock); seqno = gt->tlb_invalidation.seqno; fence->seqno = seqno; trace_xe_gt_tlb_invalidation_fence_send(xe, fence); action[1] = seqno; ret = xe_guc_ct_send_locked(&guc->ct, action, len, G2H_LEN_DW_TLB_INVALIDATE, 1); if (!ret) { spin_lock_irq(&gt->tlb_invalidation.pending_lock); if (tlb_invalidation_seqno_past(gt, seqno)) { __invalidation_fence_signal(xe, fence); } else { fence->invalidation_time = ktime_get(); list_add_tail(&fence->link, &gt->tlb_invalidation.pending_fences); if (list_is_singular(&gt->tlb_invalidation.pending_fences)) queue_delayed_work(system_wq, &gt->tlb_invalidation.fence_tdr, tlb_timeout_jiffies(gt)); } spin_unlock_irq(&gt->tlb_invalidation.pending_lock); } else { __invalidation_fence_signal(xe, fence); } if (!ret) { gt->tlb_invalidation.seqno = (gt->tlb_invalidation.seqno + 1) % TLB_INVALIDATION_SEQNO_MAX; if (!gt->tlb_invalidation.seqno) gt->tlb_invalidation.seqno = 1; } mutex_unlock(&guc->ct.lock); return ret; }"
946----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50190/bad/ice_main.c----ice_init_tx_topology,"ice_init_tx_topology(struct ice_hw *hw, const struct firmware *firmware) { u8 num_tx_sched_layers = hw->num_tx_sched_layers; struct ice_pf *pf = hw->back; struct device *dev; <S2SV_StartVul> u8 *buf_copy; <S2SV_EndVul> int err; dev = ice_pf_to_dev(pf); <S2SV_StartVul> buf_copy = kmemdup(firmware->data, firmware->size, GFP_KERNEL); <S2SV_EndVul> <S2SV_StartVul> err = ice_cfg_tx_topo(hw, buf_copy, firmware->size); <S2SV_EndVul> if (!err) { if (hw->num_tx_sched_layers > num_tx_sched_layers) dev_info(dev, ""Tx scheduling layers switching feature disabled\n""); else dev_info(dev, ""Tx scheduling layers switching feature enabled\n""); ice_deinit_hw(hw); err = ice_init_hw(hw); return err; } else if (err == -EIO) { dev_info(dev, ""DDP package does not support Tx scheduling layers switching feature - please update to the latest DDP package and try again\n""); } return 0; }","- u8 *buf_copy;
- buf_copy = kmemdup(firmware->data, firmware->size, GFP_KERNEL);
- err = ice_cfg_tx_topo(hw, buf_copy, firmware->size);
+ err = ice_cfg_tx_topo(hw, firmware->data, firmware->size);","ice_init_tx_topology(struct ice_hw *hw, const struct firmware *firmware) { u8 num_tx_sched_layers = hw->num_tx_sched_layers; struct ice_pf *pf = hw->back; struct device *dev; int err; dev = ice_pf_to_dev(pf); err = ice_cfg_tx_topo(hw, firmware->data, firmware->size); if (!err) { if (hw->num_tx_sched_layers > num_tx_sched_layers) dev_info(dev, ""Tx scheduling layers switching feature disabled\n""); else dev_info(dev, ""Tx scheduling layers switching feature enabled\n""); ice_deinit_hw(hw); err = ice_init_hw(hw); return err; } else if (err == -EIO) { dev_info(dev, ""DDP package does not support Tx scheduling layers switching feature - please update to the latest DDP package and try again\n""); } return 0; }"
1118----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53092/bad/virtio_pci_common.c----vp_del_vqs,"void vp_del_vqs(struct virtio_device *vdev) { struct virtio_pci_device *vp_dev = to_vp_device(vdev); struct virtqueue *vq, *n; int i; list_for_each_entry_safe(vq, n, &vdev->vqs, list) { <S2SV_StartVul> if (vp_dev->per_vq_vectors) { <S2SV_EndVul> <S2SV_StartVul> int v = vp_dev->vqs[vq->index]->msix_vector; <S2SV_EndVul> if (v != VIRTIO_MSI_NO_VECTOR && !vp_is_slow_path_vector(v)) { int irq = pci_irq_vector(vp_dev->pci_dev, v); irq_update_affinity_hint(irq, NULL); free_irq(irq, vq); } } <S2SV_StartVul> vp_del_vq(vq); <S2SV_EndVul> } vp_dev->per_vq_vectors = false; if (vp_dev->intx_enabled) { free_irq(vp_dev->pci_dev->irq, vp_dev); vp_dev->intx_enabled = 0; } for (i = 0; i < vp_dev->msix_used_vectors; ++i) free_irq(pci_irq_vector(vp_dev->pci_dev, i), vp_dev); if (vp_dev->msix_affinity_masks) { for (i = 0; i < vp_dev->msix_vectors; i++) free_cpumask_var(vp_dev->msix_affinity_masks[i]); } if (vp_dev->msix_enabled) { vp_dev->config_vector(vp_dev, VIRTIO_MSI_NO_VECTOR); pci_free_irq_vectors(vp_dev->pci_dev); vp_dev->msix_enabled = 0; } vp_dev->msix_vectors = 0; vp_dev->msix_used_vectors = 0; kfree(vp_dev->msix_names); vp_dev->msix_names = NULL; kfree(vp_dev->msix_affinity_masks); vp_dev->msix_affinity_masks = NULL; kfree(vp_dev->vqs); vp_dev->vqs = NULL; }","- if (vp_dev->per_vq_vectors) {
- int v = vp_dev->vqs[vq->index]->msix_vector;
- vp_del_vq(vq);
+ {
+ struct virtio_pci_device *vp_dev = to_vp_device(vdev);
+ struct virtio_pci_vq_info *info;
+ info = vp_is_avq(vdev, vq->index) ? vp_dev->admin_vq.info :
+ vp_dev->vqs[vq->index];
+ if (vp_dev->per_vq_vectors) {
+ int v = info->msix_vector;
+ }
+ }
+ vp_del_vq(vq, info);
+ }","void vp_del_vqs(struct virtio_device *vdev) { struct virtio_pci_device *vp_dev = to_vp_device(vdev); struct virtio_pci_vq_info *info; struct virtqueue *vq, *n; int i; list_for_each_entry_safe(vq, n, &vdev->vqs, list) { info = vp_is_avq(vdev, vq->index) ? vp_dev->admin_vq.info : vp_dev->vqs[vq->index]; if (vp_dev->per_vq_vectors) { int v = info->msix_vector; if (v != VIRTIO_MSI_NO_VECTOR && !vp_is_slow_path_vector(v)) { int irq = pci_irq_vector(vp_dev->pci_dev, v); irq_update_affinity_hint(irq, NULL); free_irq(irq, vq); } } vp_del_vq(vq, info); } vp_dev->per_vq_vectors = false; if (vp_dev->intx_enabled) { free_irq(vp_dev->pci_dev->irq, vp_dev); vp_dev->intx_enabled = 0; } for (i = 0; i < vp_dev->msix_used_vectors; ++i) free_irq(pci_irq_vector(vp_dev->pci_dev, i), vp_dev); if (vp_dev->msix_affinity_masks) { for (i = 0; i < vp_dev->msix_vectors; i++) free_cpumask_var(vp_dev->msix_affinity_masks[i]); } if (vp_dev->msix_enabled) { vp_dev->config_vector(vp_dev, VIRTIO_MSI_NO_VECTOR); pci_free_irq_vectors(vp_dev->pci_dev); vp_dev->msix_enabled = 0; } vp_dev->msix_vectors = 0; vp_dev->msix_used_vectors = 0; kfree(vp_dev->msix_names); vp_dev->msix_names = NULL; kfree(vp_dev->msix_affinity_masks); vp_dev->msix_affinity_masks = NULL; kfree(vp_dev->vqs); vp_dev->vqs = NULL; }"
955----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50197/bad/pinctrl-intel-platform.c----intel_platform_pinctrl_prepare_community,"static int intel_platform_pinctrl_prepare_community(struct device *dev, struct intel_community *community, struct intel_platform_pins *pins) { <S2SV_StartVul> struct fwnode_handle *child; <S2SV_EndVul> struct intel_padgroup *gpps; unsigned int group; size_t ngpps; u32 offset; int ret; ret = device_property_read_u32(dev, ""intc-gpio-pad-ownership-offset"", &offset); if (ret) return ret; community->padown_offset = offset; ret = device_property_read_u32(dev, ""intc-gpio-pad-configuration-lock-offset"", &offset); if (ret) return ret; community->padcfglock_offset = offset; ret = device_property_read_u32(dev, ""intc-gpio-host-software-pad-ownership-offset"", &offset); if (ret) return ret; community->hostown_offset = offset; ret = device_property_read_u32(dev, ""intc-gpio-gpi-interrupt-status-offset"", &offset); if (ret) return ret; community->is_offset = offset; ret = device_property_read_u32(dev, ""intc-gpio-gpi-interrupt-enable-offset"", &offset); if (ret) return ret; community->ie_offset = offset; ngpps = device_get_child_node_count(dev); if (!ngpps) return -ENODEV; gpps = devm_kcalloc(dev, ngpps, sizeof(*gpps), GFP_KERNEL); if (!gpps) return -ENOMEM; group = 0; <S2SV_StartVul> device_for_each_child_node(dev, child) { <S2SV_EndVul> struct intel_padgroup *gpp = &gpps[group]; gpp->reg_num = group; ret = intel_platform_pinctrl_prepare_group(dev, child, gpp, pins); if (ret) return ret; group++; } community->ngpps = ngpps; community->gpps = gpps; return 0; }","- struct fwnode_handle *child;
- device_for_each_child_node(dev, child) {
+ device_for_each_child_node_scoped(dev, child) {","static int intel_platform_pinctrl_prepare_community(struct device *dev, struct intel_community *community, struct intel_platform_pins *pins) { struct intel_padgroup *gpps; unsigned int group; size_t ngpps; u32 offset; int ret; ret = device_property_read_u32(dev, ""intc-gpio-pad-ownership-offset"", &offset); if (ret) return ret; community->padown_offset = offset; ret = device_property_read_u32(dev, ""intc-gpio-pad-configuration-lock-offset"", &offset); if (ret) return ret; community->padcfglock_offset = offset; ret = device_property_read_u32(dev, ""intc-gpio-host-software-pad-ownership-offset"", &offset); if (ret) return ret; community->hostown_offset = offset; ret = device_property_read_u32(dev, ""intc-gpio-gpi-interrupt-status-offset"", &offset); if (ret) return ret; community->is_offset = offset; ret = device_property_read_u32(dev, ""intc-gpio-gpi-interrupt-enable-offset"", &offset); if (ret) return ret; community->ie_offset = offset; ngpps = device_get_child_node_count(dev); if (!ngpps) return -ENODEV; gpps = devm_kcalloc(dev, ngpps, sizeof(*gpps), GFP_KERNEL); if (!gpps) return -ENOMEM; group = 0; device_for_each_child_node_scoped(dev, child) { struct intel_padgroup *gpp = &gpps[group]; gpp->reg_num = group; ret = intel_platform_pinctrl_prepare_group(dev, child, gpp, pins); if (ret) return ret; group++; } community->ngpps = ngpps; community->gpps = gpps; return 0; }"
1519----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2025-21674/bad/ipsec.c----mlx5e_xfrm_del_state,"static void mlx5e_xfrm_del_state(struct xfrm_state *x) { struct mlx5e_ipsec_sa_entry *sa_entry = to_ipsec_sa_entry(x); <S2SV_StartVul> struct mlx5_accel_esp_xfrm_attrs *attrs = &sa_entry->attrs; <S2SV_EndVul> struct mlx5e_ipsec *ipsec = sa_entry->ipsec; struct mlx5e_ipsec_sa_entry *old; if (x->xso.flags & XFRM_DEV_OFFLOAD_FLAG_ACQ) return; old = xa_erase_bh(&ipsec->sadb, sa_entry->ipsec_obj_id); WARN_ON(old != sa_entry); <S2SV_StartVul> if (attrs->mode == XFRM_MODE_TUNNEL && <S2SV_EndVul> <S2SV_StartVul> attrs->type == XFRM_DEV_OFFLOAD_PACKET) <S2SV_EndVul> <S2SV_StartVul> flush_workqueue(ipsec->wq); <S2SV_EndVul> }","- struct mlx5_accel_esp_xfrm_attrs *attrs = &sa_entry->attrs;
- if (attrs->mode == XFRM_MODE_TUNNEL &&
- attrs->type == XFRM_DEV_OFFLOAD_PACKET)
- flush_workqueue(ipsec->wq);
+ }","static void mlx5e_xfrm_del_state(struct xfrm_state *x) { struct mlx5e_ipsec_sa_entry *sa_entry = to_ipsec_sa_entry(x); struct mlx5e_ipsec *ipsec = sa_entry->ipsec; struct mlx5e_ipsec_sa_entry *old; if (x->xso.flags & XFRM_DEV_OFFLOAD_FLAG_ACQ) return; old = xa_erase_bh(&ipsec->sadb, sa_entry->ipsec_obj_id); WARN_ON(old != sa_entry); }"
1103----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53084/bad/pvr_vm.c----pvr_vm_context_release,"pvr_vm_context_release(struct kref *ref_count) { struct pvr_vm_context *vm_ctx = container_of(ref_count, struct pvr_vm_context, ref_count); if (vm_ctx->fw_mem_ctx_obj) pvr_fw_object_destroy(vm_ctx->fw_mem_ctx_obj); <S2SV_StartVul> WARN_ON(pvr_vm_unmap(vm_ctx, vm_ctx->gpuvm_mgr.mm_start, <S2SV_EndVul> <S2SV_StartVul> vm_ctx->gpuvm_mgr.mm_range)); <S2SV_EndVul> pvr_mmu_context_destroy(vm_ctx->mmu_ctx); drm_gem_private_object_fini(&vm_ctx->dummy_gem); mutex_destroy(&vm_ctx->lock); drm_gpuvm_put(&vm_ctx->gpuvm_mgr); }","- WARN_ON(pvr_vm_unmap(vm_ctx, vm_ctx->gpuvm_mgr.mm_start,
- vm_ctx->gpuvm_mgr.mm_range));
+ pvr_vm_unmap_all(vm_ctx);","pvr_vm_context_release(struct kref *ref_count) { struct pvr_vm_context *vm_ctx = container_of(ref_count, struct pvr_vm_context, ref_count); if (vm_ctx->fw_mem_ctx_obj) pvr_fw_object_destroy(vm_ctx->fw_mem_ctx_obj); pvr_vm_unmap_all(vm_ctx); pvr_mmu_context_destroy(vm_ctx->mmu_ctx); drm_gem_private_object_fini(&vm_ctx->dummy_gem); mutex_destroy(&vm_ctx->lock); drm_gpuvm_put(&vm_ctx->gpuvm_mgr); }"
434----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47672/bad/mac80211.c----iwl_mvm_mac_flush,"static void iwl_mvm_mac_flush(struct ieee80211_hw *hw, struct ieee80211_vif *vif, u32 queues, bool drop) { struct iwl_mvm *mvm = IWL_MAC80211_GET_MVM(hw); struct iwl_mvm_vif *mvmvif; struct iwl_mvm_sta *mvmsta; struct ieee80211_sta *sta; int i; u32 msk = 0; if (!vif) { iwl_mvm_flush_no_vif(mvm, queues, drop); return; } if (vif->type != NL80211_IFTYPE_STATION) return; flush_work(&mvm->add_stream_wk); mutex_lock(&mvm->mutex); mvmvif = iwl_mvm_vif_from_mac80211(vif); for (i = 0; i < ARRAY_SIZE(mvm->fw_id_to_mac_id); i++) { sta = rcu_dereference_protected(mvm->fw_id_to_mac_id[i], lockdep_is_held(&mvm->mutex)); if (IS_ERR_OR_NULL(sta)) continue; mvmsta = iwl_mvm_sta_from_mac80211(sta); if (mvmsta->vif != vif) continue; WARN_ON(i != mvmvif->ap_sta_id && !sta->tdls); if (drop) { if (iwl_mvm_flush_sta(mvm, mvmsta, false, 0)) IWL_ERR(mvm, ""flush request fail\n""); } else { msk |= mvmsta->tfd_queue_msk; if (iwl_mvm_has_new_tx_api(mvm)) iwl_mvm_wait_sta_queues_empty(mvm, mvmsta); } } mutex_unlock(&mvm->mutex); <S2SV_StartVul> if (!drop && !iwl_mvm_has_new_tx_api(mvm)) <S2SV_EndVul> iwl_trans_wait_tx_queues_empty(mvm->trans, msk); }","- if (!drop && !iwl_mvm_has_new_tx_api(mvm))
+ if (!drop && !iwl_mvm_has_new_tx_api(mvm) &&
+ !test_bit(IWL_MVM_STATUS_HW_RESTART_REQUESTED,
+ &mvm->status))","static void iwl_mvm_mac_flush(struct ieee80211_hw *hw, struct ieee80211_vif *vif, u32 queues, bool drop) { struct iwl_mvm *mvm = IWL_MAC80211_GET_MVM(hw); struct iwl_mvm_vif *mvmvif; struct iwl_mvm_sta *mvmsta; struct ieee80211_sta *sta; int i; u32 msk = 0; if (!vif) { iwl_mvm_flush_no_vif(mvm, queues, drop); return; } if (vif->type != NL80211_IFTYPE_STATION) return; flush_work(&mvm->add_stream_wk); mutex_lock(&mvm->mutex); mvmvif = iwl_mvm_vif_from_mac80211(vif); for (i = 0; i < ARRAY_SIZE(mvm->fw_id_to_mac_id); i++) { sta = rcu_dereference_protected(mvm->fw_id_to_mac_id[i], lockdep_is_held(&mvm->mutex)); if (IS_ERR_OR_NULL(sta)) continue; mvmsta = iwl_mvm_sta_from_mac80211(sta); if (mvmsta->vif != vif) continue; WARN_ON(i != mvmvif->ap_sta_id && !sta->tdls); if (drop) { if (iwl_mvm_flush_sta(mvm, mvmsta, false, 0)) IWL_ERR(mvm, ""flush request fail\n""); } else { msk |= mvmsta->tfd_queue_msk; if (iwl_mvm_has_new_tx_api(mvm)) iwl_mvm_wait_sta_queues_empty(mvm, mvmsta); } } mutex_unlock(&mvm->mutex); if (!drop && !iwl_mvm_has_new_tx_api(mvm) && !test_bit(IWL_MVM_STATUS_HW_RESTART_REQUESTED, &mvm->status)) iwl_trans_wait_tx_queues_empty(mvm->trans, msk); }"
256----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46731/bad/ppatomctrl.c----atomctrl_retrieve_ac_timing,"static int atomctrl_retrieve_ac_timing( uint8_t index, ATOM_INIT_REG_BLOCK *reg_block, pp_atomctrl_mc_reg_table *table) { uint32_t i, j; uint8_t tmem_id; ATOM_MEMORY_SETTING_DATA_BLOCK *reg_data = (ATOM_MEMORY_SETTING_DATA_BLOCK *) ((uint8_t *)reg_block + (2 * sizeof(uint16_t)) + le16_to_cpu(reg_block->usRegIndexTblSize)); uint8_t num_ranges = 0; while (*(uint32_t *)reg_data != END_OF_REG_DATA_BLOCK && num_ranges < VBIOS_MAX_AC_TIMING_ENTRIES) { tmem_id = (uint8_t)((*(uint32_t *)reg_data & MEM_ID_MASK) >> MEM_ID_SHIFT); if (index == tmem_id) { table->mc_reg_table_entry[num_ranges].mclk_max = (uint32_t)((*(uint32_t *)reg_data & CLOCK_RANGE_MASK) >> CLOCK_RANGE_SHIFT); for (i = 0, j = 1; i < table->last; i++) { if ((table->mc_reg_address[i].uc_pre_reg_data & LOW_NIBBLE_MASK) == DATA_FROM_TABLE) { table->mc_reg_table_entry[num_ranges].mc_data[i] = (uint32_t)*((uint32_t *)reg_data + j); j++; } else if ((table->mc_reg_address[i].uc_pre_reg_data & LOW_NIBBLE_MASK) == DATA_EQU_PREV) { <S2SV_StartVul> table->mc_reg_table_entry[num_ranges].mc_data[i] = <S2SV_EndVul> <S2SV_StartVul> table->mc_reg_table_entry[num_ranges].mc_data[i-1]; <S2SV_EndVul> } } num_ranges++; } reg_data = (ATOM_MEMORY_SETTING_DATA_BLOCK *) ((uint8_t *)reg_data + le16_to_cpu(reg_block->usRegDataBlkSize)) ; } PP_ASSERT_WITH_CODE((*(uint32_t *)reg_data == END_OF_REG_DATA_BLOCK), ""Invalid VramInfo table."", return -1); table->num_entries = num_ranges; return 0; }","- table->mc_reg_table_entry[num_ranges].mc_data[i] =
- table->mc_reg_table_entry[num_ranges].mc_data[i-1];
+ if (i)
+ table->mc_reg_table_entry[num_ranges].mc_data[i] =
+ table->mc_reg_table_entry[num_ranges].mc_data[i-1];","static int atomctrl_retrieve_ac_timing( uint8_t index, ATOM_INIT_REG_BLOCK *reg_block, pp_atomctrl_mc_reg_table *table) { uint32_t i, j; uint8_t tmem_id; ATOM_MEMORY_SETTING_DATA_BLOCK *reg_data = (ATOM_MEMORY_SETTING_DATA_BLOCK *) ((uint8_t *)reg_block + (2 * sizeof(uint16_t)) + le16_to_cpu(reg_block->usRegIndexTblSize)); uint8_t num_ranges = 0; while (*(uint32_t *)reg_data != END_OF_REG_DATA_BLOCK && num_ranges < VBIOS_MAX_AC_TIMING_ENTRIES) { tmem_id = (uint8_t)((*(uint32_t *)reg_data & MEM_ID_MASK) >> MEM_ID_SHIFT); if (index == tmem_id) { table->mc_reg_table_entry[num_ranges].mclk_max = (uint32_t)((*(uint32_t *)reg_data & CLOCK_RANGE_MASK) >> CLOCK_RANGE_SHIFT); for (i = 0, j = 1; i < table->last; i++) { if ((table->mc_reg_address[i].uc_pre_reg_data & LOW_NIBBLE_MASK) == DATA_FROM_TABLE) { table->mc_reg_table_entry[num_ranges].mc_data[i] = (uint32_t)*((uint32_t *)reg_data + j); j++; } else if ((table->mc_reg_address[i].uc_pre_reg_data & LOW_NIBBLE_MASK) == DATA_EQU_PREV) { if (i) table->mc_reg_table_entry[num_ranges].mc_data[i] = table->mc_reg_table_entry[num_ranges].mc_data[i-1]; } } num_ranges++; } reg_data = (ATOM_MEMORY_SETTING_DATA_BLOCK *) ((uint8_t *)reg_data + le16_to_cpu(reg_block->usRegDataBlkSize)) ; } PP_ASSERT_WITH_CODE((*(uint32_t *)reg_data == END_OF_REG_DATA_BLOCK), ""Invalid VramInfo table."", return -1); table->num_entries = num_ranges; return 0; }"
1368----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56702/bad/verifier.c----btf_check_func_arg_match,"static int btf_check_func_arg_match(struct bpf_verifier_env *env, int subprog, const struct btf *btf, struct bpf_reg_state *regs) { struct bpf_subprog_info *sub = subprog_info(env, subprog); struct bpf_verifier_log *log = &env->log; u32 i; int ret; ret = btf_prepare_func_args(env, subprog); if (ret) return ret; for (i = 0; i < sub->arg_cnt; i++) { u32 regno = i + 1; struct bpf_reg_state *reg = &regs[regno]; struct bpf_subprog_arg_info *arg = &sub->args[i]; if (arg->arg_type == ARG_ANYTHING) { if (reg->type != SCALAR_VALUE) { bpf_log(log, ""R%d is not a scalar\n"", regno); return -EINVAL; } } else if (arg->arg_type == ARG_PTR_TO_CTX) { ret = check_func_arg_reg_off(env, reg, regno, ARG_DONTCARE); if (ret < 0) return ret; if (reg->type != PTR_TO_CTX) { bpf_log(log, ""arg#%d expects pointer to ctx\n"", i); return -EINVAL; } } else if (base_type(arg->arg_type) == ARG_PTR_TO_MEM) { ret = check_func_arg_reg_off(env, reg, regno, ARG_DONTCARE); if (ret < 0) return ret; if (check_mem_reg(env, reg, regno, arg->mem_size)) return -EINVAL; if (!(arg->arg_type & PTR_MAYBE_NULL) && (reg->type & PTR_MAYBE_NULL)) { bpf_log(log, ""arg#%d is expected to be non-NULL\n"", i); return -EINVAL; } } else if (base_type(arg->arg_type) == ARG_PTR_TO_ARENA) { if (reg->type != PTR_TO_ARENA && reg->type != SCALAR_VALUE) { bpf_log(log, ""R%d is not a pointer to arena or scalar.\n"", regno); return -EINVAL; } } else if (arg->arg_type == (ARG_PTR_TO_DYNPTR | MEM_RDONLY)) { ret = check_func_arg_reg_off(env, reg, regno, ARG_PTR_TO_DYNPTR); if (ret) return ret; ret = process_dynptr_func(env, regno, -1, arg->arg_type, 0); if (ret) return ret; } else if (base_type(arg->arg_type) == ARG_PTR_TO_BTF_ID) { struct bpf_call_arg_meta meta; int err; if (register_is_null(reg) && type_may_be_null(arg->arg_type)) continue; memset(&meta, 0, sizeof(meta)); err = check_reg_type(env, regno, arg->arg_type, &arg->btf_id, &meta); err = err ?: check_func_arg_reg_off(env, reg, regno, arg->arg_type); <S2SV_StartVul> if (err) <S2SV_EndVul> <S2SV_StartVul> return err; <S2SV_EndVul> } else { bpf_log(log, ""verifier bug: unrecognized arg#%d type %d\n"", i, arg->arg_type); return -EFAULT; } } return 0; }","- if (err)
- return err;
+ bool mask;
+ mask = mask_raw_tp_reg(env, reg);
+ unmask_raw_tp_reg(reg, mask);","static int btf_check_func_arg_match(struct bpf_verifier_env *env, int subprog, const struct btf *btf, struct bpf_reg_state *regs) { struct bpf_subprog_info *sub = subprog_info(env, subprog); struct bpf_verifier_log *log = &env->log; u32 i; int ret; ret = btf_prepare_func_args(env, subprog); if (ret) return ret; for (i = 0; i < sub->arg_cnt; i++) { u32 regno = i + 1; struct bpf_reg_state *reg = &regs[regno]; struct bpf_subprog_arg_info *arg = &sub->args[i]; if (arg->arg_type == ARG_ANYTHING) { if (reg->type != SCALAR_VALUE) { bpf_log(log, ""R%d is not a scalar\n"", regno); return -EINVAL; } } else if (arg->arg_type == ARG_PTR_TO_CTX) { ret = check_func_arg_reg_off(env, reg, regno, ARG_DONTCARE); if (ret < 0) return ret; if (reg->type != PTR_TO_CTX) { bpf_log(log, ""arg#%d expects pointer to ctx\n"", i); return -EINVAL; } } else if (base_type(arg->arg_type) == ARG_PTR_TO_MEM) { ret = check_func_arg_reg_off(env, reg, regno, ARG_DONTCARE); if (ret < 0) return ret; if (check_mem_reg(env, reg, regno, arg->mem_size)) return -EINVAL; if (!(arg->arg_type & PTR_MAYBE_NULL) && (reg->type & PTR_MAYBE_NULL)) { bpf_log(log, ""arg#%d is expected to be non-NULL\n"", i); return -EINVAL; } } else if (base_type(arg->arg_type) == ARG_PTR_TO_ARENA) { if (reg->type != PTR_TO_ARENA && reg->type != SCALAR_VALUE) { bpf_log(log, ""R%d is not a pointer to arena or scalar.\n"", regno); return -EINVAL; } } else if (arg->arg_type == (ARG_PTR_TO_DYNPTR | MEM_RDONLY)) { ret = check_func_arg_reg_off(env, reg, regno, ARG_PTR_TO_DYNPTR); if (ret) return ret; ret = process_dynptr_func(env, regno, -1, arg->arg_type, 0); if (ret) return ret; } else if (base_type(arg->arg_type) == ARG_PTR_TO_BTF_ID) { struct bpf_call_arg_meta meta; bool mask; int err; if (register_is_null(reg) && type_may_be_null(arg->arg_type)) continue; memset(&meta, 0, sizeof(meta)); mask = mask_raw_tp_reg(env, reg); err = check_reg_type(env, regno, arg->arg_type, &arg->btf_id, &meta); err = err ?: check_func_arg_reg_off(env, reg, regno, arg->arg_type); unmask_raw_tp_reg(reg, mask); if (err) return err; } else { bpf_log(log, ""verifier bug: unrecognized arg#%d type %d\n"", i, arg->arg_type); return -EFAULT; } } return 0; }"
1526----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2025-21684/bad/gpio-xilinx.c----xgpio_set_multiple,"static void xgpio_set_multiple(struct gpio_chip *gc, unsigned long *mask, unsigned long *bits) { DECLARE_BITMAP(hw_mask, 64); DECLARE_BITMAP(hw_bits, 64); DECLARE_BITMAP(state, 64); unsigned long flags; struct xgpio_instance *chip = gpiochip_get_data(gc); bitmap_remap(hw_mask, mask, chip->sw_map, chip->hw_map, 64); bitmap_remap(hw_bits, bits, chip->sw_map, chip->hw_map, 64); <S2SV_StartVul> spin_lock_irqsave(&chip->gpio_lock, flags); <S2SV_EndVul> bitmap_replace(state, chip->state, hw_bits, hw_mask, 64); xgpio_write_ch_all(chip, XGPIO_DATA_OFFSET, state); bitmap_copy(chip->state, state, 64); <S2SV_StartVul> spin_unlock_irqrestore(&chip->gpio_lock, flags); <S2SV_EndVul> }","- spin_lock_irqsave(&chip->gpio_lock, flags);
- spin_unlock_irqrestore(&chip->gpio_lock, flags);
+ raw_spin_lock_irqsave(&chip->gpio_lock, flags);
+ raw_spin_unlock_irqrestore(&chip->gpio_lock, flags);","static void xgpio_set_multiple(struct gpio_chip *gc, unsigned long *mask, unsigned long *bits) { DECLARE_BITMAP(hw_mask, 64); DECLARE_BITMAP(hw_bits, 64); DECLARE_BITMAP(state, 64); unsigned long flags; struct xgpio_instance *chip = gpiochip_get_data(gc); bitmap_remap(hw_mask, mask, chip->sw_map, chip->hw_map, 64); bitmap_remap(hw_bits, bits, chip->sw_map, chip->hw_map, 64); raw_spin_lock_irqsave(&chip->gpio_lock, flags); bitmap_replace(state, chip->state, hw_bits, hw_mask, 64); xgpio_write_ch_all(chip, XGPIO_DATA_OFFSET, state); bitmap_copy(chip->state, state, 64); raw_spin_unlock_irqrestore(&chip->gpio_lock, flags); }"
1268----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56551/bad/amdgpu_device.c----amdgpu_device_fini_sw,"void amdgpu_device_fini_sw(struct amdgpu_device *adev) { int idx; bool px; <S2SV_StartVul> amdgpu_fence_driver_sw_fini(adev); <S2SV_EndVul> amdgpu_device_ip_fini(adev); amdgpu_ucode_release(&adev->firmware.gpu_info_fw); adev->accel_working = false; dma_fence_put(rcu_dereference_protected(adev->gang_submit, true)); amdgpu_reset_fini(adev); if (!amdgpu_device_has_dc_support(adev)) amdgpu_i2c_fini(adev); if (amdgpu_emu_mode != 1) amdgpu_atombios_fini(adev); kfree(adev->bios); adev->bios = NULL; kfree(adev->fru_info); adev->fru_info = NULL; px = amdgpu_device_supports_px(adev_to_drm(adev)); if (px || (!dev_is_removable(&adev->pdev->dev) && apple_gmux_detect(NULL, NULL))) vga_switcheroo_unregister_client(adev->pdev); if (px) vga_switcheroo_fini_domain_pm_ops(adev->dev); if ((adev->pdev->class >> 8) == PCI_CLASS_DISPLAY_VGA) vga_client_unregister(adev->pdev); if (drm_dev_enter(adev_to_drm(adev), &idx)) { iounmap(adev->rmmio); adev->rmmio = NULL; amdgpu_doorbell_fini(adev); drm_dev_exit(idx); } if (IS_ENABLED(CONFIG_PERF_EVENTS)) amdgpu_pmu_fini(adev); if (adev->mman.discovery_bin) amdgpu_discovery_fini(adev); amdgpu_reset_put_reset_domain(adev->reset_domain); adev->reset_domain = NULL; kfree(adev->pci_state); }","- amdgpu_fence_driver_sw_fini(adev);
+ amdgpu_fence_driver_sw_fini(adev);","void amdgpu_device_fini_sw(struct amdgpu_device *adev) { int idx; bool px; amdgpu_device_ip_fini(adev); amdgpu_fence_driver_sw_fini(adev); amdgpu_ucode_release(&adev->firmware.gpu_info_fw); adev->accel_working = false; dma_fence_put(rcu_dereference_protected(adev->gang_submit, true)); amdgpu_reset_fini(adev); if (!amdgpu_device_has_dc_support(adev)) amdgpu_i2c_fini(adev); if (amdgpu_emu_mode != 1) amdgpu_atombios_fini(adev); kfree(adev->bios); adev->bios = NULL; kfree(adev->fru_info); adev->fru_info = NULL; px = amdgpu_device_supports_px(adev_to_drm(adev)); if (px || (!dev_is_removable(&adev->pdev->dev) && apple_gmux_detect(NULL, NULL))) vga_switcheroo_unregister_client(adev->pdev); if (px) vga_switcheroo_fini_domain_pm_ops(adev->dev); if ((adev->pdev->class >> 8) == PCI_CLASS_DISPLAY_VGA) vga_client_unregister(adev->pdev); if (drm_dev_enter(adev_to_drm(adev), &idx)) { iounmap(adev->rmmio); adev->rmmio = NULL; amdgpu_doorbell_fini(adev); drm_dev_exit(idx); } if (IS_ENABLED(CONFIG_PERF_EVENTS)) amdgpu_pmu_fini(adev); if (adev->mman.discovery_bin) amdgpu_discovery_fini(adev); amdgpu_reset_put_reset_domain(adev->reset_domain); adev->reset_domain = NULL; kfree(adev->pci_state); }"
1400----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56724/bad/intel_soc_pmic_bxtwc.c----bxtwc_probe,"static int bxtwc_probe(struct platform_device *pdev) { struct device *dev = &pdev->dev; int ret; acpi_handle handle; acpi_status status; unsigned long long hrv; struct intel_soc_pmic *pmic; handle = ACPI_HANDLE(&pdev->dev); status = acpi_evaluate_integer(handle, ""_HRV"", NULL, &hrv); if (ACPI_FAILURE(status)) return dev_err_probe(dev, -ENODEV, ""Failed to get PMIC hardware revision\n""); if (hrv != BROXTON_PMIC_WC_HRV) return dev_err_probe(dev, -ENODEV, ""Invalid PMIC hardware revision: %llu\n"", hrv); pmic = devm_kzalloc(&pdev->dev, sizeof(*pmic), GFP_KERNEL); if (!pmic) return -ENOMEM; ret = platform_get_irq(pdev, 0); if (ret < 0) return ret; pmic->irq = ret; dev_set_drvdata(&pdev->dev, pmic); pmic->dev = &pdev->dev; pmic->regmap = devm_regmap_init(&pdev->dev, NULL, pmic, &bxtwc_regmap_config); if (IS_ERR(pmic->regmap)) return dev_err_probe(dev, PTR_ERR(pmic->regmap), ""Failed to initialise regmap\n""); ret = devm_regmap_add_irq_chip(&pdev->dev, pmic->regmap, pmic->irq, IRQF_ONESHOT | IRQF_SHARED, 0, &bxtwc_regmap_irq_chip, &pmic->irq_chip_data); if (ret) return dev_err_probe(dev, ret, ""Failed to add IRQ chip\n""); <S2SV_StartVul> ret = bxtwc_add_chained_irq_chip(pmic, pmic->irq_chip_data, <S2SV_EndVul> BXTWC_PWRBTN_LVL1_IRQ, <S2SV_StartVul> IRQF_ONESHOT, <S2SV_EndVul> &bxtwc_regmap_irq_chip_pwrbtn, &pmic->irq_chip_data_pwrbtn); if (ret) return dev_err_probe(dev, ret, ""Failed to add PWRBTN IRQ chip\n""); <S2SV_StartVul> ret = bxtwc_add_chained_irq_chip(pmic, pmic->irq_chip_data, <S2SV_EndVul> <S2SV_StartVul> BXTWC_TMU_LVL1_IRQ, <S2SV_EndVul> <S2SV_StartVul> IRQF_ONESHOT, <S2SV_EndVul> <S2SV_StartVul> &bxtwc_regmap_irq_chip_tmu, <S2SV_EndVul> <S2SV_StartVul> &pmic->irq_chip_data_tmu); <S2SV_EndVul> <S2SV_StartVul> if (ret) <S2SV_EndVul> <S2SV_StartVul> return dev_err_probe(dev, ret, ""Failed to add TMU IRQ chip\n""); <S2SV_EndVul> <S2SV_StartVul> ret = bxtwc_add_chained_irq_chip(pmic, pmic->irq_chip_data, <S2SV_EndVul> BXTWC_BCU_LVL1_IRQ, IRQF_ONESHOT, &bxtwc_regmap_irq_chip_bcu, &pmic->irq_chip_data_bcu); if (ret) return dev_err_probe(dev, ret, ""Failed to add BUC IRQ chip\n""); ret = bxtwc_add_chained_irq_chip(pmic, pmic->irq_chip_data, BXTWC_ADC_LVL1_IRQ, IRQF_ONESHOT, &bxtwc_regmap_irq_chip_adc, &pmic->irq_chip_data_adc); if (ret) return dev_err_probe(dev, ret, ""Failed to add ADC IRQ chip\n""); ret = bxtwc_add_chained_devices(pmic, bxt_wc_chgr_dev, ARRAY_SIZE(bxt_wc_chgr_dev), pmic->irq_chip_data, BXTWC_CHGR_LVL1_IRQ, IRQF_ONESHOT, &bxtwc_regmap_irq_chip_chgr, &pmic->irq_chip_data_chgr); if (ret) return ret; ret = bxtwc_add_chained_irq_chip(pmic, pmic->irq_chip_data, BXTWC_CRIT_LVL1_IRQ, IRQF_ONESHOT, &bxtwc_regmap_irq_chip_crit, &pmic->irq_chip_data_crit); if (ret) return dev_err_probe(dev, ret, ""Failed to add CRIT IRQ chip\n""); ret = devm_mfd_add_devices(&pdev->dev, PLATFORM_DEVID_NONE, bxt_wc_dev, ARRAY_SIZE(bxt_wc_dev), NULL, 0, NULL); if (ret) return dev_err_probe(dev, ret, ""Failed to add devices\n""); ret = sysfs_create_group(&pdev->dev.kobj, &bxtwc_group); if (ret) { dev_err(&pdev->dev, ""Failed to create sysfs group %d\n"", ret); return ret; } regmap_update_bits(pmic->regmap, BXTWC_MIRQLVL1, BXTWC_MIRQLVL1_MCHGR, 0); return 0; }","- ret = bxtwc_add_chained_irq_chip(pmic, pmic->irq_chip_data,
- IRQF_ONESHOT,
- ret = bxtwc_add_chained_irq_chip(pmic, pmic->irq_chip_data,
- BXTWC_TMU_LVL1_IRQ,
- IRQF_ONESHOT,
- &bxtwc_regmap_irq_chip_tmu,
- &pmic->irq_chip_data_tmu);
- if (ret)
- return dev_err_probe(dev, ret, ""Failed to add TMU IRQ chip\n"");
- ret = bxtwc_add_chained_irq_chip(pmic, pmic->irq_chip_data,
+ ret = bxtwc_add_chained_devices(pmic, bxt_wc_tmu_dev, ARRAY_SIZE(bxt_wc_tmu_dev),
+ pmic->irq_chip_data,
+ BXTWC_TMU_LVL1_IRQ,
+ IRQF_ONESHOT,
+ &bxtwc_regmap_irq_chip_tmu,
+ &pmic->irq_chip_data_tmu);
+ if (ret)
+ return ret;
+ IRQF_ONESHOT,","static int bxtwc_probe(struct platform_device *pdev) { struct device *dev = &pdev->dev; int ret; acpi_handle handle; acpi_status status; unsigned long long hrv; struct intel_soc_pmic *pmic; handle = ACPI_HANDLE(&pdev->dev); status = acpi_evaluate_integer(handle, ""_HRV"", NULL, &hrv); if (ACPI_FAILURE(status)) return dev_err_probe(dev, -ENODEV, ""Failed to get PMIC hardware revision\n""); if (hrv != BROXTON_PMIC_WC_HRV) return dev_err_probe(dev, -ENODEV, ""Invalid PMIC hardware revision: %llu\n"", hrv); pmic = devm_kzalloc(&pdev->dev, sizeof(*pmic), GFP_KERNEL); if (!pmic) return -ENOMEM; ret = platform_get_irq(pdev, 0); if (ret < 0) return ret; pmic->irq = ret; dev_set_drvdata(&pdev->dev, pmic); pmic->dev = &pdev->dev; pmic->regmap = devm_regmap_init(&pdev->dev, NULL, pmic, &bxtwc_regmap_config); if (IS_ERR(pmic->regmap)) return dev_err_probe(dev, PTR_ERR(pmic->regmap), ""Failed to initialise regmap\n""); ret = devm_regmap_add_irq_chip(&pdev->dev, pmic->regmap, pmic->irq, IRQF_ONESHOT | IRQF_SHARED, 0, &bxtwc_regmap_irq_chip, &pmic->irq_chip_data); if (ret) return dev_err_probe(dev, ret, ""Failed to add IRQ chip\n""); ret = bxtwc_add_chained_devices(pmic, bxt_wc_tmu_dev, ARRAY_SIZE(bxt_wc_tmu_dev), pmic->irq_chip_data, BXTWC_TMU_LVL1_IRQ, IRQF_ONESHOT, &bxtwc_regmap_irq_chip_tmu, &pmic->irq_chip_data_tmu); if (ret) return ret; ret = bxtwc_add_chained_irq_chip(pmic, pmic->irq_chip_data, BXTWC_PWRBTN_LVL1_IRQ, IRQF_ONESHOT, &bxtwc_regmap_irq_chip_pwrbtn, &pmic->irq_chip_data_pwrbtn); if (ret) return dev_err_probe(dev, ret, ""Failed to add PWRBTN IRQ chip\n""); ret = bxtwc_add_chained_irq_chip(pmic, pmic->irq_chip_data, BXTWC_BCU_LVL1_IRQ, IRQF_ONESHOT, &bxtwc_regmap_irq_chip_bcu, &pmic->irq_chip_data_bcu); if (ret) return dev_err_probe(dev, ret, ""Failed to add BUC IRQ chip\n""); ret = bxtwc_add_chained_irq_chip(pmic, pmic->irq_chip_data, BXTWC_ADC_LVL1_IRQ, IRQF_ONESHOT, &bxtwc_regmap_irq_chip_adc, &pmic->irq_chip_data_adc); if (ret) return dev_err_probe(dev, ret, ""Failed to add ADC IRQ chip\n""); ret = bxtwc_add_chained_devices(pmic, bxt_wc_chgr_dev, ARRAY_SIZE(bxt_wc_chgr_dev), pmic->irq_chip_data, BXTWC_CHGR_LVL1_IRQ, IRQF_ONESHOT, &bxtwc_regmap_irq_chip_chgr, &pmic->irq_chip_data_chgr); if (ret) return ret; ret = bxtwc_add_chained_irq_chip(pmic, pmic->irq_chip_data, BXTWC_CRIT_LVL1_IRQ, IRQF_ONESHOT, &bxtwc_regmap_irq_chip_crit, &pmic->irq_chip_data_crit); if (ret) return dev_err_probe(dev, ret, ""Failed to add CRIT IRQ chip\n""); ret = devm_mfd_add_devices(&pdev->dev, PLATFORM_DEVID_NONE, bxt_wc_dev, ARRAY_SIZE(bxt_wc_dev), NULL, 0, NULL); if (ret) return dev_err_probe(dev, ret, ""Failed to add devices\n""); ret = sysfs_create_group(&pdev->dev.kobj, &bxtwc_group); if (ret) { dev_err(&pdev->dev, ""Failed to create sysfs group %d\n"", ret); return ret; } regmap_update_bits(pmic->regmap, BXTWC_MIRQLVL1, BXTWC_MIRQLVL1_MCHGR, 0); return 0; }"
359----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46821/bad/navi10_ppt.c----navi10_emit_clk_levels,"static int navi10_emit_clk_levels(struct smu_context *smu, enum smu_clk_type clk_type, char *buf, int *offset) { uint16_t *curve_settings; int ret = 0; uint32_t cur_value = 0, value = 0; uint32_t freq_values[3] = {0}; uint32_t i, levels, mark_index = 0, count = 0; struct smu_table_context *table_context = &smu->smu_table; uint32_t gen_speed, lane_width; struct smu_dpm_context *smu_dpm = &smu->smu_dpm; struct smu_11_0_dpm_context *dpm_context = smu_dpm->dpm_context; PPTable_t *pptable = (PPTable_t *)table_context->driver_pptable; OverDriveTable_t *od_table = (OverDriveTable_t *)table_context->overdrive_table; struct smu_11_0_overdrive_table *od_settings = smu->od_settings; uint32_t min_value, max_value; switch (clk_type) { case SMU_GFXCLK: case SMU_SCLK: case SMU_SOCCLK: case SMU_MCLK: case SMU_UCLK: case SMU_FCLK: case SMU_VCLK: case SMU_DCLK: case SMU_DCEFCLK: ret = navi10_get_current_clk_freq_by_table(smu, clk_type, &cur_value); if (ret) return ret; ret = smu_v11_0_get_dpm_level_count(smu, clk_type, &count); if (ret) return ret; <S2SV_StartVul> if (!navi10_is_support_fine_grained_dpm(smu, clk_type)) { <S2SV_EndVul> for (i = 0; i < count; i++) { ret = smu_v11_0_get_dpm_freq_by_index(smu, clk_type, i, &value); if (ret) return ret; *offset += sysfs_emit_at(buf, *offset, ""%d: %uMhz %s\n"", i, value, cur_value == value ? ""*"" : """"); } } else { ret = smu_v11_0_get_dpm_freq_by_index(smu, clk_type, 0, &freq_values[0]); if (ret) return ret; ret = smu_v11_0_get_dpm_freq_by_index(smu, clk_type, count - 1, &freq_values[2]); if (ret) return ret; freq_values[1] = cur_value; mark_index = cur_value == freq_values[0] ? 0 : cur_value == freq_values[2] ? 2 : 1; levels = 3; if (mark_index != 1) { levels = 2; freq_values[1] = freq_values[2]; } for (i = 0; i < levels; i++) { *offset += sysfs_emit_at(buf, *offset, ""%d: %uMhz %s\n"", i, freq_values[i], i == mark_index ? ""*"" : """"); } } break; case SMU_PCIE: gen_speed = smu_v11_0_get_current_pcie_link_speed_level(smu); lane_width = smu_v11_0_get_current_pcie_link_width_level(smu); for (i = 0; i < NUM_LINK_LEVELS; i++) { *offset += sysfs_emit_at(buf, *offset, ""%d: %s %s %dMhz %s\n"", i, (dpm_context->dpm_tables.pcie_table.pcie_gen[i] == 0) ? ""2.5GT/s,"" : (dpm_context->dpm_tables.pcie_table.pcie_gen[i] == 1) ? ""5.0GT/s,"" : (dpm_context->dpm_tables.pcie_table.pcie_gen[i] == 2) ? ""8.0GT/s,"" : (dpm_context->dpm_tables.pcie_table.pcie_gen[i] == 3) ? ""16.0GT/s,"" : """", (dpm_context->dpm_tables.pcie_table.pcie_lane[i] == 1) ? ""x1"" : (dpm_context->dpm_tables.pcie_table.pcie_lane[i] == 2) ? ""x2"" : (dpm_context->dpm_tables.pcie_table.pcie_lane[i] == 3) ? ""x4"" : (dpm_context->dpm_tables.pcie_table.pcie_lane[i] == 4) ? ""x8"" : (dpm_context->dpm_tables.pcie_table.pcie_lane[i] == 5) ? ""x12"" : (dpm_context->dpm_tables.pcie_table.pcie_lane[i] == 6) ? ""x16"" : """", pptable->LclkFreq[i], (gen_speed == dpm_context->dpm_tables.pcie_table.pcie_gen[i]) && (lane_width == dpm_context->dpm_tables.pcie_table.pcie_lane[i]) ? ""*"" : """"); } break; case SMU_OD_SCLK: if (!smu->od_enabled || !od_table || !od_settings) return -EOPNOTSUPP; if (!navi10_od_feature_is_supported(od_settings, SMU_11_0_ODCAP_GFXCLK_LIMITS)) break; *offset += sysfs_emit_at(buf, *offset, ""OD_SCLK:\n0: %uMhz\n1: %uMhz\n"", od_table->GfxclkFmin, od_table->GfxclkFmax); break; case SMU_OD_MCLK: if (!smu->od_enabled || !od_table || !od_settings) return -EOPNOTSUPP; if (!navi10_od_feature_is_supported(od_settings, SMU_11_0_ODCAP_UCLK_MAX)) break; *offset += sysfs_emit_at(buf, *offset, ""OD_MCLK:\n1: %uMHz\n"", od_table->UclkFmax); break; case SMU_OD_VDDC_CURVE: if (!smu->od_enabled || !od_table || !od_settings) return -EOPNOTSUPP; if (!navi10_od_feature_is_supported(od_settings, SMU_11_0_ODCAP_GFXCLK_CURVE)) break; *offset += sysfs_emit_at(buf, *offset, ""OD_VDDC_CURVE:\n""); for (i = 0; i < 3; i++) { switch (i) { case 0: curve_settings = &od_table->GfxclkFreq1; break; case 1: curve_settings = &od_table->GfxclkFreq2; break; case 2: curve_settings = &od_table->GfxclkFreq3; break; default: break; } *offset += sysfs_emit_at(buf, *offset, ""%d: %uMHz %umV\n"", i, curve_settings[0], curve_settings[1] / NAVI10_VOLTAGE_SCALE); } break; case SMU_OD_RANGE: if (!smu->od_enabled || !od_table || !od_settings) return -EOPNOTSUPP; *offset += sysfs_emit_at(buf, *offset, ""%s:\n"", ""OD_RANGE""); if (navi10_od_feature_is_supported(od_settings, SMU_11_0_ODCAP_GFXCLK_LIMITS)) { navi10_od_setting_get_range(od_settings, SMU_11_0_ODSETTING_GFXCLKFMIN, &min_value, NULL); navi10_od_setting_get_range(od_settings, SMU_11_0_ODSETTING_GFXCLKFMAX, NULL, &max_value); *offset += sysfs_emit_at(buf, *offset, ""SCLK: %7uMhz %10uMhz\n"", min_value, max_value); } if (navi10_od_feature_is_supported(od_settings, SMU_11_0_ODCAP_UCLK_MAX)) { navi10_od_setting_get_range(od_settings, SMU_11_0_ODSETTING_UCLKFMAX, &min_value, &max_value); *offset += sysfs_emit_at(buf, *offset, ""MCLK: %7uMhz %10uMhz\n"", min_value, max_value); } if (navi10_od_feature_is_supported(od_settings, SMU_11_0_ODCAP_GFXCLK_CURVE)) { navi10_od_setting_get_range(od_settings, SMU_11_0_ODSETTING_VDDGFXCURVEFREQ_P1, &min_value, &max_value); *offset += sysfs_emit_at(buf, *offset, ""VDDC_CURVE_SCLK[0]: %7uMhz %10uMhz\n"", min_value, max_value); navi10_od_setting_get_range(od_settings, SMU_11_0_ODSETTING_VDDGFXCURVEVOLTAGE_P1, &min_value, &max_value); *offset += sysfs_emit_at(buf, *offset, ""VDDC_CURVE_VOLT[0]: %7dmV %11dmV\n"", min_value, max_value); navi10_od_setting_get_range(od_settings, SMU_11_0_ODSETTING_VDDGFXCURVEFREQ_P2, &min_value, &max_value); *offset += sysfs_emit_at(buf, *offset, ""VDDC_CURVE_SCLK[1]: %7uMhz %10uMhz\n"", min_value, max_value); navi10_od_setting_get_range(od_settings, SMU_11_0_ODSETTING_VDDGFXCURVEVOLTAGE_P2, &min_value, &max_value); *offset += sysfs_emit_at(buf, *offset, ""VDDC_CURVE_VOLT[1]: %7dmV %11dmV\n"", min_value, max_value); navi10_od_setting_get_range(od_settings, SMU_11_0_ODSETTING_VDDGFXCURVEFREQ_P3, &min_value, &max_value); *offset += sysfs_emit_at(buf, *offset, ""VDDC_CURVE_SCLK[2]: %7uMhz %10uMhz\n"", min_value, max_value); navi10_od_setting_get_range(od_settings, SMU_11_0_ODSETTING_VDDGFXCURVEVOLTAGE_P3, &min_value, &max_value); *offset += sysfs_emit_at(buf, *offset, ""VDDC_CURVE_VOLT[2]: %7dmV %11dmV\n"", min_value, max_value); } break; default: break; } return 0; }","- if (!navi10_is_support_fine_grained_dpm(smu, clk_type)) {
+ return ret;
+ ret = navi10_is_support_fine_grained_dpm(smu, clk_type);
+ if (ret < 0)
+ return ret;
+ if (!ret) {","static int navi10_emit_clk_levels(struct smu_context *smu, enum smu_clk_type clk_type, char *buf, int *offset) { uint16_t *curve_settings; int ret = 0; uint32_t cur_value = 0, value = 0; uint32_t freq_values[3] = {0}; uint32_t i, levels, mark_index = 0, count = 0; struct smu_table_context *table_context = &smu->smu_table; uint32_t gen_speed, lane_width; struct smu_dpm_context *smu_dpm = &smu->smu_dpm; struct smu_11_0_dpm_context *dpm_context = smu_dpm->dpm_context; PPTable_t *pptable = (PPTable_t *)table_context->driver_pptable; OverDriveTable_t *od_table = (OverDriveTable_t *)table_context->overdrive_table; struct smu_11_0_overdrive_table *od_settings = smu->od_settings; uint32_t min_value, max_value; switch (clk_type) { case SMU_GFXCLK: case SMU_SCLK: case SMU_SOCCLK: case SMU_MCLK: case SMU_UCLK: case SMU_FCLK: case SMU_VCLK: case SMU_DCLK: case SMU_DCEFCLK: ret = navi10_get_current_clk_freq_by_table(smu, clk_type, &cur_value); if (ret) return ret; ret = smu_v11_0_get_dpm_level_count(smu, clk_type, &count); if (ret) return ret; ret = navi10_is_support_fine_grained_dpm(smu, clk_type); if (ret < 0) return ret; if (!ret) { for (i = 0; i < count; i++) { ret = smu_v11_0_get_dpm_freq_by_index(smu, clk_type, i, &value); if (ret) return ret; *offset += sysfs_emit_at(buf, *offset, ""%d: %uMhz %s\n"", i, value, cur_value == value ? ""*"" : """"); } } else { ret = smu_v11_0_get_dpm_freq_by_index(smu, clk_type, 0, &freq_values[0]); if (ret) return ret; ret = smu_v11_0_get_dpm_freq_by_index(smu, clk_type, count - 1, &freq_values[2]); if (ret) return ret; freq_values[1] = cur_value; mark_index = cur_value == freq_values[0] ? 0 : cur_value == freq_values[2] ? 2 : 1; levels = 3; if (mark_index != 1) { levels = 2; freq_values[1] = freq_values[2]; } for (i = 0; i < levels; i++) { *offset += sysfs_emit_at(buf, *offset, ""%d: %uMhz %s\n"", i, freq_values[i], i == mark_index ? ""*"" : """"); } } break; case SMU_PCIE: gen_speed = smu_v11_0_get_current_pcie_link_speed_level(smu); lane_width = smu_v11_0_get_current_pcie_link_width_level(smu); for (i = 0; i < NUM_LINK_LEVELS; i++) { *offset += sysfs_emit_at(buf, *offset, ""%d: %s %s %dMhz %s\n"", i, (dpm_context->dpm_tables.pcie_table.pcie_gen[i] == 0) ? ""2.5GT/s,"" : (dpm_context->dpm_tables.pcie_table.pcie_gen[i] == 1) ? ""5.0GT/s,"" : (dpm_context->dpm_tables.pcie_table.pcie_gen[i] == 2) ? ""8.0GT/s,"" : (dpm_context->dpm_tables.pcie_table.pcie_gen[i] == 3) ? ""16.0GT/s,"" : """", (dpm_context->dpm_tables.pcie_table.pcie_lane[i] == 1) ? ""x1"" : (dpm_context->dpm_tables.pcie_table.pcie_lane[i] == 2) ? ""x2"" : (dpm_context->dpm_tables.pcie_table.pcie_lane[i] == 3) ? ""x4"" : (dpm_context->dpm_tables.pcie_table.pcie_lane[i] == 4) ? ""x8"" : (dpm_context->dpm_tables.pcie_table.pcie_lane[i] == 5) ? ""x12"" : (dpm_context->dpm_tables.pcie_table.pcie_lane[i] == 6) ? ""x16"" : """", pptable->LclkFreq[i], (gen_speed == dpm_context->dpm_tables.pcie_table.pcie_gen[i]) && (lane_width == dpm_context->dpm_tables.pcie_table.pcie_lane[i]) ? ""*"" : """"); } break; case SMU_OD_SCLK: if (!smu->od_enabled || !od_table || !od_settings) return -EOPNOTSUPP; if (!navi10_od_feature_is_supported(od_settings, SMU_11_0_ODCAP_GFXCLK_LIMITS)) break; *offset += sysfs_emit_at(buf, *offset, ""OD_SCLK:\n0: %uMhz\n1: %uMhz\n"", od_table->GfxclkFmin, od_table->GfxclkFmax); break; case SMU_OD_MCLK: if (!smu->od_enabled || !od_table || !od_settings) return -EOPNOTSUPP; if (!navi10_od_feature_is_supported(od_settings, SMU_11_0_ODCAP_UCLK_MAX)) break; *offset += sysfs_emit_at(buf, *offset, ""OD_MCLK:\n1: %uMHz\n"", od_table->UclkFmax); break; case SMU_OD_VDDC_CURVE: if (!smu->od_enabled || !od_table || !od_settings) return -EOPNOTSUPP; if (!navi10_od_feature_is_supported(od_settings, SMU_11_0_ODCAP_GFXCLK_CURVE)) break; *offset += sysfs_emit_at(buf, *offset, ""OD_VDDC_CURVE:\n""); for (i = 0; i < 3; i++) { switch (i) { case 0: curve_settings = &od_table->GfxclkFreq1; break; case 1: curve_settings = &od_table->GfxclkFreq2; break; case 2: curve_settings = &od_table->GfxclkFreq3; break; default: break; } *offset += sysfs_emit_at(buf, *offset, ""%d: %uMHz %umV\n"", i, curve_settings[0], curve_settings[1] / NAVI10_VOLTAGE_SCALE); } break; case SMU_OD_RANGE: if (!smu->od_enabled || !od_table || !od_settings) return -EOPNOTSUPP; *offset += sysfs_emit_at(buf, *offset, ""%s:\n"", ""OD_RANGE""); if (navi10_od_feature_is_supported(od_settings, SMU_11_0_ODCAP_GFXCLK_LIMITS)) { navi10_od_setting_get_range(od_settings, SMU_11_0_ODSETTING_GFXCLKFMIN, &min_value, NULL); navi10_od_setting_get_range(od_settings, SMU_11_0_ODSETTING_GFXCLKFMAX, NULL, &max_value); *offset += sysfs_emit_at(buf, *offset, ""SCLK: %7uMhz %10uMhz\n"", min_value, max_value); } if (navi10_od_feature_is_supported(od_settings, SMU_11_0_ODCAP_UCLK_MAX)) { navi10_od_setting_get_range(od_settings, SMU_11_0_ODSETTING_UCLKFMAX, &min_value, &max_value); *offset += sysfs_emit_at(buf, *offset, ""MCLK: %7uMhz %10uMhz\n"", min_value, max_value); } if (navi10_od_feature_is_supported(od_settings, SMU_11_0_ODCAP_GFXCLK_CURVE)) { navi10_od_setting_get_range(od_settings, SMU_11_0_ODSETTING_VDDGFXCURVEFREQ_P1, &min_value, &max_value); *offset += sysfs_emit_at(buf, *offset, ""VDDC_CURVE_SCLK[0]: %7uMhz %10uMhz\n"", min_value, max_value); navi10_od_setting_get_range(od_settings, SMU_11_0_ODSETTING_VDDGFXCURVEVOLTAGE_P1, &min_value, &max_value); *offset += sysfs_emit_at(buf, *offset, ""VDDC_CURVE_VOLT[0]: %7dmV %11dmV\n"", min_value, max_value); navi10_od_setting_get_range(od_settings, SMU_11_0_ODSETTING_VDDGFXCURVEFREQ_P2, &min_value, &max_value); *offset += sysfs_emit_at(buf, *offset, ""VDDC_CURVE_SCLK[1]: %7uMhz %10uMhz\n"", min_value, max_value); navi10_od_setting_get_range(od_settings, SMU_11_0_ODSETTING_VDDGFXCURVEVOLTAGE_P2, &min_value, &max_value); *offset += sysfs_emit_at(buf, *offset, ""VDDC_CURVE_VOLT[1]: %7dmV %11dmV\n"", min_value, max_value); navi10_od_setting_get_range(od_settings, SMU_11_0_ODSETTING_VDDGFXCURVEFREQ_P3, &min_value, &max_value); *offset += sysfs_emit_at(buf, *offset, ""VDDC_CURVE_SCLK[2]: %7uMhz %10uMhz\n"", min_value, max_value); navi10_od_setting_get_range(od_settings, SMU_11_0_ODSETTING_VDDGFXCURVEVOLTAGE_P3, &min_value, &max_value); *offset += sysfs_emit_at(buf, *offset, ""VDDC_CURVE_VOLT[2]: %7dmV %11dmV\n"", min_value, max_value); } break; default: break; } return 0; }"
506----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47744/bad/kvm_main.c----kvm_suspend,static int kvm_suspend(void) { <S2SV_StartVul> lockdep_assert_not_held(&kvm_lock); <S2SV_EndVul> lockdep_assert_irqs_disabled(); if (kvm_usage_count) hardware_disable_nolock(NULL); return 0; },"- lockdep_assert_not_held(&kvm_lock);
+ lockdep_assert_not_held(&kvm_usage_lock);",static int kvm_suspend(void) { lockdep_assert_not_held(&kvm_usage_lock); lockdep_assert_irqs_disabled(); if (kvm_usage_count) hardware_disable_nolock(NULL); return 0; }
1130----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53101/bad/file.c----ocfs2_setattr,"int ocfs2_setattr(struct dentry *dentry, struct iattr *attr) { int status = 0, size_change; int inode_locked = 0; struct inode *inode = d_inode(dentry); struct super_block *sb = inode->i_sb; struct ocfs2_super *osb = OCFS2_SB(sb); struct buffer_head *bh = NULL; handle_t *handle = NULL; struct dquot *transfer_to[MAXQUOTAS] = { }; int qtype; int had_lock; struct ocfs2_lock_holder oh; trace_ocfs2_setattr(inode, dentry, (unsigned long long)OCFS2_I(inode)->ip_blkno, dentry->d_name.len, dentry->d_name.name, <S2SV_StartVul> attr->ia_valid, attr->ia_mode, <S2SV_EndVul> <S2SV_StartVul> from_kuid(&init_user_ns, attr->ia_uid), <S2SV_EndVul> <S2SV_StartVul> from_kgid(&init_user_ns, attr->ia_gid)); <S2SV_EndVul> if (S_ISLNK(inode->i_mode)) attr->ia_valid &= ~ATTR_SIZE; #define OCFS2_VALID_ATTRS (ATTR_ATIME | ATTR_MTIME | ATTR_CTIME | ATTR_SIZE \ | ATTR_GID | ATTR_UID | ATTR_MODE) if (!(attr->ia_valid & OCFS2_VALID_ATTRS)) return 0; status = setattr_prepare(dentry, attr); if (status) return status; if (is_quota_modification(inode, attr)) { status = dquot_initialize(inode); if (status) return status; } size_change = S_ISREG(inode->i_mode) && attr->ia_valid & ATTR_SIZE; if (size_change) { inode_dio_wait(inode); status = ocfs2_rw_lock(inode, 1); if (status < 0) { mlog_errno(status); goto bail; } } had_lock = ocfs2_inode_lock_tracker(inode, &bh, 1, &oh); if (had_lock < 0) { status = had_lock; goto bail_unlock_rw; } else if (had_lock) { mlog(ML_ERROR, ""Another case of recursive locking:\n""); dump_stack(); } inode_locked = 1; if (size_change) { status = inode_newsize_ok(inode, attr->ia_size); if (status) goto bail_unlock; if (i_size_read(inode) >= attr->ia_size) { if (ocfs2_should_order_data(inode)) { status = ocfs2_begin_ordered_truncate(inode, attr->ia_size); if (status) goto bail_unlock; } status = ocfs2_truncate_file(inode, bh, attr->ia_size); } else status = ocfs2_extend_file(inode, bh, attr->ia_size); if (status < 0) { if (status != -ENOSPC) mlog_errno(status); status = -ENOSPC; goto bail_unlock; } } if ((attr->ia_valid & ATTR_UID && !uid_eq(attr->ia_uid, inode->i_uid)) || (attr->ia_valid & ATTR_GID && !gid_eq(attr->ia_gid, inode->i_gid))) { if (attr->ia_valid & ATTR_UID && !uid_eq(attr->ia_uid, inode->i_uid) && OCFS2_HAS_RO_COMPAT_FEATURE(sb, OCFS2_FEATURE_RO_COMPAT_USRQUOTA)) { transfer_to[USRQUOTA] = dqget(sb, make_kqid_uid(attr->ia_uid)); if (IS_ERR(transfer_to[USRQUOTA])) { status = PTR_ERR(transfer_to[USRQUOTA]); transfer_to[USRQUOTA] = NULL; goto bail_unlock; } } if (attr->ia_valid & ATTR_GID && !gid_eq(attr->ia_gid, inode->i_gid) && OCFS2_HAS_RO_COMPAT_FEATURE(sb, OCFS2_FEATURE_RO_COMPAT_GRPQUOTA)) { transfer_to[GRPQUOTA] = dqget(sb, make_kqid_gid(attr->ia_gid)); if (IS_ERR(transfer_to[GRPQUOTA])) { status = PTR_ERR(transfer_to[GRPQUOTA]); transfer_to[GRPQUOTA] = NULL; goto bail_unlock; } } down_write(&OCFS2_I(inode)->ip_alloc_sem); handle = ocfs2_start_trans(osb, OCFS2_INODE_UPDATE_CREDITS + 2 * ocfs2_quota_trans_credits(sb)); if (IS_ERR(handle)) { status = PTR_ERR(handle); mlog_errno(status); goto bail_unlock_alloc; } status = __dquot_transfer(inode, transfer_to); if (status < 0) goto bail_commit; } else { down_write(&OCFS2_I(inode)->ip_alloc_sem); handle = ocfs2_start_trans(osb, OCFS2_INODE_UPDATE_CREDITS); if (IS_ERR(handle)) { status = PTR_ERR(handle); mlog_errno(status); goto bail_unlock_alloc; } } setattr_copy(inode, attr); mark_inode_dirty(inode); status = ocfs2_mark_inode_dirty(handle, inode, bh); if (status < 0) mlog_errno(status); bail_commit: ocfs2_commit_trans(osb, handle); bail_unlock_alloc: up_write(&OCFS2_I(inode)->ip_alloc_sem); bail_unlock: if (status && inode_locked) { ocfs2_inode_unlock_tracker(inode, 1, &oh, had_lock); inode_locked = 0; } bail_unlock_rw: if (size_change) ocfs2_rw_unlock(inode, 1); bail: for (qtype = 0; qtype < OCFS2_MAXQUOTAS; qtype++) dqput(transfer_to[qtype]); if (!status && attr->ia_valid & ATTR_MODE) { status = ocfs2_acl_chmod(inode, bh); if (status < 0) mlog_errno(status); } if (inode_locked) ocfs2_inode_unlock_tracker(inode, 1, &oh, had_lock); brelse(bh); return status; }","- attr->ia_valid, attr->ia_mode,
- from_kuid(&init_user_ns, attr->ia_uid),
- from_kgid(&init_user_ns, attr->ia_gid));
+ attr->ia_valid,
+ attr->ia_valid & ATTR_MODE ? attr->ia_mode : 0,
+ attr->ia_valid & ATTR_UID ?
+ from_kuid(&init_user_ns, attr->ia_uid) : 0,
+ attr->ia_valid & ATTR_GID ?
+ from_kgid(&init_user_ns, attr->ia_gid) : 0);","int ocfs2_setattr(struct dentry *dentry, struct iattr *attr) { int status = 0, size_change; int inode_locked = 0; struct inode *inode = d_inode(dentry); struct super_block *sb = inode->i_sb; struct ocfs2_super *osb = OCFS2_SB(sb); struct buffer_head *bh = NULL; handle_t *handle = NULL; struct dquot *transfer_to[MAXQUOTAS] = { }; int qtype; int had_lock; struct ocfs2_lock_holder oh; trace_ocfs2_setattr(inode, dentry, (unsigned long long)OCFS2_I(inode)->ip_blkno, dentry->d_name.len, dentry->d_name.name, attr->ia_valid, attr->ia_valid & ATTR_MODE ? attr->ia_mode : 0, attr->ia_valid & ATTR_UID ? from_kuid(&init_user_ns, attr->ia_uid) : 0, attr->ia_valid & ATTR_GID ? from_kgid(&init_user_ns, attr->ia_gid) : 0); if (S_ISLNK(inode->i_mode)) attr->ia_valid &= ~ATTR_SIZE; #define OCFS2_VALID_ATTRS (ATTR_ATIME | ATTR_MTIME | ATTR_CTIME | ATTR_SIZE \ | ATTR_GID | ATTR_UID | ATTR_MODE) if (!(attr->ia_valid & OCFS2_VALID_ATTRS)) return 0; status = setattr_prepare(dentry, attr); if (status) return status; if (is_quota_modification(inode, attr)) { status = dquot_initialize(inode); if (status) return status; } size_change = S_ISREG(inode->i_mode) && attr->ia_valid & ATTR_SIZE; if (size_change) { inode_dio_wait(inode); status = ocfs2_rw_lock(inode, 1); if (status < 0) { mlog_errno(status); goto bail; } } had_lock = ocfs2_inode_lock_tracker(inode, &bh, 1, &oh); if (had_lock < 0) { status = had_lock; goto bail_unlock_rw; } else if (had_lock) { mlog(ML_ERROR, ""Another case of recursive locking:\n""); dump_stack(); } inode_locked = 1; if (size_change) { status = inode_newsize_ok(inode, attr->ia_size); if (status) goto bail_unlock; if (i_size_read(inode) >= attr->ia_size) { if (ocfs2_should_order_data(inode)) { status = ocfs2_begin_ordered_truncate(inode, attr->ia_size); if (status) goto bail_unlock; } status = ocfs2_truncate_file(inode, bh, attr->ia_size); } else status = ocfs2_extend_file(inode, bh, attr->ia_size); if (status < 0) { if (status != -ENOSPC) mlog_errno(status); status = -ENOSPC; goto bail_unlock; } } if ((attr->ia_valid & ATTR_UID && !uid_eq(attr->ia_uid, inode->i_uid)) || (attr->ia_valid & ATTR_GID && !gid_eq(attr->ia_gid, inode->i_gid))) { if (attr->ia_valid & ATTR_UID && !uid_eq(attr->ia_uid, inode->i_uid) && OCFS2_HAS_RO_COMPAT_FEATURE(sb, OCFS2_FEATURE_RO_COMPAT_USRQUOTA)) { transfer_to[USRQUOTA] = dqget(sb, make_kqid_uid(attr->ia_uid)); if (IS_ERR(transfer_to[USRQUOTA])) { status = PTR_ERR(transfer_to[USRQUOTA]); transfer_to[USRQUOTA] = NULL; goto bail_unlock; } } if (attr->ia_valid & ATTR_GID && !gid_eq(attr->ia_gid, inode->i_gid) && OCFS2_HAS_RO_COMPAT_FEATURE(sb, OCFS2_FEATURE_RO_COMPAT_GRPQUOTA)) { transfer_to[GRPQUOTA] = dqget(sb, make_kqid_gid(attr->ia_gid)); if (IS_ERR(transfer_to[GRPQUOTA])) { status = PTR_ERR(transfer_to[GRPQUOTA]); transfer_to[GRPQUOTA] = NULL; goto bail_unlock; } } down_write(&OCFS2_I(inode)->ip_alloc_sem); handle = ocfs2_start_trans(osb, OCFS2_INODE_UPDATE_CREDITS + 2 * ocfs2_quota_trans_credits(sb)); if (IS_ERR(handle)) { status = PTR_ERR(handle); mlog_errno(status); goto bail_unlock_alloc; } status = __dquot_transfer(inode, transfer_to); if (status < 0) goto bail_commit; } else { down_write(&OCFS2_I(inode)->ip_alloc_sem); handle = ocfs2_start_trans(osb, OCFS2_INODE_UPDATE_CREDITS); if (IS_ERR(handle)) { status = PTR_ERR(handle); mlog_errno(status); goto bail_unlock_alloc; } } setattr_copy(inode, attr); mark_inode_dirty(inode); status = ocfs2_mark_inode_dirty(handle, inode, bh); if (status < 0) mlog_errno(status); bail_commit: ocfs2_commit_trans(osb, handle); bail_unlock_alloc: up_write(&OCFS2_I(inode)->ip_alloc_sem); bail_unlock: if (status && inode_locked) { ocfs2_inode_unlock_tracker(inode, 1, &oh, had_lock); inode_locked = 0; } bail_unlock_rw: if (size_change) ocfs2_rw_unlock(inode, 1); bail: for (qtype = 0; qtype < OCFS2_MAXQUOTAS; qtype++) dqput(transfer_to[qtype]); if (!status && attr->ia_valid & ATTR_MODE) { status = ocfs2_acl_chmod(inode, bh); if (status < 0) mlog_errno(status); } if (inode_locked) ocfs2_inode_unlock_tracker(inode, 1, &oh, had_lock); brelse(bh); return status; }"
1045----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50285/bad/server.c----queue_ksmbd_work,"static int queue_ksmbd_work(struct ksmbd_conn *conn) { struct ksmbd_work *work; int err; work = ksmbd_alloc_work_struct(); if (!work) { pr_err(""allocation for work failed\n""); return -ENOMEM; } work->conn = conn; work->request_buf = conn->request_buf; conn->request_buf = NULL; <S2SV_StartVul> err = ksmbd_init_smb_server(work); <S2SV_EndVul> <S2SV_StartVul> if (err) { <S2SV_EndVul> <S2SV_StartVul> ksmbd_free_work_struct(work); <S2SV_EndVul> <S2SV_StartVul> return 0; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> ksmbd_conn_enqueue_request(work); atomic_inc(&conn->r_count); conn->last_active = jiffies; INIT_WORK(&work->work, handle_ksmbd_work); ksmbd_queue_work(work); return 0; }","- err = ksmbd_init_smb_server(work);
- if (err) {
- ksmbd_free_work_struct(work);
- return 0;
- }
+ err = ksmbd_init_smb_server(conn);
+ if (err)
+ return 0;
+ if (atomic_inc_return(&conn->mux_smb_requests) >= conn->vals->max_credits) {
+ atomic_dec_return(&conn->mux_smb_requests);
+ return -ENOSPC;
+ }","static int queue_ksmbd_work(struct ksmbd_conn *conn) { struct ksmbd_work *work; int err; err = ksmbd_init_smb_server(conn); if (err) return 0; if (atomic_inc_return(&conn->mux_smb_requests) >= conn->vals->max_credits) { atomic_dec_return(&conn->mux_smb_requests); return -ENOSPC; } work = ksmbd_alloc_work_struct(); if (!work) { pr_err(""allocation for work failed\n""); return -ENOMEM; } work->conn = conn; work->request_buf = conn->request_buf; conn->request_buf = NULL; ksmbd_conn_enqueue_request(work); atomic_inc(&conn->r_count); conn->last_active = jiffies; INIT_WORK(&work->work, handle_ksmbd_work); ksmbd_queue_work(work); return 0; }"
713----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49991/bad/kfd_process.c----kfd_process_destroy_pdds,"static void kfd_process_destroy_pdds(struct kfd_process *p) { int i; for (i = 0; i < p->n_pdds; i++) { struct kfd_process_device *pdd = p->pdds[i]; pr_debug(""Releasing pdd (topology id %d) for process (pasid 0x%x)\n"", pdd->dev->id, p->pasid); kfd_process_device_destroy_cwsr_dgpu(pdd); kfd_process_device_destroy_ib_mem(pdd); if (pdd->drm_file) { amdgpu_amdkfd_gpuvm_release_process_vm( pdd->dev->adev, pdd->drm_priv); fput(pdd->drm_file); } if (pdd->qpd.cwsr_kaddr && !pdd->qpd.cwsr_base) free_pages((unsigned long)pdd->qpd.cwsr_kaddr, get_order(KFD_CWSR_TBA_TMA_SIZE)); idr_destroy(&pdd->alloc_idr); kfd_free_process_doorbells(pdd->dev->kfd, pdd); if (pdd->dev->kfd->shared_resources.enable_mes) amdgpu_amdkfd_free_gtt_mem(pdd->dev->adev, <S2SV_StartVul> pdd->proc_ctx_bo); <S2SV_EndVul> if (pdd->runtime_inuse) { pm_runtime_mark_last_busy(adev_to_drm(pdd->dev->adev)->dev); pm_runtime_put_autosuspend(adev_to_drm(pdd->dev->adev)->dev); pdd->runtime_inuse = false; } kfree(pdd); p->pdds[i] = NULL; } p->n_pdds = 0; }","- pdd->proc_ctx_bo);
+ &pdd->proc_ctx_bo);","static void kfd_process_destroy_pdds(struct kfd_process *p) { int i; for (i = 0; i < p->n_pdds; i++) { struct kfd_process_device *pdd = p->pdds[i]; pr_debug(""Releasing pdd (topology id %d) for process (pasid 0x%x)\n"", pdd->dev->id, p->pasid); kfd_process_device_destroy_cwsr_dgpu(pdd); kfd_process_device_destroy_ib_mem(pdd); if (pdd->drm_file) { amdgpu_amdkfd_gpuvm_release_process_vm( pdd->dev->adev, pdd->drm_priv); fput(pdd->drm_file); } if (pdd->qpd.cwsr_kaddr && !pdd->qpd.cwsr_base) free_pages((unsigned long)pdd->qpd.cwsr_kaddr, get_order(KFD_CWSR_TBA_TMA_SIZE)); idr_destroy(&pdd->alloc_idr); kfd_free_process_doorbells(pdd->dev->kfd, pdd); if (pdd->dev->kfd->shared_resources.enable_mes) amdgpu_amdkfd_free_gtt_mem(pdd->dev->adev, &pdd->proc_ctx_bo); if (pdd->runtime_inuse) { pm_runtime_mark_last_busy(adev_to_drm(pdd->dev->adev)->dev); pm_runtime_put_autosuspend(adev_to_drm(pdd->dev->adev)->dev); pdd->runtime_inuse = false; } kfree(pdd); p->pdds[i] = NULL; } p->n_pdds = 0; }"
1527----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2025-21684/bad/gpio-xilinx.c----xgpio_irqhandler,"static void xgpio_irqhandler(struct irq_desc *desc) { struct xgpio_instance *chip = irq_desc_get_handler_data(desc); struct gpio_chip *gc = &chip->gc; struct irq_chip *irqchip = irq_desc_get_chip(desc); DECLARE_BITMAP(rising, 64); DECLARE_BITMAP(falling, 64); DECLARE_BITMAP(all, 64); int irq_offset; u32 status; u32 bit; status = xgpio_readreg(chip->regs + XGPIO_IPISR_OFFSET); xgpio_writereg(chip->regs + XGPIO_IPISR_OFFSET, status); chained_irq_enter(irqchip, desc); <S2SV_StartVul> spin_lock(&chip->gpio_lock); <S2SV_EndVul> xgpio_read_ch_all(chip, XGPIO_DATA_OFFSET, all); bitmap_complement(rising, chip->last_irq_read, 64); bitmap_and(rising, rising, all, 64); bitmap_and(rising, rising, chip->enable, 64); bitmap_and(rising, rising, chip->rising_edge, 64); bitmap_complement(falling, all, 64); bitmap_and(falling, falling, chip->last_irq_read, 64); bitmap_and(falling, falling, chip->enable, 64); bitmap_and(falling, falling, chip->falling_edge, 64); bitmap_copy(chip->last_irq_read, all, 64); bitmap_or(all, rising, falling, 64); <S2SV_StartVul> spin_unlock(&chip->gpio_lock); <S2SV_EndVul> dev_dbg(gc->parent, ""IRQ rising %*pb falling %*pb\n"", 64, rising, 64, falling); for_each_set_bit(bit, all, 64) { irq_offset = xgpio_from_bit(chip, bit); generic_handle_domain_irq(gc->irq.domain, irq_offset); } chained_irq_exit(irqchip, desc); }","- spin_lock(&chip->gpio_lock);
- spin_unlock(&chip->gpio_lock);
+ raw_spin_lock(&chip->gpio_lock);
+ raw_spin_unlock(&chip->gpio_lock);","static void xgpio_irqhandler(struct irq_desc *desc) { struct xgpio_instance *chip = irq_desc_get_handler_data(desc); struct gpio_chip *gc = &chip->gc; struct irq_chip *irqchip = irq_desc_get_chip(desc); DECLARE_BITMAP(rising, 64); DECLARE_BITMAP(falling, 64); DECLARE_BITMAP(all, 64); int irq_offset; u32 status; u32 bit; status = xgpio_readreg(chip->regs + XGPIO_IPISR_OFFSET); xgpio_writereg(chip->regs + XGPIO_IPISR_OFFSET, status); chained_irq_enter(irqchip, desc); raw_spin_lock(&chip->gpio_lock); xgpio_read_ch_all(chip, XGPIO_DATA_OFFSET, all); bitmap_complement(rising, chip->last_irq_read, 64); bitmap_and(rising, rising, all, 64); bitmap_and(rising, rising, chip->enable, 64); bitmap_and(rising, rising, chip->rising_edge, 64); bitmap_complement(falling, all, 64); bitmap_and(falling, falling, chip->last_irq_read, 64); bitmap_and(falling, falling, chip->enable, 64); bitmap_and(falling, falling, chip->falling_edge, 64); bitmap_copy(chip->last_irq_read, all, 64); bitmap_or(all, rising, falling, 64); raw_spin_unlock(&chip->gpio_lock); dev_dbg(gc->parent, ""IRQ rising %*pb falling %*pb\n"", 64, rising, 64, falling); for_each_set_bit(bit, all, 64) { irq_offset = xgpio_from_bit(chip, bit); generic_handle_domain_irq(gc->irq.domain, irq_offset); } chained_irq_exit(irqchip, desc); }"
46----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44931/bad/gpiolib.c----*gpiochip_get_desc,"struct gpio_desc *gpiochip_get_desc(struct gpio_chip *chip, u16 hwnum) { struct gpio_device *gdev = chip->gpiodev; if (hwnum >= gdev->ngpio) return ERR_PTR(-EINVAL); <S2SV_StartVul> return &gdev->descs[hwnum]; <S2SV_EndVul> }","- return &gdev->descs[hwnum];
+ return &gdev->descs[array_index_nospec(hwnum, gdev->ngpio)];","struct gpio_desc *gpiochip_get_desc(struct gpio_chip *chip, u16 hwnum) { struct gpio_device *gdev = chip->gpiodev; if (hwnum >= gdev->ngpio) return ERR_PTR(-EINVAL); return &gdev->descs[array_index_nospec(hwnum, gdev->ngpio)]; }"
295----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46766/bad/ice_main.c----ice_reinit_interrupt_scheme,"static int ice_reinit_interrupt_scheme(struct ice_pf *pf) { struct device *dev = ice_pf_to_dev(pf); int ret, v; ret = ice_init_interrupt_scheme(pf); if (ret) { dev_err(dev, ""Failed to re-initialize interrupt %d\n"", ret); return ret; } ice_for_each_vsi(pf, v) { if (!pf->vsi[v]) continue; ret = ice_vsi_alloc_q_vectors(pf->vsi[v]); if (ret) goto err_reinit; ice_vsi_map_rings_to_vectors(pf->vsi[v]); ice_vsi_set_napi_queues(pf->vsi[v]); <S2SV_StartVul> } <S2SV_EndVul> ret = ice_req_irq_msix_misc(pf); if (ret) { dev_err(dev, ""Setting up misc vector failed after device suspend %d\n"", ret); goto err_reinit; } return 0; err_reinit: while (v--) <S2SV_StartVul> if (pf->vsi[v]) <S2SV_EndVul> ice_vsi_free_q_vectors(pf->vsi[v]); return ret; <S2SV_StartVul> } <S2SV_EndVul>","- }
- if (pf->vsi[v])
- }
+ rtnl_lock();
+ rtnl_unlock();
+ }
+ if (pf->vsi[v]) {
+ rtnl_lock();
+ ice_vsi_clear_napi_queues(pf->vsi[v]);
+ rtnl_unlock();
+ }
+ }","static int ice_reinit_interrupt_scheme(struct ice_pf *pf) { struct device *dev = ice_pf_to_dev(pf); int ret, v; ret = ice_init_interrupt_scheme(pf); if (ret) { dev_err(dev, ""Failed to re-initialize interrupt %d\n"", ret); return ret; } ice_for_each_vsi(pf, v) { if (!pf->vsi[v]) continue; ret = ice_vsi_alloc_q_vectors(pf->vsi[v]); if (ret) goto err_reinit; ice_vsi_map_rings_to_vectors(pf->vsi[v]); rtnl_lock(); ice_vsi_set_napi_queues(pf->vsi[v]); rtnl_unlock(); } ret = ice_req_irq_msix_misc(pf); if (ret) { dev_err(dev, ""Setting up misc vector failed after device suspend %d\n"", ret); goto err_reinit; } return 0; err_reinit: while (v--) if (pf->vsi[v]) { rtnl_lock(); ice_vsi_clear_napi_queues(pf->vsi[v]); rtnl_unlock(); ice_vsi_free_q_vectors(pf->vsi[v]); } return ret; }"
398----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46858/bad/pm_netlink.c----mptcp_pm_del_add_timer,"mptcp_pm_del_add_timer(struct mptcp_sock *msk, const struct mptcp_addr_info *addr, bool check_id) { struct mptcp_pm_add_entry *entry; struct sock *sk = (struct sock *)msk; spin_lock_bh(&msk->pm.lock); entry = mptcp_lookup_anno_list_by_saddr(msk, addr); <S2SV_StartVul> if (entry && (!check_id || entry->addr.id == addr->id)) <S2SV_EndVul> entry->retrans_times = ADD_ADDR_RETRANS_MAX; spin_unlock_bh(&msk->pm.lock); <S2SV_StartVul> if (entry && (!check_id || entry->addr.id == addr->id)) <S2SV_EndVul> <S2SV_StartVul> sk_stop_timer_sync(sk, &entry->add_timer); <S2SV_EndVul> return entry; }","- if (entry && (!check_id || entry->addr.id == addr->id))
- if (entry && (!check_id || entry->addr.id == addr->id))
- sk_stop_timer_sync(sk, &entry->add_timer);
+ struct timer_list *add_timer = NULL;
+ if (entry && (!check_id || entry->addr.id == addr->id)) {
+ add_timer = &entry->add_timer;
+ }
+ if (!check_id && entry)
+ list_del(&entry->list);
+ if (add_timer)
+ sk_stop_timer_sync(sk, add_timer);
+ }","mptcp_pm_del_add_timer(struct mptcp_sock *msk, const struct mptcp_addr_info *addr, bool check_id) { struct mptcp_pm_add_entry *entry; struct sock *sk = (struct sock *)msk; struct timer_list *add_timer = NULL; spin_lock_bh(&msk->pm.lock); entry = mptcp_lookup_anno_list_by_saddr(msk, addr); if (entry && (!check_id || entry->addr.id == addr->id)) { entry->retrans_times = ADD_ADDR_RETRANS_MAX; add_timer = &entry->add_timer; } if (!check_id && entry) list_del(&entry->list); spin_unlock_bh(&msk->pm.lock); if (add_timer) sk_stop_timer_sync(sk, add_timer); return entry; }"
492----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47733/bad/main.c----netfs_init,"static int __init netfs_init(void) { int ret = -ENOMEM; netfs_request_slab = kmem_cache_create(""netfs_request"", sizeof(struct netfs_io_request), 0, SLAB_HWCACHE_ALIGN | SLAB_ACCOUNT, NULL); if (!netfs_request_slab) goto error_req; if (mempool_init_slab_pool(&netfs_request_pool, 100, netfs_request_slab) < 0) goto error_reqpool; netfs_subrequest_slab = kmem_cache_create(""netfs_subrequest"", sizeof(struct netfs_io_subrequest), 0, SLAB_HWCACHE_ALIGN | SLAB_ACCOUNT, NULL); if (!netfs_subrequest_slab) goto error_subreq; if (mempool_init_slab_pool(&netfs_subrequest_pool, 100, netfs_subrequest_slab) < 0) goto error_subreqpool; if (!proc_mkdir(""fs/netfs"", NULL)) goto error_proc; if (!proc_create_seq(""fs/netfs/requests"", S_IFREG | 0444, NULL, &netfs_requests_seq_ops)) goto error_procfile; #ifdef CONFIG_FSCACHE_STATS if (!proc_create_single(""fs/netfs/stats"", S_IFREG | 0444, NULL, netfs_stats_show)) goto error_procfile; #endif ret = fscache_init(); if (ret < 0) goto error_fscache; return 0; error_fscache: error_procfile: <S2SV_StartVul> remove_proc_entry(""fs/netfs"", NULL); <S2SV_EndVul> error_proc: mempool_exit(&netfs_subrequest_pool); error_subreqpool: kmem_cache_destroy(netfs_subrequest_slab); error_subreq: mempool_exit(&netfs_request_pool); error_reqpool: kmem_cache_destroy(netfs_request_slab); error_req: return ret; }","- remove_proc_entry(""fs/netfs"", NULL);
+ remove_proc_subtree(""fs/netfs"", NULL);","static int __init netfs_init(void) { int ret = -ENOMEM; netfs_request_slab = kmem_cache_create(""netfs_request"", sizeof(struct netfs_io_request), 0, SLAB_HWCACHE_ALIGN | SLAB_ACCOUNT, NULL); if (!netfs_request_slab) goto error_req; if (mempool_init_slab_pool(&netfs_request_pool, 100, netfs_request_slab) < 0) goto error_reqpool; netfs_subrequest_slab = kmem_cache_create(""netfs_subrequest"", sizeof(struct netfs_io_subrequest), 0, SLAB_HWCACHE_ALIGN | SLAB_ACCOUNT, NULL); if (!netfs_subrequest_slab) goto error_subreq; if (mempool_init_slab_pool(&netfs_subrequest_pool, 100, netfs_subrequest_slab) < 0) goto error_subreqpool; if (!proc_mkdir(""fs/netfs"", NULL)) goto error_proc; if (!proc_create_seq(""fs/netfs/requests"", S_IFREG | 0444, NULL, &netfs_requests_seq_ops)) goto error_procfile; #ifdef CONFIG_FSCACHE_STATS if (!proc_create_single(""fs/netfs/stats"", S_IFREG | 0444, NULL, netfs_stats_show)) goto error_procfile; #endif ret = fscache_init(); if (ret < 0) goto error_fscache; return 0; error_fscache: error_procfile: remove_proc_subtree(""fs/netfs"", NULL); error_proc: mempool_exit(&netfs_subrequest_pool); error_subreqpool: kmem_cache_destroy(netfs_subrequest_slab); error_subreq: mempool_exit(&netfs_request_pool); error_reqpool: kmem_cache_destroy(netfs_request_slab); error_req: return ret; }"
71----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44944/bad/nf_conntrack_netlink.c----ctnetlink_del_expect,"static int ctnetlink_del_expect(struct net *net, struct sock *ctnl, struct sk_buff *skb, const struct nlmsghdr *nlh, const struct nlattr * const cda[], struct netlink_ext_ack *extack) { struct nf_conntrack_expect *exp; struct nf_conntrack_tuple tuple; struct nfgenmsg *nfmsg = nlmsg_data(nlh); u_int8_t u3 = nfmsg->nfgen_family; struct nf_conntrack_zone zone; int err; if (cda[CTA_EXPECT_TUPLE]) { err = ctnetlink_parse_zone(cda[CTA_EXPECT_ZONE], &zone); if (err < 0) return err; err = ctnetlink_parse_tuple(cda, &tuple, CTA_EXPECT_TUPLE, u3, NULL); if (err < 0) return err; exp = nf_ct_expect_find_get(net, &zone, &tuple); if (!exp) return -ENOENT; if (cda[CTA_EXPECT_ID]) { __be32 id = nla_get_be32(cda[CTA_EXPECT_ID]); <S2SV_StartVul> if (ntohl(id) != (u32)(unsigned long)exp) { <S2SV_EndVul> nf_ct_expect_put(exp); return -ENOENT; } } spin_lock_bh(&nf_conntrack_expect_lock); if (del_timer(&exp->timeout)) { nf_ct_unlink_expect_report(exp, NETLINK_CB(skb).portid, nlmsg_report(nlh)); nf_ct_expect_put(exp); } spin_unlock_bh(&nf_conntrack_expect_lock); nf_ct_expect_put(exp); } else if (cda[CTA_EXPECT_HELP_NAME]) { char *name = nla_data(cda[CTA_EXPECT_HELP_NAME]); nf_ct_expect_iterate_net(net, expect_iter_name, name, NETLINK_CB(skb).portid, nlmsg_report(nlh)); } else { nf_ct_expect_iterate_net(net, expect_iter_all, NULL, NETLINK_CB(skb).portid, nlmsg_report(nlh)); } return 0; }","- if (ntohl(id) != (u32)(unsigned long)exp) {
+ if (id != nf_expect_get_id(exp)) {","static int ctnetlink_del_expect(struct net *net, struct sock *ctnl, struct sk_buff *skb, const struct nlmsghdr *nlh, const struct nlattr * const cda[], struct netlink_ext_ack *extack) { struct nf_conntrack_expect *exp; struct nf_conntrack_tuple tuple; struct nfgenmsg *nfmsg = nlmsg_data(nlh); u_int8_t u3 = nfmsg->nfgen_family; struct nf_conntrack_zone zone; int err; if (cda[CTA_EXPECT_TUPLE]) { err = ctnetlink_parse_zone(cda[CTA_EXPECT_ZONE], &zone); if (err < 0) return err; err = ctnetlink_parse_tuple(cda, &tuple, CTA_EXPECT_TUPLE, u3, NULL); if (err < 0) return err; exp = nf_ct_expect_find_get(net, &zone, &tuple); if (!exp) return -ENOENT; if (cda[CTA_EXPECT_ID]) { __be32 id = nla_get_be32(cda[CTA_EXPECT_ID]); if (id != nf_expect_get_id(exp)) { nf_ct_expect_put(exp); return -ENOENT; } } spin_lock_bh(&nf_conntrack_expect_lock); if (del_timer(&exp->timeout)) { nf_ct_unlink_expect_report(exp, NETLINK_CB(skb).portid, nlmsg_report(nlh)); nf_ct_expect_put(exp); } spin_unlock_bh(&nf_conntrack_expect_lock); nf_ct_expect_put(exp); } else if (cda[CTA_EXPECT_HELP_NAME]) { char *name = nla_data(cda[CTA_EXPECT_HELP_NAME]); nf_ct_expect_iterate_net(net, expect_iter_name, name, NETLINK_CB(skb).portid, nlmsg_report(nlh)); } else { nf_ct_expect_iterate_net(net, expect_iter_all, NULL, NETLINK_CB(skb).portid, nlmsg_report(nlh)); } return 0; }"
864----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50124/bad/iso.c----iso_conn_del,"static void iso_conn_del(struct hci_conn *hcon, int err) { struct iso_conn *conn = hcon->iso_data; struct sock *sk; struct sock *parent; if (!conn) return; BT_DBG(""hcon %p conn %p, err %d"", hcon, conn, err); iso_conn_lock(conn); <S2SV_StartVul> sk = conn->sk; <S2SV_EndVul> <S2SV_StartVul> if (sk) <S2SV_EndVul> <S2SV_StartVul> sock_hold(sk); <S2SV_EndVul> iso_conn_unlock(conn); if (sk) { lock_sock(sk); if (test_bit(BT_SK_PA_SYNC, &iso_pi(sk)->flags)) { parent = iso_get_sock_listen(&hcon->src, &hcon->dst, iso_match_conn_sync_handle, hcon); if (parent) { set_bit(BT_SK_PA_SYNC_TERM, &iso_pi(parent)->flags); sock_put(parent); } } iso_sock_clear_timer(sk); iso_chan_del(sk, err); release_sock(sk); sock_put(sk); } cancel_delayed_work_sync(&conn->timeout_work); hcon->iso_data = NULL; kfree(conn); }","- sk = conn->sk;
- if (sk)
- sock_hold(sk);
+ sk = iso_sock_hold(conn);","static void iso_conn_del(struct hci_conn *hcon, int err) { struct iso_conn *conn = hcon->iso_data; struct sock *sk; struct sock *parent; if (!conn) return; BT_DBG(""hcon %p conn %p, err %d"", hcon, conn, err); iso_conn_lock(conn); sk = iso_sock_hold(conn); iso_conn_unlock(conn); if (sk) { lock_sock(sk); if (test_bit(BT_SK_PA_SYNC, &iso_pi(sk)->flags)) { parent = iso_get_sock_listen(&hcon->src, &hcon->dst, iso_match_conn_sync_handle, hcon); if (parent) { set_bit(BT_SK_PA_SYNC_TERM, &iso_pi(parent)->flags); sock_put(parent); } } iso_sock_clear_timer(sk); iso_chan_del(sk, err); release_sock(sk); sock_put(sk); } cancel_delayed_work_sync(&conn->timeout_work); hcon->iso_data = NULL; kfree(conn); }"
586----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49908/bad/amdgpu_dm.c----amdgpu_dm_update_cursor,"static void amdgpu_dm_update_cursor(struct drm_plane *plane, struct drm_plane_state *old_plane_state, struct dc_stream_update *update) { struct amdgpu_device *adev = drm_to_adev(plane->dev); struct amdgpu_framebuffer *afb = to_amdgpu_framebuffer(plane->state->fb); struct drm_crtc *crtc = afb ? plane->state->crtc : old_plane_state->crtc; struct dm_crtc_state *crtc_state = crtc ? to_dm_crtc_state(crtc->state) : NULL; struct amdgpu_crtc *amdgpu_crtc = to_amdgpu_crtc(crtc); uint64_t address = afb ? afb->address : 0; struct dc_cursor_position position = {0}; struct dc_cursor_attributes attributes; int ret; if (!plane->state->fb && !old_plane_state->fb) return; drm_dbg_atomic(plane->dev, ""crtc_id=%d with size %d to %d\n"", amdgpu_crtc->crtc_id, plane->state->crtc_w, plane->state->crtc_h); ret = amdgpu_dm_plane_get_cursor_position(plane, crtc, &position); if (ret) return; if (!position.enable) { if (crtc_state && crtc_state->stream) { dc_stream_set_cursor_position(crtc_state->stream, &position); update->cursor_position = &crtc_state->stream->cursor_position; } return; } amdgpu_crtc->cursor_width = plane->state->crtc_w; amdgpu_crtc->cursor_height = plane->state->crtc_h; memset(&attributes, 0, sizeof(attributes)); attributes.address.high_part = upper_32_bits(address); attributes.address.low_part = lower_32_bits(address); attributes.width = plane->state->crtc_w; attributes.height = plane->state->crtc_h; attributes.color_format = CURSOR_MODE_COLOR_PRE_MULTIPLIED_ALPHA; attributes.rotation_angle = 0; attributes.attribute_flags.value = 0; if (crtc_state->cm_is_degamma_srgb && adev->dm.dc->caps.color.dpp.gamma_corr) attributes.attribute_flags.bits.ENABLE_CURSOR_DEGAMMA = 1; <S2SV_StartVul> attributes.pitch = afb->base.pitches[0] / afb->base.format->cpp[0]; <S2SV_EndVul> if (crtc_state->stream) { if (!dc_stream_set_cursor_attributes(crtc_state->stream, &attributes)) DRM_ERROR(""DC failed to set cursor attributes\n""); update->cursor_attributes = &crtc_state->stream->cursor_attributes; if (!dc_stream_set_cursor_position(crtc_state->stream, &position)) DRM_ERROR(""DC failed to set cursor position\n""); update->cursor_position = &crtc_state->stream->cursor_position; } }","- attributes.pitch = afb->base.pitches[0] / afb->base.format->cpp[0];
+ if (afb)
+ attributes.pitch = afb->base.pitches[0] / afb->base.format->cpp[0];","static void amdgpu_dm_update_cursor(struct drm_plane *plane, struct drm_plane_state *old_plane_state, struct dc_stream_update *update) { struct amdgpu_device *adev = drm_to_adev(plane->dev); struct amdgpu_framebuffer *afb = to_amdgpu_framebuffer(plane->state->fb); struct drm_crtc *crtc = afb ? plane->state->crtc : old_plane_state->crtc; struct dm_crtc_state *crtc_state = crtc ? to_dm_crtc_state(crtc->state) : NULL; struct amdgpu_crtc *amdgpu_crtc = to_amdgpu_crtc(crtc); uint64_t address = afb ? afb->address : 0; struct dc_cursor_position position = {0}; struct dc_cursor_attributes attributes; int ret; if (!plane->state->fb && !old_plane_state->fb) return; drm_dbg_atomic(plane->dev, ""crtc_id=%d with size %d to %d\n"", amdgpu_crtc->crtc_id, plane->state->crtc_w, plane->state->crtc_h); ret = amdgpu_dm_plane_get_cursor_position(plane, crtc, &position); if (ret) return; if (!position.enable) { if (crtc_state && crtc_state->stream) { dc_stream_set_cursor_position(crtc_state->stream, &position); update->cursor_position = &crtc_state->stream->cursor_position; } return; } amdgpu_crtc->cursor_width = plane->state->crtc_w; amdgpu_crtc->cursor_height = plane->state->crtc_h; memset(&attributes, 0, sizeof(attributes)); attributes.address.high_part = upper_32_bits(address); attributes.address.low_part = lower_32_bits(address); attributes.width = plane->state->crtc_w; attributes.height = plane->state->crtc_h; attributes.color_format = CURSOR_MODE_COLOR_PRE_MULTIPLIED_ALPHA; attributes.rotation_angle = 0; attributes.attribute_flags.value = 0; if (crtc_state->cm_is_degamma_srgb && adev->dm.dc->caps.color.dpp.gamma_corr) attributes.attribute_flags.bits.ENABLE_CURSOR_DEGAMMA = 1; if (afb) attributes.pitch = afb->base.pitches[0] / afb->base.format->cpp[0]; if (crtc_state->stream) { if (!dc_stream_set_cursor_attributes(crtc_state->stream, &attributes)) DRM_ERROR(""DC failed to set cursor attributes\n""); update->cursor_attributes = &crtc_state->stream->cursor_attributes; if (!dc_stream_set_cursor_position(crtc_state->stream, &position)) DRM_ERROR(""DC failed to set cursor position\n""); update->cursor_position = &crtc_state->stream->cursor_position; } }"
1494----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-57933/bad/gve_main.c----gve_xsk_pool_disable,"static int gve_xsk_pool_disable(struct net_device *dev, u16 qid) { struct gve_priv *priv = netdev_priv(dev); struct napi_struct *napi_rx; struct napi_struct *napi_tx; struct xsk_buff_pool *pool; int tx_qid; pool = xsk_get_pool_from_qid(dev, qid); if (!pool) return -EINVAL; if (qid >= priv->rx_cfg.num_queues) return -EINVAL; <S2SV_StartVul> if (!priv->xdp_prog) <S2SV_EndVul> <S2SV_StartVul> goto done; <S2SV_EndVul> <S2SV_StartVul> tx_qid = gve_xdp_tx_queue_id(priv, qid); <S2SV_EndVul> <S2SV_StartVul> if (!netif_running(dev)) { <S2SV_EndVul> <S2SV_StartVul> priv->rx[qid].xsk_pool = NULL; <S2SV_EndVul> <S2SV_StartVul> xdp_rxq_info_unreg(&priv->rx[qid].xsk_rxq); <S2SV_EndVul> <S2SV_StartVul> priv->tx[tx_qid].xsk_pool = NULL; <S2SV_EndVul> <S2SV_StartVul> goto done; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> napi_rx = &priv->ntfy_blocks[priv->rx[qid].ntfy_id].napi; napi_disable(napi_rx); napi_tx = &priv->ntfy_blocks[priv->tx[tx_qid].ntfy_id].napi; napi_disable(napi_tx); priv->rx[qid].xsk_pool = NULL; xdp_rxq_info_unreg(&priv->rx[qid].xsk_rxq); priv->tx[tx_qid].xsk_pool = NULL; smp_mb(); napi_enable(napi_rx); if (gve_rx_work_pending(&priv->rx[qid])) napi_schedule(napi_rx); napi_enable(napi_tx); if (gve_tx_clean_pending(priv, &priv->tx[tx_qid])) napi_schedule(napi_tx); done: xsk_pool_dma_unmap(pool, DMA_ATTR_SKIP_CPU_SYNC | DMA_ATTR_WEAK_ORDERING); return 0; }","- if (!priv->xdp_prog)
- goto done;
- tx_qid = gve_xdp_tx_queue_id(priv, qid);
- if (!netif_running(dev)) {
- priv->rx[qid].xsk_pool = NULL;
- xdp_rxq_info_unreg(&priv->rx[qid].xsk_rxq);
- priv->tx[tx_qid].xsk_pool = NULL;
- goto done;
- }
+ if (!priv->xdp_prog || !netif_running(dev))
+ tx_qid = gve_xdp_tx_queue_id(priv, qid);","static int gve_xsk_pool_disable(struct net_device *dev, u16 qid) { struct gve_priv *priv = netdev_priv(dev); struct napi_struct *napi_rx; struct napi_struct *napi_tx; struct xsk_buff_pool *pool; int tx_qid; pool = xsk_get_pool_from_qid(dev, qid); if (!pool) return -EINVAL; if (qid >= priv->rx_cfg.num_queues) return -EINVAL; if (!priv->xdp_prog || !netif_running(dev)) goto done; napi_rx = &priv->ntfy_blocks[priv->rx[qid].ntfy_id].napi; napi_disable(napi_rx); tx_qid = gve_xdp_tx_queue_id(priv, qid); napi_tx = &priv->ntfy_blocks[priv->tx[tx_qid].ntfy_id].napi; napi_disable(napi_tx); priv->rx[qid].xsk_pool = NULL; xdp_rxq_info_unreg(&priv->rx[qid].xsk_rxq); priv->tx[tx_qid].xsk_pool = NULL; smp_mb(); napi_enable(napi_rx); if (gve_rx_work_pending(&priv->rx[qid])) napi_schedule(napi_rx); napi_enable(napi_tx); if (gve_tx_clean_pending(priv, &priv->tx[tx_qid])) napi_schedule(napi_tx); done: xsk_pool_dma_unmap(pool, DMA_ATTR_SKIP_CPU_SYNC | DMA_ATTR_WEAK_ORDERING); return 0; }"
1022----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50261/bad/macsec.c----macsec_free_netdev,"static void macsec_free_netdev(struct net_device *dev) { struct macsec_dev *macsec = macsec_priv(dev); <S2SV_StartVul> if (macsec->secy.tx_sc.md_dst) <S2SV_EndVul> <S2SV_StartVul> metadata_dst_free(macsec->secy.tx_sc.md_dst); <S2SV_EndVul> free_percpu(macsec->stats); free_percpu(macsec->secy.tx_sc.stats); netdev_put(macsec->real_dev, &macsec->dev_tracker); }","- if (macsec->secy.tx_sc.md_dst)
- metadata_dst_free(macsec->secy.tx_sc.md_dst);
+ dst_release(&macsec->secy.tx_sc.md_dst->dst);","static void macsec_free_netdev(struct net_device *dev) { struct macsec_dev *macsec = macsec_priv(dev); dst_release(&macsec->secy.tx_sc.md_dst->dst); free_percpu(macsec->stats); free_percpu(macsec->secy.tx_sc.stats); netdev_put(macsec->real_dev, &macsec->dev_tracker); }"
693----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49986/bad/core.c----x86_android_tablet_probe,"static __init int x86_android_tablet_probe(struct platform_device *pdev) { const struct x86_dev_info *dev_info; const struct dmi_system_id *id; struct gpio_chip *chip; int i, ret = 0; id = dmi_first_match(x86_android_tablet_ids); if (!id) return -ENODEV; dev_info = id->driver_data; x86_android_tablet_device = pdev; if (dev_info->invalid_aei_gpiochip) { chip = gpiochip_find(dev_info->invalid_aei_gpiochip, gpiochip_find_match_label); if (!chip) { pr_err(""error cannot find GPIO chip %s\n"", dev_info->invalid_aei_gpiochip); return -ENODEV; } acpi_gpiochip_free_interrupts(chip); } for (i = 0; dev_info->modules && dev_info->modules[i]; i++) request_module(dev_info->modules[i]); bat_swnode = dev_info->bat_swnode; if (bat_swnode) { ret = software_node_register(bat_swnode); if (ret) return ret; } gpiod_lookup_tables = dev_info->gpiod_lookup_tables; for (i = 0; gpiod_lookup_tables && gpiod_lookup_tables[i]; i++) gpiod_add_lookup_table(gpiod_lookup_tables[i]); if (dev_info->init) { ret = dev_info->init(); if (ret < 0) { x86_android_tablet_remove(pdev); return ret; } exit_handler = dev_info->exit; } i2c_clients = kcalloc(dev_info->i2c_client_count, sizeof(*i2c_clients), GFP_KERNEL); if (!i2c_clients) { x86_android_tablet_remove(pdev); return -ENOMEM; } i2c_client_count = dev_info->i2c_client_count; for (i = 0; i < i2c_client_count; i++) { ret = x86_instantiate_i2c_client(dev_info, i); if (ret < 0) { x86_android_tablet_remove(pdev); return ret; } } pdevs = kcalloc(dev_info->pdev_count + 1, sizeof(*pdevs), GFP_KERNEL); if (!pdevs) { x86_android_tablet_remove(pdev); return -ENOMEM; } pdev_count = dev_info->pdev_count; for (i = 0; i < pdev_count; i++) { pdevs[i] = platform_device_register_full(&dev_info->pdev_info[i]); if (IS_ERR(pdevs[i])) { x86_android_tablet_remove(pdev); <S2SV_StartVul> return PTR_ERR(pdevs[i]); <S2SV_EndVul> } } serdevs = kcalloc(dev_info->serdev_count, sizeof(*serdevs), GFP_KERNEL); if (!serdevs) { x86_android_tablet_remove(pdev); return -ENOMEM; } serdev_count = dev_info->serdev_count; for (i = 0; i < serdev_count; i++) { ret = x86_instantiate_serdev(&dev_info->serdev_info[i], i); if (ret < 0) { x86_android_tablet_remove(pdev); return ret; } } if (dev_info->gpio_button_count) { struct gpio_keys_platform_data pdata = { }; struct gpio_desc *gpiod; buttons = kcalloc(dev_info->gpio_button_count, sizeof(*buttons), GFP_KERNEL); if (!buttons) { x86_android_tablet_remove(pdev); return -ENOMEM; } for (i = 0; i < dev_info->gpio_button_count; i++) { ret = x86_android_tablet_get_gpiod(dev_info->gpio_button[i].chip, dev_info->gpio_button[i].pin, &gpiod); if (ret < 0) { x86_android_tablet_remove(pdev); return ret; } buttons[i] = dev_info->gpio_button[i].button; buttons[i].gpio = desc_to_gpio(gpiod); } pdata.buttons = buttons; pdata.nbuttons = dev_info->gpio_button_count; pdevs[pdev_count] = platform_device_register_data(NULL, ""gpio-keys"", PLATFORM_DEVID_AUTO, &pdata, sizeof(pdata)); if (IS_ERR(pdevs[pdev_count])) { x86_android_tablet_remove(pdev); <S2SV_StartVul> return PTR_ERR(pdevs[pdev_count]); <S2SV_EndVul> } pdev_count++; } return 0; }","- return PTR_ERR(pdevs[i]);
- return PTR_ERR(pdevs[pdev_count]);
+ ret = PTR_ERR(pdevs[i]);
+ return ret;
+ ret = PTR_ERR(pdevs[pdev_count]);
+ return ret;","static __init int x86_android_tablet_probe(struct platform_device *pdev) { const struct x86_dev_info *dev_info; const struct dmi_system_id *id; struct gpio_chip *chip; int i, ret = 0; id = dmi_first_match(x86_android_tablet_ids); if (!id) return -ENODEV; dev_info = id->driver_data; x86_android_tablet_device = pdev; if (dev_info->invalid_aei_gpiochip) { chip = gpiochip_find(dev_info->invalid_aei_gpiochip, gpiochip_find_match_label); if (!chip) { pr_err(""error cannot find GPIO chip %s\n"", dev_info->invalid_aei_gpiochip); return -ENODEV; } acpi_gpiochip_free_interrupts(chip); } for (i = 0; dev_info->modules && dev_info->modules[i]; i++) request_module(dev_info->modules[i]); bat_swnode = dev_info->bat_swnode; if (bat_swnode) { ret = software_node_register(bat_swnode); if (ret) return ret; } gpiod_lookup_tables = dev_info->gpiod_lookup_tables; for (i = 0; gpiod_lookup_tables && gpiod_lookup_tables[i]; i++) gpiod_add_lookup_table(gpiod_lookup_tables[i]); if (dev_info->init) { ret = dev_info->init(); if (ret < 0) { x86_android_tablet_remove(pdev); return ret; } exit_handler = dev_info->exit; } i2c_clients = kcalloc(dev_info->i2c_client_count, sizeof(*i2c_clients), GFP_KERNEL); if (!i2c_clients) { x86_android_tablet_remove(pdev); return -ENOMEM; } i2c_client_count = dev_info->i2c_client_count; for (i = 0; i < i2c_client_count; i++) { ret = x86_instantiate_i2c_client(dev_info, i); if (ret < 0) { x86_android_tablet_remove(pdev); return ret; } } pdevs = kcalloc(dev_info->pdev_count + 1, sizeof(*pdevs), GFP_KERNEL); if (!pdevs) { x86_android_tablet_remove(pdev); return -ENOMEM; } pdev_count = dev_info->pdev_count; for (i = 0; i < pdev_count; i++) { pdevs[i] = platform_device_register_full(&dev_info->pdev_info[i]); if (IS_ERR(pdevs[i])) { ret = PTR_ERR(pdevs[i]); x86_android_tablet_remove(pdev); return ret; } } serdevs = kcalloc(dev_info->serdev_count, sizeof(*serdevs), GFP_KERNEL); if (!serdevs) { x86_android_tablet_remove(pdev); return -ENOMEM; } serdev_count = dev_info->serdev_count; for (i = 0; i < serdev_count; i++) { ret = x86_instantiate_serdev(&dev_info->serdev_info[i], i); if (ret < 0) { x86_android_tablet_remove(pdev); return ret; } } if (dev_info->gpio_button_count) { struct gpio_keys_platform_data pdata = { }; struct gpio_desc *gpiod; buttons = kcalloc(dev_info->gpio_button_count, sizeof(*buttons), GFP_KERNEL); if (!buttons) { x86_android_tablet_remove(pdev); return -ENOMEM; } for (i = 0; i < dev_info->gpio_button_count; i++) { ret = x86_android_tablet_get_gpiod(dev_info->gpio_button[i].chip, dev_info->gpio_button[i].pin, &gpiod); if (ret < 0) { x86_android_tablet_remove(pdev); return ret; } buttons[i] = dev_info->gpio_button[i].button; buttons[i].gpio = desc_to_gpio(gpiod); } pdata.buttons = buttons; pdata.nbuttons = dev_info->gpio_button_count; pdevs[pdev_count] = platform_device_register_data(NULL, ""gpio-keys"", PLATFORM_DEVID_AUTO, &pdata, sizeof(pdata)); if (IS_ERR(pdevs[pdev_count])) { ret = PTR_ERR(pdevs[pdev_count]); x86_android_tablet_remove(pdev); return ret; } pdev_count++; } return 0; }"
163----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-45018/bad/nf_flow_table_offload.c----nf_flow_offload_tuple,"static int nf_flow_offload_tuple(struct nf_flowtable *flowtable, struct flow_offload *flow, struct nf_flow_rule *flow_rule, enum flow_offload_tuple_dir dir, int priority, int cmd, struct flow_stats *stats, struct list_head *block_cb_list) { struct flow_cls_offload cls_flow = {}; struct flow_block_cb *block_cb; <S2SV_StartVul> struct netlink_ext_ack extack; <S2SV_EndVul> __be16 proto = ETH_P_ALL; int err, i = 0; nf_flow_offload_init(&cls_flow, proto, priority, cmd, &flow->tuplehash[dir].tuple, &extack); if (cmd == FLOW_CLS_REPLACE) cls_flow.rule = flow_rule->rule; down_read(&flowtable->flow_block_lock); list_for_each_entry(block_cb, block_cb_list, list) { err = block_cb->cb(TC_SETUP_CLSFLOWER, &cls_flow, block_cb->cb_priv); if (err < 0) continue; i++; } up_read(&flowtable->flow_block_lock); if (cmd == FLOW_CLS_STATS) memcpy(stats, &cls_flow.stats, sizeof(*stats)); return i; }","- struct netlink_ext_ack extack;
+ struct netlink_ext_ack extack = {};","static int nf_flow_offload_tuple(struct nf_flowtable *flowtable, struct flow_offload *flow, struct nf_flow_rule *flow_rule, enum flow_offload_tuple_dir dir, int priority, int cmd, struct flow_stats *stats, struct list_head *block_cb_list) { struct flow_cls_offload cls_flow = {}; struct netlink_ext_ack extack = {}; struct flow_block_cb *block_cb; __be16 proto = ETH_P_ALL; int err, i = 0; nf_flow_offload_init(&cls_flow, proto, priority, cmd, &flow->tuplehash[dir].tuple, &extack); if (cmd == FLOW_CLS_REPLACE) cls_flow.rule = flow_rule->rule; down_read(&flowtable->flow_block_lock); list_for_each_entry(block_cb, block_cb_list, list) { err = block_cb->cb(TC_SETUP_CLSFLOWER, &cls_flow, block_cb->cb_priv); if (err < 0) continue; i++; } up_read(&flowtable->flow_block_lock); if (cmd == FLOW_CLS_STATS) memcpy(stats, &cls_flow.stats, sizeof(*stats)); return i; }"
667----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49967/bad/namei.c----*do_split,"static struct ext4_dir_entry_2 *do_split(handle_t *handle, struct inode *dir, struct buffer_head **bh,struct dx_frame *frame, struct dx_hash_info *hinfo) { unsigned blocksize = dir->i_sb->s_blocksize; unsigned continued; int count; struct buffer_head *bh2; ext4_lblk_t newblock; u32 hash2; struct dx_map_entry *map; char *data1 = (*bh)->b_data, *data2; unsigned split, move, size; struct ext4_dir_entry_2 *de = NULL, *de2; int csum_size = 0; int err = 0, i; if (ext4_has_metadata_csum(dir->i_sb)) csum_size = sizeof(struct ext4_dir_entry_tail); bh2 = ext4_append(handle, dir, &newblock); if (IS_ERR(bh2)) { brelse(*bh); *bh = NULL; return (struct ext4_dir_entry_2 *) bh2; } BUFFER_TRACE(*bh, ""get_write_access""); err = ext4_journal_get_write_access(handle, dir->i_sb, *bh, EXT4_JTR_NONE); if (err) goto journal_error; BUFFER_TRACE(frame->bh, ""get_write_access""); err = ext4_journal_get_write_access(handle, dir->i_sb, frame->bh, EXT4_JTR_NONE); if (err) goto journal_error; data2 = bh2->b_data; map = (struct dx_map_entry *) (data2 + blocksize); count = dx_make_map(dir, *bh, hinfo, map); if (count < 0) { err = count; goto journal_error; } map -= count; dx_sort_map(map, count); size = 0; move = 0; for (i = count-1; i >= 0; i--) { if (size + map[i].size/2 > blocksize/2) break; size += map[i].size; move++; } if (i > 0) split = count - move; else split = count/2; hash2 = map[split].hash; <S2SV_StartVul> continued = hash2 == map[split - 1].hash; <S2SV_EndVul> dxtrace(printk(KERN_INFO ""Split block %lu at %x, %i/%i\n"", (unsigned long)dx_get_block(frame->at), hash2, split, count-split)); de2 = dx_move_dirents(dir, data1, data2, map + split, count - split, blocksize); de = dx_pack_dirents(dir, data1, blocksize); de->rec_len = ext4_rec_len_to_disk(data1 + (blocksize - csum_size) - (char *) de, blocksize); de2->rec_len = ext4_rec_len_to_disk(data2 + (blocksize - csum_size) - (char *) de2, blocksize); if (csum_size) { ext4_initialize_dirent_tail(*bh, blocksize); ext4_initialize_dirent_tail(bh2, blocksize); } dxtrace(dx_show_leaf(dir, hinfo, (struct ext4_dir_entry_2 *) data1, blocksize, 1)); dxtrace(dx_show_leaf(dir, hinfo, (struct ext4_dir_entry_2 *) data2, blocksize, 1)); if (hinfo->hash >= hash2) { swap(*bh, bh2); de = de2; } dx_insert_block(frame, hash2 + continued, newblock); err = ext4_handle_dirty_dirblock(handle, dir, bh2); if (err) goto journal_error; err = ext4_handle_dirty_dx_node(handle, dir, frame->bh); if (err) goto journal_error; brelse(bh2); dxtrace(dx_show_index(""frame"", frame->entries)); return de; journal_error: brelse(*bh); brelse(bh2); *bh = NULL; ext4_std_error(dir->i_sb, err); return ERR_PTR(err); }","- continued = hash2 == map[split - 1].hash;
+ continued = split > 0 ? hash2 == map[split - 1].hash : 0;","static struct ext4_dir_entry_2 *do_split(handle_t *handle, struct inode *dir, struct buffer_head **bh,struct dx_frame *frame, struct dx_hash_info *hinfo) { unsigned blocksize = dir->i_sb->s_blocksize; unsigned continued; int count; struct buffer_head *bh2; ext4_lblk_t newblock; u32 hash2; struct dx_map_entry *map; char *data1 = (*bh)->b_data, *data2; unsigned split, move, size; struct ext4_dir_entry_2 *de = NULL, *de2; int csum_size = 0; int err = 0, i; if (ext4_has_metadata_csum(dir->i_sb)) csum_size = sizeof(struct ext4_dir_entry_tail); bh2 = ext4_append(handle, dir, &newblock); if (IS_ERR(bh2)) { brelse(*bh); *bh = NULL; return (struct ext4_dir_entry_2 *) bh2; } BUFFER_TRACE(*bh, ""get_write_access""); err = ext4_journal_get_write_access(handle, dir->i_sb, *bh, EXT4_JTR_NONE); if (err) goto journal_error; BUFFER_TRACE(frame->bh, ""get_write_access""); err = ext4_journal_get_write_access(handle, dir->i_sb, frame->bh, EXT4_JTR_NONE); if (err) goto journal_error; data2 = bh2->b_data; map = (struct dx_map_entry *) (data2 + blocksize); count = dx_make_map(dir, *bh, hinfo, map); if (count < 0) { err = count; goto journal_error; } map -= count; dx_sort_map(map, count); size = 0; move = 0; for (i = count-1; i >= 0; i--) { if (size + map[i].size/2 > blocksize/2) break; size += map[i].size; move++; } if (i > 0) split = count - move; else split = count/2; hash2 = map[split].hash; continued = split > 0 ? hash2 == map[split - 1].hash : 0; dxtrace(printk(KERN_INFO ""Split block %lu at %x, %i/%i\n"", (unsigned long)dx_get_block(frame->at), hash2, split, count-split)); de2 = dx_move_dirents(dir, data1, data2, map + split, count - split, blocksize); de = dx_pack_dirents(dir, data1, blocksize); de->rec_len = ext4_rec_len_to_disk(data1 + (blocksize - csum_size) - (char *) de, blocksize); de2->rec_len = ext4_rec_len_to_disk(data2 + (blocksize - csum_size) - (char *) de2, blocksize); if (csum_size) { ext4_initialize_dirent_tail(*bh, blocksize); ext4_initialize_dirent_tail(bh2, blocksize); } dxtrace(dx_show_leaf(dir, hinfo, (struct ext4_dir_entry_2 *) data1, blocksize, 1)); dxtrace(dx_show_leaf(dir, hinfo, (struct ext4_dir_entry_2 *) data2, blocksize, 1)); if (hinfo->hash >= hash2) { swap(*bh, bh2); de = de2; } dx_insert_block(frame, hash2 + continued, newblock); err = ext4_handle_dirty_dirblock(handle, dir, bh2); if (err) goto journal_error; err = ext4_handle_dirty_dx_node(handle, dir, frame->bh); if (err) goto journal_error; brelse(bh2); dxtrace(dx_show_index(""frame"", frame->entries)); return de; journal_error: brelse(*bh); brelse(bh2); *bh = NULL; ext4_std_error(dir->i_sb, err); return ERR_PTR(err); }"
1440----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56764/bad/ublk_drv.c----ublk_ctrl_start_dev,"static int ublk_ctrl_start_dev(struct ublk_device *ub, struct io_uring_cmd *cmd) { const struct ublksrv_ctrl_cmd *header = io_uring_sqe_cmd(cmd->sqe); const struct ublk_param_basic *p = &ub->params.basic; int ublksrv_pid = (int)header->data[0]; struct queue_limits lim = { .logical_block_size = 1 << p->logical_bs_shift, .physical_block_size = 1 << p->physical_bs_shift, .io_min = 1 << p->io_min_shift, .io_opt = 1 << p->io_opt_shift, .max_hw_sectors = p->max_sectors, .chunk_sectors = p->chunk_sectors, .virt_boundary_mask = p->virt_boundary_mask, .max_segments = USHRT_MAX, .max_segment_size = UINT_MAX, .dma_alignment = 3, }; struct gendisk *disk; int ret = -EINVAL; if (ublksrv_pid <= 0) return -EINVAL; if (!(ub->params.types & UBLK_PARAM_TYPE_BASIC)) return -EINVAL; if (ub->params.types & UBLK_PARAM_TYPE_DISCARD) { const struct ublk_param_discard *pd = &ub->params.discard; lim.discard_alignment = pd->discard_alignment; lim.discard_granularity = pd->discard_granularity; lim.max_hw_discard_sectors = pd->max_discard_sectors; lim.max_write_zeroes_sectors = pd->max_write_zeroes_sectors; lim.max_discard_segments = pd->max_discard_segments; } if (ub->params.types & UBLK_PARAM_TYPE_ZONED) { const struct ublk_param_zoned *p = &ub->params.zoned; if (!IS_ENABLED(CONFIG_BLK_DEV_ZONED)) return -EOPNOTSUPP; lim.features |= BLK_FEAT_ZONED; lim.max_active_zones = p->max_active_zones; lim.max_open_zones = p->max_open_zones; lim.max_hw_zone_append_sectors = p->max_zone_append_sectors; } if (ub->params.basic.attrs & UBLK_ATTR_VOLATILE_CACHE) { lim.features |= BLK_FEAT_WRITE_CACHE; if (ub->params.basic.attrs & UBLK_ATTR_FUA) lim.features |= BLK_FEAT_FUA; } if (ub->params.basic.attrs & UBLK_ATTR_ROTATIONAL) lim.features |= BLK_FEAT_ROTATIONAL; if (wait_for_completion_interruptible(&ub->completion) != 0) return -EINTR; mutex_lock(&ub->mutex); if (ub->dev_info.state == UBLK_S_DEV_LIVE || test_bit(UB_STATE_USED, &ub->state)) { ret = -EEXIST; goto out_unlock; } disk = blk_mq_alloc_disk(&ub->tag_set, &lim, NULL); if (IS_ERR(disk)) { ret = PTR_ERR(disk); goto out_unlock; } sprintf(disk->disk_name, ""ublkb%d"", ub->ub_number); disk->fops = &ub_fops; disk->private_data = ub; ub->dev_info.ublksrv_pid = ublksrv_pid; ub->ub_disk = disk; ublk_apply_params(ub); if (ub->nr_privileged_daemon != ub->nr_queues_ready) set_bit(GD_SUPPRESS_PART_SCAN, &disk->state); ublk_get_device(ub); ub->dev_info.state = UBLK_S_DEV_LIVE; if (ublk_dev_is_zoned(ub)) { ret = ublk_revalidate_disk_zones(ub); if (ret) goto out_put_cdev; } ret = add_disk(disk); if (ret) goto out_put_cdev; set_bit(UB_STATE_USED, &ub->state); out_put_cdev: if (ret) { <S2SV_StartVul> ub->dev_info.state = UBLK_S_DEV_DEAD; <S2SV_EndVul> ublk_put_device(ub); } if (ret) put_disk(disk); out_unlock: mutex_unlock(&ub->mutex); return ret; }","- ub->dev_info.state = UBLK_S_DEV_DEAD;
+ ublk_detach_disk(ub);
+ }","static int ublk_ctrl_start_dev(struct ublk_device *ub, struct io_uring_cmd *cmd) { const struct ublksrv_ctrl_cmd *header = io_uring_sqe_cmd(cmd->sqe); const struct ublk_param_basic *p = &ub->params.basic; int ublksrv_pid = (int)header->data[0]; struct queue_limits lim = { .logical_block_size = 1 << p->logical_bs_shift, .physical_block_size = 1 << p->physical_bs_shift, .io_min = 1 << p->io_min_shift, .io_opt = 1 << p->io_opt_shift, .max_hw_sectors = p->max_sectors, .chunk_sectors = p->chunk_sectors, .virt_boundary_mask = p->virt_boundary_mask, .max_segments = USHRT_MAX, .max_segment_size = UINT_MAX, .dma_alignment = 3, }; struct gendisk *disk; int ret = -EINVAL; if (ublksrv_pid <= 0) return -EINVAL; if (!(ub->params.types & UBLK_PARAM_TYPE_BASIC)) return -EINVAL; if (ub->params.types & UBLK_PARAM_TYPE_DISCARD) { const struct ublk_param_discard *pd = &ub->params.discard; lim.discard_alignment = pd->discard_alignment; lim.discard_granularity = pd->discard_granularity; lim.max_hw_discard_sectors = pd->max_discard_sectors; lim.max_write_zeroes_sectors = pd->max_write_zeroes_sectors; lim.max_discard_segments = pd->max_discard_segments; } if (ub->params.types & UBLK_PARAM_TYPE_ZONED) { const struct ublk_param_zoned *p = &ub->params.zoned; if (!IS_ENABLED(CONFIG_BLK_DEV_ZONED)) return -EOPNOTSUPP; lim.features |= BLK_FEAT_ZONED; lim.max_active_zones = p->max_active_zones; lim.max_open_zones = p->max_open_zones; lim.max_hw_zone_append_sectors = p->max_zone_append_sectors; } if (ub->params.basic.attrs & UBLK_ATTR_VOLATILE_CACHE) { lim.features |= BLK_FEAT_WRITE_CACHE; if (ub->params.basic.attrs & UBLK_ATTR_FUA) lim.features |= BLK_FEAT_FUA; } if (ub->params.basic.attrs & UBLK_ATTR_ROTATIONAL) lim.features |= BLK_FEAT_ROTATIONAL; if (wait_for_completion_interruptible(&ub->completion) != 0) return -EINTR; mutex_lock(&ub->mutex); if (ub->dev_info.state == UBLK_S_DEV_LIVE || test_bit(UB_STATE_USED, &ub->state)) { ret = -EEXIST; goto out_unlock; } disk = blk_mq_alloc_disk(&ub->tag_set, &lim, NULL); if (IS_ERR(disk)) { ret = PTR_ERR(disk); goto out_unlock; } sprintf(disk->disk_name, ""ublkb%d"", ub->ub_number); disk->fops = &ub_fops; disk->private_data = ub; ub->dev_info.ublksrv_pid = ublksrv_pid; ub->ub_disk = disk; ublk_apply_params(ub); if (ub->nr_privileged_daemon != ub->nr_queues_ready) set_bit(GD_SUPPRESS_PART_SCAN, &disk->state); ublk_get_device(ub); ub->dev_info.state = UBLK_S_DEV_LIVE; if (ublk_dev_is_zoned(ub)) { ret = ublk_revalidate_disk_zones(ub); if (ret) goto out_put_cdev; } ret = add_disk(disk); if (ret) goto out_put_cdev; set_bit(UB_STATE_USED, &ub->state); out_put_cdev: if (ret) { ublk_detach_disk(ub); ublk_put_device(ub); } if (ret) put_disk(disk); out_unlock: mutex_unlock(&ub->mutex); return ret; }"
491----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47732/bad/iaa_crypto_main.c----remove_device_compression_modes,"static void remove_device_compression_modes(struct iaa_device *iaa_device) { struct iaa_device_compression_mode *device_mode; int i; for (i = 0; i < IAA_COMP_MODES_MAX; i++) { device_mode = iaa_device->compression_modes[i]; if (!device_mode) continue; <S2SV_StartVul> free_device_compression_mode(iaa_device, device_mode); <S2SV_EndVul> <S2SV_StartVul> iaa_device->compression_modes[i] = NULL; <S2SV_EndVul> if (iaa_compression_modes[i]->free) iaa_compression_modes[i]->free(device_mode); } }","- free_device_compression_mode(iaa_device, device_mode);
- iaa_device->compression_modes[i] = NULL;
+ free_device_compression_mode(iaa_device, device_mode);
+ iaa_device->compression_modes[i] = NULL;","static void remove_device_compression_modes(struct iaa_device *iaa_device) { struct iaa_device_compression_mode *device_mode; int i; for (i = 0; i < IAA_COMP_MODES_MAX; i++) { device_mode = iaa_device->compression_modes[i]; if (!device_mode) continue; if (iaa_compression_modes[i]->free) iaa_compression_modes[i]->free(device_mode); free_device_compression_mode(iaa_device, device_mode); iaa_device->compression_modes[i] = NULL; } }"
507----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47744/bad/kvm_main.c----hardware_enable_all,"static int hardware_enable_all(void) { atomic_t failed = ATOMIC_INIT(0); int r; if (system_state == SYSTEM_HALT || system_state == SYSTEM_POWER_OFF || system_state == SYSTEM_RESTART) return -EBUSY; cpus_read_lock(); <S2SV_StartVul> mutex_lock(&kvm_lock); <S2SV_EndVul> r = 0; kvm_usage_count++; if (kvm_usage_count == 1) { on_each_cpu(hardware_enable_nolock, &failed, 1); if (atomic_read(&failed)) { hardware_disable_all_nolock(); r = -EBUSY; } } <S2SV_StartVul> mutex_unlock(&kvm_lock); <S2SV_EndVul> cpus_read_unlock(); return r; }","- mutex_lock(&kvm_lock);
- mutex_unlock(&kvm_lock);
+ mutex_lock(&kvm_usage_lock);
+ mutex_unlock(&kvm_usage_lock);","static int hardware_enable_all(void) { atomic_t failed = ATOMIC_INIT(0); int r; if (system_state == SYSTEM_HALT || system_state == SYSTEM_POWER_OFF || system_state == SYSTEM_RESTART) return -EBUSY; cpus_read_lock(); mutex_lock(&kvm_usage_lock); r = 0; kvm_usage_count++; if (kvm_usage_count == 1) { on_each_cpu(hardware_enable_nolock, &failed, 1); if (atomic_read(&failed)) { hardware_disable_all_nolock(); r = -EBUSY; } } mutex_unlock(&kvm_usage_lock); cpus_read_unlock(); return r; }"
651----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49953/bad/ipsec.c----mlx5e_ipsec_handle_sw_limits,"static void mlx5e_ipsec_handle_sw_limits(struct work_struct *_work) { struct mlx5e_ipsec_dwork *dwork = container_of(_work, struct mlx5e_ipsec_dwork, dwork.work); struct mlx5e_ipsec_sa_entry *sa_entry = dwork->sa_entry; struct xfrm_state *x = sa_entry->x; if (sa_entry->attrs.drop) return; spin_lock_bh(&x->lock); <S2SV_StartVul> xfrm_state_check_expire(x); <S2SV_EndVul> if (x->km.state == XFRM_STATE_EXPIRED) { sa_entry->attrs.drop = true; spin_unlock_bh(&x->lock); mlx5e_accel_ipsec_fs_modify(sa_entry); return; } spin_unlock_bh(&x->lock); queue_delayed_work(sa_entry->ipsec->wq, &dwork->dwork, MLX5_IPSEC_RESCHED); }","- xfrm_state_check_expire(x);
+ spin_unlock_bh(&x->lock);
+ return;
+ }
+ if (x->km.state != XFRM_STATE_VALID) {
+ spin_unlock_bh(&x->lock);
+ return;
+ }
+ xfrm_state_check_expire(x);
+ spin_unlock_bh(&x->lock);","static void mlx5e_ipsec_handle_sw_limits(struct work_struct *_work) { struct mlx5e_ipsec_dwork *dwork = container_of(_work, struct mlx5e_ipsec_dwork, dwork.work); struct mlx5e_ipsec_sa_entry *sa_entry = dwork->sa_entry; struct xfrm_state *x = sa_entry->x; if (sa_entry->attrs.drop) return; spin_lock_bh(&x->lock); if (x->km.state == XFRM_STATE_EXPIRED) { sa_entry->attrs.drop = true; spin_unlock_bh(&x->lock); mlx5e_accel_ipsec_fs_modify(sa_entry); return; } if (x->km.state != XFRM_STATE_VALID) { spin_unlock_bh(&x->lock); return; } xfrm_state_check_expire(x); spin_unlock_bh(&x->lock); queue_delayed_work(sa_entry->ipsec->wq, &dwork->dwork, MLX5_IPSEC_RESCHED); }"
859----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50118/bad/super.c----btrfs_reconfigure,"static int btrfs_reconfigure(struct fs_context *fc) { struct super_block *sb = fc->root->d_sb; struct btrfs_fs_info *fs_info = btrfs_sb(sb); struct btrfs_fs_context *ctx = fc->fs_private; struct btrfs_fs_context old_ctx; int ret = 0; bool mount_reconfigure = (fc->s_fs_info != NULL); btrfs_info_to_ctx(fs_info, &old_ctx); if (mount_reconfigure) ctx->mount_opt = old_ctx.mount_opt; sync_filesystem(sb); set_bit(BTRFS_FS_STATE_REMOUNTING, &fs_info->fs_state); <S2SV_StartVul> if (!mount_reconfigure && <S2SV_EndVul> <S2SV_StartVul> !btrfs_check_options(fs_info, &ctx->mount_opt, fc->sb_flags)) <S2SV_EndVul> return -EINVAL; ret = btrfs_check_features(fs_info, !(fc->sb_flags & SB_RDONLY)); if (ret < 0) return ret; btrfs_ctx_to_info(fs_info, ctx); btrfs_remount_begin(fs_info, old_ctx.mount_opt, fc->sb_flags); btrfs_resize_thread_pool(fs_info, fs_info->thread_pool_size, old_ctx.thread_pool_size); if ((bool)btrfs_test_opt(fs_info, FREE_SPACE_TREE) != (bool)btrfs_fs_compat_ro(fs_info, FREE_SPACE_TREE) && (!sb_rdonly(sb) || (fc->sb_flags & SB_RDONLY))) { btrfs_warn(fs_info, ""remount supports changing free space tree only from RO to RW""); if (btrfs_fs_compat_ro(fs_info, FREE_SPACE_TREE)) { btrfs_set_opt(fs_info->mount_opt, FREE_SPACE_TREE); btrfs_clear_opt(fs_info->mount_opt, SPACE_CACHE); } if (btrfs_free_space_cache_v1_active(fs_info)) { btrfs_clear_opt(fs_info->mount_opt, FREE_SPACE_TREE); btrfs_set_opt(fs_info->mount_opt, SPACE_CACHE); } } ret = 0; if (!sb_rdonly(sb) && (fc->sb_flags & SB_RDONLY)) ret = btrfs_remount_ro(fs_info); else if (sb_rdonly(sb) && !(fc->sb_flags & SB_RDONLY)) ret = btrfs_remount_rw(fs_info); if (ret) goto restore; if ((fc->sb_flags & SB_POSIXACL) != (sb->s_flags & SB_POSIXACL)) fc->sb_flags_mask |= SB_POSIXACL; btrfs_emit_options(fs_info, &old_ctx); wake_up_process(fs_info->transaction_kthread); btrfs_remount_cleanup(fs_info, old_ctx.mount_opt); btrfs_clear_oneshot_options(fs_info); clear_bit(BTRFS_FS_STATE_REMOUNTING, &fs_info->fs_state); return 0; restore: btrfs_ctx_to_info(fs_info, &old_ctx); btrfs_remount_cleanup(fs_info, old_ctx.mount_opt); clear_bit(BTRFS_FS_STATE_REMOUNTING, &fs_info->fs_state); return ret; }","- if (!mount_reconfigure &&
- !btrfs_check_options(fs_info, &ctx->mount_opt, fc->sb_flags))
+ if (!btrfs_check_options(fs_info, &ctx->mount_opt, fc->sb_flags))","static int btrfs_reconfigure(struct fs_context *fc) { struct super_block *sb = fc->root->d_sb; struct btrfs_fs_info *fs_info = btrfs_sb(sb); struct btrfs_fs_context *ctx = fc->fs_private; struct btrfs_fs_context old_ctx; int ret = 0; bool mount_reconfigure = (fc->s_fs_info != NULL); btrfs_info_to_ctx(fs_info, &old_ctx); if (mount_reconfigure) ctx->mount_opt = old_ctx.mount_opt; sync_filesystem(sb); set_bit(BTRFS_FS_STATE_REMOUNTING, &fs_info->fs_state); if (!btrfs_check_options(fs_info, &ctx->mount_opt, fc->sb_flags)) return -EINVAL; ret = btrfs_check_features(fs_info, !(fc->sb_flags & SB_RDONLY)); if (ret < 0) return ret; btrfs_ctx_to_info(fs_info, ctx); btrfs_remount_begin(fs_info, old_ctx.mount_opt, fc->sb_flags); btrfs_resize_thread_pool(fs_info, fs_info->thread_pool_size, old_ctx.thread_pool_size); if ((bool)btrfs_test_opt(fs_info, FREE_SPACE_TREE) != (bool)btrfs_fs_compat_ro(fs_info, FREE_SPACE_TREE) && (!sb_rdonly(sb) || (fc->sb_flags & SB_RDONLY))) { btrfs_warn(fs_info, ""remount supports changing free space tree only from RO to RW""); if (btrfs_fs_compat_ro(fs_info, FREE_SPACE_TREE)) { btrfs_set_opt(fs_info->mount_opt, FREE_SPACE_TREE); btrfs_clear_opt(fs_info->mount_opt, SPACE_CACHE); } if (btrfs_free_space_cache_v1_active(fs_info)) { btrfs_clear_opt(fs_info->mount_opt, FREE_SPACE_TREE); btrfs_set_opt(fs_info->mount_opt, SPACE_CACHE); } } ret = 0; if (!sb_rdonly(sb) && (fc->sb_flags & SB_RDONLY)) ret = btrfs_remount_ro(fs_info); else if (sb_rdonly(sb) && !(fc->sb_flags & SB_RDONLY)) ret = btrfs_remount_rw(fs_info); if (ret) goto restore; if ((fc->sb_flags & SB_POSIXACL) != (sb->s_flags & SB_POSIXACL)) fc->sb_flags_mask |= SB_POSIXACL; btrfs_emit_options(fs_info, &old_ctx); wake_up_process(fs_info->transaction_kthread); btrfs_remount_cleanup(fs_info, old_ctx.mount_opt); btrfs_clear_oneshot_options(fs_info); clear_bit(BTRFS_FS_STATE_REMOUNTING, &fs_info->fs_state); return 0; restore: btrfs_ctx_to_info(fs_info, &old_ctx); btrfs_remount_cleanup(fs_info, old_ctx.mount_opt); clear_bit(BTRFS_FS_STATE_REMOUNTING, &fs_info->fs_state); return ret; }"
1051----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50292/bad/stm32_spdifrx.c----stm32_spdifrx_remove,static void stm32_spdifrx_remove(struct platform_device *pdev) { struct stm32_spdifrx_data *spdifrx = platform_get_drvdata(pdev); <S2SV_StartVul> if (spdifrx->ctrl_chan) <S2SV_EndVul> dma_release_channel(spdifrx->ctrl_chan); if (spdifrx->dmab) snd_dma_free_pages(spdifrx->dmab); snd_dmaengine_pcm_unregister(&pdev->dev); snd_soc_unregister_component(&pdev->dev); pm_runtime_disable(&pdev->dev); },"- if (spdifrx->ctrl_chan)
+ if (!IS_ERR(spdifrx->ctrl_chan))",static void stm32_spdifrx_remove(struct platform_device *pdev) { struct stm32_spdifrx_data *spdifrx = platform_get_drvdata(pdev); if (!IS_ERR(spdifrx->ctrl_chan)) dma_release_channel(spdifrx->ctrl_chan); if (spdifrx->dmab) snd_dma_free_pages(spdifrx->dmab); snd_dmaengine_pcm_unregister(&pdev->dev); snd_soc_unregister_component(&pdev->dev); pm_runtime_disable(&pdev->dev); }
1114----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53090/bad/rxrpc.c----afs_put_call,"void afs_put_call(struct afs_call *call) { struct afs_net *net = call->net; unsigned int debug_id = call->debug_id; bool zero; int r, o; zero = __refcount_dec_and_test(&call->ref, &r); o = atomic_read(&net->nr_outstanding_calls); trace_afs_call(debug_id, afs_call_trace_put, r - 1, o, <S2SV_StartVul> __builtin_return_address(0)); <S2SV_EndVul> <S2SV_StartVul> if (zero) { <S2SV_EndVul> <S2SV_StartVul> ASSERT(!work_pending(&call->async_work)); <S2SV_EndVul> <S2SV_StartVul> ASSERT(call->type->name != NULL); <S2SV_EndVul> <S2SV_StartVul> rxrpc_kernel_put_peer(call->peer); <S2SV_EndVul> <S2SV_StartVul> if (call->rxcall) { <S2SV_EndVul> <S2SV_StartVul> rxrpc_kernel_shutdown_call(net->socket, call->rxcall); <S2SV_EndVul> <S2SV_StartVul> rxrpc_kernel_put_call(net->socket, call->rxcall); <S2SV_EndVul> <S2SV_StartVul> call->rxcall = NULL; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> if (call->type->destructor) <S2SV_EndVul> <S2SV_StartVul> call->type->destructor(call); <S2SV_EndVul> <S2SV_StartVul> afs_unuse_server_notime(call->net, call->server, afs_server_trace_put_call); <S2SV_EndVul> <S2SV_StartVul> kfree(call->request); <S2SV_EndVul> <S2SV_StartVul> trace_afs_call(call->debug_id, afs_call_trace_free, 0, o, <S2SV_EndVul> <S2SV_StartVul> __builtin_return_address(0)); <S2SV_EndVul> <S2SV_StartVul> kfree(call); <S2SV_EndVul> <S2SV_StartVul> o = atomic_dec_return(&net->nr_outstanding_calls); <S2SV_EndVul> <S2SV_StartVul> if (o == 0) <S2SV_EndVul> <S2SV_StartVul> wake_up_var(&net->nr_outstanding_calls); <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul>","- __builtin_return_address(0));
- if (zero) {
- ASSERT(!work_pending(&call->async_work));
- ASSERT(call->type->name != NULL);
- rxrpc_kernel_put_peer(call->peer);
- if (call->rxcall) {
- rxrpc_kernel_shutdown_call(net->socket, call->rxcall);
- rxrpc_kernel_put_call(net->socket, call->rxcall);
- call->rxcall = NULL;
- }
- if (call->type->destructor)
- call->type->destructor(call);
- afs_unuse_server_notime(call->net, call->server, afs_server_trace_put_call);
- kfree(call->request);
- trace_afs_call(call->debug_id, afs_call_trace_free, 0, o,
- __builtin_return_address(0));
- kfree(call);
- o = atomic_dec_return(&net->nr_outstanding_calls);
- if (o == 0)
- wake_up_var(&net->nr_outstanding_calls);
- }
- }
+ trace_afs_call(debug_id, afs_call_trace_put, r - 1, o,
+ __builtin_return_address(0));
+ if (zero)
+ afs_free_call(call);
+ }","void afs_put_call(struct afs_call *call) { struct afs_net *net = call->net; unsigned int debug_id = call->debug_id; bool zero; int r, o; zero = __refcount_dec_and_test(&call->ref, &r); o = atomic_read(&net->nr_outstanding_calls); trace_afs_call(debug_id, afs_call_trace_put, r - 1, o, __builtin_return_address(0)); if (zero) afs_free_call(call); }"
1038----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50278/bad/dm-cache-target.c----cache_preresume,"static int cache_preresume(struct dm_target *ti) { int r = 0; struct cache *cache = ti->private; dm_cblock_t csize = get_cache_dev_size(cache); <S2SV_StartVul> if (!cache->sized) { <S2SV_EndVul> <S2SV_StartVul> r = resize_cache_dev(cache, csize); <S2SV_EndVul> <S2SV_StartVul> if (r) <S2SV_EndVul> <S2SV_StartVul> return r; <S2SV_EndVul> <S2SV_StartVul> cache->sized = true; <S2SV_EndVul> <S2SV_StartVul> } else if (csize != cache->cache_size) { <S2SV_EndVul> if (!can_resize(cache, csize)) return -EINVAL; <S2SV_StartVul> r = resize_cache_dev(cache, csize); <S2SV_EndVul> <S2SV_StartVul> if (r) <S2SV_EndVul> <S2SV_StartVul> return r; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> if (!cache->loaded_mappings) { r = dm_cache_load_mappings(cache->cmd, cache->policy, load_mapping, cache); if (r) { DMERR(""%s: could not load cache mappings"", cache_device_name(cache)); metadata_operation_failed(cache, ""dm_cache_load_mappings"", r); return r; } cache->loaded_mappings = true; } if (!cache->loaded_discards) { struct discard_load_info li; clear_bitset(cache->discard_bitset, from_dblock(cache->discard_nr_blocks)); discard_load_info_init(cache, &li); r = dm_cache_load_discards(cache->cmd, load_discard, &li); if (r) { DMERR(""%s: could not load origin discards"", cache_device_name(cache)); metadata_operation_failed(cache, ""dm_cache_load_discards"", r); return r; } set_discard_range(&li); cache->loaded_discards = true; } return r; }","- if (!cache->sized) {
- r = resize_cache_dev(cache, csize);
- if (r)
- return r;
- cache->sized = true;
- } else if (csize != cache->cache_size) {
- r = resize_cache_dev(cache, csize);
- if (r)
- return r;
- }
+ if (!cache->sized || csize != cache->cache_size) {
+ cache->sized = true;
+ }","static int cache_preresume(struct dm_target *ti) { int r = 0; struct cache *cache = ti->private; dm_cblock_t csize = get_cache_dev_size(cache); if (!cache->sized || csize != cache->cache_size) { if (!can_resize(cache, csize)) return -EINVAL; r = resize_cache_dev(cache, csize); if (r) return r; cache->sized = true; } if (!cache->loaded_mappings) { r = dm_cache_load_mappings(cache->cmd, cache->policy, load_mapping, cache); if (r) { DMERR(""%s: could not load cache mappings"", cache_device_name(cache)); metadata_operation_failed(cache, ""dm_cache_load_mappings"", r); return r; } cache->loaded_mappings = true; } if (!cache->loaded_discards) { struct discard_load_info li; clear_bitset(cache->discard_bitset, from_dblock(cache->discard_nr_blocks)); discard_load_info_init(cache, &li); r = dm_cache_load_discards(cache->cmd, load_discard, &li); if (r) { DMERR(""%s: could not load origin discards"", cache_device_name(cache)); metadata_operation_failed(cache, ""dm_cache_load_discards"", r); return r; } set_discard_range(&li); cache->loaded_discards = true; } return r; }"
775----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50049/bad/dc.c----dc_validate_boot_timing,"bool dc_validate_boot_timing(const struct dc *dc, const struct dc_sink *sink, struct dc_crtc_timing *crtc_timing) { struct timing_generator *tg; struct stream_encoder *se = NULL; struct dc_crtc_timing hw_crtc_timing = {0}; struct dc_link *link = sink->link; unsigned int i, enc_inst, tg_inst = 0; if (sink->sink_signal != SIGNAL_TYPE_EDP) { return false; } if (dc->debug.force_odm_combine) return false; if (!link->link_enc->funcs->is_dig_enabled(link->link_enc)) return false; enc_inst = link->link_enc->funcs->get_dig_frontend(link->link_enc); if (enc_inst == ENGINE_ID_UNKNOWN) return false; for (i = 0; i < dc->res_pool->stream_enc_count; i++) { if (dc->res_pool->stream_enc[i]->id == enc_inst) { se = dc->res_pool->stream_enc[i]; tg_inst = dc->res_pool->stream_enc[i]->funcs->dig_source_otg( dc->res_pool->stream_enc[i]); break; } } if (i == dc->res_pool->stream_enc_count) return false; if (tg_inst >= dc->res_pool->timing_generator_count) return false; if (tg_inst != link->link_enc->preferred_engine) return false; tg = dc->res_pool->timing_generators[tg_inst]; if (!tg->funcs->get_hw_timing) return false; if (!tg->funcs->get_hw_timing(tg, &hw_crtc_timing)) return false; if (crtc_timing->h_total != hw_crtc_timing.h_total) return false; if (crtc_timing->h_border_left != hw_crtc_timing.h_border_left) return false; if (crtc_timing->h_addressable != hw_crtc_timing.h_addressable) return false; if (crtc_timing->h_border_right != hw_crtc_timing.h_border_right) return false; if (crtc_timing->h_front_porch != hw_crtc_timing.h_front_porch) return false; if (crtc_timing->h_sync_width != hw_crtc_timing.h_sync_width) return false; if (crtc_timing->v_total != hw_crtc_timing.v_total) return false; if (crtc_timing->v_border_top != hw_crtc_timing.v_border_top) return false; if (crtc_timing->v_addressable != hw_crtc_timing.v_addressable) return false; if (crtc_timing->v_border_bottom != hw_crtc_timing.v_border_bottom) return false; if (crtc_timing->v_front_porch != hw_crtc_timing.v_front_porch) return false; if (crtc_timing->v_sync_width != hw_crtc_timing.v_sync_width) return false; if (crtc_timing->flags.DSC) return false; if (dc_is_dp_signal(link->connector_signal)) { unsigned int pix_clk_100hz = 0; uint32_t numOdmPipes = 1; uint32_t id_src[4] = {0}; dc->res_pool->dp_clock_source->funcs->get_pixel_clk_frequency_100hz( dc->res_pool->dp_clock_source, tg_inst, &pix_clk_100hz); if (tg->funcs->get_optc_source) tg->funcs->get_optc_source(tg, &numOdmPipes, &id_src[0], &id_src[1]); if (numOdmPipes == 2) { pix_clk_100hz *= 2; } else if (numOdmPipes == 4) { pix_clk_100hz *= 4; } else if (se && se->funcs->get_pixels_per_cycle) { uint32_t pixels_per_cycle = se->funcs->get_pixels_per_cycle(se); if (pixels_per_cycle != 1 && !dc->debug.enable_dp_dig_pixel_rate_div_policy) return false; pix_clk_100hz *= pixels_per_cycle; } if (crtc_timing->pix_clk_100hz != pix_clk_100hz) return false; <S2SV_StartVul> if (!se->funcs->dp_get_pixel_format) <S2SV_EndVul> return false; if (!se->funcs->dp_get_pixel_format( se, &hw_crtc_timing.pixel_encoding, &hw_crtc_timing.display_color_depth)) return false; if (hw_crtc_timing.display_color_depth != crtc_timing->display_color_depth) return false; if (hw_crtc_timing.pixel_encoding != crtc_timing->pixel_encoding) return false; } if (link->dpcd_caps.dprx_feature.bits.VSC_SDP_COLORIMETRY_SUPPORTED) { return false; } if (link->dpcd_caps.channel_coding_cap.bits.DP_128b_132b_SUPPORTED) return false; if (dc->link_srv->edp_is_ilr_optimization_required(link, crtc_timing)) { DC_LOG_EVENT_LINK_TRAINING(""Seamless boot disabled to optimize eDP link rate\n""); return false; } return true; }","- if (!se->funcs->dp_get_pixel_format)
+ if (!se || !se->funcs->dp_get_pixel_format)","bool dc_validate_boot_timing(const struct dc *dc, const struct dc_sink *sink, struct dc_crtc_timing *crtc_timing) { struct timing_generator *tg; struct stream_encoder *se = NULL; struct dc_crtc_timing hw_crtc_timing = {0}; struct dc_link *link = sink->link; unsigned int i, enc_inst, tg_inst = 0; if (sink->sink_signal != SIGNAL_TYPE_EDP) { return false; } if (dc->debug.force_odm_combine) return false; if (!link->link_enc->funcs->is_dig_enabled(link->link_enc)) return false; enc_inst = link->link_enc->funcs->get_dig_frontend(link->link_enc); if (enc_inst == ENGINE_ID_UNKNOWN) return false; for (i = 0; i < dc->res_pool->stream_enc_count; i++) { if (dc->res_pool->stream_enc[i]->id == enc_inst) { se = dc->res_pool->stream_enc[i]; tg_inst = dc->res_pool->stream_enc[i]->funcs->dig_source_otg( dc->res_pool->stream_enc[i]); break; } } if (i == dc->res_pool->stream_enc_count) return false; if (tg_inst >= dc->res_pool->timing_generator_count) return false; if (tg_inst != link->link_enc->preferred_engine) return false; tg = dc->res_pool->timing_generators[tg_inst]; if (!tg->funcs->get_hw_timing) return false; if (!tg->funcs->get_hw_timing(tg, &hw_crtc_timing)) return false; if (crtc_timing->h_total != hw_crtc_timing.h_total) return false; if (crtc_timing->h_border_left != hw_crtc_timing.h_border_left) return false; if (crtc_timing->h_addressable != hw_crtc_timing.h_addressable) return false; if (crtc_timing->h_border_right != hw_crtc_timing.h_border_right) return false; if (crtc_timing->h_front_porch != hw_crtc_timing.h_front_porch) return false; if (crtc_timing->h_sync_width != hw_crtc_timing.h_sync_width) return false; if (crtc_timing->v_total != hw_crtc_timing.v_total) return false; if (crtc_timing->v_border_top != hw_crtc_timing.v_border_top) return false; if (crtc_timing->v_addressable != hw_crtc_timing.v_addressable) return false; if (crtc_timing->v_border_bottom != hw_crtc_timing.v_border_bottom) return false; if (crtc_timing->v_front_porch != hw_crtc_timing.v_front_porch) return false; if (crtc_timing->v_sync_width != hw_crtc_timing.v_sync_width) return false; if (crtc_timing->flags.DSC) return false; if (dc_is_dp_signal(link->connector_signal)) { unsigned int pix_clk_100hz = 0; uint32_t numOdmPipes = 1; uint32_t id_src[4] = {0}; dc->res_pool->dp_clock_source->funcs->get_pixel_clk_frequency_100hz( dc->res_pool->dp_clock_source, tg_inst, &pix_clk_100hz); if (tg->funcs->get_optc_source) tg->funcs->get_optc_source(tg, &numOdmPipes, &id_src[0], &id_src[1]); if (numOdmPipes == 2) { pix_clk_100hz *= 2; } else if (numOdmPipes == 4) { pix_clk_100hz *= 4; } else if (se && se->funcs->get_pixels_per_cycle) { uint32_t pixels_per_cycle = se->funcs->get_pixels_per_cycle(se); if (pixels_per_cycle != 1 && !dc->debug.enable_dp_dig_pixel_rate_div_policy) return false; pix_clk_100hz *= pixels_per_cycle; } if (crtc_timing->pix_clk_100hz != pix_clk_100hz) return false; if (!se || !se->funcs->dp_get_pixel_format) return false; if (!se->funcs->dp_get_pixel_format( se, &hw_crtc_timing.pixel_encoding, &hw_crtc_timing.display_color_depth)) return false; if (hw_crtc_timing.display_color_depth != crtc_timing->display_color_depth) return false; if (hw_crtc_timing.pixel_encoding != crtc_timing->pixel_encoding) return false; } if (link->dpcd_caps.dprx_feature.bits.VSC_SDP_COLORIMETRY_SUPPORTED) { return false; } if (link->dpcd_caps.channel_coding_cap.bits.DP_128b_132b_SUPPORTED) return false; if (dc->link_srv->edp_is_ilr_optimization_required(link, crtc_timing)) { DC_LOG_EVENT_LINK_TRAINING(""Seamless boot disabled to optimize eDP link rate\n""); return false; } return true; }"
487----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47729/bad/xe_vm.c----*xe_vm_create,"struct xe_vm *xe_vm_create(struct xe_device *xe, u32 flags) { struct drm_gem_object *vm_resv_obj; struct xe_vm *vm; int err, number_tiles = 0; struct xe_tile *tile; u8 id; vm = kzalloc(sizeof(*vm), GFP_KERNEL); if (!vm) return ERR_PTR(-ENOMEM); vm->xe = xe; vm->size = 1ull << xe->info.va_bits; vm->flags = flags; init_rwsem(&vm->lock); mutex_init(&vm->snap_mutex); INIT_LIST_HEAD(&vm->rebind_list); INIT_LIST_HEAD(&vm->userptr.repin_list); INIT_LIST_HEAD(&vm->userptr.invalidated); init_rwsem(&vm->userptr.notifier_lock); spin_lock_init(&vm->userptr.invalidated_lock); INIT_WORK(&vm->destroy_work, vm_destroy_work_func); INIT_LIST_HEAD(&vm->preempt.exec_queues); vm->preempt.min_run_period_ms = 10; for_each_tile(tile, xe, id) xe_range_fence_tree_init(&vm->rftree[id]); vm->pt_ops = &xelp_pt_ops; if (flags & XE_VM_FLAG_LR_MODE) xe_pm_runtime_get_noresume(xe); vm_resv_obj = drm_gpuvm_resv_object_alloc(&xe->drm); if (!vm_resv_obj) { err = -ENOMEM; goto err_no_resv; } drm_gpuvm_init(&vm->gpuvm, ""Xe VM"", DRM_GPUVM_RESV_PROTECTED, &xe->drm, vm_resv_obj, 0, vm->size, 0, 0, &gpuvm_ops); drm_gem_object_put(vm_resv_obj); err = xe_vm_lock(vm, true); if (err) goto err_close; if (IS_DGFX(xe) && xe->info.vram_flags & XE_VRAM_FLAGS_NEED64K) vm->flags |= XE_VM_FLAG_64K; for_each_tile(tile, xe, id) { if (flags & XE_VM_FLAG_MIGRATION && tile->id != XE_VM_FLAG_TILE_ID(flags)) continue; vm->pt_root[id] = xe_pt_create(vm, tile, xe->info.vm_max_level); if (IS_ERR(vm->pt_root[id])) { err = PTR_ERR(vm->pt_root[id]); vm->pt_root[id] = NULL; goto err_unlock_close; } } if (xe_vm_has_scratch(vm)) { for_each_tile(tile, xe, id) { if (!vm->pt_root[id]) continue; err = xe_vm_create_scratch(xe, tile, vm); if (err) goto err_unlock_close; } vm->batch_invalidate_tlb = true; } if (vm->flags & XE_VM_FLAG_LR_MODE) { INIT_WORK(&vm->preempt.rebind_work, preempt_rebind_work_func); vm->batch_invalidate_tlb = false; } for_each_tile(tile, xe, id) { if (!vm->pt_root[id]) continue; xe_pt_populate_empty(tile, vm, vm->pt_root[id]); } xe_vm_unlock(vm); if (!(flags & XE_VM_FLAG_MIGRATION)) { for_each_tile(tile, xe, id) { <S2SV_StartVul> struct xe_gt *gt = tile->primary_gt; <S2SV_EndVul> <S2SV_StartVul> struct xe_vm *migrate_vm; <S2SV_EndVul> struct xe_exec_queue *q; u32 create_flags = EXEC_QUEUE_FLAG_VM; if (!vm->pt_root[id]) continue; <S2SV_StartVul> migrate_vm = xe_migrate_get_vm(tile->migrate); <S2SV_EndVul> <S2SV_StartVul> q = xe_exec_queue_create_class(xe, gt, migrate_vm, <S2SV_EndVul> <S2SV_StartVul> XE_ENGINE_CLASS_COPY, <S2SV_EndVul> <S2SV_StartVul> create_flags); <S2SV_EndVul> <S2SV_StartVul> xe_vm_put(migrate_vm); <S2SV_EndVul> if (IS_ERR(q)) { err = PTR_ERR(q); goto err_close; } vm->q[id] = q; number_tiles++; } } if (number_tiles > 1) vm->composite_fence_ctx = dma_fence_context_alloc(1); mutex_lock(&xe->usm.lock); if (flags & XE_VM_FLAG_FAULT_MODE) xe->usm.num_vm_in_fault_mode++; else if (!(flags & XE_VM_FLAG_MIGRATION)) xe->usm.num_vm_in_non_fault_mode++; mutex_unlock(&xe->usm.lock); trace_xe_vm_create(vm); return vm; err_unlock_close: xe_vm_unlock(vm); err_close: xe_vm_close_and_put(vm); return ERR_PTR(err); err_no_resv: mutex_destroy(&vm->snap_mutex); for_each_tile(tile, xe, id) xe_range_fence_tree_fini(&vm->rftree[id]); kfree(vm); if (flags & XE_VM_FLAG_LR_MODE) xe_pm_runtime_put(xe); return ERR_PTR(err); }","- struct xe_gt *gt = tile->primary_gt;
- struct xe_vm *migrate_vm;
- migrate_vm = xe_migrate_get_vm(tile->migrate);
- q = xe_exec_queue_create_class(xe, gt, migrate_vm,
- XE_ENGINE_CLASS_COPY,
- create_flags);
- xe_vm_put(migrate_vm);
+ q = xe_exec_queue_create_bind(xe, tile, create_flags, 0);","struct xe_vm *xe_vm_create(struct xe_device *xe, u32 flags) { struct drm_gem_object *vm_resv_obj; struct xe_vm *vm; int err, number_tiles = 0; struct xe_tile *tile; u8 id; vm = kzalloc(sizeof(*vm), GFP_KERNEL); if (!vm) return ERR_PTR(-ENOMEM); vm->xe = xe; vm->size = 1ull << xe->info.va_bits; vm->flags = flags; init_rwsem(&vm->lock); mutex_init(&vm->snap_mutex); INIT_LIST_HEAD(&vm->rebind_list); INIT_LIST_HEAD(&vm->userptr.repin_list); INIT_LIST_HEAD(&vm->userptr.invalidated); init_rwsem(&vm->userptr.notifier_lock); spin_lock_init(&vm->userptr.invalidated_lock); INIT_WORK(&vm->destroy_work, vm_destroy_work_func); INIT_LIST_HEAD(&vm->preempt.exec_queues); vm->preempt.min_run_period_ms = 10; for_each_tile(tile, xe, id) xe_range_fence_tree_init(&vm->rftree[id]); vm->pt_ops = &xelp_pt_ops; if (flags & XE_VM_FLAG_LR_MODE) xe_pm_runtime_get_noresume(xe); vm_resv_obj = drm_gpuvm_resv_object_alloc(&xe->drm); if (!vm_resv_obj) { err = -ENOMEM; goto err_no_resv; } drm_gpuvm_init(&vm->gpuvm, ""Xe VM"", DRM_GPUVM_RESV_PROTECTED, &xe->drm, vm_resv_obj, 0, vm->size, 0, 0, &gpuvm_ops); drm_gem_object_put(vm_resv_obj); err = xe_vm_lock(vm, true); if (err) goto err_close; if (IS_DGFX(xe) && xe->info.vram_flags & XE_VRAM_FLAGS_NEED64K) vm->flags |= XE_VM_FLAG_64K; for_each_tile(tile, xe, id) { if (flags & XE_VM_FLAG_MIGRATION && tile->id != XE_VM_FLAG_TILE_ID(flags)) continue; vm->pt_root[id] = xe_pt_create(vm, tile, xe->info.vm_max_level); if (IS_ERR(vm->pt_root[id])) { err = PTR_ERR(vm->pt_root[id]); vm->pt_root[id] = NULL; goto err_unlock_close; } } if (xe_vm_has_scratch(vm)) { for_each_tile(tile, xe, id) { if (!vm->pt_root[id]) continue; err = xe_vm_create_scratch(xe, tile, vm); if (err) goto err_unlock_close; } vm->batch_invalidate_tlb = true; } if (vm->flags & XE_VM_FLAG_LR_MODE) { INIT_WORK(&vm->preempt.rebind_work, preempt_rebind_work_func); vm->batch_invalidate_tlb = false; } for_each_tile(tile, xe, id) { if (!vm->pt_root[id]) continue; xe_pt_populate_empty(tile, vm, vm->pt_root[id]); } xe_vm_unlock(vm); if (!(flags & XE_VM_FLAG_MIGRATION)) { for_each_tile(tile, xe, id) { struct xe_exec_queue *q; u32 create_flags = EXEC_QUEUE_FLAG_VM; if (!vm->pt_root[id]) continue; q = xe_exec_queue_create_bind(xe, tile, create_flags, 0); if (IS_ERR(q)) { err = PTR_ERR(q); goto err_close; } vm->q[id] = q; number_tiles++; } } if (number_tiles > 1) vm->composite_fence_ctx = dma_fence_context_alloc(1); mutex_lock(&xe->usm.lock); if (flags & XE_VM_FLAG_FAULT_MODE) xe->usm.num_vm_in_fault_mode++; else if (!(flags & XE_VM_FLAG_MIGRATION)) xe->usm.num_vm_in_non_fault_mode++; mutex_unlock(&xe->usm.lock); trace_xe_vm_create(vm); return vm; err_unlock_close: xe_vm_unlock(vm); err_close: xe_vm_close_and_put(vm); return ERR_PTR(err); err_no_resv: mutex_destroy(&vm->snap_mutex); for_each_tile(tile, xe, id) xe_range_fence_tree_fini(&vm->rftree[id]); kfree(vm); if (flags & XE_VM_FLAG_LR_MODE) xe_pm_runtime_put(xe); return ERR_PTR(err); }"
1395----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56722/bad/hns_roce_hw_v2.c----free_mr_send_cmd_to_hw,"static void free_mr_send_cmd_to_hw(struct hns_roce_dev *hr_dev) { struct hns_roce_v2_priv *priv = hr_dev->priv; struct hns_roce_v2_free_mr *free_mr = &priv->free_mr; struct ib_wc wc[ARRAY_SIZE(free_mr->rsv_qp)]; struct ib_device *ibdev = &hr_dev->ib_dev; struct hns_roce_qp *hr_qp; unsigned long end; int cqe_cnt = 0; int npolled; int ret; int i; if (priv->handle->rinfo.reset_state == HNS_ROCE_STATE_RST_INIT || priv->handle->rinfo.instance_state == HNS_ROCE_STATE_INIT || hr_dev->state == HNS_ROCE_DEVICE_STATE_UNINIT) return; mutex_lock(&free_mr->mutex); for (i = 0; i < ARRAY_SIZE(free_mr->rsv_qp); i++) { hr_qp = free_mr->rsv_qp[i]; ret = free_mr_post_send_lp_wqe(hr_qp); if (ret) { <S2SV_StartVul> ibdev_err(ibdev, <S2SV_EndVul> <S2SV_StartVul> ""failed to send wqe (qp:0x%lx) for free mr, ret = %d.\n"", <S2SV_EndVul> <S2SV_StartVul> hr_qp->qpn, ret); <S2SV_EndVul> break; <S2SV_StartVul> } <S2SV_EndVul> cqe_cnt++; } end = msecs_to_jiffies(HNS_ROCE_V2_FREE_MR_TIMEOUT) + jiffies; while (cqe_cnt) { npolled = hns_roce_v2_poll_cq(&free_mr->rsv_cq->ib_cq, cqe_cnt, wc); if (npolled < 0) { <S2SV_StartVul> ibdev_err(ibdev, <S2SV_EndVul> <S2SV_StartVul> ""failed to poll cqe for free mr, remain %d cqe.\n"", <S2SV_EndVul> <S2SV_StartVul> cqe_cnt); <S2SV_EndVul> goto out; <S2SV_StartVul> } <S2SV_EndVul> if (time_after(jiffies, end)) { <S2SV_StartVul> ibdev_err(ibdev, <S2SV_EndVul> <S2SV_StartVul> ""failed to poll cqe for free mr and timeout, remain %d cqe.\n"", <S2SV_EndVul> <S2SV_StartVul> cqe_cnt); <S2SV_EndVul> goto out; <S2SV_StartVul> } <S2SV_EndVul> cqe_cnt -= npolled; } out: mutex_unlock(&free_mr->mutex); }","- ibdev_err(ibdev,
- ""failed to send wqe (qp:0x%lx) for free mr, ret = %d.\n"",
- hr_qp->qpn, ret);
- }
- ibdev_err(ibdev,
- ""failed to poll cqe for free mr, remain %d cqe.\n"",
- cqe_cnt);
- }
- ibdev_err(ibdev,
- ""failed to poll cqe for free mr and timeout, remain %d cqe.\n"",
- cqe_cnt);
- }
+ ibdev_err_ratelimited(ibdev,
+ ""failed to send wqe (qp:0x%lx) for free mr, ret = %d.\n"",
+ hr_qp->qpn, ret);
+ ibdev_err_ratelimited(ibdev,
+ ""failed to poll cqe for free mr, remain %d cqe.\n"",
+ cqe_cnt);
+ ibdev_err_ratelimited(ibdev,
+ ""failed to poll cqe for free mr and timeout, remain %d cqe.\n"",
+ cqe_cnt);","static void free_mr_send_cmd_to_hw(struct hns_roce_dev *hr_dev) { struct hns_roce_v2_priv *priv = hr_dev->priv; struct hns_roce_v2_free_mr *free_mr = &priv->free_mr; struct ib_wc wc[ARRAY_SIZE(free_mr->rsv_qp)]; struct ib_device *ibdev = &hr_dev->ib_dev; struct hns_roce_qp *hr_qp; unsigned long end; int cqe_cnt = 0; int npolled; int ret; int i; if (priv->handle->rinfo.reset_state == HNS_ROCE_STATE_RST_INIT || priv->handle->rinfo.instance_state == HNS_ROCE_STATE_INIT || hr_dev->state == HNS_ROCE_DEVICE_STATE_UNINIT) return; mutex_lock(&free_mr->mutex); for (i = 0; i < ARRAY_SIZE(free_mr->rsv_qp); i++) { hr_qp = free_mr->rsv_qp[i]; ret = free_mr_post_send_lp_wqe(hr_qp); if (ret) { ibdev_err_ratelimited(ibdev, ""failed to send wqe (qp:0x%lx) for free mr, ret = %d.\n"", hr_qp->qpn, ret); break; } cqe_cnt++; } end = msecs_to_jiffies(HNS_ROCE_V2_FREE_MR_TIMEOUT) + jiffies; while (cqe_cnt) { npolled = hns_roce_v2_poll_cq(&free_mr->rsv_cq->ib_cq, cqe_cnt, wc); if (npolled < 0) { ibdev_err_ratelimited(ibdev, ""failed to poll cqe for free mr, remain %d cqe.\n"", cqe_cnt); goto out; } if (time_after(jiffies, end)) { ibdev_err_ratelimited(ibdev, ""failed to poll cqe for free mr and timeout, remain %d cqe.\n"", cqe_cnt); goto out; } cqe_cnt -= npolled; } out: mutex_unlock(&free_mr->mutex); }"
781----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50058/bad/serial_core.c----uart_shutdown,"static void uart_shutdown(struct tty_struct *tty, struct uart_state *state) { struct uart_port *uport = uart_port_check(state); struct tty_port *port = &state->port; unsigned long flags = 0; char *xmit_buf = NULL; if (tty) set_bit(TTY_IO_ERROR, &tty->flags); if (tty_port_initialized(port)) { tty_port_set_initialized(port, 0); <S2SV_StartVul> if (uport && uart_console(uport) && tty) { <S2SV_EndVul> <S2SV_StartVul> uport->cons->cflag = tty->termios.c_cflag; <S2SV_EndVul> <S2SV_StartVul> uport->cons->ispeed = tty->termios.c_ispeed; <S2SV_EndVul> <S2SV_StartVul> uport->cons->ospeed = tty->termios.c_ospeed; <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> if (!tty || C_HUPCL(tty)) <S2SV_EndVul> <S2SV_StartVul> uart_port_dtr_rts(uport, 0); <S2SV_EndVul> uart_port_shutdown(port); <S2SV_StartVul> } <S2SV_EndVul> tty_port_set_suspended(port, 0); uart_port_lock(state, flags); xmit_buf = state->xmit.buf; state->xmit.buf = NULL; uart_port_unlock(uport, flags); if (xmit_buf) free_page((unsigned long)xmit_buf); }","- if (uport && uart_console(uport) && tty) {
- uport->cons->cflag = tty->termios.c_cflag;
- uport->cons->ispeed = tty->termios.c_ispeed;
- uport->cons->ospeed = tty->termios.c_ospeed;
- }
- if (!tty || C_HUPCL(tty))
- uart_port_dtr_rts(uport, 0);
- }
+ if (uport) {
+ if (uart_console(uport) && tty) {
+ uport->cons->cflag = tty->termios.c_cflag;
+ uport->cons->ispeed = tty->termios.c_ispeed;
+ uport->cons->ospeed = tty->termios.c_ospeed;
+ }
+ if (!tty || C_HUPCL(tty))
+ uart_port_dtr_rts(uport, 0);
+ }
+ }","static void uart_shutdown(struct tty_struct *tty, struct uart_state *state) { struct uart_port *uport = uart_port_check(state); struct tty_port *port = &state->port; unsigned long flags = 0; char *xmit_buf = NULL; if (tty) set_bit(TTY_IO_ERROR, &tty->flags); if (tty_port_initialized(port)) { tty_port_set_initialized(port, 0); if (uport) { if (uart_console(uport) && tty) { uport->cons->cflag = tty->termios.c_cflag; uport->cons->ispeed = tty->termios.c_ispeed; uport->cons->ospeed = tty->termios.c_ospeed; } if (!tty || C_HUPCL(tty)) uart_port_dtr_rts(uport, 0); } uart_port_shutdown(port); } tty_port_set_suspended(port, 0); uart_port_lock(state, flags); xmit_buf = state->xmit.buf; state->xmit.buf = NULL; uart_port_unlock(uport, flags); if (xmit_buf) free_page((unsigned long)xmit_buf); }"
595----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49917/bad/dcn30_hwseq.c----dcn30_init_hw,"void dcn30_init_hw(struct dc *dc) { struct abm **abms = dc->res_pool->multiple_abms; struct dce_hwseq *hws = dc->hwseq; struct dc_bios *dcb = dc->ctx->dc_bios; struct resource_pool *res_pool = dc->res_pool; int i; int edp_num; uint32_t backlight = MAX_BACKLIGHT_LEVEL; uint32_t user_level = MAX_BACKLIGHT_LEVEL; <S2SV_StartVul> if (dc->clk_mgr && dc->clk_mgr->funcs->init_clocks) <S2SV_EndVul> dc->clk_mgr->funcs->init_clocks(dc->clk_mgr); if (res_pool->dccg->funcs->dccg_init) res_pool->dccg->funcs->dccg_init(res_pool->dccg); if (!dcb->funcs->is_accelerated_mode(dcb)) { hws->funcs.bios_golden_init(dc); hws->funcs.disable_vga(dc->hwseq); } if (dc->debug.enable_mem_low_power.bits.dmcu) { if (dc->debug.disable_dmcu || dc->config.disable_dmcu) { REG_UPDATE(DMU_MEM_PWR_CNTL, DMCU_ERAM_MEM_PWR_FORCE, 3); } } if (dc->debug.enable_mem_low_power.bits.optc) { REG_SET_2(ODM_MEM_PWR_CTRL3, 0, ODM_MEM_UNASSIGNED_PWR_MODE, 3, ODM_MEM_VBLANK_PWR_MODE, 1); } if (dc->debug.enable_mem_low_power.bits.vga) { REG_UPDATE(MMHUBBUB_MEM_PWR_CNTL, VGA_MEM_PWR_FORCE, 1); } if (dc->ctx->dc_bios->fw_info_valid) { res_pool->ref_clocks.xtalin_clock_inKhz = dc->ctx->dc_bios->fw_info.pll_info.crystal_frequency; if (res_pool->hubbub) { (res_pool->dccg->funcs->get_dccg_ref_freq)(res_pool->dccg, dc->ctx->dc_bios->fw_info.pll_info.crystal_frequency, &res_pool->ref_clocks.dccg_ref_clock_inKhz); (res_pool->hubbub->funcs->get_dchub_ref_freq)(res_pool->hubbub, res_pool->ref_clocks.dccg_ref_clock_inKhz, &res_pool->ref_clocks.dchub_ref_clock_inKhz); } else { res_pool->ref_clocks.dccg_ref_clock_inKhz = res_pool->ref_clocks.xtalin_clock_inKhz; res_pool->ref_clocks.dchub_ref_clock_inKhz = res_pool->ref_clocks.xtalin_clock_inKhz; } } else ASSERT_CRITICAL(false); for (i = 0; i < dc->link_count; i++) { struct dc_link *link = dc->links[i]; link->link_enc->funcs->hw_init(link->link_enc); if (link->link_enc->funcs->is_dig_enabled && link->link_enc->funcs->is_dig_enabled(link->link_enc)) { link->link_status.link_active = true; if (link->link_enc->funcs->fec_is_active && link->link_enc->funcs->fec_is_active(link->link_enc)) link->fec_state = dc_link_fec_enabled; } } dc->link_srv->blank_all_dp_displays(dc); if (hws->funcs.enable_power_gating_plane) hws->funcs.enable_power_gating_plane(dc->hwseq, true); if (dcb->funcs->is_accelerated_mode(dcb) || !dc->config.seamless_boot_edp_requested) { hws->funcs.init_pipes(dc, dc->current_state); if (dc->res_pool->hubbub->funcs->allow_self_refresh_control) dc->res_pool->hubbub->funcs->allow_self_refresh_control(dc->res_pool->hubbub, !dc->res_pool->hubbub->ctx->dc->debug.disable_stutter); } if (!dc->config.seamless_boot_edp_requested) { struct dc_link *edp_links[MAX_NUM_EDP]; struct dc_link *edp_link = NULL; dc_get_edp_links(dc, edp_links, &edp_num); if (edp_num) edp_link = edp_links[0]; if (edp_link && edp_link->link_enc->funcs->is_dig_enabled && edp_link->link_enc->funcs->is_dig_enabled(edp_link->link_enc) && dc->hwss.edp_backlight_control && hws->funcs.power_down && dc->hwss.edp_power_control) { dc->hwss.edp_backlight_control(edp_link, false); hws->funcs.power_down(dc); dc->hwss.edp_power_control(edp_link, false); } else { for (i = 0; i < dc->link_count; i++) { struct dc_link *link = dc->links[i]; if (link->link_enc->funcs->is_dig_enabled && link->link_enc->funcs->is_dig_enabled(link->link_enc) && hws->funcs.power_down) { hws->funcs.power_down(dc); break; } } } } for (i = 0; i < res_pool->audio_count; i++) { struct audio *audio = res_pool->audios[i]; audio->funcs->hw_init(audio); } for (i = 0; i < dc->link_count; i++) { struct dc_link *link = dc->links[i]; if (link->panel_cntl) { backlight = link->panel_cntl->funcs->hw_init(link->panel_cntl); user_level = link->panel_cntl->stored_backlight_registers.USER_LEVEL; } } for (i = 0; i < dc->res_pool->pipe_count; i++) { if (abms[i] != NULL) abms[i]->funcs->abm_init(abms[i], backlight, user_level); } REG_WRITE(DIO_MEM_PWR_CTRL, 0); if (!dc->debug.disable_clock_gate) { REG_WRITE(DCCG_GATE_DISABLE_CNTL, 0); REG_WRITE(DCCG_GATE_DISABLE_CNTL2, 0); REG_UPDATE(DCFCLK_CNTL, DCFCLK_GATE_DIS, 0); } if (!dcb->funcs->is_accelerated_mode(dcb) && dc->res_pool->hubbub->funcs->init_watermarks) dc->res_pool->hubbub->funcs->init_watermarks(dc->res_pool->hubbub); <S2SV_StartVul> if (dc->clk_mgr->funcs->notify_wm_ranges) <S2SV_EndVul> dc->clk_mgr->funcs->notify_wm_ranges(dc->clk_mgr); <S2SV_StartVul> if (dc->clk_mgr->funcs->set_hard_max_memclk && !dc->clk_mgr->dc_mode_softmax_enabled) <S2SV_EndVul> dc->clk_mgr->funcs->set_hard_max_memclk(dc->clk_mgr); if (dc->res_pool->hubbub->funcs->force_pstate_change_control) dc->res_pool->hubbub->funcs->force_pstate_change_control( dc->res_pool->hubbub, false, false); if (dc->res_pool->hubbub->funcs->init_crb) dc->res_pool->hubbub->funcs->init_crb(dc->res_pool->hubbub); dc_dmub_srv_query_caps_cmd(dc->ctx->dmub_srv); dc->caps.dmub_caps.psr = dc->ctx->dmub_srv->dmub->feature_caps.psr; dc->caps.dmub_caps.mclk_sw = dc->ctx->dmub_srv->dmub->feature_caps.fw_assisted_mclk_switch_ver; }","- if (dc->clk_mgr && dc->clk_mgr->funcs->init_clocks)
- if (dc->clk_mgr->funcs->notify_wm_ranges)
- if (dc->clk_mgr->funcs->set_hard_max_memclk && !dc->clk_mgr->dc_mode_softmax_enabled)
+ if (dc->clk_mgr && dc->clk_mgr->funcs && dc->clk_mgr->funcs->init_clocks)
+ if (dc->clk_mgr && dc->clk_mgr->funcs && dc->clk_mgr->funcs->notify_wm_ranges)
+ if (dc->clk_mgr && dc->clk_mgr->funcs && dc->clk_mgr->funcs->set_hard_max_memclk &&
+ !dc->clk_mgr->dc_mode_softmax_enabled)","void dcn30_init_hw(struct dc *dc) { struct abm **abms = dc->res_pool->multiple_abms; struct dce_hwseq *hws = dc->hwseq; struct dc_bios *dcb = dc->ctx->dc_bios; struct resource_pool *res_pool = dc->res_pool; int i; int edp_num; uint32_t backlight = MAX_BACKLIGHT_LEVEL; uint32_t user_level = MAX_BACKLIGHT_LEVEL; if (dc->clk_mgr && dc->clk_mgr->funcs && dc->clk_mgr->funcs->init_clocks) dc->clk_mgr->funcs->init_clocks(dc->clk_mgr); if (res_pool->dccg->funcs->dccg_init) res_pool->dccg->funcs->dccg_init(res_pool->dccg); if (!dcb->funcs->is_accelerated_mode(dcb)) { hws->funcs.bios_golden_init(dc); hws->funcs.disable_vga(dc->hwseq); } if (dc->debug.enable_mem_low_power.bits.dmcu) { if (dc->debug.disable_dmcu || dc->config.disable_dmcu) { REG_UPDATE(DMU_MEM_PWR_CNTL, DMCU_ERAM_MEM_PWR_FORCE, 3); } } if (dc->debug.enable_mem_low_power.bits.optc) { REG_SET_2(ODM_MEM_PWR_CTRL3, 0, ODM_MEM_UNASSIGNED_PWR_MODE, 3, ODM_MEM_VBLANK_PWR_MODE, 1); } if (dc->debug.enable_mem_low_power.bits.vga) { REG_UPDATE(MMHUBBUB_MEM_PWR_CNTL, VGA_MEM_PWR_FORCE, 1); } if (dc->ctx->dc_bios->fw_info_valid) { res_pool->ref_clocks.xtalin_clock_inKhz = dc->ctx->dc_bios->fw_info.pll_info.crystal_frequency; if (res_pool->hubbub) { (res_pool->dccg->funcs->get_dccg_ref_freq)(res_pool->dccg, dc->ctx->dc_bios->fw_info.pll_info.crystal_frequency, &res_pool->ref_clocks.dccg_ref_clock_inKhz); (res_pool->hubbub->funcs->get_dchub_ref_freq)(res_pool->hubbub, res_pool->ref_clocks.dccg_ref_clock_inKhz, &res_pool->ref_clocks.dchub_ref_clock_inKhz); } else { res_pool->ref_clocks.dccg_ref_clock_inKhz = res_pool->ref_clocks.xtalin_clock_inKhz; res_pool->ref_clocks.dchub_ref_clock_inKhz = res_pool->ref_clocks.xtalin_clock_inKhz; } } else ASSERT_CRITICAL(false); for (i = 0; i < dc->link_count; i++) { struct dc_link *link = dc->links[i]; link->link_enc->funcs->hw_init(link->link_enc); if (link->link_enc->funcs->is_dig_enabled && link->link_enc->funcs->is_dig_enabled(link->link_enc)) { link->link_status.link_active = true; if (link->link_enc->funcs->fec_is_active && link->link_enc->funcs->fec_is_active(link->link_enc)) link->fec_state = dc_link_fec_enabled; } } dc->link_srv->blank_all_dp_displays(dc); if (hws->funcs.enable_power_gating_plane) hws->funcs.enable_power_gating_plane(dc->hwseq, true); if (dcb->funcs->is_accelerated_mode(dcb) || !dc->config.seamless_boot_edp_requested) { hws->funcs.init_pipes(dc, dc->current_state); if (dc->res_pool->hubbub->funcs->allow_self_refresh_control) dc->res_pool->hubbub->funcs->allow_self_refresh_control(dc->res_pool->hubbub, !dc->res_pool->hubbub->ctx->dc->debug.disable_stutter); } if (!dc->config.seamless_boot_edp_requested) { struct dc_link *edp_links[MAX_NUM_EDP]; struct dc_link *edp_link = NULL; dc_get_edp_links(dc, edp_links, &edp_num); if (edp_num) edp_link = edp_links[0]; if (edp_link && edp_link->link_enc->funcs->is_dig_enabled && edp_link->link_enc->funcs->is_dig_enabled(edp_link->link_enc) && dc->hwss.edp_backlight_control && hws->funcs.power_down && dc->hwss.edp_power_control) { dc->hwss.edp_backlight_control(edp_link, false); hws->funcs.power_down(dc); dc->hwss.edp_power_control(edp_link, false); } else { for (i = 0; i < dc->link_count; i++) { struct dc_link *link = dc->links[i]; if (link->link_enc->funcs->is_dig_enabled && link->link_enc->funcs->is_dig_enabled(link->link_enc) && hws->funcs.power_down) { hws->funcs.power_down(dc); break; } } } } for (i = 0; i < res_pool->audio_count; i++) { struct audio *audio = res_pool->audios[i]; audio->funcs->hw_init(audio); } for (i = 0; i < dc->link_count; i++) { struct dc_link *link = dc->links[i]; if (link->panel_cntl) { backlight = link->panel_cntl->funcs->hw_init(link->panel_cntl); user_level = link->panel_cntl->stored_backlight_registers.USER_LEVEL; } } for (i = 0; i < dc->res_pool->pipe_count; i++) { if (abms[i] != NULL) abms[i]->funcs->abm_init(abms[i], backlight, user_level); } REG_WRITE(DIO_MEM_PWR_CTRL, 0); if (!dc->debug.disable_clock_gate) { REG_WRITE(DCCG_GATE_DISABLE_CNTL, 0); REG_WRITE(DCCG_GATE_DISABLE_CNTL2, 0); REG_UPDATE(DCFCLK_CNTL, DCFCLK_GATE_DIS, 0); } if (!dcb->funcs->is_accelerated_mode(dcb) && dc->res_pool->hubbub->funcs->init_watermarks) dc->res_pool->hubbub->funcs->init_watermarks(dc->res_pool->hubbub); if (dc->clk_mgr && dc->clk_mgr->funcs && dc->clk_mgr->funcs->notify_wm_ranges) dc->clk_mgr->funcs->notify_wm_ranges(dc->clk_mgr); if (dc->clk_mgr && dc->clk_mgr->funcs && dc->clk_mgr->funcs->set_hard_max_memclk && !dc->clk_mgr->dc_mode_softmax_enabled) dc->clk_mgr->funcs->set_hard_max_memclk(dc->clk_mgr); if (dc->res_pool->hubbub->funcs->force_pstate_change_control) dc->res_pool->hubbub->funcs->force_pstate_change_control( dc->res_pool->hubbub, false, false); if (dc->res_pool->hubbub->funcs->init_crb) dc->res_pool->hubbub->funcs->init_crb(dc->res_pool->hubbub); dc_dmub_srv_query_caps_cmd(dc->ctx->dmub_srv); dc->caps.dmub_caps.psr = dc->ctx->dmub_srv->dmub->feature_caps.psr; dc->caps.dmub_caps.mclk_sw = dc->ctx->dmub_srv->dmub->feature_caps.fw_assisted_mclk_switch_ver; }"
227----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46705/bad/xe_mmio.c----mmio_fini,"static void mmio_fini(void *arg) { struct xe_device *xe = arg; pci_iounmap(to_pci_dev(xe->drm.dev), xe->mmio.regs); <S2SV_StartVul> if (xe->mem.vram.mapping) <S2SV_EndVul> <S2SV_StartVul> iounmap(xe->mem.vram.mapping); <S2SV_EndVul> <S2SV_StartVul> xe->mem.vram.mapping = NULL; <S2SV_EndVul> xe->mmio.regs = NULL; }","- if (xe->mem.vram.mapping)
- iounmap(xe->mem.vram.mapping);
- xe->mem.vram.mapping = NULL;
+ }","static void mmio_fini(void *arg) { struct xe_device *xe = arg; pci_iounmap(to_pci_dev(xe->drm.dev), xe->mmio.regs); xe->mmio.regs = NULL; }"
883----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50136/bad/eswitch.c----mlx5_eswitch_enable_locked,"int mlx5_eswitch_enable_locked(struct mlx5_eswitch *esw, int num_vfs) { int err; devl_assert_locked(priv_to_devlink(esw->dev)); if (!MLX5_CAP_ESW_FLOWTABLE_FDB(esw->dev, ft_support)) { esw_warn(esw->dev, ""FDB is not supported, aborting ...\n""); return -EOPNOTSUPP; } mlx5_eswitch_get_devlink_param(esw); err = mlx5_esw_acls_ns_init(esw); if (err) return err; mlx5_eswitch_update_num_of_vfs(esw, num_vfs); MLX5_NB_INIT(&esw->nb, eswitch_vport_event, NIC_VPORT_CHANGE); mlx5_eq_notifier_register(esw->dev, &esw->nb); if (esw->mode == MLX5_ESWITCH_LEGACY) { err = esw_legacy_enable(esw); } else { mlx5_rescan_drivers(esw->dev); err = esw_offloads_enable(esw); } if (err) <S2SV_StartVul> goto abort; <S2SV_EndVul> esw->fdb_table.flags |= MLX5_ESW_FDB_CREATED; mlx5_eswitch_event_handler_register(esw); esw_info(esw->dev, ""Enable: mode(%s), nvfs(%d), necvfs(%d), active vports(%d)\n"", esw->mode == MLX5_ESWITCH_LEGACY ? ""LEGACY"" : ""OFFLOADS"", esw->esw_funcs.num_vfs, esw->esw_funcs.num_ec_vfs, esw->enabled_vports); mlx5_esw_mode_change_notify(esw, esw->mode); return 0; <S2SV_StartVul> abort: <S2SV_EndVul> mlx5_esw_acls_ns_cleanup(esw); return err; }","- goto abort;
- abort:
+ goto err_esw_enable;
+ err_esw_enable:
+ mlx5_eq_notifier_unregister(esw->dev, &esw->nb);","int mlx5_eswitch_enable_locked(struct mlx5_eswitch *esw, int num_vfs) { int err; devl_assert_locked(priv_to_devlink(esw->dev)); if (!MLX5_CAP_ESW_FLOWTABLE_FDB(esw->dev, ft_support)) { esw_warn(esw->dev, ""FDB is not supported, aborting ...\n""); return -EOPNOTSUPP; } mlx5_eswitch_get_devlink_param(esw); err = mlx5_esw_acls_ns_init(esw); if (err) return err; mlx5_eswitch_update_num_of_vfs(esw, num_vfs); MLX5_NB_INIT(&esw->nb, eswitch_vport_event, NIC_VPORT_CHANGE); mlx5_eq_notifier_register(esw->dev, &esw->nb); if (esw->mode == MLX5_ESWITCH_LEGACY) { err = esw_legacy_enable(esw); } else { mlx5_rescan_drivers(esw->dev); err = esw_offloads_enable(esw); } if (err) goto err_esw_enable; esw->fdb_table.flags |= MLX5_ESW_FDB_CREATED; mlx5_eswitch_event_handler_register(esw); esw_info(esw->dev, ""Enable: mode(%s), nvfs(%d), necvfs(%d), active vports(%d)\n"", esw->mode == MLX5_ESWITCH_LEGACY ? ""LEGACY"" : ""OFFLOADS"", esw->esw_funcs.num_vfs, esw->esw_funcs.num_ec_vfs, esw->enabled_vports); mlx5_esw_mode_change_notify(esw, esw->mode); return 0; err_esw_enable: mlx5_eq_notifier_unregister(esw->dev, &esw->nb); mlx5_esw_acls_ns_cleanup(esw); return err; }"
741----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50017/bad/ident_map.c----ident_pud_init,"static int ident_pud_init(struct x86_mapping_info *info, pud_t *pud_page, unsigned long addr, unsigned long end) { unsigned long next; for (; addr < end; addr = next) { pud_t *pud = pud_page + pud_index(addr); pmd_t *pmd; next = (addr & PUD_MASK) + PUD_SIZE; if (next > end) next = end; <S2SV_StartVul> if (info->direct_gbpages) { <S2SV_EndVul> <S2SV_StartVul> pud_t pudval; <S2SV_EndVul> <S2SV_StartVul> if (pud_present(*pud)) <S2SV_EndVul> <S2SV_StartVul> continue; <S2SV_EndVul> <S2SV_StartVul> addr &= PUD_MASK; <S2SV_EndVul> pudval = __pud((addr - info->offset) | info->page_flag); set_pud(pud, pudval); <S2SV_StartVul> continue; <S2SV_EndVul> } if (pud_present(*pud)) { pmd = pmd_offset(pud, 0); ident_pmd_init(info, pmd, addr, next); continue; } pmd = (pmd_t *)info->alloc_pgt_page(info->context); if (!pmd) return -ENOMEM; ident_pmd_init(info, pmd, addr, next); set_pud(pud, __pud(__pa(pmd) | info->kernpg_flag)); } return 0; }","- if (info->direct_gbpages) {
- pud_t pudval;
- if (pud_present(*pud))
- continue;
- addr &= PUD_MASK;
- continue;
+ bool use_gbpage;
+ if (pud_leaf(*pud))
+ continue;
+ use_gbpage = info->direct_gbpages;
+ use_gbpage &= ((addr & ~PUD_MASK) == 0);
+ use_gbpage &= ((next & ~PUD_MASK) == 0);
+ use_gbpage &= !pud_present(*pud);
+ if (use_gbpage) {
+ pud_t pudval;
+ continue;","static int ident_pud_init(struct x86_mapping_info *info, pud_t *pud_page, unsigned long addr, unsigned long end) { unsigned long next; for (; addr < end; addr = next) { pud_t *pud = pud_page + pud_index(addr); pmd_t *pmd; bool use_gbpage; next = (addr & PUD_MASK) + PUD_SIZE; if (next > end) next = end; if (pud_leaf(*pud)) continue; use_gbpage = info->direct_gbpages; use_gbpage &= ((addr & ~PUD_MASK) == 0); use_gbpage &= ((next & ~PUD_MASK) == 0); use_gbpage &= !pud_present(*pud); if (use_gbpage) { pud_t pudval; pudval = __pud((addr - info->offset) | info->page_flag); set_pud(pud, pudval); continue; } if (pud_present(*pud)) { pmd = pmd_offset(pud, 0); ident_pmd_init(info, pmd, addr, next); continue; } pmd = (pmd_t *)info->alloc_pgt_page(info->context); if (!pmd) return -ENOMEM; ident_pmd_init(info, pmd, addr, next); set_pud(pud, __pud(__pa(pmd) | info->kernpg_flag)); } return 0; }"
1389----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56722/bad/hns_roce_hw_v2.c----free_mr_post_send_lp_wqe,"static int free_mr_post_send_lp_wqe(struct hns_roce_qp *hr_qp) { struct hns_roce_dev *hr_dev = to_hr_dev(hr_qp->ibqp.device); struct ib_device *ibdev = &hr_dev->ib_dev; const struct ib_send_wr *bad_wr; struct ib_rdma_wr rdma_wr = {}; struct ib_send_wr *send_wr; int ret; send_wr = &rdma_wr.wr; send_wr->opcode = IB_WR_RDMA_WRITE; ret = hns_roce_v2_post_send(&hr_qp->ibqp, send_wr, &bad_wr); if (ret) { <S2SV_StartVul> ibdev_err(ibdev, ""failed to post wqe for free mr, ret = %d.\n"", <S2SV_EndVul> <S2SV_StartVul> ret); <S2SV_EndVul> return ret; <S2SV_StartVul> } <S2SV_EndVul> return 0; }","- ibdev_err(ibdev, ""failed to post wqe for free mr, ret = %d.\n"",
- ret);
- }
+ ibdev_err_ratelimited(ibdev, ""failed to post wqe for free mr, ret = %d.\n"",
+ ret);","static int free_mr_post_send_lp_wqe(struct hns_roce_qp *hr_qp) { struct hns_roce_dev *hr_dev = to_hr_dev(hr_qp->ibqp.device); struct ib_device *ibdev = &hr_dev->ib_dev; const struct ib_send_wr *bad_wr; struct ib_rdma_wr rdma_wr = {}; struct ib_send_wr *send_wr; int ret; send_wr = &rdma_wr.wr; send_wr->opcode = IB_WR_RDMA_WRITE; ret = hns_roce_v2_post_send(&hr_qp->ibqp, send_wr, &bad_wr); if (ret) { ibdev_err_ratelimited(ibdev, ""failed to post wqe for free mr, ret = %d.\n"", ret); return ret; } return 0; }"
220----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46700/bad/mes_v11_0.c----mes_v11_0_submit_pkt_and_poll_completion,"static int mes_v11_0_submit_pkt_and_poll_completion(struct amdgpu_mes *mes, void *pkt, int size, int api_status_off) { union MESAPI__QUERY_MES_STATUS mes_status_pkt; signed long timeout = 3000000; struct amdgpu_device *adev = mes->adev; struct amdgpu_ring *ring = &mes->ring; struct MES_API_STATUS *api_status; union MESAPI__MISC *x_pkt = pkt; const char *op_str, *misc_op_str; unsigned long flags; u64 status_gpu_addr; <S2SV_StartVul> u32 status_offset; <S2SV_EndVul> u64 *status_ptr; signed long r; int ret; if (x_pkt->header.opcode >= MES_SCH_API_MAX) return -EINVAL; if (amdgpu_emu_mode) { timeout *= 100; } else if (amdgpu_sriov_vf(adev)) { timeout = 15 * 600 * 1000; } ret = amdgpu_device_wb_get(adev, &status_offset); if (ret) return ret; status_gpu_addr = adev->wb.gpu_addr + (status_offset * 4); status_ptr = (u64 *)&adev->wb.wb[status_offset]; *status_ptr = 0; spin_lock_irqsave(&mes->ring_lock, flags); r = amdgpu_ring_alloc(ring, (size + sizeof(mes_status_pkt)) / 4); if (r) goto error_unlock_free; api_status = (struct MES_API_STATUS *)((char *)pkt + api_status_off); api_status->api_completion_fence_addr = status_gpu_addr; api_status->api_completion_fence_value = 1; amdgpu_ring_write_multiple(ring, pkt, size / 4); memset(&mes_status_pkt, 0, sizeof(mes_status_pkt)); mes_status_pkt.header.type = MES_API_TYPE_SCHEDULER; mes_status_pkt.header.opcode = MES_SCH_API_QUERY_SCHEDULER_STATUS; mes_status_pkt.header.dwsize = API_FRAME_SIZE_IN_DWORDS; mes_status_pkt.api_status.api_completion_fence_addr = ring->fence_drv.gpu_addr; <S2SV_StartVul> mes_status_pkt.api_status.api_completion_fence_value = <S2SV_EndVul> <S2SV_StartVul> ++ring->fence_drv.sync_seq; <S2SV_EndVul> amdgpu_ring_write_multiple(ring, &mes_status_pkt, sizeof(mes_status_pkt) / 4); amdgpu_ring_commit(ring); spin_unlock_irqrestore(&mes->ring_lock, flags); op_str = mes_v11_0_get_op_string(x_pkt); misc_op_str = mes_v11_0_get_misc_op_string(x_pkt); if (misc_op_str) dev_dbg(adev->dev, ""MES msg=%s (%s) was emitted\n"", op_str, misc_op_str); else if (op_str) dev_dbg(adev->dev, ""MES msg=%s was emitted\n"", op_str); else dev_dbg(adev->dev, ""MES msg=%d was emitted\n"", x_pkt->header.opcode); <S2SV_StartVul> r = amdgpu_fence_wait_polling(ring, ring->fence_drv.sync_seq, timeout); <S2SV_EndVul> if (r < 1 || !*status_ptr) { if (misc_op_str) dev_err(adev->dev, ""MES failed to respond to msg=%s (%s)\n"", op_str, misc_op_str); else if (op_str) dev_err(adev->dev, ""MES failed to respond to msg=%s\n"", op_str); else dev_err(adev->dev, ""MES failed to respond to msg=%d\n"", x_pkt->header.opcode); while (halt_if_hws_hang) schedule(); r = -ETIMEDOUT; goto error_wb_free; } amdgpu_device_wb_free(adev, status_offset); return 0; error_unlock_free: spin_unlock_irqrestore(&mes->ring_lock, flags); error_wb_free: amdgpu_device_wb_free(adev, status_offset); return r; }","- u32 status_offset;
- mes_status_pkt.api_status.api_completion_fence_value =
- ++ring->fence_drv.sync_seq;
- r = amdgpu_fence_wait_polling(ring, ring->fence_drv.sync_seq, timeout);
+ u32 seq, status_offset;
+ seq = ++ring->fence_drv.sync_seq;
+ r = amdgpu_fence_wait_polling(ring,
+ seq - ring->fence_drv.num_fences_mask,
+ timeout);
+ if (r < 1)
+ goto error_undo;
+ mes_status_pkt.api_status.api_completion_fence_value = seq;
+ r = amdgpu_fence_wait_polling(ring, seq, timeout);
+ error_undo:
+ dev_err(adev->dev, ""MES ring buffer is full.\n"");
+ amdgpu_ring_undo(ring);","static int mes_v11_0_submit_pkt_and_poll_completion(struct amdgpu_mes *mes, void *pkt, int size, int api_status_off) { union MESAPI__QUERY_MES_STATUS mes_status_pkt; signed long timeout = 3000000; struct amdgpu_device *adev = mes->adev; struct amdgpu_ring *ring = &mes->ring; struct MES_API_STATUS *api_status; union MESAPI__MISC *x_pkt = pkt; const char *op_str, *misc_op_str; unsigned long flags; u64 status_gpu_addr; u32 seq, status_offset; u64 *status_ptr; signed long r; int ret; if (x_pkt->header.opcode >= MES_SCH_API_MAX) return -EINVAL; if (amdgpu_emu_mode) { timeout *= 100; } else if (amdgpu_sriov_vf(adev)) { timeout = 15 * 600 * 1000; } ret = amdgpu_device_wb_get(adev, &status_offset); if (ret) return ret; status_gpu_addr = adev->wb.gpu_addr + (status_offset * 4); status_ptr = (u64 *)&adev->wb.wb[status_offset]; *status_ptr = 0; spin_lock_irqsave(&mes->ring_lock, flags); r = amdgpu_ring_alloc(ring, (size + sizeof(mes_status_pkt)) / 4); if (r) goto error_unlock_free; seq = ++ring->fence_drv.sync_seq; r = amdgpu_fence_wait_polling(ring, seq - ring->fence_drv.num_fences_mask, timeout); if (r < 1) goto error_undo; api_status = (struct MES_API_STATUS *)((char *)pkt + api_status_off); api_status->api_completion_fence_addr = status_gpu_addr; api_status->api_completion_fence_value = 1; amdgpu_ring_write_multiple(ring, pkt, size / 4); memset(&mes_status_pkt, 0, sizeof(mes_status_pkt)); mes_status_pkt.header.type = MES_API_TYPE_SCHEDULER; mes_status_pkt.header.opcode = MES_SCH_API_QUERY_SCHEDULER_STATUS; mes_status_pkt.header.dwsize = API_FRAME_SIZE_IN_DWORDS; mes_status_pkt.api_status.api_completion_fence_addr = ring->fence_drv.gpu_addr; mes_status_pkt.api_status.api_completion_fence_value = seq; amdgpu_ring_write_multiple(ring, &mes_status_pkt, sizeof(mes_status_pkt) / 4); amdgpu_ring_commit(ring); spin_unlock_irqrestore(&mes->ring_lock, flags); op_str = mes_v11_0_get_op_string(x_pkt); misc_op_str = mes_v11_0_get_misc_op_string(x_pkt); if (misc_op_str) dev_dbg(adev->dev, ""MES msg=%s (%s) was emitted\n"", op_str, misc_op_str); else if (op_str) dev_dbg(adev->dev, ""MES msg=%s was emitted\n"", op_str); else dev_dbg(adev->dev, ""MES msg=%d was emitted\n"", x_pkt->header.opcode); r = amdgpu_fence_wait_polling(ring, seq, timeout); if (r < 1 || !*status_ptr) { if (misc_op_str) dev_err(adev->dev, ""MES failed to respond to msg=%s (%s)\n"", op_str, misc_op_str); else if (op_str) dev_err(adev->dev, ""MES failed to respond to msg=%s\n"", op_str); else dev_err(adev->dev, ""MES failed to respond to msg=%d\n"", x_pkt->header.opcode); while (halt_if_hws_hang) schedule(); r = -ETIMEDOUT; goto error_wb_free; } amdgpu_device_wb_free(adev, status_offset); return 0; error_undo: dev_err(adev->dev, ""MES ring buffer is full.\n""); amdgpu_ring_undo(ring); error_unlock_free: spin_unlock_irqrestore(&mes->ring_lock, flags); error_wb_free: amdgpu_device_wb_free(adev, status_offset); return r; }"
1263----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56544/bad/udmabuf.c----vmap_udmabuf,"static int vmap_udmabuf(struct dma_buf *buf, struct iosys_map *map) { struct udmabuf *ubuf = buf->priv; struct page **pages; void *vaddr; pgoff_t pg; dma_resv_assert_held(buf->resv); <S2SV_StartVul> pages = kmalloc_array(ubuf->pagecount, sizeof(*pages), GFP_KERNEL); <S2SV_EndVul> if (!pages) return -ENOMEM; for (pg = 0; pg < ubuf->pagecount; pg++) pages[pg] = &ubuf->folios[pg]->page; vaddr = vm_map_ram(pages, ubuf->pagecount, -1); <S2SV_StartVul> kfree(pages); <S2SV_EndVul> if (!vaddr) return -EINVAL; iosys_map_set_vaddr(map, vaddr); return 0; }","- pages = kmalloc_array(ubuf->pagecount, sizeof(*pages), GFP_KERNEL);
- kfree(pages);
+ pages = kvmalloc_array(ubuf->pagecount, sizeof(*pages), GFP_KERNEL);
+ kvfree(pages);","static int vmap_udmabuf(struct dma_buf *buf, struct iosys_map *map) { struct udmabuf *ubuf = buf->priv; struct page **pages; void *vaddr; pgoff_t pg; dma_resv_assert_held(buf->resv); pages = kvmalloc_array(ubuf->pagecount, sizeof(*pages), GFP_KERNEL); if (!pages) return -ENOMEM; for (pg = 0; pg < ubuf->pagecount; pg++) pages[pg] = &ubuf->folios[pg]->page; vaddr = vm_map_ram(pages, ubuf->pagecount, -1); kvfree(pages); if (!vaddr) return -EINVAL; iosys_map_set_vaddr(map, vaddr); return 0; }"
1105----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53085/bad/tpm-interface.c----tpm_get_random,"int tpm_get_random(struct tpm_chip *chip, u8 *out, size_t max) { int rc; if (!out || max > TPM_MAX_RNG_DATA) return -EINVAL; chip = tpm_find_get_ops(chip); if (!chip) return -ENODEV; if (chip->flags & TPM_CHIP_FLAG_TPM2) rc = tpm2_get_random(chip, out, max); else rc = tpm1_get_random(chip, out, max); <S2SV_StartVul> tpm_put_ops(chip); <S2SV_EndVul> return rc; <S2SV_StartVul> } <S2SV_EndVul>","- tpm_put_ops(chip);
- }
+ if (chip->flags & TPM_CHIP_FLAG_SUSPENDED) {
+ rc = 0;
+ goto out;
+ }
+ out:
+ tpm_put_ops(chip);
+ }","int tpm_get_random(struct tpm_chip *chip, u8 *out, size_t max) { int rc; if (!out || max > TPM_MAX_RNG_DATA) return -EINVAL; chip = tpm_find_get_ops(chip); if (!chip) return -ENODEV; if (chip->flags & TPM_CHIP_FLAG_SUSPENDED) { rc = 0; goto out; } if (chip->flags & TPM_CHIP_FLAG_TPM2) rc = tpm2_get_random(chip, out, max); else rc = tpm1_get_random(chip, out, max); out: tpm_put_ops(chip); return rc; }"
871----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50129/bad/pse_core.c----pse_release_pis,static void pse_release_pis(struct pse_controller_dev *pcdev) { int i; <S2SV_StartVul> for (i = 0; i <= pcdev->nr_lines; i++) { <S2SV_EndVul> of_node_put(pcdev->pi[i].pairset[0].np); of_node_put(pcdev->pi[i].pairset[1].np); of_node_put(pcdev->pi[i].np); } kfree(pcdev->pi); },"- for (i = 0; i <= pcdev->nr_lines; i++) {
+ for (i = 0; i < pcdev->nr_lines; i++) {",static void pse_release_pis(struct pse_controller_dev *pcdev) { int i; for (i = 0; i < pcdev->nr_lines; i++) { of_node_put(pcdev->pi[i].pairset[0].np); of_node_put(pcdev->pi[i].pairset[1].np); of_node_put(pcdev->pi[i].np); } kfree(pcdev->pi); }
1132----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53104/bad/uvc_driver.c----uvc_parse_format,"static int uvc_parse_format(struct uvc_device *dev, struct uvc_streaming *streaming, struct uvc_format *format, struct uvc_frame *frames, u32 **intervals, const unsigned char *buffer, int buflen) { struct usb_interface *intf = streaming->intf; struct usb_host_interface *alts = intf->cur_altsetting; const struct uvc_format_desc *fmtdesc; struct uvc_frame *frame; const unsigned char *start = buffer; unsigned int width_multiplier = 1; unsigned int interval; unsigned int i, n; u8 ftype; format->type = buffer[2]; format->index = buffer[3]; format->frames = frames; switch (buffer[2]) { case UVC_VS_FORMAT_UNCOMPRESSED: case UVC_VS_FORMAT_FRAME_BASED: n = buffer[2] == UVC_VS_FORMAT_UNCOMPRESSED ? 27 : 28; if (buflen < n) { uvc_dbg(dev, DESCR, ""device %d videostreaming interface %d FORMAT error\n"", dev->udev->devnum, alts->desc.bInterfaceNumber); return -EINVAL; } fmtdesc = uvc_format_by_guid(&buffer[5]); if (!fmtdesc) { dev_info(&streaming->intf->dev, ""Unknown video format %pUl\n"", &buffer[5]); return 0; } format->fcc = fmtdesc->fcc; format->bpp = buffer[21]; if (dev->quirks & UVC_QUIRK_FORCE_Y8) { if (format->fcc == V4L2_PIX_FMT_YUYV) { format->fcc = V4L2_PIX_FMT_GREY; format->bpp = 8; width_multiplier = 2; } } if (dev->quirks & UVC_QUIRK_FORCE_BPP) { const struct v4l2_format_info *info = v4l2_format_info(format->fcc); if (info) { unsigned int div = info->hdiv * info->vdiv; n = info->bpp[0] * div; for (i = 1; i < info->comp_planes; i++) n += info->bpp[i]; format->bpp = DIV_ROUND_UP(8 * n, div); } } if (buffer[2] == UVC_VS_FORMAT_UNCOMPRESSED) { ftype = UVC_VS_FRAME_UNCOMPRESSED; } else { ftype = UVC_VS_FRAME_FRAME_BASED; if (buffer[27]) format->flags = UVC_FMT_FLAG_COMPRESSED; } break; case UVC_VS_FORMAT_MJPEG: if (buflen < 11) { uvc_dbg(dev, DESCR, ""device %d videostreaming interface %d FORMAT error\n"", dev->udev->devnum, alts->desc.bInterfaceNumber); return -EINVAL; } format->fcc = V4L2_PIX_FMT_MJPEG; format->flags = UVC_FMT_FLAG_COMPRESSED; format->bpp = 0; ftype = UVC_VS_FRAME_MJPEG; break; case UVC_VS_FORMAT_DV: if (buflen < 9) { uvc_dbg(dev, DESCR, ""device %d videostreaming interface %d FORMAT error\n"", dev->udev->devnum, alts->desc.bInterfaceNumber); return -EINVAL; } if ((buffer[8] & 0x7f) > 2) { uvc_dbg(dev, DESCR, ""device %d videostreaming interface %d: unknown DV format %u\n"", dev->udev->devnum, alts->desc.bInterfaceNumber, buffer[8]); return -EINVAL; } format->fcc = V4L2_PIX_FMT_DV; format->flags = UVC_FMT_FLAG_COMPRESSED | UVC_FMT_FLAG_STREAM; format->bpp = 0; ftype = 0; frame = &frames[0]; memset(frame, 0, sizeof(*frame)); frame->bFrameIntervalType = 1; frame->dwDefaultFrameInterval = 1; frame->dwFrameInterval = *intervals; *(*intervals)++ = 1; format->nframes = 1; break; case UVC_VS_FORMAT_MPEG2TS: case UVC_VS_FORMAT_STREAM_BASED: default: uvc_dbg(dev, DESCR, ""device %d videostreaming interface %d unsupported format %u\n"", dev->udev->devnum, alts->desc.bInterfaceNumber, buffer[2]); return -EINVAL; } uvc_dbg(dev, DESCR, ""Found format %p4cc"", &format->fcc); buflen -= buffer[0]; buffer += buffer[0]; <S2SV_StartVul> while (buflen > 2 && buffer[1] == USB_DT_CS_INTERFACE && <S2SV_EndVul> buffer[2] == ftype) { unsigned int maxIntervalIndex; frame = &frames[format->nframes]; if (ftype != UVC_VS_FRAME_FRAME_BASED) n = buflen > 25 ? buffer[25] : 0; else n = buflen > 21 ? buffer[21] : 0; n = n ? n : 3; if (buflen < 26 + 4*n) { uvc_dbg(dev, DESCR, ""device %d videostreaming interface %d FRAME error\n"", dev->udev->devnum, alts->desc.bInterfaceNumber); return -EINVAL; } frame->bFrameIndex = buffer[3]; frame->bmCapabilities = buffer[4]; frame->wWidth = get_unaligned_le16(&buffer[5]) * width_multiplier; frame->wHeight = get_unaligned_le16(&buffer[7]); frame->dwMinBitRate = get_unaligned_le32(&buffer[9]); frame->dwMaxBitRate = get_unaligned_le32(&buffer[13]); if (ftype != UVC_VS_FRAME_FRAME_BASED) { frame->dwMaxVideoFrameBufferSize = get_unaligned_le32(&buffer[17]); frame->dwDefaultFrameInterval = get_unaligned_le32(&buffer[21]); frame->bFrameIntervalType = buffer[25]; } else { frame->dwMaxVideoFrameBufferSize = 0; frame->dwDefaultFrameInterval = get_unaligned_le32(&buffer[17]); frame->bFrameIntervalType = buffer[21]; } frame->dwFrameInterval = *intervals; for (i = 0; i < n; ++i) { interval = get_unaligned_le32(&buffer[26+4*i]); (*intervals)[i] = interval ? interval : 1; } if (!(format->flags & UVC_FMT_FLAG_COMPRESSED)) frame->dwMaxVideoFrameBufferSize = format->bpp * frame->wWidth * frame->wHeight / 8; maxIntervalIndex = frame->bFrameIntervalType ? n - 1 : 1; frame->dwDefaultFrameInterval = clamp(frame->dwDefaultFrameInterval, frame->dwFrameInterval[0], frame->dwFrameInterval[maxIntervalIndex]); if (dev->quirks & UVC_QUIRK_RESTRICT_FRAME_RATE) { frame->bFrameIntervalType = 1; (*intervals)[0] = frame->dwDefaultFrameInterval; } uvc_dbg(dev, DESCR, ""- %ux%u (%u.%u fps)\n"", frame->wWidth, frame->wHeight, 10000000 / frame->dwDefaultFrameInterval, (100000000 / frame->dwDefaultFrameInterval) % 10); format->nframes++; *intervals += n; buflen -= buffer[0]; buffer += buffer[0]; } if (buflen > 2 && buffer[1] == USB_DT_CS_INTERFACE && buffer[2] == UVC_VS_STILL_IMAGE_FRAME) { buflen -= buffer[0]; buffer += buffer[0]; } if (buflen > 2 && buffer[1] == USB_DT_CS_INTERFACE && buffer[2] == UVC_VS_COLORFORMAT) { if (buflen < 6) { uvc_dbg(dev, DESCR, ""device %d videostreaming interface %d COLORFORMAT error\n"", dev->udev->devnum, alts->desc.bInterfaceNumber); return -EINVAL; } format->colorspace = uvc_colorspace(buffer[3]); format->xfer_func = uvc_xfer_func(buffer[4]); format->ycbcr_enc = uvc_ycbcr_enc(buffer[5]); buflen -= buffer[0]; buffer += buffer[0]; } else { format->colorspace = V4L2_COLORSPACE_SRGB; } return buffer - start; }","- while (buflen > 2 && buffer[1] == USB_DT_CS_INTERFACE &&
+ while (ftype && buflen > 2 && buffer[1] == USB_DT_CS_INTERFACE &&","static int uvc_parse_format(struct uvc_device *dev, struct uvc_streaming *streaming, struct uvc_format *format, struct uvc_frame *frames, u32 **intervals, const unsigned char *buffer, int buflen) { struct usb_interface *intf = streaming->intf; struct usb_host_interface *alts = intf->cur_altsetting; const struct uvc_format_desc *fmtdesc; struct uvc_frame *frame; const unsigned char *start = buffer; unsigned int width_multiplier = 1; unsigned int interval; unsigned int i, n; u8 ftype; format->type = buffer[2]; format->index = buffer[3]; format->frames = frames; switch (buffer[2]) { case UVC_VS_FORMAT_UNCOMPRESSED: case UVC_VS_FORMAT_FRAME_BASED: n = buffer[2] == UVC_VS_FORMAT_UNCOMPRESSED ? 27 : 28; if (buflen < n) { uvc_dbg(dev, DESCR, ""device %d videostreaming interface %d FORMAT error\n"", dev->udev->devnum, alts->desc.bInterfaceNumber); return -EINVAL; } fmtdesc = uvc_format_by_guid(&buffer[5]); if (!fmtdesc) { dev_info(&streaming->intf->dev, ""Unknown video format %pUl\n"", &buffer[5]); return 0; } format->fcc = fmtdesc->fcc; format->bpp = buffer[21]; if (dev->quirks & UVC_QUIRK_FORCE_Y8) { if (format->fcc == V4L2_PIX_FMT_YUYV) { format->fcc = V4L2_PIX_FMT_GREY; format->bpp = 8; width_multiplier = 2; } } if (dev->quirks & UVC_QUIRK_FORCE_BPP) { const struct v4l2_format_info *info = v4l2_format_info(format->fcc); if (info) { unsigned int div = info->hdiv * info->vdiv; n = info->bpp[0] * div; for (i = 1; i < info->comp_planes; i++) n += info->bpp[i]; format->bpp = DIV_ROUND_UP(8 * n, div); } } if (buffer[2] == UVC_VS_FORMAT_UNCOMPRESSED) { ftype = UVC_VS_FRAME_UNCOMPRESSED; } else { ftype = UVC_VS_FRAME_FRAME_BASED; if (buffer[27]) format->flags = UVC_FMT_FLAG_COMPRESSED; } break; case UVC_VS_FORMAT_MJPEG: if (buflen < 11) { uvc_dbg(dev, DESCR, ""device %d videostreaming interface %d FORMAT error\n"", dev->udev->devnum, alts->desc.bInterfaceNumber); return -EINVAL; } format->fcc = V4L2_PIX_FMT_MJPEG; format->flags = UVC_FMT_FLAG_COMPRESSED; format->bpp = 0; ftype = UVC_VS_FRAME_MJPEG; break; case UVC_VS_FORMAT_DV: if (buflen < 9) { uvc_dbg(dev, DESCR, ""device %d videostreaming interface %d FORMAT error\n"", dev->udev->devnum, alts->desc.bInterfaceNumber); return -EINVAL; } if ((buffer[8] & 0x7f) > 2) { uvc_dbg(dev, DESCR, ""device %d videostreaming interface %d: unknown DV format %u\n"", dev->udev->devnum, alts->desc.bInterfaceNumber, buffer[8]); return -EINVAL; } format->fcc = V4L2_PIX_FMT_DV; format->flags = UVC_FMT_FLAG_COMPRESSED | UVC_FMT_FLAG_STREAM; format->bpp = 0; ftype = 0; frame = &frames[0]; memset(frame, 0, sizeof(*frame)); frame->bFrameIntervalType = 1; frame->dwDefaultFrameInterval = 1; frame->dwFrameInterval = *intervals; *(*intervals)++ = 1; format->nframes = 1; break; case UVC_VS_FORMAT_MPEG2TS: case UVC_VS_FORMAT_STREAM_BASED: default: uvc_dbg(dev, DESCR, ""device %d videostreaming interface %d unsupported format %u\n"", dev->udev->devnum, alts->desc.bInterfaceNumber, buffer[2]); return -EINVAL; } uvc_dbg(dev, DESCR, ""Found format %p4cc"", &format->fcc); buflen -= buffer[0]; buffer += buffer[0]; while (ftype && buflen > 2 && buffer[1] == USB_DT_CS_INTERFACE && buffer[2] == ftype) { unsigned int maxIntervalIndex; frame = &frames[format->nframes]; if (ftype != UVC_VS_FRAME_FRAME_BASED) n = buflen > 25 ? buffer[25] : 0; else n = buflen > 21 ? buffer[21] : 0; n = n ? n : 3; if (buflen < 26 + 4*n) { uvc_dbg(dev, DESCR, ""device %d videostreaming interface %d FRAME error\n"", dev->udev->devnum, alts->desc.bInterfaceNumber); return -EINVAL; } frame->bFrameIndex = buffer[3]; frame->bmCapabilities = buffer[4]; frame->wWidth = get_unaligned_le16(&buffer[5]) * width_multiplier; frame->wHeight = get_unaligned_le16(&buffer[7]); frame->dwMinBitRate = get_unaligned_le32(&buffer[9]); frame->dwMaxBitRate = get_unaligned_le32(&buffer[13]); if (ftype != UVC_VS_FRAME_FRAME_BASED) { frame->dwMaxVideoFrameBufferSize = get_unaligned_le32(&buffer[17]); frame->dwDefaultFrameInterval = get_unaligned_le32(&buffer[21]); frame->bFrameIntervalType = buffer[25]; } else { frame->dwMaxVideoFrameBufferSize = 0; frame->dwDefaultFrameInterval = get_unaligned_le32(&buffer[17]); frame->bFrameIntervalType = buffer[21]; } frame->dwFrameInterval = *intervals; for (i = 0; i < n; ++i) { interval = get_unaligned_le32(&buffer[26+4*i]); (*intervals)[i] = interval ? interval : 1; } if (!(format->flags & UVC_FMT_FLAG_COMPRESSED)) frame->dwMaxVideoFrameBufferSize = format->bpp * frame->wWidth * frame->wHeight / 8; maxIntervalIndex = frame->bFrameIntervalType ? n - 1 : 1; frame->dwDefaultFrameInterval = clamp(frame->dwDefaultFrameInterval, frame->dwFrameInterval[0], frame->dwFrameInterval[maxIntervalIndex]); if (dev->quirks & UVC_QUIRK_RESTRICT_FRAME_RATE) { frame->bFrameIntervalType = 1; (*intervals)[0] = frame->dwDefaultFrameInterval; } uvc_dbg(dev, DESCR, ""- %ux%u (%u.%u fps)\n"", frame->wWidth, frame->wHeight, 10000000 / frame->dwDefaultFrameInterval, (100000000 / frame->dwDefaultFrameInterval) % 10); format->nframes++; *intervals += n; buflen -= buffer[0]; buffer += buffer[0]; } if (buflen > 2 && buffer[1] == USB_DT_CS_INTERFACE && buffer[2] == UVC_VS_STILL_IMAGE_FRAME) { buflen -= buffer[0]; buffer += buffer[0]; } if (buflen > 2 && buffer[1] == USB_DT_CS_INTERFACE && buffer[2] == UVC_VS_COLORFORMAT) { if (buflen < 6) { uvc_dbg(dev, DESCR, ""device %d videostreaming interface %d COLORFORMAT error\n"", dev->udev->devnum, alts->desc.bInterfaceNumber); return -EINVAL; } format->colorspace = uvc_colorspace(buffer[3]); format->xfer_func = uvc_xfer_func(buffer[4]); format->ycbcr_enc = uvc_ycbcr_enc(buffer[5]); buflen -= buffer[0]; buffer += buffer[0]; } else { format->colorspace = V4L2_COLORSPACE_SRGB; } return buffer - start; }"
967----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50210/bad/posix-clock.c----pc_clock_settime,"static int pc_clock_settime(clockid_t id, const struct timespec64 *ts) { struct posix_clock_desc cd; int err; err = get_clock_desc(id, &cd); if (err) return err; if ((cd.fp->f_mode & FMODE_WRITE) == 0) { err = -EACCES; goto out; } <S2SV_StartVul> if (!timespec64_valid_strict(ts)) <S2SV_EndVul> <S2SV_StartVul> return -EINVAL; <S2SV_EndVul> if (cd.clk->ops.clock_settime) err = cd.clk->ops.clock_settime(cd.clk, ts); else err = -EOPNOTSUPP; out: put_clock_desc(&cd); return err; }","- if (!timespec64_valid_strict(ts))
- return -EINVAL;
+ if (!timespec64_valid_strict(ts))
+ return -EINVAL;","static int pc_clock_settime(clockid_t id, const struct timespec64 *ts) { struct posix_clock_desc cd; int err; if (!timespec64_valid_strict(ts)) return -EINVAL; err = get_clock_desc(id, &cd); if (err) return err; if ((cd.fp->f_mode & FMODE_WRITE) == 0) { err = -EACCES; goto out; } if (cd.clk->ops.clock_settime) err = cd.clk->ops.clock_settime(cd.clk, ts); else err = -EOPNOTSUPP; out: put_clock_desc(&cd); return err; }"
419----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47660/bad/fsnotify.c----__fsnotify_update_child_dentry_flags,"void __fsnotify_update_child_dentry_flags(struct inode *inode) { struct dentry *alias; int watched; if (!S_ISDIR(inode->i_mode)) return; watched = fsnotify_inode_watches_children(inode); spin_lock(&inode->i_lock); hlist_for_each_entry(alias, &inode->i_dentry, d_u.d_alias) { struct dentry *child; spin_lock(&alias->d_lock); list_for_each_entry(child, &alias->d_subdirs, d_child) { if (!child->d_inode) continue; spin_lock_nested(&child->d_lock, DENTRY_D_LOCK_NESTED); <S2SV_StartVul> if (watched) <S2SV_EndVul> <S2SV_StartVul> child->d_flags |= DCACHE_FSNOTIFY_PARENT_WATCHED; <S2SV_EndVul> <S2SV_StartVul> else <S2SV_EndVul> <S2SV_StartVul> child->d_flags &= ~DCACHE_FSNOTIFY_PARENT_WATCHED; <S2SV_EndVul> spin_unlock(&child->d_lock); } spin_unlock(&alias->d_lock); } spin_unlock(&inode->i_lock); }","- if (watched)
- child->d_flags |= DCACHE_FSNOTIFY_PARENT_WATCHED;
- else
- child->d_flags &= ~DCACHE_FSNOTIFY_PARENT_WATCHED;","#include <linux/dcache.h> #include <linux/fs.h> #include <linux/gfp.h> #include <linux/init.h> #include <linux/module.h> #include <linux/mount.h> #include <linux/srcu.h> #include <linux/fsnotify_backend.h> #include ""fsnotify.h"" void __fsnotify_inode_delete(struct inode *inode) { fsnotify_clear_marks_by_inode(inode); }"
1246----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-54683/bad/xt_IDLETIMER.c----idletimer_tg_destroy_v1,"static void idletimer_tg_destroy_v1(const struct xt_tgdtor_param *par) { const struct idletimer_tg_info_v1 *info = par->targinfo; pr_debug(""destroy targinfo %s\n"", info->label); mutex_lock(&list_mutex); <S2SV_StartVul> if (--info->timer->refcnt == 0) { <S2SV_EndVul> <S2SV_StartVul> pr_debug(""deleting timer %s\n"", info->label); <S2SV_EndVul> <S2SV_StartVul> list_del(&info->timer->entry); <S2SV_EndVul> <S2SV_StartVul> if (info->timer->timer_type & XT_IDLETIMER_ALARM) { <S2SV_EndVul> <S2SV_StartVul> alarm_cancel(&info->timer->alarm); <S2SV_EndVul> <S2SV_StartVul> } else { <S2SV_EndVul> <S2SV_StartVul> timer_shutdown_sync(&info->timer->timer); <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> cancel_work_sync(&info->timer->work); <S2SV_EndVul> <S2SV_StartVul> sysfs_remove_file(idletimer_tg_kobj, &info->timer->attr.attr); <S2SV_EndVul> <S2SV_StartVul> kfree(info->timer->attr.attr.name); <S2SV_EndVul> <S2SV_StartVul> kfree(info->timer); <S2SV_EndVul> <S2SV_StartVul> } else { <S2SV_EndVul> pr_debug(""decreased refcnt of timer %s to %u\n"", info->label, info->timer->refcnt); <S2SV_StartVul> } <S2SV_EndVul> mutex_unlock(&list_mutex); <S2SV_StartVul> } <S2SV_EndVul>","- if (--info->timer->refcnt == 0) {
- pr_debug(""deleting timer %s\n"", info->label);
- list_del(&info->timer->entry);
- if (info->timer->timer_type & XT_IDLETIMER_ALARM) {
- alarm_cancel(&info->timer->alarm);
- } else {
- timer_shutdown_sync(&info->timer->timer);
- }
- cancel_work_sync(&info->timer->work);
- sysfs_remove_file(idletimer_tg_kobj, &info->timer->attr.attr);
- kfree(info->timer->attr.attr.name);
- kfree(info->timer);
- } else {
- }
- }
+ if (--info->timer->refcnt > 0) {
+ mutex_unlock(&list_mutex);
+ return;
+ }
+ pr_debug(""deleting timer %s\n"", info->label);
+ list_del(&info->timer->entry);
+ mutex_unlock(&list_mutex);
+ if (info->timer->timer_type & XT_IDLETIMER_ALARM) {
+ alarm_cancel(&info->timer->alarm);
+ } else {
+ timer_shutdown_sync(&info->timer->timer);
+ }
+ cancel_work_sync(&info->timer->work);
+ sysfs_remove_file(idletimer_tg_kobj, &info->timer->attr.attr);
+ kfree(info->timer->attr.attr.name);
+ kfree(info->timer);
+ }","static void idletimer_tg_destroy_v1(const struct xt_tgdtor_param *par) { const struct idletimer_tg_info_v1 *info = par->targinfo; pr_debug(""destroy targinfo %s\n"", info->label); mutex_lock(&list_mutex); if (--info->timer->refcnt > 0) { pr_debug(""decreased refcnt of timer %s to %u\n"", info->label, info->timer->refcnt); mutex_unlock(&list_mutex); return; } pr_debug(""deleting timer %s\n"", info->label); list_del(&info->timer->entry); mutex_unlock(&list_mutex); if (info->timer->timer_type & XT_IDLETIMER_ALARM) { alarm_cancel(&info->timer->alarm); } else { timer_shutdown_sync(&info->timer->timer); } cancel_work_sync(&info->timer->work); sysfs_remove_file(idletimer_tg_kobj, &info->timer->attr.attr); kfree(info->timer->attr.attr.name); kfree(info->timer); }"
1061----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50303/bad/resource.c----walk_system_ram_res_rev,"int walk_system_ram_res_rev(u64 start, u64 end, void *arg, int (*func)(struct resource *, void *)) { struct resource res, *rams; int rams_size = 16, i; unsigned long flags; int ret = -1; rams = kvcalloc(rams_size, sizeof(struct resource), GFP_KERNEL); if (!rams) return ret; flags = IORESOURCE_SYSTEM_RAM | IORESOURCE_BUSY; i = 0; while ((start < end) && (!find_next_iomem_res(start, end, flags, IORES_DESC_NONE, &res))) { if (i >= rams_size) { struct resource *rams_new; rams_new = kvrealloc(rams, rams_size * sizeof(struct resource), (rams_size + 16) * sizeof(struct resource), GFP_KERNEL); if (!rams_new) goto out; rams = rams_new; rams_size += 16; } <S2SV_StartVul> rams[i].start = res.start; <S2SV_EndVul> <S2SV_StartVul> rams[i++].end = res.end; <S2SV_EndVul> start = res.end + 1; } for (i--; i >= 0; i--) { ret = (*func)(&rams[i], arg); if (ret) break; } out: kvfree(rams); return ret; }","- rams[i].start = res.start;
- rams[i++].end = res.end;
+ rams[i++] = res;","int walk_system_ram_res_rev(u64 start, u64 end, void *arg, int (*func)(struct resource *, void *)) { struct resource res, *rams; int rams_size = 16, i; unsigned long flags; int ret = -1; rams = kvcalloc(rams_size, sizeof(struct resource), GFP_KERNEL); if (!rams) return ret; flags = IORESOURCE_SYSTEM_RAM | IORESOURCE_BUSY; i = 0; while ((start < end) && (!find_next_iomem_res(start, end, flags, IORES_DESC_NONE, &res))) { if (i >= rams_size) { struct resource *rams_new; rams_new = kvrealloc(rams, rams_size * sizeof(struct resource), (rams_size + 16) * sizeof(struct resource), GFP_KERNEL); if (!rams_new) goto out; rams = rams_new; rams_size += 16; } rams[i++] = res; start = res.end + 1; } for (i--; i >= 0; i--) { ret = (*func)(&rams[i], arg); if (ret) break; } out: kvfree(rams); return ret; }"
1028----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50269/bad/sunxi.c----sunxi_musb_exit,"static int sunxi_musb_exit(struct musb *musb) { struct sunxi_glue *glue = dev_get_drvdata(musb->controller->parent); pm_runtime_put(musb->controller); cancel_work_sync(&glue->work); if (test_bit(SUNXI_MUSB_FL_PHY_ON, &glue->flags)) phy_power_off(glue->phy); phy_exit(glue->phy); if (test_bit(SUNXI_MUSB_FL_HAS_RESET, &glue->flags)) reset_control_assert(glue->rst); clk_disable_unprepare(glue->clk); if (test_bit(SUNXI_MUSB_FL_HAS_SRAM, &glue->flags)) sunxi_sram_release(musb->controller->parent); <S2SV_StartVul> devm_usb_put_phy(glue->dev, glue->xceiv); <S2SV_EndVul> return 0; }","- devm_usb_put_phy(glue->dev, glue->xceiv);","static int sunxi_musb_exit(struct musb *musb) { struct sunxi_glue *glue = dev_get_drvdata(musb->controller->parent); pm_runtime_put(musb->controller); cancel_work_sync(&glue->work); if (test_bit(SUNXI_MUSB_FL_PHY_ON, &glue->flags)) phy_power_off(glue->phy); phy_exit(glue->phy); if (test_bit(SUNXI_MUSB_FL_HAS_RESET, &glue->flags)) reset_control_assert(glue->rst); clk_disable_unprepare(glue->clk); if (test_bit(SUNXI_MUSB_FL_HAS_SRAM, &glue->flags)) sunxi_sram_release(musb->controller->parent); return 0; }"
266----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46749/bad/btnxpuart.c----btnxpuart_flush,static int btnxpuart_flush(struct hci_dev *hdev) { struct btnxpuart_dev *nxpdev = hci_get_drvdata(hdev); serdev_device_write_flush(nxpdev->serdev); skb_queue_purge(&nxpdev->txq); cancel_work_sync(&nxpdev->tx_work); <S2SV_StartVul> kfree_skb(nxpdev->rx_skb); <S2SV_EndVul> <S2SV_StartVul> nxpdev->rx_skb = NULL; <S2SV_EndVul> return 0; },"- kfree_skb(nxpdev->rx_skb);
- nxpdev->rx_skb = NULL;
+ if (!IS_ERR_OR_NULL(nxpdev->rx_skb)) {
+ kfree_skb(nxpdev->rx_skb);
+ nxpdev->rx_skb = NULL;
+ }
+ }",static int btnxpuart_flush(struct hci_dev *hdev) { struct btnxpuart_dev *nxpdev = hci_get_drvdata(hdev); serdev_device_write_flush(nxpdev->serdev); skb_queue_purge(&nxpdev->txq); cancel_work_sync(&nxpdev->tx_work); if (!IS_ERR_OR_NULL(nxpdev->rx_skb)) { kfree_skb(nxpdev->rx_skb); nxpdev->rx_skb = NULL; } return 0; }
1477----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-57895/bad/smb2pdu.c----set_file_basic_info,"static int set_file_basic_info(struct ksmbd_file *fp, struct smb2_file_basic_info *file_info, struct ksmbd_share_config *share) { struct iattr attrs; struct file *filp; struct inode *inode; struct mnt_idmap *idmap; int rc = 0; if (!(fp->daccess & FILE_WRITE_ATTRIBUTES_LE)) return -EACCES; attrs.ia_valid = 0; filp = fp->filp; inode = file_inode(filp); idmap = file_mnt_idmap(filp); if (file_info->CreationTime) fp->create_time = le64_to_cpu(file_info->CreationTime); if (file_info->LastAccessTime) { attrs.ia_atime = ksmbd_NTtimeToUnix(file_info->LastAccessTime); attrs.ia_valid |= (ATTR_ATIME | ATTR_ATIME_SET); } <S2SV_StartVul> attrs.ia_valid |= ATTR_CTIME; <S2SV_EndVul> if (file_info->ChangeTime) <S2SV_StartVul> attrs.ia_ctime = ksmbd_NTtimeToUnix(file_info->ChangeTime); <S2SV_EndVul> <S2SV_StartVul> else <S2SV_EndVul> <S2SV_StartVul> attrs.ia_ctime = inode_get_ctime(inode); <S2SV_EndVul> if (file_info->LastWriteTime) { attrs.ia_mtime = ksmbd_NTtimeToUnix(file_info->LastWriteTime); <S2SV_StartVul> attrs.ia_valid |= (ATTR_MTIME | ATTR_MTIME_SET); <S2SV_EndVul> } if (file_info->Attributes) { if (!S_ISDIR(inode->i_mode) && file_info->Attributes & FILE_ATTRIBUTE_DIRECTORY_LE) { pr_err(""can't change a file to a directory\n""); return -EINVAL; } if (!(S_ISDIR(inode->i_mode) && file_info->Attributes == FILE_ATTRIBUTE_NORMAL_LE)) fp->f_ci->m_fattr = file_info->Attributes | (fp->f_ci->m_fattr & FILE_ATTRIBUTE_DIRECTORY_LE); } if (test_share_config_flag(share, KSMBD_SHARE_FLAG_STORE_DOS_ATTRS) && (file_info->CreationTime || file_info->Attributes)) { struct xattr_dos_attrib da = {0}; da.version = 4; da.itime = fp->itime; da.create_time = fp->create_time; da.attr = le32_to_cpu(fp->f_ci->m_fattr); da.flags = XATTR_DOSINFO_ATTRIB | XATTR_DOSINFO_CREATE_TIME | XATTR_DOSINFO_ITIME; rc = ksmbd_vfs_set_dos_attrib_xattr(idmap, &filp->f_path, &da, true); if (rc) ksmbd_debug(SMB, ""failed to restore file attribute in EA\n""); rc = 0; } if (attrs.ia_valid) { struct dentry *dentry = filp->f_path.dentry; struct inode *inode = d_inode(dentry); if (IS_IMMUTABLE(inode) || IS_APPEND(inode)) return -EACCES; inode_lock(inode); <S2SV_StartVul> inode_set_ctime_to_ts(inode, attrs.ia_ctime); <S2SV_EndVul> <S2SV_StartVul> attrs.ia_valid &= ~ATTR_CTIME; <S2SV_EndVul> rc = notify_change(idmap, dentry, &attrs, NULL); inode_unlock(inode); } return rc; }","- attrs.ia_valid |= ATTR_CTIME;
- attrs.ia_ctime = ksmbd_NTtimeToUnix(file_info->ChangeTime);
- else
- attrs.ia_ctime = inode_get_ctime(inode);
- attrs.ia_valid |= (ATTR_MTIME | ATTR_MTIME_SET);
- inode_set_ctime_to_ts(inode, attrs.ia_ctime);
- attrs.ia_valid &= ~ATTR_CTIME;
+ inode_set_ctime_to_ts(inode,
+ ksmbd_NTtimeToUnix(file_info->ChangeTime));
+ attrs.ia_valid |= (ATTR_MTIME | ATTR_MTIME_SET | ATTR_CTIME);","static int set_file_basic_info(struct ksmbd_file *fp, struct smb2_file_basic_info *file_info, struct ksmbd_share_config *share) { struct iattr attrs; struct file *filp; struct inode *inode; struct mnt_idmap *idmap; int rc = 0; if (!(fp->daccess & FILE_WRITE_ATTRIBUTES_LE)) return -EACCES; attrs.ia_valid = 0; filp = fp->filp; inode = file_inode(filp); idmap = file_mnt_idmap(filp); if (file_info->CreationTime) fp->create_time = le64_to_cpu(file_info->CreationTime); if (file_info->LastAccessTime) { attrs.ia_atime = ksmbd_NTtimeToUnix(file_info->LastAccessTime); attrs.ia_valid |= (ATTR_ATIME | ATTR_ATIME_SET); } if (file_info->ChangeTime) inode_set_ctime_to_ts(inode, ksmbd_NTtimeToUnix(file_info->ChangeTime)); if (file_info->LastWriteTime) { attrs.ia_mtime = ksmbd_NTtimeToUnix(file_info->LastWriteTime); attrs.ia_valid |= (ATTR_MTIME | ATTR_MTIME_SET | ATTR_CTIME); } if (file_info->Attributes) { if (!S_ISDIR(inode->i_mode) && file_info->Attributes & FILE_ATTRIBUTE_DIRECTORY_LE) { pr_err(""can't change a file to a directory\n""); return -EINVAL; } if (!(S_ISDIR(inode->i_mode) && file_info->Attributes == FILE_ATTRIBUTE_NORMAL_LE)) fp->f_ci->m_fattr = file_info->Attributes | (fp->f_ci->m_fattr & FILE_ATTRIBUTE_DIRECTORY_LE); } if (test_share_config_flag(share, KSMBD_SHARE_FLAG_STORE_DOS_ATTRS) && (file_info->CreationTime || file_info->Attributes)) { struct xattr_dos_attrib da = {0}; da.version = 4; da.itime = fp->itime; da.create_time = fp->create_time; da.attr = le32_to_cpu(fp->f_ci->m_fattr); da.flags = XATTR_DOSINFO_ATTRIB | XATTR_DOSINFO_CREATE_TIME | XATTR_DOSINFO_ITIME; rc = ksmbd_vfs_set_dos_attrib_xattr(idmap, &filp->f_path, &da, true); if (rc) ksmbd_debug(SMB, ""failed to restore file attribute in EA\n""); rc = 0; } if (attrs.ia_valid) { struct dentry *dentry = filp->f_path.dentry; struct inode *inode = d_inode(dentry); if (IS_IMMUTABLE(inode) || IS_APPEND(inode)) return -EACCES; inode_lock(inode); rc = notify_change(idmap, dentry, &attrs, NULL); inode_unlock(inode); } return rc; }"
536----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49858/bad/tpm.c----efi_retrieve_tpm2_eventlog,"void efi_retrieve_tpm2_eventlog(void) { efi_guid_t tcg2_guid = EFI_TCG2_PROTOCOL_GUID; efi_guid_t linux_eventlog_guid = LINUX_EFI_TPM_EVENT_LOG_GUID; efi_status_t status; efi_physical_addr_t log_location = 0, log_last_entry = 0; struct linux_efi_tpm_eventlog *log_tbl = NULL; struct efi_tcg2_final_events_table *final_events_table = NULL; unsigned long first_entry_addr, last_entry_addr; size_t log_size, last_entry_size; efi_bool_t truncated; int version = EFI_TCG2_EVENT_LOG_FORMAT_TCG_2; efi_tcg2_protocol_t *tcg2_protocol = NULL; int final_events_size = 0; status = efi_bs_call(locate_protocol, &tcg2_guid, NULL, (void **)&tcg2_protocol); if (status != EFI_SUCCESS) return; status = efi_call_proto(tcg2_protocol, get_event_log, version, &log_location, &log_last_entry, &truncated); if (status != EFI_SUCCESS || !log_location) { version = EFI_TCG2_EVENT_LOG_FORMAT_TCG_1_2; status = efi_call_proto(tcg2_protocol, get_event_log, version, &log_location, &log_last_entry, &truncated); if (status != EFI_SUCCESS || !log_location) return; } first_entry_addr = (unsigned long) log_location; if (!log_last_entry) { log_size = 0; } else { last_entry_addr = (unsigned long) log_last_entry; if (version == EFI_TCG2_EVENT_LOG_FORMAT_TCG_2) { last_entry_size = __calc_tpm2_event_size((void *)last_entry_addr, (void *)(long)log_location, false); } else { last_entry_size = sizeof(struct tcpa_event) + ((struct tcpa_event *) last_entry_addr)->event_size; } log_size = log_last_entry - log_location + last_entry_size; } <S2SV_StartVul> status = efi_bs_call(allocate_pool, EFI_LOADER_DATA, <S2SV_EndVul> sizeof(*log_tbl) + log_size, (void **)&log_tbl); if (status != EFI_SUCCESS) { efi_err(""Unable to allocate memory for event log\n""); return; } if (version == EFI_TCG2_EVENT_LOG_FORMAT_TCG_2) final_events_table = get_efi_config_table(LINUX_EFI_TPM_FINAL_LOG_GUID); if (final_events_table && final_events_table->nr_events) { struct tcg_pcr_event2_head *header; int offset; void *data; int event_size; int i = final_events_table->nr_events; data = (void *)final_events_table; offset = sizeof(final_events_table->version) + sizeof(final_events_table->nr_events); while (i > 0) { header = data + offset + final_events_size; event_size = __calc_tpm2_event_size(header, (void *)(long)log_location, false); final_events_size += event_size; i--; } } memset(log_tbl, 0, sizeof(*log_tbl) + log_size); log_tbl->size = log_size; log_tbl->final_events_preboot_size = final_events_size; log_tbl->version = version; memcpy(log_tbl->log, (void *) first_entry_addr, log_size); status = efi_bs_call(install_configuration_table, &linux_eventlog_guid, log_tbl); if (status != EFI_SUCCESS) goto err_free; return; err_free: efi_bs_call(free_pool, log_tbl); }","- status = efi_bs_call(allocate_pool, EFI_LOADER_DATA,
+ status = efi_bs_call(allocate_pool, EFI_ACPI_RECLAIM_MEMORY,","void efi_retrieve_tpm2_eventlog(void) { efi_guid_t tcg2_guid = EFI_TCG2_PROTOCOL_GUID; efi_guid_t linux_eventlog_guid = LINUX_EFI_TPM_EVENT_LOG_GUID; efi_status_t status; efi_physical_addr_t log_location = 0, log_last_entry = 0; struct linux_efi_tpm_eventlog *log_tbl = NULL; struct efi_tcg2_final_events_table *final_events_table = NULL; unsigned long first_entry_addr, last_entry_addr; size_t log_size, last_entry_size; efi_bool_t truncated; int version = EFI_TCG2_EVENT_LOG_FORMAT_TCG_2; efi_tcg2_protocol_t *tcg2_protocol = NULL; int final_events_size = 0; status = efi_bs_call(locate_protocol, &tcg2_guid, NULL, (void **)&tcg2_protocol); if (status != EFI_SUCCESS) return; status = efi_call_proto(tcg2_protocol, get_event_log, version, &log_location, &log_last_entry, &truncated); if (status != EFI_SUCCESS || !log_location) { version = EFI_TCG2_EVENT_LOG_FORMAT_TCG_1_2; status = efi_call_proto(tcg2_protocol, get_event_log, version, &log_location, &log_last_entry, &truncated); if (status != EFI_SUCCESS || !log_location) return; } first_entry_addr = (unsigned long) log_location; if (!log_last_entry) { log_size = 0; } else { last_entry_addr = (unsigned long) log_last_entry; if (version == EFI_TCG2_EVENT_LOG_FORMAT_TCG_2) { last_entry_size = __calc_tpm2_event_size((void *)last_entry_addr, (void *)(long)log_location, false); } else { last_entry_size = sizeof(struct tcpa_event) + ((struct tcpa_event *) last_entry_addr)->event_size; } log_size = log_last_entry - log_location + last_entry_size; } status = efi_bs_call(allocate_pool, EFI_ACPI_RECLAIM_MEMORY, sizeof(*log_tbl) + log_size, (void **)&log_tbl); if (status != EFI_SUCCESS) { efi_err(""Unable to allocate memory for event log\n""); return; } if (version == EFI_TCG2_EVENT_LOG_FORMAT_TCG_2) final_events_table = get_efi_config_table(LINUX_EFI_TPM_FINAL_LOG_GUID); if (final_events_table && final_events_table->nr_events) { struct tcg_pcr_event2_head *header; int offset; void *data; int event_size; int i = final_events_table->nr_events; data = (void *)final_events_table; offset = sizeof(final_events_table->version) + sizeof(final_events_table->nr_events); while (i > 0) { header = data + offset + final_events_size; event_size = __calc_tpm2_event_size(header, (void *)(long)log_location, false); final_events_size += event_size; i--; } } memset(log_tbl, 0, sizeof(*log_tbl) + log_size); log_tbl->size = log_size; log_tbl->final_events_preboot_size = final_events_size; log_tbl->version = version; memcpy(log_tbl->log, (void *) first_entry_addr, log_size); status = efi_bs_call(install_configuration_table, &linux_eventlog_guid, log_tbl); if (status != EFI_SUCCESS) goto err_free; return; err_free: efi_bs_call(free_pool, log_tbl); }"
570----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49892/bad/display_rq_dlg_calc_20v2.c----calculate_ttu_cursor,"static void calculate_ttu_cursor(struct display_mode_lib *mode_lib, double *refcyc_per_req_delivery_pre_cur, double *refcyc_per_req_delivery_cur, double refclk_freq_in_mhz, double ref_freq_to_pix_freq, double hscale_pixel_rate_l, double hscl_ratio, double vratio_pre_l, double vratio_l, unsigned int cur_width, enum cursor_bpp cur_bpp); #include ""../dml_inline_defs.h"" static unsigned int get_bytes_per_element(enum source_format_class source_format, bool is_chroma) { <S2SV_StartVul> unsigned int ret_val = 0; <S2SV_EndVul> if (source_format == dm_444_16) { if (!is_chroma) ret_val = 2; } else if (source_format == dm_444_32) { if (!is_chroma) ret_val = 4; } else if (source_format == dm_444_64) { if (!is_chroma) ret_val = 8; } else if (source_format == dm_420_8) { if (is_chroma) ret_val = 2; else ret_val = 1; } else if (source_format == dm_420_10) { if (is_chroma) ret_val = 4; else ret_val = 2; } else if (source_format == dm_444_8) { ret_val = 1; } return ret_val; }","- unsigned int ret_val = 0;
+ unsigned int ret_val = 1;","static void calculate_ttu_cursor(struct display_mode_lib *mode_lib, double *refcyc_per_req_delivery_pre_cur, double *refcyc_per_req_delivery_cur, double refclk_freq_in_mhz, double ref_freq_to_pix_freq, double hscale_pixel_rate_l, double hscl_ratio, double vratio_pre_l, double vratio_l, unsigned int cur_width, enum cursor_bpp cur_bpp); #include ""../dml_inline_defs.h"" static unsigned int get_bytes_per_element(enum source_format_class source_format, bool is_chroma) { unsigned int ret_val = 1; if (source_format == dm_444_16) { if (!is_chroma) ret_val = 2; } else if (source_format == dm_444_32) { if (!is_chroma) ret_val = 4; } else if (source_format == dm_444_64) { if (!is_chroma) ret_val = 8; } else if (source_format == dm_420_8) { if (is_chroma) ret_val = 2; else ret_val = 1; } else if (source_format == dm_420_10) { if (is_chroma) ret_val = 4; else ret_val = 2; } else if (source_format == dm_444_8) { ret_val = 1; } return ret_val; }"
1037----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50277/bad/dm.c----*alloc_dev,"static struct mapped_device *alloc_dev(int minor) { int r, numa_node_id = dm_get_numa_node(); struct dax_device *dax_dev; struct mapped_device *md; void *old_md; md = kvzalloc_node(sizeof(*md), GFP_KERNEL, numa_node_id); if (!md) { DMERR(""unable to allocate device, out of memory.""); return NULL; } if (!try_module_get(THIS_MODULE)) goto bad_module_get; if (minor == DM_ANY_MINOR) r = next_free_minor(&minor); else r = specific_minor(minor); if (r < 0) goto bad_minor; r = init_srcu_struct(&md->io_barrier); if (r < 0) goto bad_io_barrier; md->numa_node_id = numa_node_id; md->init_tio_pdu = false; md->type = DM_TYPE_NONE; mutex_init(&md->suspend_lock); mutex_init(&md->type_lock); mutex_init(&md->table_devices_lock); spin_lock_init(&md->deferred_lock); atomic_set(&md->holders, 1); atomic_set(&md->open_count, 0); atomic_set(&md->event_nr, 0); atomic_set(&md->uevent_seq, 0); INIT_LIST_HEAD(&md->uevent_list); INIT_LIST_HEAD(&md->table_devices); spin_lock_init(&md->uevent_lock); md->disk = blk_alloc_disk(NULL, md->numa_node_id); <S2SV_StartVul> if (IS_ERR(md->disk)) <S2SV_EndVul> goto bad; md->queue = md->disk->queue; init_waitqueue_head(&md->wait); INIT_WORK(&md->work, dm_wq_work); INIT_WORK(&md->requeue_work, dm_wq_requeue_work); init_waitqueue_head(&md->eventq); init_completion(&md->kobj_holder.completion); md->requeue_list = NULL; md->swap_bios = get_swap_bios(); sema_init(&md->swap_bios_semaphore, md->swap_bios); mutex_init(&md->swap_bios_lock); md->disk->major = _major; md->disk->first_minor = minor; md->disk->minors = 1; md->disk->flags |= GENHD_FL_NO_PART; md->disk->fops = &dm_blk_dops; md->disk->private_data = md; sprintf(md->disk->disk_name, ""dm-%d"", minor); dax_dev = alloc_dax(md, &dm_dax_ops); if (IS_ERR(dax_dev)) { if (PTR_ERR(dax_dev) != -EOPNOTSUPP) goto bad; } else { set_dax_nocache(dax_dev); set_dax_nomc(dax_dev); md->dax_dev = dax_dev; if (dax_add_host(dax_dev, md->disk)) goto bad; } format_dev_t(md->name, MKDEV(_major, minor)); md->wq = alloc_workqueue(""kdmflush/%s"", WQ_MEM_RECLAIM, 0, md->name); if (!md->wq) goto bad; md->pending_io = alloc_percpu(unsigned long); if (!md->pending_io) goto bad; r = dm_stats_init(&md->stats); if (r < 0) goto bad; spin_lock(&_minor_lock); old_md = idr_replace(&_minor_idr, md, minor); spin_unlock(&_minor_lock); BUG_ON(old_md != MINOR_ALLOCED); return md; bad: cleanup_mapped_device(md); bad_io_barrier: free_minor(minor); bad_minor: module_put(THIS_MODULE); bad_module_get: kvfree(md); return NULL; }","- if (IS_ERR(md->disk))
+ if (IS_ERR(md->disk)) {
+ md->disk = NULL;
+ }","static struct mapped_device *alloc_dev(int minor) { int r, numa_node_id = dm_get_numa_node(); struct dax_device *dax_dev; struct mapped_device *md; void *old_md; md = kvzalloc_node(sizeof(*md), GFP_KERNEL, numa_node_id); if (!md) { DMERR(""unable to allocate device, out of memory.""); return NULL; } if (!try_module_get(THIS_MODULE)) goto bad_module_get; if (minor == DM_ANY_MINOR) r = next_free_minor(&minor); else r = specific_minor(minor); if (r < 0) goto bad_minor; r = init_srcu_struct(&md->io_barrier); if (r < 0) goto bad_io_barrier; md->numa_node_id = numa_node_id; md->init_tio_pdu = false; md->type = DM_TYPE_NONE; mutex_init(&md->suspend_lock); mutex_init(&md->type_lock); mutex_init(&md->table_devices_lock); spin_lock_init(&md->deferred_lock); atomic_set(&md->holders, 1); atomic_set(&md->open_count, 0); atomic_set(&md->event_nr, 0); atomic_set(&md->uevent_seq, 0); INIT_LIST_HEAD(&md->uevent_list); INIT_LIST_HEAD(&md->table_devices); spin_lock_init(&md->uevent_lock); md->disk = blk_alloc_disk(NULL, md->numa_node_id); if (IS_ERR(md->disk)) { md->disk = NULL; goto bad; } md->queue = md->disk->queue; init_waitqueue_head(&md->wait); INIT_WORK(&md->work, dm_wq_work); INIT_WORK(&md->requeue_work, dm_wq_requeue_work); init_waitqueue_head(&md->eventq); init_completion(&md->kobj_holder.completion); md->requeue_list = NULL; md->swap_bios = get_swap_bios(); sema_init(&md->swap_bios_semaphore, md->swap_bios); mutex_init(&md->swap_bios_lock); md->disk->major = _major; md->disk->first_minor = minor; md->disk->minors = 1; md->disk->flags |= GENHD_FL_NO_PART; md->disk->fops = &dm_blk_dops; md->disk->private_data = md; sprintf(md->disk->disk_name, ""dm-%d"", minor); dax_dev = alloc_dax(md, &dm_dax_ops); if (IS_ERR(dax_dev)) { if (PTR_ERR(dax_dev) != -EOPNOTSUPP) goto bad; } else { set_dax_nocache(dax_dev); set_dax_nomc(dax_dev); md->dax_dev = dax_dev; if (dax_add_host(dax_dev, md->disk)) goto bad; } format_dev_t(md->name, MKDEV(_major, minor)); md->wq = alloc_workqueue(""kdmflush/%s"", WQ_MEM_RECLAIM, 0, md->name); if (!md->wq) goto bad; md->pending_io = alloc_percpu(unsigned long); if (!md->pending_io) goto bad; r = dm_stats_init(&md->stats); if (r < 0) goto bad; spin_lock(&_minor_lock); old_md = idr_replace(&_minor_idr, md, minor); spin_unlock(&_minor_lock); BUG_ON(old_md != MINOR_ALLOCED); return md; bad: cleanup_mapped_device(md); bad_io_barrier: free_minor(minor); bad_minor: module_put(THIS_MODULE); bad_module_get: kvfree(md); return NULL; }"
478----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47723/bad/jfs_dmap.c----dbMount,"int dbMount(struct inode *ipbmap) { struct bmap *bmp; struct dbmap_disk *dbmp_le; struct metapage *mp; int i, err; bmp = kmalloc(sizeof(struct bmap), GFP_KERNEL); if (bmp == NULL) return -ENOMEM; mp = read_metapage(ipbmap, BMAPBLKNO << JFS_SBI(ipbmap->i_sb)->l2nbperpage, PSIZE, 0); if (mp == NULL) { err = -EIO; goto err_kfree_bmp; } dbmp_le = (struct dbmap_disk *) mp->data; bmp->db_mapsize = le64_to_cpu(dbmp_le->dn_mapsize); bmp->db_nfree = le64_to_cpu(dbmp_le->dn_nfree); bmp->db_l2nbperpage = le32_to_cpu(dbmp_le->dn_l2nbperpage); if (bmp->db_l2nbperpage > L2PSIZE - L2MINBLOCKSIZE || bmp->db_l2nbperpage < 0) { err = -EINVAL; goto err_release_metapage; } bmp->db_numag = le32_to_cpu(dbmp_le->dn_numag); <S2SV_StartVul> if (!bmp->db_numag) { <S2SV_EndVul> err = -EINVAL; goto err_release_metapage; } bmp->db_maxlevel = le32_to_cpu(dbmp_le->dn_maxlevel); bmp->db_maxag = le32_to_cpu(dbmp_le->dn_maxag); bmp->db_agpref = le32_to_cpu(dbmp_le->dn_agpref); if (bmp->db_maxag >= MAXAG || bmp->db_maxag < 0 || bmp->db_agpref >= MAXAG || bmp->db_agpref < 0) { err = -EINVAL; goto err_release_metapage; } bmp->db_aglevel = le32_to_cpu(dbmp_le->dn_aglevel); bmp->db_agheight = le32_to_cpu(dbmp_le->dn_agheight); bmp->db_agwidth = le32_to_cpu(dbmp_le->dn_agwidth); bmp->db_agstart = le32_to_cpu(dbmp_le->dn_agstart); bmp->db_agl2size = le32_to_cpu(dbmp_le->dn_agl2size); if (bmp->db_agl2size > L2MAXL2SIZE - L2MAXAG || bmp->db_agl2size < 0) { err = -EINVAL; goto err_release_metapage; } if (((bmp->db_mapsize - 1) >> bmp->db_agl2size) > MAXAG) { err = -EINVAL; goto err_release_metapage; } for (i = 0; i < MAXAG; i++) bmp->db_agfree[i] = le64_to_cpu(dbmp_le->dn_agfree[i]); bmp->db_agsize = le64_to_cpu(dbmp_le->dn_agsize); bmp->db_maxfreebud = dbmp_le->dn_maxfreebud; release_metapage(mp); bmp->db_ipbmap = ipbmap; JFS_SBI(ipbmap->i_sb)->bmap = bmp; memset(bmp->db_active, 0, sizeof(bmp->db_active)); BMAP_LOCK_INIT(bmp); return (0); err_release_metapage: release_metapage(mp); err_kfree_bmp: kfree(bmp); return err; }","- if (!bmp->db_numag) {
+ if (!bmp->db_numag || bmp->db_numag >= MAXAG) {","int dbMount(struct inode *ipbmap) { struct bmap *bmp; struct dbmap_disk *dbmp_le; struct metapage *mp; int i, err; bmp = kmalloc(sizeof(struct bmap), GFP_KERNEL); if (bmp == NULL) return -ENOMEM; mp = read_metapage(ipbmap, BMAPBLKNO << JFS_SBI(ipbmap->i_sb)->l2nbperpage, PSIZE, 0); if (mp == NULL) { err = -EIO; goto err_kfree_bmp; } dbmp_le = (struct dbmap_disk *) mp->data; bmp->db_mapsize = le64_to_cpu(dbmp_le->dn_mapsize); bmp->db_nfree = le64_to_cpu(dbmp_le->dn_nfree); bmp->db_l2nbperpage = le32_to_cpu(dbmp_le->dn_l2nbperpage); if (bmp->db_l2nbperpage > L2PSIZE - L2MINBLOCKSIZE || bmp->db_l2nbperpage < 0) { err = -EINVAL; goto err_release_metapage; } bmp->db_numag = le32_to_cpu(dbmp_le->dn_numag); if (!bmp->db_numag || bmp->db_numag >= MAXAG) { err = -EINVAL; goto err_release_metapage; } bmp->db_maxlevel = le32_to_cpu(dbmp_le->dn_maxlevel); bmp->db_maxag = le32_to_cpu(dbmp_le->dn_maxag); bmp->db_agpref = le32_to_cpu(dbmp_le->dn_agpref); if (bmp->db_maxag >= MAXAG || bmp->db_maxag < 0 || bmp->db_agpref >= MAXAG || bmp->db_agpref < 0) { err = -EINVAL; goto err_release_metapage; } bmp->db_aglevel = le32_to_cpu(dbmp_le->dn_aglevel); bmp->db_agheight = le32_to_cpu(dbmp_le->dn_agheight); bmp->db_agwidth = le32_to_cpu(dbmp_le->dn_agwidth); bmp->db_agstart = le32_to_cpu(dbmp_le->dn_agstart); bmp->db_agl2size = le32_to_cpu(dbmp_le->dn_agl2size); if (bmp->db_agl2size > L2MAXL2SIZE - L2MAXAG || bmp->db_agl2size < 0) { err = -EINVAL; goto err_release_metapage; } if (((bmp->db_mapsize - 1) >> bmp->db_agl2size) > MAXAG) { err = -EINVAL; goto err_release_metapage; } for (i = 0; i < MAXAG; i++) bmp->db_agfree[i] = le64_to_cpu(dbmp_le->dn_agfree[i]); bmp->db_agsize = le64_to_cpu(dbmp_le->dn_agsize); bmp->db_maxfreebud = dbmp_le->dn_maxfreebud; release_metapage(mp); bmp->db_ipbmap = ipbmap; JFS_SBI(ipbmap->i_sb)->bmap = bmp; memset(bmp->db_active, 0, sizeof(bmp->db_active)); BMAP_LOCK_INIT(bmp); return (0); err_release_metapage: release_metapage(mp); err_kfree_bmp: kfree(bmp); return err; }"
1307----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56606/bad/af_packet.c----packet_create,"static int packet_create(struct net *net, struct socket *sock, int protocol, int kern) { struct sock *sk; struct packet_sock *po; __be16 proto = (__force __be16)protocol; int err; if (!ns_capable(net->user_ns, CAP_NET_RAW)) return -EPERM; if (sock->type != SOCK_DGRAM && sock->type != SOCK_RAW && sock->type != SOCK_PACKET) return -ESOCKTNOSUPPORT; sock->state = SS_UNCONNECTED; err = -ENOBUFS; sk = sk_alloc(net, PF_PACKET, GFP_KERNEL, &packet_proto, kern); if (sk == NULL) goto out; sock->ops = &packet_ops; if (sock->type == SOCK_PACKET) sock->ops = &packet_ops_spkt; sock_init_data(sock, sk); <S2SV_StartVul> po = pkt_sk(sk); <S2SV_EndVul> init_completion(&po->skb_completion); sk->sk_family = PF_PACKET; po->num = proto; <S2SV_StartVul> err = packet_alloc_pending(po); <S2SV_EndVul> <S2SV_StartVul> if (err) <S2SV_EndVul> <S2SV_StartVul> goto out2; <S2SV_EndVul> packet_cached_dev_reset(po); sk->sk_destruct = packet_sock_destruct; spin_lock_init(&po->bind_lock); mutex_init(&po->pg_vec_lock); po->rollover = NULL; po->prot_hook.func = packet_rcv; if (sock->type == SOCK_PACKET) po->prot_hook.func = packet_rcv_spkt; po->prot_hook.af_packet_priv = sk; po->prot_hook.af_packet_net = sock_net(sk); if (proto) { po->prot_hook.type = proto; __register_prot_hook(sk); } mutex_lock(&net->packet.sklist_lock); sk_add_node_tail_rcu(sk, &net->packet.sklist); mutex_unlock(&net->packet.sklist_lock); sock_prot_inuse_add(net, &packet_proto, 1); return 0; <S2SV_StartVul> out2: <S2SV_EndVul> sk_free(sk); out: return err; }","- po = pkt_sk(sk);
- err = packet_alloc_pending(po);
- if (err)
- goto out2;
- out2:
+ po = pkt_sk(sk);
+ err = packet_alloc_pending(po);
+ if (err)
+ goto out_sk_free;
+ out_sk_free:","static int packet_create(struct net *net, struct socket *sock, int protocol, int kern) { struct sock *sk; struct packet_sock *po; __be16 proto = (__force __be16)protocol; int err; if (!ns_capable(net->user_ns, CAP_NET_RAW)) return -EPERM; if (sock->type != SOCK_DGRAM && sock->type != SOCK_RAW && sock->type != SOCK_PACKET) return -ESOCKTNOSUPPORT; sock->state = SS_UNCONNECTED; err = -ENOBUFS; sk = sk_alloc(net, PF_PACKET, GFP_KERNEL, &packet_proto, kern); if (sk == NULL) goto out; sock->ops = &packet_ops; if (sock->type == SOCK_PACKET) sock->ops = &packet_ops_spkt; po = pkt_sk(sk); err = packet_alloc_pending(po); if (err) goto out_sk_free; sock_init_data(sock, sk); init_completion(&po->skb_completion); sk->sk_family = PF_PACKET; po->num = proto; packet_cached_dev_reset(po); sk->sk_destruct = packet_sock_destruct; spin_lock_init(&po->bind_lock); mutex_init(&po->pg_vec_lock); po->rollover = NULL; po->prot_hook.func = packet_rcv; if (sock->type == SOCK_PACKET) po->prot_hook.func = packet_rcv_spkt; po->prot_hook.af_packet_priv = sk; po->prot_hook.af_packet_net = sock_net(sk); if (proto) { po->prot_hook.type = proto; __register_prot_hook(sk); } mutex_lock(&net->packet.sklist_lock); sk_add_node_tail_rcu(sk, &net->packet.sklist); mutex_unlock(&net->packet.sklist_lock); sock_prot_inuse_add(net, &packet_proto, 1); return 0; out_sk_free: sk_free(sk); out: return err; }"
369----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46833/bad/hclge_err.c----hclge_query_reg_info_of_ssu,"static void hclge_query_reg_info_of_ssu(struct hclge_dev *hdev) { u32 loop_para[HCLGE_MOD_MSG_PARA_ARRAY_MAX_SIZE] = {0}; struct hclge_mod_reg_common_msg msg; <S2SV_StartVul> u8 i, j, num; <S2SV_EndVul> <S2SV_StartVul> u32 loop_time; <S2SV_EndVul> num = ARRAY_SIZE(hclge_ssu_reg_common_msg); for (i = 0; i < num; i++) { msg = hclge_ssu_reg_common_msg[i]; if (!hclge_err_mod_check_support_cmd(msg.cmd, hdev)) continue; loop_time = 1; loop_para[0] = 0; if (msg.need_para) { <S2SV_StartVul> loop_time = hdev->ae_dev->dev_specs.tnl_num; <S2SV_EndVul> for (j = 0; j < loop_time; j++) loop_para[j] = j + 1; } hclge_query_reg_info(hdev, &msg, loop_time, loop_para); } }","- u8 i, j, num;
- u32 loop_time;
- loop_time = hdev->ae_dev->dev_specs.tnl_num;
+ u8 i, j, num, loop_time;
+ loop_time = min(hdev->ae_dev->dev_specs.tnl_num,
+ HCLGE_MOD_MSG_PARA_ARRAY_MAX_SIZE);","static void hclge_query_reg_info_of_ssu(struct hclge_dev *hdev) { u32 loop_para[HCLGE_MOD_MSG_PARA_ARRAY_MAX_SIZE] = {0}; struct hclge_mod_reg_common_msg msg; u8 i, j, num, loop_time; num = ARRAY_SIZE(hclge_ssu_reg_common_msg); for (i = 0; i < num; i++) { msg = hclge_ssu_reg_common_msg[i]; if (!hclge_err_mod_check_support_cmd(msg.cmd, hdev)) continue; loop_time = 1; loop_para[0] = 0; if (msg.need_para) { loop_time = min(hdev->ae_dev->dev_specs.tnl_num, HCLGE_MOD_MSG_PARA_ARRAY_MAX_SIZE); for (j = 0; j < loop_time; j++) loop_para[j] = j + 1; } hclge_query_reg_info(hdev, &msg, loop_time, loop_para); } }"
740----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50015/bad/file.c----ext4_dio_write_iter,"static ssize_t ext4_dio_write_iter(struct kiocb *iocb, struct iov_iter *from) { ssize_t ret; handle_t *handle; struct inode *inode = file_inode(iocb->ki_filp); loff_t offset = iocb->ki_pos; size_t count = iov_iter_count(from); const struct iomap_ops *iomap_ops = &ext4_iomap_ops; bool extend = false, unaligned_io = false; bool ilock_shared = true; if (ext4_unaligned_io(inode, from, offset)) { unaligned_io = true; ilock_shared = false; } if (offset + count > i_size_read(inode)) ilock_shared = false; if (iocb->ki_flags & IOCB_NOWAIT) { if (ilock_shared) { if (!inode_trylock_shared(inode)) return -EAGAIN; } else { if (!inode_trylock(inode)) return -EAGAIN; } } else { if (ilock_shared) inode_lock_shared(inode); else inode_lock(inode); } if (!ext4_dio_supported(inode)) { if (ilock_shared) inode_unlock_shared(inode); else inode_unlock(inode); return ext4_buffered_write_iter(iocb, from); } ret = ext4_dio_write_checks(iocb, from, &ilock_shared, &extend); if (ret <= 0) return ret; if ((iocb->ki_flags & IOCB_NOWAIT) && (unaligned_io || extend)) { ret = -EAGAIN; goto out; } ext4_clear_inode_state(inode, EXT4_STATE_MAY_INLINE_DATA); offset = iocb->ki_pos; count = ret; if (unaligned_io) inode_dio_wait(inode); if (extend) { handle = ext4_journal_start(inode, EXT4_HT_INODE, 2); if (IS_ERR(handle)) { ret = PTR_ERR(handle); goto out; } ret = ext4_orphan_add(handle, inode); if (ret) { ext4_journal_stop(handle); goto out; } ext4_journal_stop(handle); } if (ilock_shared) iomap_ops = &ext4_iomap_overwrite_ops; ret = iomap_dio_rw(iocb, from, iomap_ops, &ext4_dio_write_ops, is_sync_kiocb(iocb) || unaligned_io || extend); if (ret == -ENOTBLK) ret = 0; if (extend) { WARN_ON_ONCE(ret == -EIOCBQUEUED); <S2SV_StartVul> ext4_inode_extension_cleanup(inode, ret); <S2SV_EndVul> } out: if (ilock_shared) inode_unlock_shared(inode); else inode_unlock(inode); if (ret >= 0 && iov_iter_count(from)) { ssize_t err; loff_t endbyte; offset = iocb->ki_pos; err = ext4_buffered_write_iter(iocb, from); if (err < 0) return err; ret += err; endbyte = offset + err - 1; err = filemap_write_and_wait_range(iocb->ki_filp->f_mapping, offset, endbyte); if (!err) invalidate_mapping_pages(iocb->ki_filp->f_mapping, offset >> PAGE_SHIFT, endbyte >> PAGE_SHIFT); } return ret; }","- ext4_inode_extension_cleanup(inode, ret);
+ ext4_inode_extension_cleanup(inode, ret < 0);","static ssize_t ext4_dio_write_iter(struct kiocb *iocb, struct iov_iter *from) { ssize_t ret; handle_t *handle; struct inode *inode = file_inode(iocb->ki_filp); loff_t offset = iocb->ki_pos; size_t count = iov_iter_count(from); const struct iomap_ops *iomap_ops = &ext4_iomap_ops; bool extend = false, unaligned_io = false; bool ilock_shared = true; if (ext4_unaligned_io(inode, from, offset)) { unaligned_io = true; ilock_shared = false; } if (offset + count > i_size_read(inode)) ilock_shared = false; if (iocb->ki_flags & IOCB_NOWAIT) { if (ilock_shared) { if (!inode_trylock_shared(inode)) return -EAGAIN; } else { if (!inode_trylock(inode)) return -EAGAIN; } } else { if (ilock_shared) inode_lock_shared(inode); else inode_lock(inode); } if (!ext4_dio_supported(inode)) { if (ilock_shared) inode_unlock_shared(inode); else inode_unlock(inode); return ext4_buffered_write_iter(iocb, from); } ret = ext4_dio_write_checks(iocb, from, &ilock_shared, &extend); if (ret <= 0) return ret; if ((iocb->ki_flags & IOCB_NOWAIT) && (unaligned_io || extend)) { ret = -EAGAIN; goto out; } ext4_clear_inode_state(inode, EXT4_STATE_MAY_INLINE_DATA); offset = iocb->ki_pos; count = ret; if (unaligned_io) inode_dio_wait(inode); if (extend) { handle = ext4_journal_start(inode, EXT4_HT_INODE, 2); if (IS_ERR(handle)) { ret = PTR_ERR(handle); goto out; } ret = ext4_orphan_add(handle, inode); if (ret) { ext4_journal_stop(handle); goto out; } ext4_journal_stop(handle); } if (ilock_shared) iomap_ops = &ext4_iomap_overwrite_ops; ret = iomap_dio_rw(iocb, from, iomap_ops, &ext4_dio_write_ops, is_sync_kiocb(iocb) || unaligned_io || extend); if (ret == -ENOTBLK) ret = 0; if (extend) { WARN_ON_ONCE(ret == -EIOCBQUEUED); ext4_inode_extension_cleanup(inode, ret < 0); } out: if (ilock_shared) inode_unlock_shared(inode); else inode_unlock(inode); if (ret >= 0 && iov_iter_count(from)) { ssize_t err; loff_t endbyte; offset = iocb->ki_pos; err = ext4_buffered_write_iter(iocb, from); if (err < 0) return err; ret += err; endbyte = offset + err - 1; err = filemap_write_and_wait_range(iocb->ki_filp->f_mapping, offset, endbyte); if (!err) invalidate_mapping_pages(iocb->ki_filp->f_mapping, offset >> PAGE_SHIFT, endbyte >> PAGE_SHIFT); } return ret; }"
1235----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53689/bad/blk-mq.c----__blk_mq_update_nr_hw_queues,"static void __blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set, int nr_hw_queues) { struct request_queue *q; LIST_HEAD(head); int prev_nr_hw_queues = set->nr_hw_queues; int i; lockdep_assert_held(&set->tag_list_lock); if (set->nr_maps == 1 && nr_hw_queues > nr_cpu_ids) nr_hw_queues = nr_cpu_ids; if (nr_hw_queues < 1) return; if (set->nr_maps == 1 && nr_hw_queues == set->nr_hw_queues) return; <S2SV_StartVul> list_for_each_entry(q, &set->tag_list, tag_set_list) <S2SV_EndVul> blk_mq_freeze_queue(q); list_for_each_entry(q, &set->tag_list, tag_set_list) if (!blk_mq_elv_switch_none(&head, q)) goto switch_back; list_for_each_entry(q, &set->tag_list, tag_set_list) { blk_mq_debugfs_unregister_hctxs(q); blk_mq_sysfs_unregister_hctxs(q); } if (blk_mq_realloc_tag_set_tags(set, nr_hw_queues) < 0) goto reregister; fallback: blk_mq_update_queue_map(set); list_for_each_entry(q, &set->tag_list, tag_set_list) { struct queue_limits lim; blk_mq_realloc_hw_ctxs(set, q); if (q->nr_hw_queues != set->nr_hw_queues) { int i = prev_nr_hw_queues; pr_warn(""Increasing nr_hw_queues to %d fails, fallback to %d\n"", nr_hw_queues, prev_nr_hw_queues); for (; i < set->nr_hw_queues; i++) __blk_mq_free_map_and_rqs(set, i); set->nr_hw_queues = prev_nr_hw_queues; goto fallback; } lim = queue_limits_start_update(q); if (blk_mq_can_poll(set)) lim.features |= BLK_FEAT_POLL; else lim.features &= ~BLK_FEAT_POLL; if (queue_limits_commit_update(q, &lim) < 0) pr_warn(""updating the poll flag failed\n""); blk_mq_map_swqueue(q); } reregister: list_for_each_entry(q, &set->tag_list, tag_set_list) { blk_mq_sysfs_register_hctxs(q); blk_mq_debugfs_register_hctxs(q); } switch_back: list_for_each_entry(q, &set->tag_list, tag_set_list) blk_mq_elv_switch_back(&head, q); <S2SV_StartVul> list_for_each_entry(q, &set->tag_list, tag_set_list) <S2SV_EndVul> blk_mq_unfreeze_queue(q); for (i = set->nr_hw_queues; i < prev_nr_hw_queues; i++) __blk_mq_free_map_and_rqs(set, i); }","- list_for_each_entry(q, &set->tag_list, tag_set_list)
- list_for_each_entry(q, &set->tag_list, tag_set_list)
+ list_for_each_entry(q, &set->tag_list, tag_set_list) {
+ mutex_lock(&q->sysfs_dir_lock);
+ mutex_lock(&q->sysfs_lock);
+ }
+ list_for_each_entry(q, &set->tag_list, tag_set_list) {
+ mutex_unlock(&q->sysfs_lock);
+ mutex_unlock(&q->sysfs_dir_lock);
+ }","static void __blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set, int nr_hw_queues) { struct request_queue *q; LIST_HEAD(head); int prev_nr_hw_queues = set->nr_hw_queues; int i; lockdep_assert_held(&set->tag_list_lock); if (set->nr_maps == 1 && nr_hw_queues > nr_cpu_ids) nr_hw_queues = nr_cpu_ids; if (nr_hw_queues < 1) return; if (set->nr_maps == 1 && nr_hw_queues == set->nr_hw_queues) return; list_for_each_entry(q, &set->tag_list, tag_set_list) { mutex_lock(&q->sysfs_dir_lock); mutex_lock(&q->sysfs_lock); blk_mq_freeze_queue(q); } list_for_each_entry(q, &set->tag_list, tag_set_list) if (!blk_mq_elv_switch_none(&head, q)) goto switch_back; list_for_each_entry(q, &set->tag_list, tag_set_list) { blk_mq_debugfs_unregister_hctxs(q); blk_mq_sysfs_unregister_hctxs(q); } if (blk_mq_realloc_tag_set_tags(set, nr_hw_queues) < 0) goto reregister; fallback: blk_mq_update_queue_map(set); list_for_each_entry(q, &set->tag_list, tag_set_list) { struct queue_limits lim; blk_mq_realloc_hw_ctxs(set, q); if (q->nr_hw_queues != set->nr_hw_queues) { int i = prev_nr_hw_queues; pr_warn(""Increasing nr_hw_queues to %d fails, fallback to %d\n"", nr_hw_queues, prev_nr_hw_queues); for (; i < set->nr_hw_queues; i++) __blk_mq_free_map_and_rqs(set, i); set->nr_hw_queues = prev_nr_hw_queues; goto fallback; } lim = queue_limits_start_update(q); if (blk_mq_can_poll(set)) lim.features |= BLK_FEAT_POLL; else lim.features &= ~BLK_FEAT_POLL; if (queue_limits_commit_update(q, &lim) < 0) pr_warn(""updating the poll flag failed\n""); blk_mq_map_swqueue(q); } reregister: list_for_each_entry(q, &set->tag_list, tag_set_list) { blk_mq_sysfs_register_hctxs(q); blk_mq_debugfs_register_hctxs(q); } switch_back: list_for_each_entry(q, &set->tag_list, tag_set_list) blk_mq_elv_switch_back(&head, q); list_for_each_entry(q, &set->tag_list, tag_set_list) { blk_mq_unfreeze_queue(q); mutex_unlock(&q->sysfs_lock); mutex_unlock(&q->sysfs_dir_lock); } for (i = set->nr_hw_queues; i < prev_nr_hw_queues; i++) __blk_mq_free_map_and_rqs(set, i); }"
710----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49991/bad/kfd_device.c----kgd2kfd_device_exit,"void kgd2kfd_device_exit(struct kfd_dev *kfd) { if (kfd->init_complete) { kfd_cleanup_nodes(kfd, kfd->num_nodes); kfd_doorbell_fini(kfd); ida_destroy(&kfd->doorbell_ida); kfd_gtt_sa_fini(kfd); <S2SV_StartVul> amdgpu_amdkfd_free_gtt_mem(kfd->adev, kfd->gtt_mem); <S2SV_EndVul> } kfree(kfd); }","- amdgpu_amdkfd_free_gtt_mem(kfd->adev, kfd->gtt_mem);
+ amdgpu_amdkfd_free_gtt_mem(kfd->adev, &kfd->gtt_mem);","void kgd2kfd_device_exit(struct kfd_dev *kfd) { if (kfd->init_complete) { kfd_cleanup_nodes(kfd, kfd->num_nodes); kfd_doorbell_fini(kfd); ida_destroy(&kfd->doorbell_ida); kfd_gtt_sa_fini(kfd); amdgpu_amdkfd_free_gtt_mem(kfd->adev, &kfd->gtt_mem); } kfree(kfd); }"
16----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-43892/bad/memcontrol.c----*mem_cgroup_alloc,"static struct mem_cgroup *mem_cgroup_alloc(void) { struct mem_cgroup *memcg; int node; int __maybe_unused i; long error = -ENOMEM; memcg = kzalloc(struct_size(memcg, nodeinfo, nr_node_ids), GFP_KERNEL); if (!memcg) return ERR_PTR(error); <S2SV_StartVul> memcg->id.id = idr_alloc(&mem_cgroup_idr, NULL, <S2SV_EndVul> <S2SV_StartVul> 1, MEM_CGROUP_ID_MAX + 1, GFP_KERNEL); <S2SV_EndVul> if (memcg->id.id < 0) { error = memcg->id.id; goto fail; } memcg->vmstats = kzalloc(sizeof(struct memcg_vmstats), GFP_KERNEL); if (!memcg->vmstats) goto fail; memcg->vmstats_percpu = alloc_percpu_gfp(struct memcg_vmstats_percpu, GFP_KERNEL_ACCOUNT); if (!memcg->vmstats_percpu) goto fail; for_each_node(node) if (alloc_mem_cgroup_per_node_info(memcg, node)) goto fail; if (memcg_wb_domain_init(memcg, GFP_KERNEL)) goto fail; INIT_WORK(&memcg->high_work, high_work_func); INIT_LIST_HEAD(&memcg->oom_notify); mutex_init(&memcg->thresholds_lock); spin_lock_init(&memcg->move_lock); vmpressure_init(&memcg->vmpressure); INIT_LIST_HEAD(&memcg->event_list); spin_lock_init(&memcg->event_list_lock); memcg->socket_pressure = jiffies; #ifdef CONFIG_MEMCG_KMEM memcg->kmemcg_id = -1; INIT_LIST_HEAD(&memcg->objcg_list); #endif #ifdef CONFIG_CGROUP_WRITEBACK INIT_LIST_HEAD(&memcg->cgwb_list); for (i = 0; i < MEMCG_CGWB_FRN_CNT; i++) memcg->cgwb_frn[i].done = __WB_COMPLETION_INIT(&memcg_cgwb_frn_waitq); #endif #ifdef CONFIG_TRANSPARENT_HUGEPAGE spin_lock_init(&memcg->deferred_split_queue.split_queue_lock); INIT_LIST_HEAD(&memcg->deferred_split_queue.split_queue); memcg->deferred_split_queue.split_queue_len = 0; #endif lru_gen_init_memcg(memcg); return memcg; fail: mem_cgroup_id_remove(memcg); __mem_cgroup_free(memcg); return ERR_PTR(error); }","- memcg->id.id = idr_alloc(&mem_cgroup_idr, NULL,
- 1, MEM_CGROUP_ID_MAX + 1, GFP_KERNEL);
+ memcg->id.id = mem_cgroup_alloc_id();","static struct mem_cgroup *mem_cgroup_alloc(void) { struct mem_cgroup *memcg; int node; int __maybe_unused i; long error = -ENOMEM; memcg = kzalloc(struct_size(memcg, nodeinfo, nr_node_ids), GFP_KERNEL); if (!memcg) return ERR_PTR(error); memcg->id.id = mem_cgroup_alloc_id(); if (memcg->id.id < 0) { error = memcg->id.id; goto fail; } memcg->vmstats = kzalloc(sizeof(struct memcg_vmstats), GFP_KERNEL); if (!memcg->vmstats) goto fail; memcg->vmstats_percpu = alloc_percpu_gfp(struct memcg_vmstats_percpu, GFP_KERNEL_ACCOUNT); if (!memcg->vmstats_percpu) goto fail; for_each_node(node) if (alloc_mem_cgroup_per_node_info(memcg, node)) goto fail; if (memcg_wb_domain_init(memcg, GFP_KERNEL)) goto fail; INIT_WORK(&memcg->high_work, high_work_func); INIT_LIST_HEAD(&memcg->oom_notify); mutex_init(&memcg->thresholds_lock); spin_lock_init(&memcg->move_lock); vmpressure_init(&memcg->vmpressure); INIT_LIST_HEAD(&memcg->event_list); spin_lock_init(&memcg->event_list_lock); memcg->socket_pressure = jiffies; #ifdef CONFIG_MEMCG_KMEM memcg->kmemcg_id = -1; INIT_LIST_HEAD(&memcg->objcg_list); #endif #ifdef CONFIG_CGROUP_WRITEBACK INIT_LIST_HEAD(&memcg->cgwb_list); for (i = 0; i < MEMCG_CGWB_FRN_CNT; i++) memcg->cgwb_frn[i].done = __WB_COMPLETION_INIT(&memcg_cgwb_frn_waitq); #endif #ifdef CONFIG_TRANSPARENT_HUGEPAGE spin_lock_init(&memcg->deferred_split_queue.split_queue_lock); INIT_LIST_HEAD(&memcg->deferred_split_queue.split_queue); memcg->deferred_split_queue.split_queue_len = 0; #endif lru_gen_init_memcg(memcg); return memcg; fail: mem_cgroup_id_remove(memcg); __mem_cgroup_free(memcg); return ERR_PTR(error); }"
1034----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50274/bad/idpf_virtchnl.c----idpf_handle_event_link,"static void idpf_handle_event_link(struct idpf_adapter *adapter, const struct virtchnl2_event *v2e) { struct idpf_netdev_priv *np; struct idpf_vport *vport; vport = idpf_vid_to_vport(adapter, le32_to_cpu(v2e->vport_id)); if (!vport) { dev_err_ratelimited(&adapter->pdev->dev, ""Failed to find vport_id %d for link event\n"", v2e->vport_id); return; } np = netdev_priv(vport->netdev); <S2SV_StartVul> vport->link_speed_mbps = le32_to_cpu(v2e->link_speed); <S2SV_EndVul> if (vport->link_up == v2e->link_status) return; vport->link_up = v2e->link_status; if (np->state != __IDPF_VPORT_UP) return; if (vport->link_up) { netif_tx_start_all_queues(vport->netdev); netif_carrier_on(vport->netdev); } else { netif_tx_stop_all_queues(vport->netdev); netif_carrier_off(vport->netdev); } }","- vport->link_speed_mbps = le32_to_cpu(v2e->link_speed);
+ np->link_speed_mbps = le32_to_cpu(v2e->link_speed);","static void idpf_handle_event_link(struct idpf_adapter *adapter, const struct virtchnl2_event *v2e) { struct idpf_netdev_priv *np; struct idpf_vport *vport; vport = idpf_vid_to_vport(adapter, le32_to_cpu(v2e->vport_id)); if (!vport) { dev_err_ratelimited(&adapter->pdev->dev, ""Failed to find vport_id %d for link event\n"", v2e->vport_id); return; } np = netdev_priv(vport->netdev); np->link_speed_mbps = le32_to_cpu(v2e->link_speed); if (vport->link_up == v2e->link_status) return; vport->link_up = v2e->link_status; if (np->state != __IDPF_VPORT_UP) return; if (vport->link_up) { netif_tx_start_all_queues(vport->netdev); netif_carrier_on(vport->netdev); } else { netif_tx_stop_all_queues(vport->netdev); netif_carrier_off(vport->netdev); } }"
278----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46758/bad/lm95234.c----tcrit1_store,"static ssize_t tcrit1_store(struct device *dev, struct device_attribute *attr, const char *buf, size_t count) { struct lm95234_data *data = dev_get_drvdata(dev); int index = to_sensor_dev_attr(attr)->index; int ret = lm95234_update_device(data); long val; if (ret) return ret; ret = kstrtol(buf, 10, &val); if (ret < 0) return ret; <S2SV_StartVul> val = clamp_val(DIV_ROUND_CLOSEST(val, 1000), 0, 255); <S2SV_EndVul> mutex_lock(&data->update_lock); data->tcrit1[index] = val; i2c_smbus_write_byte_data(data->client, LM95234_REG_TCRIT1(index), val); mutex_unlock(&data->update_lock); return count; }","- val = clamp_val(DIV_ROUND_CLOSEST(val, 1000), 0, 255);
+ val = DIV_ROUND_CLOSEST(clamp_val(val, 0, 255000), 1000);","static ssize_t tcrit1_store(struct device *dev, struct device_attribute *attr, const char *buf, size_t count) { struct lm95234_data *data = dev_get_drvdata(dev); int index = to_sensor_dev_attr(attr)->index; int ret = lm95234_update_device(data); long val; if (ret) return ret; ret = kstrtol(buf, 10, &val); if (ret < 0) return ret; val = DIV_ROUND_CLOSEST(clamp_val(val, 0, 255000), 1000); mutex_lock(&data->update_lock); data->tcrit1[index] = val; i2c_smbus_write_byte_data(data->client, LM95234_REG_TCRIT1(index), val); mutex_unlock(&data->update_lock); return count; }"
464----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47705/bad/core.c----blk_add_partition,"static bool blk_add_partition(struct gendisk *disk, struct parsed_partitions *state, int p) { sector_t size = state->parts[p].size; sector_t from = state->parts[p].from; struct block_device *part; if (!size) return true; if (from >= get_capacity(disk)) { printk(KERN_WARNING ""%s: p%d start %llu is beyond EOD, "", disk->disk_name, p, (unsigned long long) from); if (disk_unlock_native_capacity(disk)) return false; return true; } if (from + size > get_capacity(disk)) { printk(KERN_WARNING ""%s: p%d size %llu extends beyond EOD, "", disk->disk_name, p, (unsigned long long) size); if (disk_unlock_native_capacity(disk)) return false; size = get_capacity(disk) - from; } part = add_partition(disk, p, from, size, state->parts[p].flags, &state->parts[p].info); <S2SV_StartVul> if (IS_ERR(part) && PTR_ERR(part) != -ENXIO) { <S2SV_EndVul> <S2SV_StartVul> printk(KERN_ERR "" %s: p%d could not be added: %pe\n"", <S2SV_EndVul> <S2SV_StartVul> disk->disk_name, p, part); <S2SV_EndVul> return true; } if (IS_BUILTIN(CONFIG_BLK_DEV_MD) && (state->parts[p].flags & ADDPART_FLAG_RAID)) md_autodetect_dev(part->bd_dev); return true; }","- if (IS_ERR(part) && PTR_ERR(part) != -ENXIO) {
- printk(KERN_ERR "" %s: p%d could not be added: %pe\n"",
- disk->disk_name, p, part);
+ if (IS_ERR(part)) {
+ if (PTR_ERR(part) != -ENXIO) {
+ printk(KERN_ERR "" %s: p%d could not be added: %pe\n"",
+ disk->disk_name, p, part);
+ }
+ }","static bool blk_add_partition(struct gendisk *disk, struct parsed_partitions *state, int p) { sector_t size = state->parts[p].size; sector_t from = state->parts[p].from; struct block_device *part; if (!size) return true; if (from >= get_capacity(disk)) { printk(KERN_WARNING ""%s: p%d start %llu is beyond EOD, "", disk->disk_name, p, (unsigned long long) from); if (disk_unlock_native_capacity(disk)) return false; return true; } if (from + size > get_capacity(disk)) { printk(KERN_WARNING ""%s: p%d size %llu extends beyond EOD, "", disk->disk_name, p, (unsigned long long) size); if (disk_unlock_native_capacity(disk)) return false; size = get_capacity(disk) - from; } part = add_partition(disk, p, from, size, state->parts[p].flags, &state->parts[p].info); if (IS_ERR(part)) { if (PTR_ERR(part) != -ENXIO) { printk(KERN_ERR "" %s: p%d could not be added: %pe\n"", disk->disk_name, p, part); } return true; } if (IS_BUILTIN(CONFIG_BLK_DEV_MD) && (state->parts[p].flags & ADDPART_FLAG_RAID)) md_autodetect_dev(part->bd_dev); return true; }"
1408----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56739/bad/interface.c----rtc_timer_do_work,"void rtc_timer_do_work(struct work_struct *work) { struct rtc_timer *timer; struct timerqueue_node *next; ktime_t now; struct rtc_time tm; struct rtc_device *rtc = container_of(work, struct rtc_device, irqwork); mutex_lock(&rtc->ops_lock); again: <S2SV_StartVul> __rtc_read_time(rtc, &tm); <S2SV_EndVul> now = rtc_tm_to_ktime(tm); while ((next = timerqueue_getnext(&rtc->timerqueue))) { if (next->expires > now) break; timer = container_of(next, struct rtc_timer, node); timerqueue_del(&rtc->timerqueue, &timer->node); trace_rtc_timer_dequeue(timer); timer->enabled = 0; if (timer->func) timer->func(timer->rtc); trace_rtc_timer_fired(timer); if (ktime_to_ns(timer->period)) { timer->node.expires = ktime_add(timer->node.expires, timer->period); timer->enabled = 1; timerqueue_add(&rtc->timerqueue, &timer->node); trace_rtc_timer_enqueue(timer); } } if (next) { struct rtc_wkalrm alarm; int err; int retry = 3; alarm.time = rtc_ktime_to_tm(next->expires); alarm.enabled = 1; reprogram: err = __rtc_set_alarm(rtc, &alarm); if (err == -ETIME) { goto again; } else if (err) { if (retry-- > 0) goto reprogram; timer = container_of(next, struct rtc_timer, node); timerqueue_del(&rtc->timerqueue, &timer->node); trace_rtc_timer_dequeue(timer); timer->enabled = 0; dev_err(&rtc->dev, ""__rtc_set_alarm: err=%d\n"", err); goto again; } } else { rtc_alarm_disable(rtc); } pm_relax(rtc->dev.parent); mutex_unlock(&rtc->ops_lock); }","- __rtc_read_time(rtc, &tm);
+ int err;
+ err = __rtc_read_time(rtc, &tm);
+ if (err) {
+ mutex_unlock(&rtc->ops_lock);
+ return;
+ }","void rtc_timer_do_work(struct work_struct *work) { struct rtc_timer *timer; struct timerqueue_node *next; ktime_t now; struct rtc_time tm; int err; struct rtc_device *rtc = container_of(work, struct rtc_device, irqwork); mutex_lock(&rtc->ops_lock); again: err = __rtc_read_time(rtc, &tm); if (err) { mutex_unlock(&rtc->ops_lock); return; } now = rtc_tm_to_ktime(tm); while ((next = timerqueue_getnext(&rtc->timerqueue))) { if (next->expires > now) break; timer = container_of(next, struct rtc_timer, node); timerqueue_del(&rtc->timerqueue, &timer->node); trace_rtc_timer_dequeue(timer); timer->enabled = 0; if (timer->func) timer->func(timer->rtc); trace_rtc_timer_fired(timer); if (ktime_to_ns(timer->period)) { timer->node.expires = ktime_add(timer->node.expires, timer->period); timer->enabled = 1; timerqueue_add(&rtc->timerqueue, &timer->node); trace_rtc_timer_enqueue(timer); } } if (next) { struct rtc_wkalrm alarm; int err; int retry = 3; alarm.time = rtc_ktime_to_tm(next->expires); alarm.enabled = 1; reprogram: err = __rtc_set_alarm(rtc, &alarm); if (err == -ETIME) { goto again; } else if (err) { if (retry-- > 0) goto reprogram; timer = container_of(next, struct rtc_timer, node); timerqueue_del(&rtc->timerqueue, &timer->node); trace_rtc_timer_dequeue(timer); timer->enabled = 0; dev_err(&rtc->dev, ""__rtc_set_alarm: err=%d\n"", err); goto again; } } else { rtc_alarm_disable(rtc); } pm_relax(rtc->dev.parent); mutex_unlock(&rtc->ops_lock); }"
1473----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-57890/bad/uverbs_cmd.c----*uverbs_request_next_ptr,"static const void __user *uverbs_request_next_ptr(struct uverbs_req_iter *iter, size_t len) { const void __user *res = iter->cur; <S2SV_StartVul> if (iter->cur + len > iter->end) <S2SV_EndVul> return (void __force __user *)ERR_PTR(-ENOSPC); iter->cur += len; return res; }","- if (iter->cur + len > iter->end)
+ if (len > iter->end - iter->cur)","static const void __user *uverbs_request_next_ptr(struct uverbs_req_iter *iter, size_t len) { const void __user *res = iter->cur; if (len > iter->end - iter->cur) return (void __force __user *)ERR_PTR(-ENOSPC); iter->cur += len; return res; }"
421----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47661/bad/dcn21_hwseq.c----dmub_abm_set_pipe,"bool dmub_abm_set_pipe(struct abm *abm, uint32_t otg_inst, uint32_t option, uint32_t panel_inst, uint32_t pwrseq_inst) <S2SV_StartVul> { <S2SV_EndVul> union dmub_rb_cmd cmd; struct dc_context *dc = abm->ctx; uint32_t ramping_boundary = 0xFFFF; memset(&cmd, 0, sizeof(cmd)); cmd.abm_set_pipe.header.type = DMUB_CMD__ABM; cmd.abm_set_pipe.header.sub_type = DMUB_CMD__ABM_SET_PIPE; cmd.abm_set_pipe.abm_set_pipe_data.otg_inst = otg_inst; cmd.abm_set_pipe.abm_set_pipe_data.pwrseq_inst = pwrseq_inst; cmd.abm_set_pipe.abm_set_pipe_data.set_pipe_option = option; cmd.abm_set_pipe.abm_set_pipe_data.panel_inst = panel_inst; cmd.abm_set_pipe.abm_set_pipe_data.ramping_boundary = ramping_boundary; cmd.abm_set_pipe.header.payload_bytes = sizeof(struct dmub_cmd_abm_set_pipe_data); dc_wake_and_execute_dmub_cmd(dc, &cmd, DM_DMUB_WAIT_TYPE_WAIT); return true; }","- {
+ {","bool dmub_abm_set_pipe(struct abm *abm, uint32_t otg_inst, uint32_t option, uint32_t panel_inst, uint32_t pwrseq_inst) { union dmub_rb_cmd cmd; struct dc_context *dc = abm->ctx; uint8_t ramping_boundary = 0xFF; memset(&cmd, 0, sizeof(cmd)); cmd.abm_set_pipe.header.type = DMUB_CMD__ABM; cmd.abm_set_pipe.header.sub_type = DMUB_CMD__ABM_SET_PIPE; cmd.abm_set_pipe.abm_set_pipe_data.otg_inst = otg_inst; cmd.abm_set_pipe.abm_set_pipe_data.pwrseq_inst = pwrseq_inst; cmd.abm_set_pipe.abm_set_pipe_data.set_pipe_option = option; cmd.abm_set_pipe.abm_set_pipe_data.panel_inst = panel_inst; cmd.abm_set_pipe.abm_set_pipe_data.ramping_boundary = ramping_boundary; cmd.abm_set_pipe.header.payload_bytes = sizeof(struct dmub_cmd_abm_set_pipe_data); dc_wake_and_execute_dmub_cmd(dc, &cmd, DM_DMUB_WAIT_TYPE_WAIT); return true; }"
753----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50028/bad/thermal_netlink.c----thermal_genl_cmd_tz_get_gov,"static int thermal_genl_cmd_tz_get_gov(struct param *p) { struct sk_buff *msg = p->msg; <S2SV_StartVul> struct thermal_zone_device *tz; <S2SV_EndVul> int id, ret = 0; if (!p->attrs[THERMAL_GENL_ATTR_TZ_ID]) return -EINVAL; id = nla_get_u32(p->attrs[THERMAL_GENL_ATTR_TZ_ID]); <S2SV_StartVul> tz = thermal_zone_get_by_id(id); <S2SV_EndVul> if (!tz) return -EINVAL; mutex_lock(&tz->lock); if (nla_put_u32(msg, THERMAL_GENL_ATTR_TZ_ID, id) || nla_put_string(msg, THERMAL_GENL_ATTR_TZ_GOV_NAME, tz->governor->name)) ret = -EMSGSIZE; mutex_unlock(&tz->lock); return ret; }","- struct thermal_zone_device *tz;
- tz = thermal_zone_get_by_id(id);
+ CLASS(thermal_zone_get_by_id, tz)(id);","static int thermal_genl_cmd_tz_get_gov(struct param *p) { struct sk_buff *msg = p->msg; int id, ret = 0; if (!p->attrs[THERMAL_GENL_ATTR_TZ_ID]) return -EINVAL; id = nla_get_u32(p->attrs[THERMAL_GENL_ATTR_TZ_ID]); CLASS(thermal_zone_get_by_id, tz)(id); if (!tz) return -EINVAL; mutex_lock(&tz->lock); if (nla_put_u32(msg, THERMAL_GENL_ATTR_TZ_ID, id) || nla_put_string(msg, THERMAL_GENL_ATTR_TZ_GOV_NAME, tz->governor->name)) ret = -EMSGSIZE; mutex_unlock(&tz->lock); return ret; }"
1224----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53235/bad/data.c----erofs_init_metabuf,"void erofs_init_metabuf(struct erofs_buf *buf, struct super_block *sb) { struct erofs_sb_info *sbi = EROFS_SB(sb); <S2SV_StartVul> if (erofs_is_fileio_mode(sbi)) <S2SV_EndVul> <S2SV_StartVul> buf->mapping = file_inode(sbi->fdev)->i_mapping; <S2SV_EndVul> <S2SV_StartVul> else if (erofs_is_fscache_mode(sb)) <S2SV_EndVul> buf->mapping = sbi->s_fscache->inode->i_mapping; else buf->mapping = sb->s_bdev->bd_mapping; }","- if (erofs_is_fileio_mode(sbi))
- buf->mapping = file_inode(sbi->fdev)->i_mapping;
- else if (erofs_is_fscache_mode(sb))
+ buf->file = NULL;
+ if (erofs_is_fileio_mode(sbi)) {
+ buf->mapping = buf->file->f_mapping;
+ } else if (erofs_is_fscache_mode(sb))","void erofs_init_metabuf(struct erofs_buf *buf, struct super_block *sb) { struct erofs_sb_info *sbi = EROFS_SB(sb); buf->file = NULL; if (erofs_is_fileio_mode(sbi)) { buf->file = sbi->fdev; buf->mapping = buf->file->f_mapping; } else if (erofs_is_fscache_mode(sb)) buf->mapping = sbi->s_fscache->inode->i_mapping; else buf->mapping = sb->s_bdev->bd_mapping; }"
1127----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53098/bad/xe_sync.c----*user_fence_create,"static struct xe_user_fence *user_fence_create(struct xe_device *xe, u64 addr, u64 value) { struct xe_user_fence *ufence; u64 __user *ptr = u64_to_user_ptr(addr); <S2SV_StartVul> if (!access_ok(ptr, sizeof(*ptr))) <S2SV_EndVul> return ERR_PTR(-EFAULT); ufence = kzalloc(sizeof(*ufence), GFP_KERNEL); if (!ufence) return ERR_PTR(-ENOMEM); ufence->xe = xe; kref_init(&ufence->refcount); ufence->addr = ptr; ufence->value = value; ufence->mm = current->mm; mmgrab(ufence->mm); return ufence; }","- if (!access_ok(ptr, sizeof(*ptr)))
+ u64 __maybe_unused prefetch_val;
+ if (get_user(prefetch_val, ptr))","static struct xe_user_fence *user_fence_create(struct xe_device *xe, u64 addr, u64 value) { struct xe_user_fence *ufence; u64 __user *ptr = u64_to_user_ptr(addr); u64 __maybe_unused prefetch_val; if (get_user(prefetch_val, ptr)) return ERR_PTR(-EFAULT); ufence = kzalloc(sizeof(*ufence), GFP_KERNEL); if (!ufence) return ERR_PTR(-ENOMEM); ufence->xe = xe; kref_init(&ufence->refcount); ufence->addr = ptr; ufence->value = value; ufence->mm = current->mm; mmgrab(ufence->mm); return ufence; }"
837----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50100/bad/dummy_hcd.c----dummy_urb_dequeue,"static int dummy_urb_dequeue(struct usb_hcd *hcd, struct urb *urb, int status) { struct dummy_hcd *dum_hcd; unsigned long flags; int rc; dum_hcd = hcd_to_dummy_hcd(hcd); spin_lock_irqsave(&dum_hcd->dum->lock, flags); rc = usb_hcd_check_unlink_urb(hcd, urb, status); <S2SV_StartVul> if (!rc && dum_hcd->rh_state != DUMMY_RH_RUNNING && <S2SV_EndVul> <S2SV_StartVul> !list_empty(&dum_hcd->urbp_list)) <S2SV_EndVul> hrtimer_start(&dum_hcd->timer, ns_to_ktime(0), HRTIMER_MODE_REL_SOFT); spin_unlock_irqrestore(&dum_hcd->dum->lock, flags); return rc; }","- if (!rc && dum_hcd->rh_state != DUMMY_RH_RUNNING &&
- !list_empty(&dum_hcd->urbp_list))
+ if (rc == 0 && !dum_hcd->timer_pending) {
+ dum_hcd->timer_pending = 1;
+ }","static int dummy_urb_dequeue(struct usb_hcd *hcd, struct urb *urb, int status) { struct dummy_hcd *dum_hcd; unsigned long flags; int rc; dum_hcd = hcd_to_dummy_hcd(hcd); spin_lock_irqsave(&dum_hcd->dum->lock, flags); rc = usb_hcd_check_unlink_urb(hcd, urb, status); if (rc == 0 && !dum_hcd->timer_pending) { dum_hcd->timer_pending = 1; hrtimer_start(&dum_hcd->timer, ns_to_ktime(0), HRTIMER_MODE_REL_SOFT); } spin_unlock_irqrestore(&dum_hcd->dum->lock, flags); return rc; }"
653----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49955/bad/battery.c----battery_hook_register,"void battery_hook_register(struct acpi_battery_hook *hook) { struct acpi_battery *battery; mutex_lock(&hook_mutex); <S2SV_StartVul> INIT_LIST_HEAD(&hook->list); <S2SV_EndVul> list_add(&hook->list, &battery_hook_list); list_for_each_entry(battery, &acpi_battery_list, list) { if (hook->add_battery(battery->bat, hook)) { pr_err(""extension failed to load: %s"", hook->name); battery_hook_unregister_unlocked(hook); goto end; } power_supply_changed(battery->bat); } pr_info(""new extension: %s\n"", hook->name); end: mutex_unlock(&hook_mutex); }",- INIT_LIST_HEAD(&hook->list);,"void battery_hook_register(struct acpi_battery_hook *hook) { struct acpi_battery *battery; mutex_lock(&hook_mutex); list_add(&hook->list, &battery_hook_list); list_for_each_entry(battery, &acpi_battery_list, list) { if (hook->add_battery(battery->bat, hook)) { pr_err(""extension failed to load: %s"", hook->name); battery_hook_unregister_unlocked(hook); goto end; } power_supply_changed(battery->bat); } pr_info(""new extension: %s\n"", hook->name); end: mutex_unlock(&hook_mutex); }"
832----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50098/bad/ufshcd.c----ufshcd_wl_shutdown,"static void ufshcd_wl_shutdown(struct device *dev) { struct scsi_device *sdev = to_scsi_device(dev); struct ufs_hba *hba = shost_priv(sdev->host); down(&hba->host_sem); hba->shutting_down = true; up(&hba->host_sem); ufshcd_rpm_get_sync(hba); scsi_device_quiesce(sdev); shost_for_each_device(sdev, hba->host) { if (sdev == hba->ufs_device_wlun) continue; <S2SV_StartVul> scsi_device_quiesce(sdev); <S2SV_EndVul> } __ufshcd_wl_suspend(hba, UFS_SHUTDOWN_PM); if (ufshcd_is_ufs_dev_poweroff(hba) && ufshcd_is_link_off(hba)) ufshcd_suspend(hba); hba->is_powered = false; }","- scsi_device_quiesce(sdev);
+ mutex_lock(&sdev->state_mutex);
+ scsi_device_set_state(sdev, SDEV_OFFLINE);
+ mutex_unlock(&sdev->state_mutex);","static void ufshcd_wl_shutdown(struct device *dev) { struct scsi_device *sdev = to_scsi_device(dev); struct ufs_hba *hba = shost_priv(sdev->host); down(&hba->host_sem); hba->shutting_down = true; up(&hba->host_sem); ufshcd_rpm_get_sync(hba); scsi_device_quiesce(sdev); shost_for_each_device(sdev, hba->host) { if (sdev == hba->ufs_device_wlun) continue; mutex_lock(&sdev->state_mutex); scsi_device_set_state(sdev, SDEV_OFFLINE); mutex_unlock(&sdev->state_mutex); } __ufshcd_wl_suspend(hba, UFS_SHUTDOWN_PM); if (ufshcd_is_ufs_dev_poweroff(hba) && ufshcd_is_link_off(hba)) ufshcd_suspend(hba); hba->is_powered = false; }"
1323----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56635/bad/link_watch.c----default_operstate,"static unsigned int default_operstate(const struct net_device *dev) { if (netif_testing(dev)) return IF_OPER_TESTING; if (!netif_carrier_ok(dev)) { int iflink = dev_get_iflink(dev); struct net_device *peer; <S2SV_StartVul> if (iflink == dev->ifindex) <S2SV_EndVul> return IF_OPER_DOWN; peer = __dev_get_by_index(dev_net(dev), iflink); if (!peer) return IF_OPER_DOWN; return netif_carrier_ok(peer) ? IF_OPER_DOWN : IF_OPER_LOWERLAYERDOWN; } if (netif_dormant(dev)) return IF_OPER_DORMANT; return IF_OPER_UP; }","- if (iflink == dev->ifindex)
+ if (dev->reg_state == NETREG_UNREGISTERED ||
+ iflink == dev->ifindex)
+ ASSERT_RTNL();","static unsigned int default_operstate(const struct net_device *dev) { if (netif_testing(dev)) return IF_OPER_TESTING; if (!netif_carrier_ok(dev)) { int iflink = dev_get_iflink(dev); struct net_device *peer; if (dev->reg_state == NETREG_UNREGISTERED || iflink == dev->ifindex) return IF_OPER_DOWN; ASSERT_RTNL(); peer = __dev_get_by_index(dev_net(dev), iflink); if (!peer) return IF_OPER_DOWN; return netif_carrier_ok(peer) ? IF_OPER_DOWN : IF_OPER_LOWERLAYERDOWN; } if (netif_dormant(dev)) return IF_OPER_DORMANT; return IF_OPER_UP; }"
1204----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53207/bad/mgmt.c----read_local_oob_ext_data_complete,"static void read_local_oob_ext_data_complete(struct hci_dev *hdev, void *data, int err) { const struct mgmt_cp_read_local_oob_ext_data *mgmt_cp; struct mgmt_rp_read_local_oob_ext_data *mgmt_rp; u8 *h192, *r192, *h256, *r256; struct mgmt_pending_cmd *cmd = data; struct sk_buff *skb = cmd->skb; u8 status = mgmt_status(err); u16 eir_len; <S2SV_StartVul> if (cmd != pending_find(MGMT_OP_READ_LOCAL_OOB_EXT_DATA, hdev)) <S2SV_EndVul> return; if (!status) { if (!skb) status = MGMT_STATUS_FAILED; else if (IS_ERR(skb)) status = mgmt_status(PTR_ERR(skb)); else status = mgmt_status(skb->data[0]); } bt_dev_dbg(hdev, ""status %u"", status); mgmt_cp = cmd->param; if (status) { status = mgmt_status(status); eir_len = 0; h192 = NULL; r192 = NULL; h256 = NULL; r256 = NULL; } else if (!bredr_sc_enabled(hdev)) { struct hci_rp_read_local_oob_data *rp; if (skb->len != sizeof(*rp)) { status = MGMT_STATUS_FAILED; eir_len = 0; } else { status = MGMT_STATUS_SUCCESS; rp = (void *)skb->data; eir_len = 5 + 18 + 18; h192 = rp->hash; r192 = rp->rand; h256 = NULL; r256 = NULL; } } else { struct hci_rp_read_local_oob_ext_data *rp; if (skb->len != sizeof(*rp)) { status = MGMT_STATUS_FAILED; eir_len = 0; } else { status = MGMT_STATUS_SUCCESS; rp = (void *)skb->data; if (hci_dev_test_flag(hdev, HCI_SC_ONLY)) { eir_len = 5 + 18 + 18; h192 = NULL; r192 = NULL; } else { eir_len = 5 + 18 + 18 + 18 + 18; h192 = rp->hash192; r192 = rp->rand192; } h256 = rp->hash256; r256 = rp->rand256; } } mgmt_rp = kmalloc(sizeof(*mgmt_rp) + eir_len, GFP_KERNEL); if (!mgmt_rp) goto done; if (eir_len == 0) goto send_rsp; eir_len = eir_append_data(mgmt_rp->eir, 0, EIR_CLASS_OF_DEV, hdev->dev_class, 3); if (h192 && r192) { eir_len = eir_append_data(mgmt_rp->eir, eir_len, EIR_SSP_HASH_C192, h192, 16); eir_len = eir_append_data(mgmt_rp->eir, eir_len, EIR_SSP_RAND_R192, r192, 16); } if (h256 && r256) { eir_len = eir_append_data(mgmt_rp->eir, eir_len, EIR_SSP_HASH_C256, h256, 16); eir_len = eir_append_data(mgmt_rp->eir, eir_len, EIR_SSP_RAND_R256, r256, 16); } send_rsp: mgmt_rp->type = mgmt_cp->type; mgmt_rp->eir_len = cpu_to_le16(eir_len); err = mgmt_cmd_complete(cmd->sk, hdev->id, MGMT_OP_READ_LOCAL_OOB_EXT_DATA, status, mgmt_rp, sizeof(*mgmt_rp) + eir_len); if (err < 0 || status) goto done; hci_sock_set_flag(cmd->sk, HCI_MGMT_OOB_DATA_EVENTS); err = mgmt_limited_event(MGMT_EV_LOCAL_OOB_DATA_UPDATED, hdev, mgmt_rp, sizeof(*mgmt_rp) + eir_len, HCI_MGMT_OOB_DATA_EVENTS, cmd->sk); done: if (skb && !IS_ERR(skb)) kfree_skb(skb); kfree(mgmt_rp); mgmt_pending_remove(cmd); }","- if (cmd != pending_find(MGMT_OP_READ_LOCAL_OOB_EXT_DATA, hdev))
+ if (err == -ECANCELED ||
+ cmd != pending_find(MGMT_OP_READ_LOCAL_OOB_EXT_DATA, hdev))
+ return;","static void read_local_oob_ext_data_complete(struct hci_dev *hdev, void *data, int err) { const struct mgmt_cp_read_local_oob_ext_data *mgmt_cp; struct mgmt_rp_read_local_oob_ext_data *mgmt_rp; u8 *h192, *r192, *h256, *r256; struct mgmt_pending_cmd *cmd = data; struct sk_buff *skb = cmd->skb; u8 status = mgmt_status(err); u16 eir_len; if (err == -ECANCELED || cmd != pending_find(MGMT_OP_READ_LOCAL_OOB_EXT_DATA, hdev)) return; if (!status) { if (!skb) status = MGMT_STATUS_FAILED; else if (IS_ERR(skb)) status = mgmt_status(PTR_ERR(skb)); else status = mgmt_status(skb->data[0]); } bt_dev_dbg(hdev, ""status %u"", status); mgmt_cp = cmd->param; if (status) { status = mgmt_status(status); eir_len = 0; h192 = NULL; r192 = NULL; h256 = NULL; r256 = NULL; } else if (!bredr_sc_enabled(hdev)) { struct hci_rp_read_local_oob_data *rp; if (skb->len != sizeof(*rp)) { status = MGMT_STATUS_FAILED; eir_len = 0; } else { status = MGMT_STATUS_SUCCESS; rp = (void *)skb->data; eir_len = 5 + 18 + 18; h192 = rp->hash; r192 = rp->rand; h256 = NULL; r256 = NULL; } } else { struct hci_rp_read_local_oob_ext_data *rp; if (skb->len != sizeof(*rp)) { status = MGMT_STATUS_FAILED; eir_len = 0; } else { status = MGMT_STATUS_SUCCESS; rp = (void *)skb->data; if (hci_dev_test_flag(hdev, HCI_SC_ONLY)) { eir_len = 5 + 18 + 18; h192 = NULL; r192 = NULL; } else { eir_len = 5 + 18 + 18 + 18 + 18; h192 = rp->hash192; r192 = rp->rand192; } h256 = rp->hash256; r256 = rp->rand256; } } mgmt_rp = kmalloc(sizeof(*mgmt_rp) + eir_len, GFP_KERNEL); if (!mgmt_rp) goto done; if (eir_len == 0) goto send_rsp; eir_len = eir_append_data(mgmt_rp->eir, 0, EIR_CLASS_OF_DEV, hdev->dev_class, 3); if (h192 && r192) { eir_len = eir_append_data(mgmt_rp->eir, eir_len, EIR_SSP_HASH_C192, h192, 16); eir_len = eir_append_data(mgmt_rp->eir, eir_len, EIR_SSP_RAND_R192, r192, 16); } if (h256 && r256) { eir_len = eir_append_data(mgmt_rp->eir, eir_len, EIR_SSP_HASH_C256, h256, 16); eir_len = eir_append_data(mgmt_rp->eir, eir_len, EIR_SSP_RAND_R256, r256, 16); } send_rsp: mgmt_rp->type = mgmt_cp->type; mgmt_rp->eir_len = cpu_to_le16(eir_len); err = mgmt_cmd_complete(cmd->sk, hdev->id, MGMT_OP_READ_LOCAL_OOB_EXT_DATA, status, mgmt_rp, sizeof(*mgmt_rp) + eir_len); if (err < 0 || status) goto done; hci_sock_set_flag(cmd->sk, HCI_MGMT_OOB_DATA_EVENTS); err = mgmt_limited_event(MGMT_EV_LOCAL_OOB_DATA_UPDATED, hdev, mgmt_rp, sizeof(*mgmt_rp) + eir_len, HCI_MGMT_OOB_DATA_EVENTS, cmd->sk); done: if (skb && !IS_ERR(skb)) kfree_skb(skb); kfree(mgmt_rp); mgmt_pending_remove(cmd); }"
178----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-45026/bad/dasd_eckd.c----dasd_eckd_build_check_tcw,"dasd_eckd_build_check_tcw(struct dasd_device *base, struct format_data_t *fdata, int enable_pav, struct eckd_count *fmt_buffer, int rpt) { struct dasd_eckd_private *start_priv; struct dasd_device *startdev = NULL; struct tidaw *last_tidaw = NULL; struct dasd_ccw_req *cqr; struct itcw *itcw; int itcw_size; int count; int rc; int i; if (enable_pav) startdev = dasd_alias_get_start_dev(base); if (!startdev) startdev = base; start_priv = startdev->private; count = rpt * (fdata->stop_unit - fdata->start_unit + 1); itcw_size = itcw_calc_size(0, count, 0); cqr = dasd_fmalloc_request(DASD_ECKD_MAGIC, 0, itcw_size, startdev); if (IS_ERR(cqr)) return cqr; start_priv->count++; itcw = itcw_init(cqr->data, itcw_size, ITCW_OP_READ, 0, count, 0); if (IS_ERR(itcw)) { rc = -EINVAL; goto out_err; } cqr->cpaddr = itcw_get_tcw(itcw); rc = prepare_itcw(itcw, fdata->start_unit, fdata->stop_unit, DASD_ECKD_CCW_READ_COUNT_MT, base, startdev, 0, count, sizeof(struct eckd_count), count * sizeof(struct eckd_count), 0, rpt); if (rc) goto out_err; for (i = 0; i < count; i++) { last_tidaw = itcw_add_tidaw(itcw, 0, fmt_buffer++, sizeof(struct eckd_count)); if (IS_ERR(last_tidaw)) { rc = -EINVAL; goto out_err; } } last_tidaw->flags |= TIDAW_FLAGS_LAST; itcw_finalize(itcw); cqr->cpmode = 1; cqr->startdev = startdev; cqr->memdev = startdev; cqr->basedev = base; cqr->retries = startdev->default_retries; cqr->expires = startdev->default_expires * HZ; cqr->buildclk = get_tod_clock(); cqr->status = DASD_CQR_FILLED; <S2SV_StartVul> set_bit(DASD_CQR_SUPPRESS_FP, &cqr->flags); <S2SV_EndVul> <S2SV_StartVul> set_bit(DASD_CQR_SUPPRESS_IL, &cqr->flags); <S2SV_EndVul> return cqr; out_err: dasd_sfree_request(cqr, startdev); return ERR_PTR(rc); }","- set_bit(DASD_CQR_SUPPRESS_FP, &cqr->flags);
- set_bit(DASD_CQR_SUPPRESS_IL, &cqr->flags);","dasd_eckd_build_check_tcw(struct dasd_device *base, struct format_data_t *fdata, int enable_pav, struct eckd_count *fmt_buffer, int rpt) { struct dasd_eckd_private *start_priv; struct dasd_device *startdev = NULL; struct tidaw *last_tidaw = NULL; struct dasd_ccw_req *cqr; struct itcw *itcw; int itcw_size; int count; int rc; int i; if (enable_pav) startdev = dasd_alias_get_start_dev(base); if (!startdev) startdev = base; start_priv = startdev->private; count = rpt * (fdata->stop_unit - fdata->start_unit + 1); itcw_size = itcw_calc_size(0, count, 0); cqr = dasd_fmalloc_request(DASD_ECKD_MAGIC, 0, itcw_size, startdev); if (IS_ERR(cqr)) return cqr; start_priv->count++; itcw = itcw_init(cqr->data, itcw_size, ITCW_OP_READ, 0, count, 0); if (IS_ERR(itcw)) { rc = -EINVAL; goto out_err; } cqr->cpaddr = itcw_get_tcw(itcw); rc = prepare_itcw(itcw, fdata->start_unit, fdata->stop_unit, DASD_ECKD_CCW_READ_COUNT_MT, base, startdev, 0, count, sizeof(struct eckd_count), count * sizeof(struct eckd_count), 0, rpt); if (rc) goto out_err; for (i = 0; i < count; i++) { last_tidaw = itcw_add_tidaw(itcw, 0, fmt_buffer++, sizeof(struct eckd_count)); if (IS_ERR(last_tidaw)) { rc = -EINVAL; goto out_err; } } last_tidaw->flags |= TIDAW_FLAGS_LAST; itcw_finalize(itcw); cqr->cpmode = 1; cqr->startdev = startdev; cqr->memdev = startdev; cqr->basedev = base; cqr->retries = startdev->default_retries; cqr->expires = startdev->default_expires * HZ; cqr->buildclk = get_tod_clock(); cqr->status = DASD_CQR_FILLED; set_bit(DASD_CQR_SUPPRESS_IL, &cqr->flags); return cqr; out_err: dasd_sfree_request(cqr, startdev); return ERR_PTR(rc); }"
52----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44935/bad/input.c----__sctp_hash_endpoint,"static int __sctp_hash_endpoint(struct sctp_endpoint *ep) { struct sock *sk = ep->base.sk; struct net *net = sock_net(sk); struct sctp_hashbucket *head; ep->hashent = sctp_ep_hashfn(net, ep->base.bind_addr.port); head = &sctp_ep_hashtable[ep->hashent]; if (sk->sk_reuseport) { bool any = sctp_is_ep_boundall(sk); struct sctp_endpoint *ep2; struct list_head *list; <S2SV_StartVul> int cnt = 0, err = 1; <S2SV_EndVul> list_for_each(list, &ep->base.bind_addr.address_list) cnt++; sctp_for_each_hentry(ep2, &head->chain) { struct sock *sk2 = ep2->base.sk; if (!net_eq(sock_net(sk2), net) || sk2 == sk || !uid_eq(sock_i_uid(sk2), sock_i_uid(sk)) || !sk2->sk_reuseport) continue; err = sctp_bind_addrs_check(sctp_sk(sk2), sctp_sk(sk), cnt); if (!err) { err = reuseport_add_sock(sk, sk2, any); if (err) <S2SV_StartVul> return err; <S2SV_EndVul> break; } else if (err < 0) { <S2SV_StartVul> return err; <S2SV_EndVul> } } if (err) { err = reuseport_alloc(sk, any); if (err) <S2SV_StartVul> return err; <S2SV_EndVul> } } <S2SV_StartVul> write_lock(&head->lock); <S2SV_EndVul> hlist_add_head(&ep->node, &head->chain); write_unlock(&head->lock); <S2SV_StartVul> return 0; <S2SV_EndVul> }","- int cnt = 0, err = 1;
- return err;
- return err;
- return err;
- write_lock(&head->lock);
- return 0;
+ int err = 0;
+ write_lock(&head->lock);
+ int cnt = 0;
+ err = 1;
+ goto out;
+ goto out;
+ goto out;
+ out:
+ return err;","static int __sctp_hash_endpoint(struct sctp_endpoint *ep) { struct sock *sk = ep->base.sk; struct net *net = sock_net(sk); struct sctp_hashbucket *head; int err = 0; ep->hashent = sctp_ep_hashfn(net, ep->base.bind_addr.port); head = &sctp_ep_hashtable[ep->hashent]; write_lock(&head->lock); if (sk->sk_reuseport) { bool any = sctp_is_ep_boundall(sk); struct sctp_endpoint *ep2; struct list_head *list; int cnt = 0; err = 1; list_for_each(list, &ep->base.bind_addr.address_list) cnt++; sctp_for_each_hentry(ep2, &head->chain) { struct sock *sk2 = ep2->base.sk; if (!net_eq(sock_net(sk2), net) || sk2 == sk || !uid_eq(sock_i_uid(sk2), sock_i_uid(sk)) || !sk2->sk_reuseport) continue; err = sctp_bind_addrs_check(sctp_sk(sk2), sctp_sk(sk), cnt); if (!err) { err = reuseport_add_sock(sk, sk2, any); if (err) goto out; break; } else if (err < 0) { goto out; } } if (err) { err = reuseport_alloc(sk, any); if (err) goto out; } } hlist_add_head(&ep->node, &head->chain); out: write_unlock(&head->lock); return err; }"
1136----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53110/bad/vp_vdpa.c----vp_vdpa_probe,"static int vp_vdpa_probe(struct pci_dev *pdev, const struct pci_device_id *id) { struct vp_vdpa_mgmtdev *vp_vdpa_mgtdev = NULL; struct vdpa_mgmt_dev *mgtdev; struct device *dev = &pdev->dev; struct virtio_pci_modern_device *mdev = NULL; struct virtio_device_id *mdev_id = NULL; int err; vp_vdpa_mgtdev = kzalloc(sizeof(*vp_vdpa_mgtdev), GFP_KERNEL); if (!vp_vdpa_mgtdev) return -ENOMEM; mgtdev = &vp_vdpa_mgtdev->mgtdev; mgtdev->ops = &vp_vdpa_mdev_ops; mgtdev->device = dev; mdev = kzalloc(sizeof(struct virtio_pci_modern_device), GFP_KERNEL); if (!mdev) { err = -ENOMEM; goto mdev_err; } <S2SV_StartVul> mdev_id = kzalloc(sizeof(struct virtio_device_id), GFP_KERNEL); <S2SV_EndVul> if (!mdev_id) { err = -ENOMEM; goto mdev_id_err; } vp_vdpa_mgtdev->mdev = mdev; mdev->pci_dev = pdev; err = pcim_enable_device(pdev); if (err) { goto probe_err; } err = vp_modern_probe(mdev); if (err) { dev_err(&pdev->dev, ""Failed to probe modern PCI device\n""); goto probe_err; } <S2SV_StartVul> mdev_id->device = mdev->id.device; <S2SV_EndVul> <S2SV_StartVul> mdev_id->vendor = mdev->id.vendor; <S2SV_EndVul> mgtdev->id_table = mdev_id; mgtdev->max_supported_vqs = vp_modern_get_num_queues(mdev); mgtdev->supported_features = vp_modern_get_features(mdev); mgtdev->config_attr_mask = (1 << VDPA_ATTR_DEV_FEATURES); pci_set_master(pdev); pci_set_drvdata(pdev, vp_vdpa_mgtdev); err = vdpa_mgmtdev_register(mgtdev); if (err) { dev_err(&pdev->dev, ""Failed to register vdpa mgmtdev device\n""); goto register_err; } return 0; register_err: vp_modern_remove(vp_vdpa_mgtdev->mdev); probe_err: kfree(mdev_id); mdev_id_err: kfree(mdev); mdev_err: kfree(vp_vdpa_mgtdev); return err; }","- mdev_id = kzalloc(sizeof(struct virtio_device_id), GFP_KERNEL);
- mdev_id->device = mdev->id.device;
- mdev_id->vendor = mdev->id.vendor;
+ mdev_id = kcalloc(2, sizeof(struct virtio_device_id), GFP_KERNEL);
+ mdev_id[0].device = mdev->id.device;
+ mdev_id[0].vendor = mdev->id.vendor;","static int vp_vdpa_probe(struct pci_dev *pdev, const struct pci_device_id *id) { struct vp_vdpa_mgmtdev *vp_vdpa_mgtdev = NULL; struct vdpa_mgmt_dev *mgtdev; struct device *dev = &pdev->dev; struct virtio_pci_modern_device *mdev = NULL; struct virtio_device_id *mdev_id = NULL; int err; vp_vdpa_mgtdev = kzalloc(sizeof(*vp_vdpa_mgtdev), GFP_KERNEL); if (!vp_vdpa_mgtdev) return -ENOMEM; mgtdev = &vp_vdpa_mgtdev->mgtdev; mgtdev->ops = &vp_vdpa_mdev_ops; mgtdev->device = dev; mdev = kzalloc(sizeof(struct virtio_pci_modern_device), GFP_KERNEL); if (!mdev) { err = -ENOMEM; goto mdev_err; } mdev_id = kcalloc(2, sizeof(struct virtio_device_id), GFP_KERNEL); if (!mdev_id) { err = -ENOMEM; goto mdev_id_err; } vp_vdpa_mgtdev->mdev = mdev; mdev->pci_dev = pdev; err = pcim_enable_device(pdev); if (err) { goto probe_err; } err = vp_modern_probe(mdev); if (err) { dev_err(&pdev->dev, ""Failed to probe modern PCI device\n""); goto probe_err; } mdev_id[0].device = mdev->id.device; mdev_id[0].vendor = mdev->id.vendor; mgtdev->id_table = mdev_id; mgtdev->max_supported_vqs = vp_modern_get_num_queues(mdev); mgtdev->supported_features = vp_modern_get_features(mdev); mgtdev->config_attr_mask = (1 << VDPA_ATTR_DEV_FEATURES); pci_set_master(pdev); pci_set_drvdata(pdev, vp_vdpa_mgtdev); err = vdpa_mgmtdev_register(mgtdev); if (err) { dev_err(&pdev->dev, ""Failed to register vdpa mgmtdev device\n""); goto register_err; } return 0; register_err: vp_modern_remove(vp_vdpa_mgtdev->mdev); probe_err: kfree(mdev_id); mdev_id_err: kfree(mdev); mdev_err: kfree(vp_vdpa_mgtdev); return err; }"
208----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46692/bad/qcom_scm-smc.c----scm_get_wq_ctx,"int scm_get_wq_ctx(u32 *wq_ctx, u32 *flags, u32 *more_pending) { int ret; struct arm_smccc_res get_wq_res; struct arm_smccc_args get_wq_ctx = {0}; <S2SV_StartVul> get_wq_ctx.args[0] = ARM_SMCCC_CALL_VAL(ARM_SMCCC_STD_CALL, <S2SV_EndVul> ARM_SMCCC_SMC_64, ARM_SMCCC_OWNER_SIP, SCM_SMC_FNID(QCOM_SCM_SVC_WAITQ, QCOM_SCM_WAITQ_GET_WQ_CTX)); __scm_smc_do_quirk(&get_wq_ctx, &get_wq_res); ret = get_wq_res.a0; if (ret) return ret; *wq_ctx = get_wq_res.a1; *flags = get_wq_res.a2; *more_pending = get_wq_res.a3; return 0; }","- get_wq_ctx.args[0] = ARM_SMCCC_CALL_VAL(ARM_SMCCC_STD_CALL,
+ get_wq_ctx.args[0] = ARM_SMCCC_CALL_VAL(ARM_SMCCC_FAST_CALL,","int scm_get_wq_ctx(u32 *wq_ctx, u32 *flags, u32 *more_pending) { int ret; struct arm_smccc_res get_wq_res; struct arm_smccc_args get_wq_ctx = {0}; get_wq_ctx.args[0] = ARM_SMCCC_CALL_VAL(ARM_SMCCC_FAST_CALL, ARM_SMCCC_SMC_64, ARM_SMCCC_OWNER_SIP, SCM_SMC_FNID(QCOM_SCM_SVC_WAITQ, QCOM_SCM_WAITQ_GET_WQ_CTX)); __scm_smc_do_quirk(&get_wq_ctx, &get_wq_res); ret = get_wq_res.a0; if (ret) return ret; *wq_ctx = get_wq_res.a1; *flags = get_wq_res.a2; *more_pending = get_wq_res.a3; return 0; }"
1532----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2025-21684/bad/gpio-xilinx.c----xgpio_probe,"static int xgpio_probe(struct platform_device *pdev) { struct xgpio_instance *chip; int status = 0; struct device_node *np = pdev->dev.of_node; u32 is_dual = 0; u32 cells = 2; u32 width[2]; u32 state[2]; u32 dir[2]; struct gpio_irq_chip *girq; u32 temp; chip = devm_kzalloc(&pdev->dev, sizeof(*chip), GFP_KERNEL); if (!chip) return -ENOMEM; platform_set_drvdata(pdev, chip); of_property_read_u32(np, ""xlnx,is-dual"", &is_dual); memset32(width, 0, ARRAY_SIZE(width)); memset32(state, 0, ARRAY_SIZE(state)); memset32(dir, 0xFFFFFFFF, ARRAY_SIZE(dir)); of_property_read_u32(np, ""xlnx,dout-default"", &state[0]); of_property_read_u32(np, ""xlnx,dout-default-2"", &state[1]); bitmap_from_arr32(chip->state, state, 64); of_property_read_u32(np, ""xlnx,tri-default"", &dir[0]); of_property_read_u32(np, ""xlnx,tri-default-2"", &dir[1]); bitmap_from_arr32(chip->dir, dir, 64); if (of_property_read_u32(np, ""#gpio-cells"", &cells)) dev_dbg(&pdev->dev, ""Missing gpio-cells property\n""); if (cells != 2) { dev_err(&pdev->dev, ""#gpio-cells mismatch\n""); return -EINVAL; } if (of_property_read_u32(np, ""xlnx,gpio-width"", &width[0])) width[0] = 32; if (width[0] > 32) return -EINVAL; if (is_dual && of_property_read_u32(np, ""xlnx,gpio2-width"", &width[1])) width[1] = 32; if (width[1] > 32) return -EINVAL; bitmap_set(chip->sw_map, 0, width[0] + width[1]); bitmap_set(chip->hw_map, 0, width[0]); bitmap_set(chip->hw_map, 32, width[1]); <S2SV_StartVul> spin_lock_init(&chip->gpio_lock); <S2SV_EndVul> chip->gc.base = -1; chip->gc.ngpio = bitmap_weight(chip->hw_map, 64); chip->gc.parent = &pdev->dev; chip->gc.direction_input = xgpio_dir_in; chip->gc.direction_output = xgpio_dir_out; chip->gc.of_gpio_n_cells = cells; chip->gc.get = xgpio_get; chip->gc.set = xgpio_set; chip->gc.request = xgpio_request; chip->gc.free = xgpio_free; chip->gc.set_multiple = xgpio_set_multiple; chip->gc.label = dev_name(&pdev->dev); chip->regs = devm_platform_ioremap_resource(pdev, 0); if (IS_ERR(chip->regs)) { dev_err(&pdev->dev, ""failed to ioremap memory resource\n""); return PTR_ERR(chip->regs); } chip->clk = devm_clk_get_optional(&pdev->dev, NULL); if (IS_ERR(chip->clk)) return dev_err_probe(&pdev->dev, PTR_ERR(chip->clk), ""input clock not found.\n""); status = clk_prepare_enable(chip->clk); if (status < 0) { dev_err(&pdev->dev, ""Failed to prepare clk\n""); return status; } pm_runtime_get_noresume(&pdev->dev); pm_runtime_set_active(&pdev->dev); pm_runtime_enable(&pdev->dev); xgpio_save_regs(chip); chip->irq = platform_get_irq_optional(pdev, 0); if (chip->irq <= 0) goto skip_irq; xgpio_writereg(chip->regs + XGPIO_IPIER_OFFSET, 0); temp = xgpio_readreg(chip->regs + XGPIO_IPISR_OFFSET); xgpio_writereg(chip->regs + XGPIO_IPISR_OFFSET, temp); xgpio_writereg(chip->regs + XGPIO_GIER_OFFSET, XGPIO_GIER_IE); girq = &chip->gc.irq; gpio_irq_chip_set_chip(girq, &xgpio_irq_chip); girq->parent_handler = xgpio_irqhandler; girq->num_parents = 1; girq->parents = devm_kcalloc(&pdev->dev, 1, sizeof(*girq->parents), GFP_KERNEL); if (!girq->parents) { status = -ENOMEM; goto err_pm_put; } girq->parents[0] = chip->irq; girq->default_type = IRQ_TYPE_NONE; girq->handler = handle_bad_irq; skip_irq: status = devm_gpiochip_add_data(&pdev->dev, &chip->gc, chip); if (status) { dev_err(&pdev->dev, ""failed to add GPIO chip\n""); goto err_pm_put; } pm_runtime_put(&pdev->dev); return 0; err_pm_put: pm_runtime_disable(&pdev->dev); pm_runtime_put_noidle(&pdev->dev); clk_disable_unprepare(chip->clk); return status; }","- spin_lock_init(&chip->gpio_lock);
+ raw_spin_lock_init(&chip->gpio_lock);","static int xgpio_probe(struct platform_device *pdev) { struct xgpio_instance *chip; int status = 0; struct device_node *np = pdev->dev.of_node; u32 is_dual = 0; u32 cells = 2; u32 width[2]; u32 state[2]; u32 dir[2]; struct gpio_irq_chip *girq; u32 temp; chip = devm_kzalloc(&pdev->dev, sizeof(*chip), GFP_KERNEL); if (!chip) return -ENOMEM; platform_set_drvdata(pdev, chip); of_property_read_u32(np, ""xlnx,is-dual"", &is_dual); memset32(width, 0, ARRAY_SIZE(width)); memset32(state, 0, ARRAY_SIZE(state)); memset32(dir, 0xFFFFFFFF, ARRAY_SIZE(dir)); of_property_read_u32(np, ""xlnx,dout-default"", &state[0]); of_property_read_u32(np, ""xlnx,dout-default-2"", &state[1]); bitmap_from_arr32(chip->state, state, 64); of_property_read_u32(np, ""xlnx,tri-default"", &dir[0]); of_property_read_u32(np, ""xlnx,tri-default-2"", &dir[1]); bitmap_from_arr32(chip->dir, dir, 64); if (of_property_read_u32(np, ""#gpio-cells"", &cells)) dev_dbg(&pdev->dev, ""Missing gpio-cells property\n""); if (cells != 2) { dev_err(&pdev->dev, ""#gpio-cells mismatch\n""); return -EINVAL; } if (of_property_read_u32(np, ""xlnx,gpio-width"", &width[0])) width[0] = 32; if (width[0] > 32) return -EINVAL; if (is_dual && of_property_read_u32(np, ""xlnx,gpio2-width"", &width[1])) width[1] = 32; if (width[1] > 32) return -EINVAL; bitmap_set(chip->sw_map, 0, width[0] + width[1]); bitmap_set(chip->hw_map, 0, width[0]); bitmap_set(chip->hw_map, 32, width[1]); raw_spin_lock_init(&chip->gpio_lock); chip->gc.base = -1; chip->gc.ngpio = bitmap_weight(chip->hw_map, 64); chip->gc.parent = &pdev->dev; chip->gc.direction_input = xgpio_dir_in; chip->gc.direction_output = xgpio_dir_out; chip->gc.of_gpio_n_cells = cells; chip->gc.get = xgpio_get; chip->gc.set = xgpio_set; chip->gc.request = xgpio_request; chip->gc.free = xgpio_free; chip->gc.set_multiple = xgpio_set_multiple; chip->gc.label = dev_name(&pdev->dev); chip->regs = devm_platform_ioremap_resource(pdev, 0); if (IS_ERR(chip->regs)) { dev_err(&pdev->dev, ""failed to ioremap memory resource\n""); return PTR_ERR(chip->regs); } chip->clk = devm_clk_get_optional(&pdev->dev, NULL); if (IS_ERR(chip->clk)) return dev_err_probe(&pdev->dev, PTR_ERR(chip->clk), ""input clock not found.\n""); status = clk_prepare_enable(chip->clk); if (status < 0) { dev_err(&pdev->dev, ""Failed to prepare clk\n""); return status; } pm_runtime_get_noresume(&pdev->dev); pm_runtime_set_active(&pdev->dev); pm_runtime_enable(&pdev->dev); xgpio_save_regs(chip); chip->irq = platform_get_irq_optional(pdev, 0); if (chip->irq <= 0) goto skip_irq; xgpio_writereg(chip->regs + XGPIO_IPIER_OFFSET, 0); temp = xgpio_readreg(chip->regs + XGPIO_IPISR_OFFSET); xgpio_writereg(chip->regs + XGPIO_IPISR_OFFSET, temp); xgpio_writereg(chip->regs + XGPIO_GIER_OFFSET, XGPIO_GIER_IE); girq = &chip->gc.irq; gpio_irq_chip_set_chip(girq, &xgpio_irq_chip); girq->parent_handler = xgpio_irqhandler; girq->num_parents = 1; girq->parents = devm_kcalloc(&pdev->dev, 1, sizeof(*girq->parents), GFP_KERNEL); if (!girq->parents) { status = -ENOMEM; goto err_pm_put; } girq->parents[0] = chip->irq; girq->default_type = IRQ_TYPE_NONE; girq->handler = handle_bad_irq; skip_irq: status = devm_gpiochip_add_data(&pdev->dev, &chip->gc, chip); if (status) { dev_err(&pdev->dev, ""failed to add GPIO chip\n""); goto err_pm_put; } pm_runtime_put(&pdev->dev); return 0; err_pm_put: pm_runtime_disable(&pdev->dev); pm_runtime_put_noidle(&pdev->dev); clk_disable_unprepare(chip->clk); return status; }"
1113----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53089/bad/vcpu.c----kvm_arch_vcpu_create,"int kvm_arch_vcpu_create(struct kvm_vcpu *vcpu) { unsigned long timer_hz; struct loongarch_csrs *csr; vcpu->arch.vpid = 0; vcpu->arch.flush_gpa = INVALID_GPA; <S2SV_StartVul> hrtimer_init(&vcpu->arch.swtimer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS_PINNED); <S2SV_EndVul> vcpu->arch.swtimer.function = kvm_swtimer_wakeup; vcpu->arch.handle_exit = kvm_handle_exit; vcpu->arch.guest_eentry = (unsigned long)kvm_loongarch_ops->exc_entry; vcpu->arch.csr = kzalloc(sizeof(struct loongarch_csrs), GFP_KERNEL); if (!vcpu->arch.csr) return -ENOMEM; vcpu->arch.host_ecfg = (read_csr_ecfg() & CSR_ECFG_VS); vcpu->arch.last_sched_cpu = -1; timer_hz = calc_const_freq(); kvm_init_timer(vcpu, timer_hz); csr = vcpu->arch.csr; kvm_write_sw_gcsr(csr, LOONGARCH_CSR_CRMD, CSR_CRMD_DA); kvm_write_sw_gcsr(csr, LOONGARCH_CSR_TMID, vcpu->vcpu_id); kvm_write_sw_gcsr(csr, LOONGARCH_CSR_CPUID, KVM_MAX_PHYID); csr->csrs[LOONGARCH_CSR_GINTC] = 0; return 0; }","- hrtimer_init(&vcpu->arch.swtimer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS_PINNED);
+ hrtimer_init(&vcpu->arch.swtimer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS_PINNED_HARD);","int kvm_arch_vcpu_create(struct kvm_vcpu *vcpu) { unsigned long timer_hz; struct loongarch_csrs *csr; vcpu->arch.vpid = 0; vcpu->arch.flush_gpa = INVALID_GPA; hrtimer_init(&vcpu->arch.swtimer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS_PINNED_HARD); vcpu->arch.swtimer.function = kvm_swtimer_wakeup; vcpu->arch.handle_exit = kvm_handle_exit; vcpu->arch.guest_eentry = (unsigned long)kvm_loongarch_ops->exc_entry; vcpu->arch.csr = kzalloc(sizeof(struct loongarch_csrs), GFP_KERNEL); if (!vcpu->arch.csr) return -ENOMEM; vcpu->arch.host_ecfg = (read_csr_ecfg() & CSR_ECFG_VS); vcpu->arch.last_sched_cpu = -1; timer_hz = calc_const_freq(); kvm_init_timer(vcpu, timer_hz); csr = vcpu->arch.csr; kvm_write_sw_gcsr(csr, LOONGARCH_CSR_CRMD, CSR_CRMD_DA); kvm_write_sw_gcsr(csr, LOONGARCH_CSR_TMID, vcpu->vcpu_id); kvm_write_sw_gcsr(csr, LOONGARCH_CSR_CPUID, KVM_MAX_PHYID); csr->csrs[LOONGARCH_CSR_GINTC] = 0; return 0; }"
1147----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53124/bad/ipv6.c----dccp_v6_do_rcv,"static int dccp_v6_do_rcv(struct sock *sk, struct sk_buff *skb) { struct ipv6_pinfo *np = inet6_sk(sk); struct sk_buff *opt_skb = NULL; if (skb->protocol == htons(ETH_P_IP)) return dccp_v4_do_rcv(sk, skb); if (sk_filter(sk, skb)) goto discard; <S2SV_StartVul> if (np->rxopt.all) <S2SV_EndVul> opt_skb = skb_clone_and_charge_r(skb, sk); if (sk->sk_state == DCCP_OPEN) { if (dccp_rcv_established(sk, skb, dccp_hdr(skb), skb->len)) goto reset; if (opt_skb) goto ipv6_pktoptions; return 0; } if (dccp_rcv_state_process(sk, skb, dccp_hdr(skb), skb->len)) goto reset; if (opt_skb) goto ipv6_pktoptions; return 0; reset: dccp_v6_ctl_send_reset(sk, skb); discard: if (opt_skb != NULL) __kfree_skb(opt_skb); kfree_skb(skb); return 0; ipv6_pktoptions: if (!((1 << sk->sk_state) & (DCCPF_CLOSED | DCCPF_LISTEN))) { if (np->rxopt.bits.rxinfo || np->rxopt.bits.rxoinfo) np->mcast_oif = inet6_iif(opt_skb); if (np->rxopt.bits.rxhlim || np->rxopt.bits.rxohlim) np->mcast_hops = ipv6_hdr(opt_skb)->hop_limit; if (np->rxopt.bits.rxflow || np->rxopt.bits.rxtclass) np->rcv_flowinfo = ip6_flowinfo(ipv6_hdr(opt_skb)); if (np->repflow) np->flow_label = ip6_flowlabel(ipv6_hdr(opt_skb)); if (ipv6_opt_accepted(sk, opt_skb, &DCCP_SKB_CB(opt_skb)->header.h6)) { memmove(IP6CB(opt_skb), &DCCP_SKB_CB(opt_skb)->header.h6, sizeof(struct inet6_skb_parm)); opt_skb = xchg(&np->pktoptions, opt_skb); } else { __kfree_skb(opt_skb); opt_skb = xchg(&np->pktoptions, NULL); } } kfree_skb(opt_skb); return 0; }","- if (np->rxopt.all)
+ if (np->rxopt.all && sk->sk_state != DCCP_LISTEN)","static int dccp_v6_do_rcv(struct sock *sk, struct sk_buff *skb) { struct ipv6_pinfo *np = inet6_sk(sk); struct sk_buff *opt_skb = NULL; if (skb->protocol == htons(ETH_P_IP)) return dccp_v4_do_rcv(sk, skb); if (sk_filter(sk, skb)) goto discard; if (np->rxopt.all && sk->sk_state != DCCP_LISTEN) opt_skb = skb_clone_and_charge_r(skb, sk); if (sk->sk_state == DCCP_OPEN) { if (dccp_rcv_established(sk, skb, dccp_hdr(skb), skb->len)) goto reset; if (opt_skb) goto ipv6_pktoptions; return 0; } if (dccp_rcv_state_process(sk, skb, dccp_hdr(skb), skb->len)) goto reset; if (opt_skb) goto ipv6_pktoptions; return 0; reset: dccp_v6_ctl_send_reset(sk, skb); discard: if (opt_skb != NULL) __kfree_skb(opt_skb); kfree_skb(skb); return 0; ipv6_pktoptions: if (!((1 << sk->sk_state) & (DCCPF_CLOSED | DCCPF_LISTEN))) { if (np->rxopt.bits.rxinfo || np->rxopt.bits.rxoinfo) np->mcast_oif = inet6_iif(opt_skb); if (np->rxopt.bits.rxhlim || np->rxopt.bits.rxohlim) np->mcast_hops = ipv6_hdr(opt_skb)->hop_limit; if (np->rxopt.bits.rxflow || np->rxopt.bits.rxtclass) np->rcv_flowinfo = ip6_flowinfo(ipv6_hdr(opt_skb)); if (np->repflow) np->flow_label = ip6_flowlabel(ipv6_hdr(opt_skb)); if (ipv6_opt_accepted(sk, opt_skb, &DCCP_SKB_CB(opt_skb)->header.h6)) { memmove(IP6CB(opt_skb), &DCCP_SKB_CB(opt_skb)->header.h6, sizeof(struct inet6_skb_parm)); opt_skb = xchg(&np->pktoptions, opt_skb); } else { __kfree_skb(opt_skb); opt_skb = xchg(&np->pktoptions, NULL); } } kfree_skb(opt_skb); return 0; }"
887----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50139/bad/sys_regs.c----reset_clidr,"static u64 reset_clidr(struct kvm_vcpu *vcpu, const struct sys_reg_desc *r) { u64 ctr_el0 = read_sanitised_ftr_reg(SYS_CTR_EL0); u64 clidr; u8 loc; if ((ctr_el0 & CTR_EL0_IDC)) { loc = (ctr_el0 & CTR_EL0_DIC) ? 1 : 2; clidr = CACHE_TYPE_UNIFIED << CLIDR_CTYPE_SHIFT(loc); } else { loc = 1; clidr = 1 << CLIDR_LOUU_SHIFT; clidr |= 1 << CLIDR_LOUIS_SHIFT; clidr |= CACHE_TYPE_DATA << CLIDR_CTYPE_SHIFT(1); } if (!(ctr_el0 & CTR_EL0_DIC)) clidr |= CACHE_TYPE_INST << CLIDR_CTYPE_SHIFT(1); clidr |= loc << CLIDR_LOC_SHIFT; if (kvm_has_mte(vcpu->kvm)) <S2SV_StartVul> clidr |= 2 << CLIDR_TTYPE_SHIFT(loc); <S2SV_EndVul> __vcpu_sys_reg(vcpu, r->reg) = clidr; return __vcpu_sys_reg(vcpu, r->reg); }","- clidr |= 2 << CLIDR_TTYPE_SHIFT(loc);
+ clidr |= 2ULL << CLIDR_TTYPE_SHIFT(loc);","static u64 reset_clidr(struct kvm_vcpu *vcpu, const struct sys_reg_desc *r) { u64 ctr_el0 = read_sanitised_ftr_reg(SYS_CTR_EL0); u64 clidr; u8 loc; if ((ctr_el0 & CTR_EL0_IDC)) { loc = (ctr_el0 & CTR_EL0_DIC) ? 1 : 2; clidr = CACHE_TYPE_UNIFIED << CLIDR_CTYPE_SHIFT(loc); } else { loc = 1; clidr = 1 << CLIDR_LOUU_SHIFT; clidr |= 1 << CLIDR_LOUIS_SHIFT; clidr |= CACHE_TYPE_DATA << CLIDR_CTYPE_SHIFT(1); } if (!(ctr_el0 & CTR_EL0_DIC)) clidr |= CACHE_TYPE_INST << CLIDR_CTYPE_SHIFT(1); clidr |= loc << CLIDR_LOC_SHIFT; if (kvm_has_mte(vcpu->kvm)) clidr |= 2ULL << CLIDR_TTYPE_SHIFT(loc); __vcpu_sys_reg(vcpu, r->reg) = clidr; return __vcpu_sys_reg(vcpu, r->reg); }"
113----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44972/bad/extent_io.c----extent_write_locked_range,"void extent_write_locked_range(struct inode *inode, struct page *locked_page, u64 start, u64 end, struct writeback_control *wbc, bool pages_dirty) { bool found_error = false; int ret = 0; struct address_space *mapping = inode->i_mapping; struct btrfs_fs_info *fs_info = inode_to_fs_info(inode); const u32 sectorsize = fs_info->sectorsize; loff_t i_size = i_size_read(inode); u64 cur = start; struct btrfs_bio_ctrl bio_ctrl = { .wbc = wbc, .opf = REQ_OP_WRITE | wbc_to_write_flags(wbc), }; if (wbc->no_cgroup_owner) bio_ctrl.opf |= REQ_BTRFS_CGROUP_PUNT; ASSERT(IS_ALIGNED(start, sectorsize) && IS_ALIGNED(end + 1, sectorsize)); while (cur <= end) { u64 cur_end = min(round_down(cur, PAGE_SIZE) + PAGE_SIZE - 1, end); u32 cur_len = cur_end + 1 - cur; struct page *page; int nr = 0; page = find_get_page(mapping, cur >> PAGE_SHIFT); ASSERT(PageLocked(page)); <S2SV_StartVul> if (pages_dirty && page != locked_page) { <S2SV_EndVul> ASSERT(PageDirty(page)); <S2SV_StartVul> clear_page_dirty_for_io(page); <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul> ret = __extent_writepage_io(BTRFS_I(inode), page, cur, cur_len, &bio_ctrl, i_size, &nr); if (ret == 1) goto next_page; if (nr == 0) { set_page_writeback(page); end_page_writeback(page); } if (ret) { btrfs_mark_ordered_io_finished(BTRFS_I(inode), page, cur, cur_len, !ret); mapping_set_error(page->mapping, ret); } btrfs_folio_unlock_writer(fs_info, page_folio(page), cur, cur_len); if (ret < 0) found_error = true; next_page: put_page(page); cur = cur_end + 1; } submit_write_bio(&bio_ctrl, found_error ? ret : 0); }","- if (pages_dirty && page != locked_page) {
- clear_page_dirty_for_io(page);
- }
+ if (pages_dirty && page != locked_page)","void extent_write_locked_range(struct inode *inode, struct page *locked_page, u64 start, u64 end, struct writeback_control *wbc, bool pages_dirty) { bool found_error = false; int ret = 0; struct address_space *mapping = inode->i_mapping; struct btrfs_fs_info *fs_info = inode_to_fs_info(inode); const u32 sectorsize = fs_info->sectorsize; loff_t i_size = i_size_read(inode); u64 cur = start; struct btrfs_bio_ctrl bio_ctrl = { .wbc = wbc, .opf = REQ_OP_WRITE | wbc_to_write_flags(wbc), }; if (wbc->no_cgroup_owner) bio_ctrl.opf |= REQ_BTRFS_CGROUP_PUNT; ASSERT(IS_ALIGNED(start, sectorsize) && IS_ALIGNED(end + 1, sectorsize)); while (cur <= end) { u64 cur_end = min(round_down(cur, PAGE_SIZE) + PAGE_SIZE - 1, end); u32 cur_len = cur_end + 1 - cur; struct page *page; int nr = 0; page = find_get_page(mapping, cur >> PAGE_SHIFT); ASSERT(PageLocked(page)); if (pages_dirty && page != locked_page) ASSERT(PageDirty(page)); ret = __extent_writepage_io(BTRFS_I(inode), page, cur, cur_len, &bio_ctrl, i_size, &nr); if (ret == 1) goto next_page; if (nr == 0) { set_page_writeback(page); end_page_writeback(page); } if (ret) { btrfs_mark_ordered_io_finished(BTRFS_I(inode), page, cur, cur_len, !ret); mapping_set_error(page->mapping, ret); } btrfs_folio_unlock_writer(fs_info, page_folio(page), cur, cur_len); if (ret < 0) found_error = true; next_page: put_page(page); cur = cur_end + 1; } submit_write_bio(&bio_ctrl, found_error ? ret : 0); }"
1203----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53207/bad/mgmt.c----set_name_complete,"static void set_name_complete(struct hci_dev *hdev, void *data, int err) { struct mgmt_pending_cmd *cmd = data; struct mgmt_cp_set_local_name *cp = cmd->param; u8 status = mgmt_status(err); <S2SV_StartVul> bt_dev_dbg(hdev, ""err %d"", err); <S2SV_EndVul> <S2SV_StartVul> if (cmd != pending_find(MGMT_OP_SET_LOCAL_NAME, hdev)) <S2SV_EndVul> return; if (status) { mgmt_cmd_status(cmd->sk, hdev->id, MGMT_OP_SET_LOCAL_NAME, status); } else { mgmt_cmd_complete(cmd->sk, hdev->id, MGMT_OP_SET_LOCAL_NAME, 0, cp, sizeof(*cp)); if (hci_dev_test_flag(hdev, HCI_LE_ADV)) hci_cmd_sync_queue(hdev, name_changed_sync, NULL, NULL); } mgmt_pending_remove(cmd); }","- bt_dev_dbg(hdev, ""err %d"", err);
- if (cmd != pending_find(MGMT_OP_SET_LOCAL_NAME, hdev))
+ bt_dev_dbg(hdev, ""err %d"", err);
+ if (err == -ECANCELED ||
+ cmd != pending_find(MGMT_OP_SET_LOCAL_NAME, hdev))
+ return;","static void set_name_complete(struct hci_dev *hdev, void *data, int err) { struct mgmt_pending_cmd *cmd = data; struct mgmt_cp_set_local_name *cp = cmd->param; u8 status = mgmt_status(err); bt_dev_dbg(hdev, ""err %d"", err); if (err == -ECANCELED || cmd != pending_find(MGMT_OP_SET_LOCAL_NAME, hdev)) return; if (status) { mgmt_cmd_status(cmd->sk, hdev->id, MGMT_OP_SET_LOCAL_NAME, status); } else { mgmt_cmd_complete(cmd->sk, hdev->id, MGMT_OP_SET_LOCAL_NAME, 0, cp, sizeof(*cp)); if (hci_dev_test_flag(hdev, HCI_LE_ADV)) hci_cmd_sync_queue(hdev, name_changed_sync, NULL, NULL); } mgmt_pending_remove(cmd); }"
451----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47691/bad/file.c----f2fs_ioc_shutdown,"static int f2fs_ioc_shutdown(struct file *filp, unsigned long arg) { struct inode *inode = file_inode(filp); struct f2fs_sb_info *sbi = F2FS_I_SB(inode); __u32 in; int ret; bool need_drop = false, readonly = false; if (!capable(CAP_SYS_ADMIN)) return -EPERM; if (get_user(in, (__u32 __user *)arg)) return -EFAULT; if (in != F2FS_GOING_DOWN_FULLSYNC) { ret = mnt_want_write_file(filp); if (ret) { if (ret != -EROFS) return ret; in = F2FS_GOING_DOWN_NOSYNC; readonly = true; } else { need_drop = true; } } <S2SV_StartVul> ret = f2fs_do_shutdown(sbi, in, readonly); <S2SV_EndVul> if (need_drop) mnt_drop_write_file(filp); return ret; }","- ret = f2fs_do_shutdown(sbi, in, readonly);
+ ret = f2fs_do_shutdown(sbi, in, readonly, true);","static int f2fs_ioc_shutdown(struct file *filp, unsigned long arg) { struct inode *inode = file_inode(filp); struct f2fs_sb_info *sbi = F2FS_I_SB(inode); __u32 in; int ret; bool need_drop = false, readonly = false; if (!capable(CAP_SYS_ADMIN)) return -EPERM; if (get_user(in, (__u32 __user *)arg)) return -EFAULT; if (in != F2FS_GOING_DOWN_FULLSYNC) { ret = mnt_want_write_file(filp); if (ret) { if (ret != -EROFS) return ret; in = F2FS_GOING_DOWN_NOSYNC; readonly = true; } else { need_drop = true; } } ret = f2fs_do_shutdown(sbi, in, readonly, true); if (need_drop) mnt_drop_write_file(filp); return ret; }"
580----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49903/bad/jfs_discard.c----jfs_ioc_trim,"int jfs_ioc_trim(struct inode *ip, struct fstrim_range *range) { struct inode *ipbmap = JFS_SBI(ip->i_sb)->ipbmap; <S2SV_StartVul> struct bmap *bmp = JFS_SBI(ip->i_sb)->bmap; <S2SV_EndVul> struct super_block *sb = ipbmap->i_sb; int agno, agno_end; u64 start, end, minlen; u64 trimmed = 0; start = range->start >> sb->s_blocksize_bits; end = start + (range->len >> sb->s_blocksize_bits) - 1; minlen = range->minlen >> sb->s_blocksize_bits; if (minlen == 0) minlen = 1; if (minlen > bmp->db_agsize || start >= bmp->db_mapsize || <S2SV_StartVul> range->len < sb->s_blocksize) <S2SV_EndVul> return -EINVAL; if (end >= bmp->db_mapsize) end = bmp->db_mapsize - 1; agno = BLKTOAG(start, JFS_SBI(ip->i_sb)); agno_end = BLKTOAG(end, JFS_SBI(ip->i_sb)); while (agno <= agno_end) { trimmed += dbDiscardAG(ip, agno, minlen); agno++; } range->len = trimmed << sb->s_blocksize_bits; return 0; }","- struct bmap *bmp = JFS_SBI(ip->i_sb)->bmap;
- range->len < sb->s_blocksize)
+ struct bmap *bmp;
+ down_read(&sb->s_umount);
+ bmp = JFS_SBI(ip->i_sb)->bmap;
+ range->len < sb->s_blocksize) {
+ up_read(&sb->s_umount);
+ }
+ }
+ up_read(&sb->s_umount);","int jfs_ioc_trim(struct inode *ip, struct fstrim_range *range) { struct inode *ipbmap = JFS_SBI(ip->i_sb)->ipbmap; struct bmap *bmp; struct super_block *sb = ipbmap->i_sb; int agno, agno_end; u64 start, end, minlen; u64 trimmed = 0; start = range->start >> sb->s_blocksize_bits; end = start + (range->len >> sb->s_blocksize_bits) - 1; minlen = range->minlen >> sb->s_blocksize_bits; if (minlen == 0) minlen = 1; down_read(&sb->s_umount); bmp = JFS_SBI(ip->i_sb)->bmap; if (minlen > bmp->db_agsize || start >= bmp->db_mapsize || range->len < sb->s_blocksize) { up_read(&sb->s_umount); return -EINVAL; } if (end >= bmp->db_mapsize) end = bmp->db_mapsize - 1; agno = BLKTOAG(start, JFS_SBI(ip->i_sb)); agno_end = BLKTOAG(end, JFS_SBI(ip->i_sb)); while (agno <= agno_end) { trimmed += dbDiscardAG(ip, agno, minlen); agno++; } up_read(&sb->s_umount); range->len = trimmed << sb->s_blocksize_bits; return 0; }"
1150----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53129/bad/rockchip_drm_vop.c----vop_plane_atomic_async_check,"static int vop_plane_atomic_async_check(struct drm_plane *plane, struct drm_atomic_state *state) { struct drm_plane_state *new_plane_state = drm_atomic_get_new_plane_state(state, plane); struct vop_win *vop_win = to_vop_win(plane); const struct vop_win_data *win = vop_win->data; int min_scale = win->phy->scl ? FRAC_16_16(1, 8) : DRM_PLANE_NO_SCALING; int max_scale = win->phy->scl ? FRAC_16_16(8, 1) : DRM_PLANE_NO_SCALING; struct drm_crtc_state *crtc_state; if (plane != new_plane_state->crtc->cursor) return -EINVAL; if (!plane->state) return -EINVAL; if (!plane->state->fb) return -EINVAL; <S2SV_StartVul> if (state) <S2SV_EndVul> <S2SV_StartVul> crtc_state = drm_atomic_get_existing_crtc_state(state, <S2SV_EndVul> <S2SV_StartVul> new_plane_state->crtc); <S2SV_EndVul> else crtc_state = plane->crtc->state; return drm_atomic_helper_check_plane_state(plane->state, crtc_state, min_scale, max_scale, true, true); }","- if (state)
- crtc_state = drm_atomic_get_existing_crtc_state(state,
- new_plane_state->crtc);
+ crtc_state = drm_atomic_get_existing_crtc_state(state, new_plane_state->crtc);
+ if (!crtc_state)","static int vop_plane_atomic_async_check(struct drm_plane *plane, struct drm_atomic_state *state) { struct drm_plane_state *new_plane_state = drm_atomic_get_new_plane_state(state, plane); struct vop_win *vop_win = to_vop_win(plane); const struct vop_win_data *win = vop_win->data; int min_scale = win->phy->scl ? FRAC_16_16(1, 8) : DRM_PLANE_NO_SCALING; int max_scale = win->phy->scl ? FRAC_16_16(8, 1) : DRM_PLANE_NO_SCALING; struct drm_crtc_state *crtc_state; if (plane != new_plane_state->crtc->cursor) return -EINVAL; if (!plane->state) return -EINVAL; if (!plane->state->fb) return -EINVAL; crtc_state = drm_atomic_get_existing_crtc_state(state, new_plane_state->crtc); if (!crtc_state) crtc_state = plane->crtc->state; return drm_atomic_helper_check_plane_state(plane->state, crtc_state, min_scale, max_scale, true, true); }"
828----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50095/bad/mad.c----timeout_sends,"static void timeout_sends(struct work_struct *work) { struct ib_mad_agent_private *mad_agent_priv; struct ib_mad_send_wr_private *mad_send_wr; struct ib_mad_send_wc mad_send_wc; unsigned long flags, delay; mad_agent_priv = container_of(work, struct ib_mad_agent_private, timed_work.work); mad_send_wc.vendor_err = 0; spin_lock_irqsave(&mad_agent_priv->lock, flags); while (!list_empty(&mad_agent_priv->wait_list)) { mad_send_wr = list_entry(mad_agent_priv->wait_list.next, struct ib_mad_send_wr_private, agent_list); if (time_after(mad_send_wr->timeout, jiffies)) { delay = mad_send_wr->timeout - jiffies; if ((long)delay <= 0) delay = 1; queue_delayed_work(mad_agent_priv->qp_info-> port_priv->wq, &mad_agent_priv->timed_work, delay); break; } <S2SV_StartVul> list_del(&mad_send_wr->agent_list); <S2SV_EndVul> if (mad_send_wr->status == IB_WC_SUCCESS && !retry_send(mad_send_wr)) continue; <S2SV_StartVul> spin_unlock_irqrestore(&mad_agent_priv->lock, flags); <S2SV_EndVul> if (mad_send_wr->status == IB_WC_SUCCESS) mad_send_wc.status = IB_WC_RESP_TIMEOUT_ERR; else mad_send_wc.status = mad_send_wr->status; mad_send_wc.send_buf = &mad_send_wr->send_buf; mad_agent_priv->agent.send_handler(&mad_agent_priv->agent, &mad_send_wc); deref_mad_agent(mad_agent_priv); <S2SV_StartVul> spin_lock_irqsave(&mad_agent_priv->lock, flags); <S2SV_EndVul> } <S2SV_StartVul> spin_unlock_irqrestore(&mad_agent_priv->lock, flags); <S2SV_EndVul> }","- list_del(&mad_send_wr->agent_list);
- spin_unlock_irqrestore(&mad_agent_priv->lock, flags);
- spin_lock_irqsave(&mad_agent_priv->lock, flags);
- spin_unlock_irqrestore(&mad_agent_priv->lock, flags);
+ }
+ list_del_init(&mad_send_wr->agent_list);
+ list_add_tail(&mad_send_wr->agent_list, &local_list);
+ }
+ spin_unlock_irqrestore(&mad_agent_priv->lock, flags);
+ list_for_each_entry_safe(mad_send_wr, n, &local_list, agent_list) {
+ }
+ }","static void timeout_sends(struct work_struct *work) { struct ib_mad_send_wr_private *mad_send_wr, *n; struct ib_mad_agent_private *mad_agent_priv; struct ib_mad_send_wc mad_send_wc; struct list_head local_list; unsigned long flags, delay; mad_agent_priv = container_of(work, struct ib_mad_agent_private, timed_work.work); mad_send_wc.vendor_err = 0; INIT_LIST_HEAD(&local_list); spin_lock_irqsave(&mad_agent_priv->lock, flags); while (!list_empty(&mad_agent_priv->wait_list)) { mad_send_wr = list_entry(mad_agent_priv->wait_list.next, struct ib_mad_send_wr_private, agent_list); if (time_after(mad_send_wr->timeout, jiffies)) { delay = mad_send_wr->timeout - jiffies; if ((long)delay <= 0) delay = 1; queue_delayed_work(mad_agent_priv->qp_info-> port_priv->wq, &mad_agent_priv->timed_work, delay); break; } list_del_init(&mad_send_wr->agent_list); if (mad_send_wr->status == IB_WC_SUCCESS && !retry_send(mad_send_wr)) continue; list_add_tail(&mad_send_wr->agent_list, &local_list); } spin_unlock_irqrestore(&mad_agent_priv->lock, flags); list_for_each_entry_safe(mad_send_wr, n, &local_list, agent_list) { if (mad_send_wr->status == IB_WC_SUCCESS) mad_send_wc.status = IB_WC_RESP_TIMEOUT_ERR; else mad_send_wc.status = mad_send_wr->status; mad_send_wc.send_buf = &mad_send_wr->send_buf; mad_agent_priv->agent.send_handler(&mad_agent_priv->agent, &mad_send_wc); deref_mad_agent(mad_agent_priv); } }"
246----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46720/bad/amdgpu_device.c----amdgpu_device_gpu_recover,"int amdgpu_device_gpu_recover(struct amdgpu_device *adev, struct amdgpu_job *job, struct amdgpu_reset_context *reset_context) { struct list_head device_list, *device_list_handle = NULL; bool job_signaled = false; struct amdgpu_hive_info *hive = NULL; struct amdgpu_device *tmp_adev = NULL; int i, r = 0; bool need_emergency_restart = false; bool audio_suspended = false; need_emergency_restart = amdgpu_ras_need_emergency_restart(adev); if (need_emergency_restart && amdgpu_ras_get_context(adev) && amdgpu_ras_get_context(adev)->reboot) { DRM_WARN(""Emergency reboot.""); ksys_sync_helper(); emergency_restart(); } dev_info(adev->dev, ""GPU %s begin!\n"", need_emergency_restart ? ""jobs stop"":""reset""); if (!amdgpu_sriov_vf(adev)) hive = amdgpu_get_xgmi_hive(adev); if (hive) mutex_lock(&hive->hive_lock); reset_context->job = job; reset_context->hive = hive; INIT_LIST_HEAD(&device_list); <S2SV_StartVul> if (!amdgpu_sriov_vf(adev) && (adev->gmc.xgmi.num_physical_nodes > 1)) { <S2SV_EndVul> list_for_each_entry(tmp_adev, &hive->device_list, gmc.xgmi.head) { list_add_tail(&tmp_adev->reset_list, &device_list); if (adev->shutdown) tmp_adev->shutdown = true; } if (!list_is_first(&adev->reset_list, &device_list)) list_rotate_to_front(&adev->reset_list, &device_list); device_list_handle = &device_list; } else { list_add_tail(&adev->reset_list, &device_list); device_list_handle = &device_list; } if (!amdgpu_sriov_vf(adev)) { r = amdgpu_device_health_check(device_list_handle); if (r) goto end_reset; } tmp_adev = list_first_entry(device_list_handle, struct amdgpu_device, reset_list); amdgpu_device_lock_reset_domain(tmp_adev->reset_domain); list_for_each_entry(tmp_adev, device_list_handle, reset_list) { amdgpu_device_set_mp1_state(tmp_adev); if (!amdgpu_device_suspend_display_audio(tmp_adev)) audio_suspended = true; amdgpu_ras_set_error_query_ready(tmp_adev, false); cancel_delayed_work_sync(&tmp_adev->delayed_init_work); if (!amdgpu_sriov_vf(tmp_adev)) amdgpu_amdkfd_pre_reset(tmp_adev); amdgpu_unregister_gpu_instance(tmp_adev); drm_fb_helper_set_suspend_unlocked(adev_to_drm(tmp_adev)->fb_helper, true); if (!need_emergency_restart && amdgpu_device_ip_need_full_reset(tmp_adev)) amdgpu_ras_suspend(tmp_adev); for (i = 0; i < AMDGPU_MAX_RINGS; ++i) { struct amdgpu_ring *ring = tmp_adev->rings[i]; if (!amdgpu_ring_sched_ready(ring)) continue; drm_sched_stop(&ring->sched, job ? &job->base : NULL); if (need_emergency_restart) amdgpu_job_stop_all_jobs_on_sched(&ring->sched); } atomic_inc(&tmp_adev->gpu_reset_counter); } if (need_emergency_restart) goto skip_sched_resume; if (job && dma_fence_is_signaled(&job->hw_fence)) { job_signaled = true; dev_info(adev->dev, ""Guilty job already signaled, skipping HW reset""); goto skip_hw_reset; } retry: list_for_each_entry(tmp_adev, device_list_handle, reset_list) { r = amdgpu_device_pre_asic_reset(tmp_adev, reset_context); if (r) { dev_err(tmp_adev->dev, ""GPU pre asic reset failed with err, %d for drm dev, %s "", r, adev_to_drm(tmp_adev)->unique); tmp_adev->asic_reset_res = r; } if (!amdgpu_sriov_vf(tmp_adev)) amdgpu_device_stop_pending_resets(tmp_adev); } if (amdgpu_sriov_vf(adev)) { r = amdgpu_device_reset_sriov(adev, job ? false : true); if (r) adev->asic_reset_res = r; if (amdgpu_ip_version(adev, GC_HWIP, 0) == IP_VERSION(9, 4, 2) || amdgpu_ip_version(adev, GC_HWIP, 0) == IP_VERSION(9, 4, 3) || amdgpu_ip_version(adev, GC_HWIP, 0) == IP_VERSION(11, 0, 3)) amdgpu_ras_resume(adev); } else { r = amdgpu_do_asic_reset(device_list_handle, reset_context); if (r && r == -EAGAIN) goto retry; } skip_hw_reset: list_for_each_entry(tmp_adev, device_list_handle, reset_list) { for (i = 0; i < AMDGPU_MAX_RINGS; ++i) { struct amdgpu_ring *ring = tmp_adev->rings[i]; if (!amdgpu_ring_sched_ready(ring)) continue; drm_sched_start(&ring->sched, true); } if (!drm_drv_uses_atomic_modeset(adev_to_drm(tmp_adev)) && !job_signaled) drm_helper_resume_force_mode(adev_to_drm(tmp_adev)); if (tmp_adev->asic_reset_res) r = tmp_adev->asic_reset_res; tmp_adev->asic_reset_res = 0; if (r) { dev_info(tmp_adev->dev, ""GPU reset(%d) failed\n"", atomic_read(&tmp_adev->gpu_reset_counter)); amdgpu_vf_error_put(tmp_adev, AMDGIM_ERROR_VF_GPU_RESET_FAIL, 0, r); } else { dev_info(tmp_adev->dev, ""GPU reset(%d) succeeded!\n"", atomic_read(&tmp_adev->gpu_reset_counter)); if (amdgpu_acpi_smart_shift_update(adev_to_drm(tmp_adev), AMDGPU_SS_DEV_D0)) DRM_WARN(""smart shift update failed\n""); } } skip_sched_resume: list_for_each_entry(tmp_adev, device_list_handle, reset_list) { if (!need_emergency_restart && !amdgpu_sriov_vf(tmp_adev)) amdgpu_amdkfd_post_reset(tmp_adev); if (!adev->kfd.init_complete) amdgpu_amdkfd_device_init(adev); if (audio_suspended) amdgpu_device_resume_display_audio(tmp_adev); amdgpu_device_unset_mp1_state(tmp_adev); amdgpu_ras_set_error_query_ready(tmp_adev, true); } tmp_adev = list_first_entry(device_list_handle, struct amdgpu_device, reset_list); amdgpu_device_unlock_reset_domain(tmp_adev->reset_domain); end_reset: if (hive) { mutex_unlock(&hive->hive_lock); amdgpu_put_xgmi_hive(hive); } if (r) dev_info(adev->dev, ""GPU reset end with ret = %d\n"", r); atomic_set(&adev->reset_domain->reset_res, r); return r; }","- if (!amdgpu_sriov_vf(adev) && (adev->gmc.xgmi.num_physical_nodes > 1)) {
+ if (!amdgpu_sriov_vf(adev) && (adev->gmc.xgmi.num_physical_nodes > 1) && hive) {","int amdgpu_device_gpu_recover(struct amdgpu_device *adev, struct amdgpu_job *job, struct amdgpu_reset_context *reset_context) { struct list_head device_list, *device_list_handle = NULL; bool job_signaled = false; struct amdgpu_hive_info *hive = NULL; struct amdgpu_device *tmp_adev = NULL; int i, r = 0; bool need_emergency_restart = false; bool audio_suspended = false; need_emergency_restart = amdgpu_ras_need_emergency_restart(adev); if (need_emergency_restart && amdgpu_ras_get_context(adev) && amdgpu_ras_get_context(adev)->reboot) { DRM_WARN(""Emergency reboot.""); ksys_sync_helper(); emergency_restart(); } dev_info(adev->dev, ""GPU %s begin!\n"", need_emergency_restart ? ""jobs stop"":""reset""); if (!amdgpu_sriov_vf(adev)) hive = amdgpu_get_xgmi_hive(adev); if (hive) mutex_lock(&hive->hive_lock); reset_context->job = job; reset_context->hive = hive; INIT_LIST_HEAD(&device_list); if (!amdgpu_sriov_vf(adev) && (adev->gmc.xgmi.num_physical_nodes > 1) && hive) { list_for_each_entry(tmp_adev, &hive->device_list, gmc.xgmi.head) { list_add_tail(&tmp_adev->reset_list, &device_list); if (adev->shutdown) tmp_adev->shutdown = true; } if (!list_is_first(&adev->reset_list, &device_list)) list_rotate_to_front(&adev->reset_list, &device_list); device_list_handle = &device_list; } else { list_add_tail(&adev->reset_list, &device_list); device_list_handle = &device_list; } if (!amdgpu_sriov_vf(adev)) { r = amdgpu_device_health_check(device_list_handle); if (r) goto end_reset; } tmp_adev = list_first_entry(device_list_handle, struct amdgpu_device, reset_list); amdgpu_device_lock_reset_domain(tmp_adev->reset_domain); list_for_each_entry(tmp_adev, device_list_handle, reset_list) { amdgpu_device_set_mp1_state(tmp_adev); if (!amdgpu_device_suspend_display_audio(tmp_adev)) audio_suspended = true; amdgpu_ras_set_error_query_ready(tmp_adev, false); cancel_delayed_work_sync(&tmp_adev->delayed_init_work); if (!amdgpu_sriov_vf(tmp_adev)) amdgpu_amdkfd_pre_reset(tmp_adev); amdgpu_unregister_gpu_instance(tmp_adev); drm_fb_helper_set_suspend_unlocked(adev_to_drm(tmp_adev)->fb_helper, true); if (!need_emergency_restart && amdgpu_device_ip_need_full_reset(tmp_adev)) amdgpu_ras_suspend(tmp_adev); for (i = 0; i < AMDGPU_MAX_RINGS; ++i) { struct amdgpu_ring *ring = tmp_adev->rings[i]; if (!amdgpu_ring_sched_ready(ring)) continue; drm_sched_stop(&ring->sched, job ? &job->base : NULL); if (need_emergency_restart) amdgpu_job_stop_all_jobs_on_sched(&ring->sched); } atomic_inc(&tmp_adev->gpu_reset_counter); } if (need_emergency_restart) goto skip_sched_resume; if (job && dma_fence_is_signaled(&job->hw_fence)) { job_signaled = true; dev_info(adev->dev, ""Guilty job already signaled, skipping HW reset""); goto skip_hw_reset; } retry: list_for_each_entry(tmp_adev, device_list_handle, reset_list) { r = amdgpu_device_pre_asic_reset(tmp_adev, reset_context); if (r) { dev_err(tmp_adev->dev, ""GPU pre asic reset failed with err, %d for drm dev, %s "", r, adev_to_drm(tmp_adev)->unique); tmp_adev->asic_reset_res = r; } if (!amdgpu_sriov_vf(tmp_adev)) amdgpu_device_stop_pending_resets(tmp_adev); } if (amdgpu_sriov_vf(adev)) { r = amdgpu_device_reset_sriov(adev, job ? false : true); if (r) adev->asic_reset_res = r; if (amdgpu_ip_version(adev, GC_HWIP, 0) == IP_VERSION(9, 4, 2) || amdgpu_ip_version(adev, GC_HWIP, 0) == IP_VERSION(9, 4, 3) || amdgpu_ip_version(adev, GC_HWIP, 0) == IP_VERSION(11, 0, 3)) amdgpu_ras_resume(adev); } else { r = amdgpu_do_asic_reset(device_list_handle, reset_context); if (r && r == -EAGAIN) goto retry; } skip_hw_reset: list_for_each_entry(tmp_adev, device_list_handle, reset_list) { for (i = 0; i < AMDGPU_MAX_RINGS; ++i) { struct amdgpu_ring *ring = tmp_adev->rings[i]; if (!amdgpu_ring_sched_ready(ring)) continue; drm_sched_start(&ring->sched, true); } if (!drm_drv_uses_atomic_modeset(adev_to_drm(tmp_adev)) && !job_signaled) drm_helper_resume_force_mode(adev_to_drm(tmp_adev)); if (tmp_adev->asic_reset_res) r = tmp_adev->asic_reset_res; tmp_adev->asic_reset_res = 0; if (r) { dev_info(tmp_adev->dev, ""GPU reset(%d) failed\n"", atomic_read(&tmp_adev->gpu_reset_counter)); amdgpu_vf_error_put(tmp_adev, AMDGIM_ERROR_VF_GPU_RESET_FAIL, 0, r); } else { dev_info(tmp_adev->dev, ""GPU reset(%d) succeeded!\n"", atomic_read(&tmp_adev->gpu_reset_counter)); if (amdgpu_acpi_smart_shift_update(adev_to_drm(tmp_adev), AMDGPU_SS_DEV_D0)) DRM_WARN(""smart shift update failed\n""); } } skip_sched_resume: list_for_each_entry(tmp_adev, device_list_handle, reset_list) { if (!need_emergency_restart && !amdgpu_sriov_vf(tmp_adev)) amdgpu_amdkfd_post_reset(tmp_adev); if (!adev->kfd.init_complete) amdgpu_amdkfd_device_init(adev); if (audio_suspended) amdgpu_device_resume_display_audio(tmp_adev); amdgpu_device_unset_mp1_state(tmp_adev); amdgpu_ras_set_error_query_ready(tmp_adev, true); } tmp_adev = list_first_entry(device_list_handle, struct amdgpu_device, reset_list); amdgpu_device_unlock_reset_domain(tmp_adev->reset_domain); end_reset: if (hive) { mutex_unlock(&hive->hive_lock); amdgpu_put_xgmi_hive(hive); } if (r) dev_info(adev->dev, ""GPU reset end with ret = %d\n"", r); atomic_set(&adev->reset_domain->reset_res, r); return r; }"
1036----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50276/bad/mse102x.c----mse102x_tx_frame_spi,"static int mse102x_tx_frame_spi(struct mse102x_net *mse, struct sk_buff *txp, unsigned int pad) { struct mse102x_net_spi *mses = to_mse102x_spi(mse); struct spi_transfer *xfer = &mses->spi_xfer; struct spi_message *msg = &mses->spi_msg; <S2SV_StartVul> struct sk_buff *tskb; <S2SV_EndVul> int ret; netif_dbg(mse, tx_queued, mse->ndev, ""%s: skb %p, %d@%p\n"", __func__, txp, txp->len, txp->data); if ((skb_headroom(txp) < DET_SOF_LEN) || (skb_tailroom(txp) < DET_DFT_LEN + pad)) { tskb = skb_copy_expand(txp, DET_SOF_LEN, DET_DFT_LEN + pad, GFP_KERNEL); if (!tskb) return -ENOMEM; <S2SV_StartVul> dev_kfree_skb(txp); <S2SV_EndVul> txp = tskb; } mse102x_push_header(txp); if (pad) skb_put_zero(txp, pad); mse102x_put_footer(txp); xfer->tx_buf = txp->data; xfer->rx_buf = NULL; xfer->len = txp->len; ret = spi_sync(mses->spidev, msg); if (ret < 0) { netdev_err(mse->ndev, ""%s: spi_sync() failed: %d\n"", __func__, ret); mse->stats.xfer_err++; } return ret; }","- struct sk_buff *tskb;
- dev_kfree_skb(txp);
+ struct sk_buff *tskb = NULL;
+ dev_kfree_skb(tskb);","static int mse102x_tx_frame_spi(struct mse102x_net *mse, struct sk_buff *txp, unsigned int pad) { struct mse102x_net_spi *mses = to_mse102x_spi(mse); struct spi_transfer *xfer = &mses->spi_xfer; struct spi_message *msg = &mses->spi_msg; struct sk_buff *tskb = NULL; int ret; netif_dbg(mse, tx_queued, mse->ndev, ""%s: skb %p, %d@%p\n"", __func__, txp, txp->len, txp->data); if ((skb_headroom(txp) < DET_SOF_LEN) || (skb_tailroom(txp) < DET_DFT_LEN + pad)) { tskb = skb_copy_expand(txp, DET_SOF_LEN, DET_DFT_LEN + pad, GFP_KERNEL); if (!tskb) return -ENOMEM; txp = tskb; } mse102x_push_header(txp); if (pad) skb_put_zero(txp, pad); mse102x_put_footer(txp); xfer->tx_buf = txp->data; xfer->rx_buf = NULL; xfer->len = txp->len; ret = spi_sync(mses->spidev, msg); if (ret < 0) { netdev_err(mse->ndev, ""%s: spi_sync() failed: %d\n"", __func__, ret); mse->stats.xfer_err++; } dev_kfree_skb(tskb); return ret; }"
1209----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53213/bad/lan78xx.c----lan78xx_probe,"static int lan78xx_probe(struct usb_interface *intf, const struct usb_device_id *id) { struct usb_host_endpoint *ep_blkin, *ep_blkout, *ep_intr; struct lan78xx_net *dev; struct net_device *netdev; struct usb_device *udev; int ret; unsigned int maxp; unsigned int period; u8 *buf = NULL; udev = interface_to_usbdev(intf); udev = usb_get_dev(udev); netdev = alloc_etherdev(sizeof(struct lan78xx_net)); if (!netdev) { dev_err(&intf->dev, ""Error: OOM\n""); ret = -ENOMEM; goto out1; } SET_NETDEV_DEV(netdev, &intf->dev); dev = netdev_priv(netdev); dev->udev = udev; dev->intf = intf; dev->net = netdev; dev->msg_enable = netif_msg_init(msg_level, NETIF_MSG_DRV | NETIF_MSG_PROBE | NETIF_MSG_LINK); skb_queue_head_init(&dev->rxq); skb_queue_head_init(&dev->txq); skb_queue_head_init(&dev->rxq_done); skb_queue_head_init(&dev->txq_pend); skb_queue_head_init(&dev->rxq_overflow); mutex_init(&dev->phy_mutex); mutex_init(&dev->dev_mutex); ret = lan78xx_urb_config_init(dev); if (ret < 0) goto out2; ret = lan78xx_alloc_tx_resources(dev); if (ret < 0) goto out2; ret = lan78xx_alloc_rx_resources(dev); if (ret < 0) goto out3; netdev->max_mtu = MAX_SINGLE_PACKET_SIZE; netif_set_tso_max_size(netdev, LAN78XX_TSO_SIZE(dev)); netif_napi_add(netdev, &dev->napi, lan78xx_poll); INIT_DELAYED_WORK(&dev->wq, lan78xx_delayedwork); init_usb_anchor(&dev->deferred); netdev->netdev_ops = &lan78xx_netdev_ops; netdev->watchdog_timeo = TX_TIMEOUT_JIFFIES; netdev->ethtool_ops = &lan78xx_ethtool_ops; dev->delta = 1; timer_setup(&dev->stat_monitor, lan78xx_stat_monitor, 0); mutex_init(&dev->stats.access_lock); if (intf->cur_altsetting->desc.bNumEndpoints < 3) { ret = -ENODEV; goto out4; } dev->pipe_in = usb_rcvbulkpipe(udev, BULK_IN_PIPE); ep_blkin = usb_pipe_endpoint(udev, dev->pipe_in); if (!ep_blkin || !usb_endpoint_is_bulk_in(&ep_blkin->desc)) { ret = -ENODEV; goto out4; } dev->pipe_out = usb_sndbulkpipe(udev, BULK_OUT_PIPE); ep_blkout = usb_pipe_endpoint(udev, dev->pipe_out); if (!ep_blkout || !usb_endpoint_is_bulk_out(&ep_blkout->desc)) { ret = -ENODEV; goto out4; } ep_intr = &intf->cur_altsetting->endpoint[2]; if (!usb_endpoint_is_int_in(&ep_intr->desc)) { ret = -ENODEV; goto out4; } dev->pipe_intr = usb_rcvintpipe(dev->udev, usb_endpoint_num(&ep_intr->desc)); ret = lan78xx_bind(dev, intf); if (ret < 0) goto out4; period = ep_intr->desc.bInterval; maxp = usb_maxpacket(dev->udev, dev->pipe_intr); <S2SV_StartVul> buf = kmalloc(maxp, GFP_KERNEL); <S2SV_EndVul> <S2SV_StartVul> if (!buf) { <S2SV_EndVul> ret = -ENOMEM; goto out5; } <S2SV_StartVul> dev->urb_intr = usb_alloc_urb(0, GFP_KERNEL); <S2SV_EndVul> <S2SV_StartVul> if (!dev->urb_intr) { <S2SV_EndVul> ret = -ENOMEM; <S2SV_StartVul> goto out6; <S2SV_EndVul> <S2SV_StartVul> } else { <S2SV_EndVul> <S2SV_StartVul> usb_fill_int_urb(dev->urb_intr, dev->udev, <S2SV_EndVul> <S2SV_StartVul> dev->pipe_intr, buf, maxp, <S2SV_EndVul> <S2SV_StartVul> intr_complete, dev, period); <S2SV_EndVul> <S2SV_StartVul> dev->urb_intr->transfer_flags |= URB_FREE_BUFFER; <S2SV_EndVul> } dev->maxpacket = usb_maxpacket(dev->udev, dev->pipe_out); if (dev->maxpacket == 0) { ret = -ENODEV; <S2SV_StartVul> goto out6; <S2SV_EndVul> } intf->needs_remote_wakeup = 1; ret = lan78xx_phy_init(dev); if (ret < 0) <S2SV_StartVul> goto out7; <S2SV_EndVul> ret = register_netdev(netdev); if (ret != 0) { netif_err(dev, probe, netdev, ""couldn't register the device\n""); goto out8; } usb_set_intfdata(intf, dev); ret = device_set_wakeup_enable(&udev->dev, true); pm_runtime_set_autosuspend_delay(&udev->dev, DEFAULT_AUTOSUSPEND_DELAY); return 0; out8: phy_disconnect(netdev->phydev); <S2SV_StartVul> out7: <S2SV_EndVul> usb_free_urb(dev->urb_intr); <S2SV_StartVul> out6: <S2SV_EndVul> <S2SV_StartVul> kfree(buf); <S2SV_EndVul> out5: lan78xx_unbind(dev, intf); out4: netif_napi_del(&dev->napi); lan78xx_free_rx_resources(dev); out3: lan78xx_free_tx_resources(dev); out2: free_netdev(netdev); out1: usb_put_dev(udev); return ret; }","- buf = kmalloc(maxp, GFP_KERNEL);
- if (!buf) {
- dev->urb_intr = usb_alloc_urb(0, GFP_KERNEL);
- if (!dev->urb_intr) {
- goto out6;
- } else {
- usb_fill_int_urb(dev->urb_intr, dev->udev,
- dev->pipe_intr, buf, maxp,
- intr_complete, dev, period);
- dev->urb_intr->transfer_flags |= URB_FREE_BUFFER;
- goto out6;
- goto out7;
- out7:
- out6:
- kfree(buf);
+ dev->urb_intr = usb_alloc_urb(0, GFP_KERNEL);
+ if (!dev->urb_intr) {
+ buf = kmalloc(maxp, GFP_KERNEL);
+ if (!buf) {
+ goto free_urbs;
+ usb_fill_int_urb(dev->urb_intr, dev->udev,
+ dev->pipe_intr, buf, maxp,
+ intr_complete, dev, period);
+ dev->urb_intr->transfer_flags |= URB_FREE_BUFFER;
+ goto free_urbs;
+ goto free_urbs;
+ free_urbs:","static int lan78xx_probe(struct usb_interface *intf, const struct usb_device_id *id) { struct usb_host_endpoint *ep_blkin, *ep_blkout, *ep_intr; struct lan78xx_net *dev; struct net_device *netdev; struct usb_device *udev; int ret; unsigned int maxp; unsigned int period; u8 *buf = NULL; udev = interface_to_usbdev(intf); udev = usb_get_dev(udev); netdev = alloc_etherdev(sizeof(struct lan78xx_net)); if (!netdev) { dev_err(&intf->dev, ""Error: OOM\n""); ret = -ENOMEM; goto out1; } SET_NETDEV_DEV(netdev, &intf->dev); dev = netdev_priv(netdev); dev->udev = udev; dev->intf = intf; dev->net = netdev; dev->msg_enable = netif_msg_init(msg_level, NETIF_MSG_DRV | NETIF_MSG_PROBE | NETIF_MSG_LINK); skb_queue_head_init(&dev->rxq); skb_queue_head_init(&dev->txq); skb_queue_head_init(&dev->rxq_done); skb_queue_head_init(&dev->txq_pend); skb_queue_head_init(&dev->rxq_overflow); mutex_init(&dev->phy_mutex); mutex_init(&dev->dev_mutex); ret = lan78xx_urb_config_init(dev); if (ret < 0) goto out2; ret = lan78xx_alloc_tx_resources(dev); if (ret < 0) goto out2; ret = lan78xx_alloc_rx_resources(dev); if (ret < 0) goto out3; netdev->max_mtu = MAX_SINGLE_PACKET_SIZE; netif_set_tso_max_size(netdev, LAN78XX_TSO_SIZE(dev)); netif_napi_add(netdev, &dev->napi, lan78xx_poll); INIT_DELAYED_WORK(&dev->wq, lan78xx_delayedwork); init_usb_anchor(&dev->deferred); netdev->netdev_ops = &lan78xx_netdev_ops; netdev->watchdog_timeo = TX_TIMEOUT_JIFFIES; netdev->ethtool_ops = &lan78xx_ethtool_ops; dev->delta = 1; timer_setup(&dev->stat_monitor, lan78xx_stat_monitor, 0); mutex_init(&dev->stats.access_lock); if (intf->cur_altsetting->desc.bNumEndpoints < 3) { ret = -ENODEV; goto out4; } dev->pipe_in = usb_rcvbulkpipe(udev, BULK_IN_PIPE); ep_blkin = usb_pipe_endpoint(udev, dev->pipe_in); if (!ep_blkin || !usb_endpoint_is_bulk_in(&ep_blkin->desc)) { ret = -ENODEV; goto out4; } dev->pipe_out = usb_sndbulkpipe(udev, BULK_OUT_PIPE); ep_blkout = usb_pipe_endpoint(udev, dev->pipe_out); if (!ep_blkout || !usb_endpoint_is_bulk_out(&ep_blkout->desc)) { ret = -ENODEV; goto out4; } ep_intr = &intf->cur_altsetting->endpoint[2]; if (!usb_endpoint_is_int_in(&ep_intr->desc)) { ret = -ENODEV; goto out4; } dev->pipe_intr = usb_rcvintpipe(dev->udev, usb_endpoint_num(&ep_intr->desc)); ret = lan78xx_bind(dev, intf); if (ret < 0) goto out4; period = ep_intr->desc.bInterval; maxp = usb_maxpacket(dev->udev, dev->pipe_intr); dev->urb_intr = usb_alloc_urb(0, GFP_KERNEL); if (!dev->urb_intr) { ret = -ENOMEM; goto out5; } buf = kmalloc(maxp, GFP_KERNEL); if (!buf) { ret = -ENOMEM; goto free_urbs; } usb_fill_int_urb(dev->urb_intr, dev->udev, dev->pipe_intr, buf, maxp, intr_complete, dev, period); dev->urb_intr->transfer_flags |= URB_FREE_BUFFER; dev->maxpacket = usb_maxpacket(dev->udev, dev->pipe_out); if (dev->maxpacket == 0) { ret = -ENODEV; goto free_urbs; } intf->needs_remote_wakeup = 1; ret = lan78xx_phy_init(dev); if (ret < 0) goto free_urbs; ret = register_netdev(netdev); if (ret != 0) { netif_err(dev, probe, netdev, ""couldn't register the device\n""); goto out8; } usb_set_intfdata(intf, dev); ret = device_set_wakeup_enable(&udev->dev, true); pm_runtime_set_autosuspend_delay(&udev->dev, DEFAULT_AUTOSUSPEND_DELAY); return 0; out8: phy_disconnect(netdev->phydev); free_urbs: usb_free_urb(dev->urb_intr); out5: lan78xx_unbind(dev, intf); out4: netif_napi_del(&dev->napi); lan78xx_free_rx_resources(dev); out3: lan78xx_free_tx_resources(dev); out2: free_netdev(netdev); out1: usb_put_dev(udev); return ret; }"
1076----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53060/bad/amdgpu_acpi.c----*amdgpu_atif_call,"static union acpi_object *amdgpu_atif_call(struct amdgpu_atif *atif, int function, struct acpi_buffer *params) { acpi_status status; union acpi_object *obj; union acpi_object atif_arg_elements[2]; struct acpi_object_list atif_arg; struct acpi_buffer buffer = { ACPI_ALLOCATE_BUFFER, NULL }; atif_arg.count = 2; atif_arg.pointer = &atif_arg_elements[0]; atif_arg_elements[0].type = ACPI_TYPE_INTEGER; atif_arg_elements[0].integer.value = function; if (params) { atif_arg_elements[1].type = ACPI_TYPE_BUFFER; atif_arg_elements[1].buffer.length = params->length; atif_arg_elements[1].buffer.pointer = params->pointer; } else { atif_arg_elements[1].type = ACPI_TYPE_INTEGER; atif_arg_elements[1].integer.value = 0; } status = acpi_evaluate_object(atif->handle, NULL, &atif_arg, &buffer); obj = (union acpi_object *)buffer.pointer; <S2SV_StartVul> if (ACPI_FAILURE(status) && status != AE_NOT_FOUND) { <S2SV_EndVul> DRM_DEBUG_DRIVER(""failed to evaluate ATIF got %s\n"", acpi_format_exception(status)); kfree(obj); return NULL; } if (obj->type != ACPI_TYPE_BUFFER) { DRM_DEBUG_DRIVER(""bad object returned from ATIF: %d\n"", obj->type); kfree(obj); return NULL; } return obj; }","- if (ACPI_FAILURE(status) && status != AE_NOT_FOUND) {
+ if (ACPI_FAILURE(status)) {","static union acpi_object *amdgpu_atif_call(struct amdgpu_atif *atif, int function, struct acpi_buffer *params) { acpi_status status; union acpi_object *obj; union acpi_object atif_arg_elements[2]; struct acpi_object_list atif_arg; struct acpi_buffer buffer = { ACPI_ALLOCATE_BUFFER, NULL }; atif_arg.count = 2; atif_arg.pointer = &atif_arg_elements[0]; atif_arg_elements[0].type = ACPI_TYPE_INTEGER; atif_arg_elements[0].integer.value = function; if (params) { atif_arg_elements[1].type = ACPI_TYPE_BUFFER; atif_arg_elements[1].buffer.length = params->length; atif_arg_elements[1].buffer.pointer = params->pointer; } else { atif_arg_elements[1].type = ACPI_TYPE_INTEGER; atif_arg_elements[1].integer.value = 0; } status = acpi_evaluate_object(atif->handle, NULL, &atif_arg, &buffer); obj = (union acpi_object *)buffer.pointer; if (ACPI_FAILURE(status)) { DRM_DEBUG_DRIVER(""failed to evaluate ATIF got %s\n"", acpi_format_exception(status)); kfree(obj); return NULL; } if (obj->type != ACPI_TYPE_BUFFER) { DRM_DEBUG_DRIVER(""bad object returned from ATIF: %d\n"", obj->type); kfree(obj); return NULL; } return obj; }"
954----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50196/bad/pinctrl-ocelot.c----ocelot_irq_handler,"static void ocelot_irq_handler(struct irq_desc *desc) { struct irq_chip *parent_chip = irq_desc_get_chip(desc); struct gpio_chip *chip = irq_desc_get_handler_data(desc); struct ocelot_pinctrl *info = gpiochip_get_data(chip); unsigned int id_reg = OCELOT_GPIO_INTR_IDENT * info->stride; unsigned int reg = 0, irq, i; unsigned long irqs; for (i = 0; i < info->stride; i++) { regmap_read(info->map, id_reg + 4 * i, &reg); if (!reg) continue; <S2SV_StartVul> chained_irq_enter(parent_chip, desc); <S2SV_EndVul> irqs = reg; for_each_set_bit(irq, &irqs, min(32U, info->desc->npins - 32 * i)) generic_handle_domain_irq(chip->irq.domain, irq + 32 * i); <S2SV_StartVul> chained_irq_exit(parent_chip, desc); <S2SV_EndVul> } }","- chained_irq_enter(parent_chip, desc);
- chained_irq_exit(parent_chip, desc);
+ chained_irq_enter(parent_chip, desc);
+ chained_irq_exit(parent_chip, desc);","static void ocelot_irq_handler(struct irq_desc *desc) { struct irq_chip *parent_chip = irq_desc_get_chip(desc); struct gpio_chip *chip = irq_desc_get_handler_data(desc); struct ocelot_pinctrl *info = gpiochip_get_data(chip); unsigned int id_reg = OCELOT_GPIO_INTR_IDENT * info->stride; unsigned int reg = 0, irq, i; unsigned long irqs; chained_irq_enter(parent_chip, desc); for (i = 0; i < info->stride; i++) { regmap_read(info->map, id_reg + 4 * i, &reg); if (!reg) continue; irqs = reg; for_each_set_bit(irq, &irqs, min(32U, info->desc->npins - 32 * i)) generic_handle_domain_irq(chip->irq.domain, irq + 32 * i); } chained_irq_exit(parent_chip, desc); }"
1226----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53238/bad/btmtk.c----btmtk_usb_isointf_init,"static int btmtk_usb_isointf_init(struct hci_dev *hdev) { struct btmtk_data *btmtk_data = hci_get_priv(hdev); u8 iso_param[2] = { 0x08, 0x01 }; struct sk_buff *skb; int err; <S2SV_StartVul> init_usb_anchor(&btmtk_data->isopkt_anchor); <S2SV_EndVul> spin_lock_init(&btmtk_data->isorxlock); __set_mtk_intr_interface(hdev); err = btmtk_submit_intr_urb(hdev, GFP_KERNEL); if (err < 0) { usb_kill_anchored_urbs(&btmtk_data->isopkt_anchor); bt_dev_err(hdev, ""ISO intf not support (%d)"", err); return err; } skb = __hci_cmd_sync(hdev, 0xfd98, sizeof(iso_param), iso_param, HCI_INIT_TIMEOUT); if (IS_ERR(skb)) { bt_dev_err(hdev, ""Failed to apply iso setting (%ld)"", PTR_ERR(skb)); return PTR_ERR(skb); } kfree_skb(skb); return 0; }",- init_usb_anchor(&btmtk_data->isopkt_anchor);,"static int btmtk_usb_isointf_init(struct hci_dev *hdev) { struct btmtk_data *btmtk_data = hci_get_priv(hdev); u8 iso_param[2] = { 0x08, 0x01 }; struct sk_buff *skb; int err; spin_lock_init(&btmtk_data->isorxlock); __set_mtk_intr_interface(hdev); err = btmtk_submit_intr_urb(hdev, GFP_KERNEL); if (err < 0) { usb_kill_anchored_urbs(&btmtk_data->isopkt_anchor); bt_dev_err(hdev, ""ISO intf not support (%d)"", err); return err; } skb = __hci_cmd_sync(hdev, 0xfd98, sizeof(iso_param), iso_param, HCI_INIT_TIMEOUT); if (IS_ERR(skb)) { bt_dev_err(hdev, ""Failed to apply iso setting (%ld)"", PTR_ERR(skb)); return PTR_ERR(skb); } kfree_skb(skb); return 0; }"
276----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46758/bad/lm95234.c----tcrit1_hyst_store,"static ssize_t tcrit1_hyst_store(struct device *dev, struct device_attribute *attr, const char *buf, size_t count) { struct lm95234_data *data = dev_get_drvdata(dev); int index = to_sensor_dev_attr(attr)->index; int ret = lm95234_update_device(data); long val; if (ret) return ret; ret = kstrtol(buf, 10, &val); if (ret < 0) return ret; <S2SV_StartVul> val = DIV_ROUND_CLOSEST(val, 1000); <S2SV_EndVul> val = clamp_val((int)data->tcrit1[index] - val, 0, 31); mutex_lock(&data->update_lock); data->thyst = val; i2c_smbus_write_byte_data(data->client, LM95234_REG_TCRIT_HYST, val); mutex_unlock(&data->update_lock); return count; }","- val = DIV_ROUND_CLOSEST(val, 1000);
+ val = DIV_ROUND_CLOSEST(clamp_val(val, -255000, 255000), 1000);","static ssize_t tcrit1_hyst_store(struct device *dev, struct device_attribute *attr, const char *buf, size_t count) { struct lm95234_data *data = dev_get_drvdata(dev); int index = to_sensor_dev_attr(attr)->index; int ret = lm95234_update_device(data); long val; if (ret) return ret; ret = kstrtol(buf, 10, &val); if (ret < 0) return ret; val = DIV_ROUND_CLOSEST(clamp_val(val, -255000, 255000), 1000); val = clamp_val((int)data->tcrit1[index] - val, 0, 31); mutex_lock(&data->update_lock); data->thyst = val; i2c_smbus_write_byte_data(data->client, LM95234_REG_TCRIT_HYST, val); mutex_unlock(&data->update_lock); return count; }"
78----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44952/bad/core.c----uevent_show,"static ssize_t uevent_show(struct device *dev, struct device_attribute *attr, char *buf) { struct kobject *top_kobj; struct kset *kset; struct kobj_uevent_env *env = NULL; int i; int len = 0; int retval; top_kobj = &dev->kobj; while (!top_kobj->kset && top_kobj->parent) top_kobj = top_kobj->parent; if (!top_kobj->kset) goto out; kset = top_kobj->kset; if (!kset->uevent_ops || !kset->uevent_ops->uevent) goto out; if (kset->uevent_ops && kset->uevent_ops->filter) if (!kset->uevent_ops->filter(kset, &dev->kobj)) goto out; env = kzalloc(sizeof(struct kobj_uevent_env), GFP_KERNEL); if (!env) return -ENOMEM; <S2SV_StartVul> device_lock(dev); <S2SV_EndVul> retval = kset->uevent_ops->uevent(kset, &dev->kobj, env); <S2SV_StartVul> device_unlock(dev); <S2SV_EndVul> if (retval) goto out; for (i = 0; i < env->envp_idx; i++) len += sysfs_emit_at(buf, len, ""%s\n"", env->envp[i]); out: kfree(env); return len; }","- device_lock(dev);
- device_unlock(dev);","static ssize_t uevent_show(struct device *dev, struct device_attribute *attr, char *buf) { struct kobject *top_kobj; struct kset *kset; struct kobj_uevent_env *env = NULL; int i; int len = 0; int retval; top_kobj = &dev->kobj; while (!top_kobj->kset && top_kobj->parent) top_kobj = top_kobj->parent; if (!top_kobj->kset) goto out; kset = top_kobj->kset; if (!kset->uevent_ops || !kset->uevent_ops->uevent) goto out; if (kset->uevent_ops && kset->uevent_ops->filter) if (!kset->uevent_ops->filter(kset, &dev->kobj)) goto out; env = kzalloc(sizeof(struct kobj_uevent_env), GFP_KERNEL); if (!env) return -ENOMEM; retval = kset->uevent_ops->uevent(kset, &dev->kobj, env); if (retval) goto out; for (i = 0; i < env->envp_idx; i++) len += sysfs_emit_at(buf, len, ""%s\n"", env->envp[i]); out: kfree(env); return len; }"
576----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49898/bad/dcn32_fpu.c----subvp_vblank_schedulable,"static bool subvp_vblank_schedulable(struct dc *dc, struct dc_state *context) { struct pipe_ctx *pipe = NULL; struct pipe_ctx *subvp_pipe = NULL; bool found = false; bool schedulable = false; uint32_t i = 0; uint8_t vblank_index = 0; uint16_t prefetch_us = 0; uint16_t mall_region_us = 0; uint16_t vblank_frame_us = 0; uint16_t subvp_active_us = 0; uint16_t vblank_blank_us = 0; uint16_t max_vblank_mallregion = 0; struct dc_crtc_timing *main_timing = NULL; struct dc_crtc_timing *phantom_timing = NULL; struct dc_crtc_timing *vblank_timing = NULL; for (i = 0; i < dc->res_pool->pipe_count; i++) { pipe = &context->res_ctx.pipe_ctx[i]; if (!pipe->stream || !pipe->plane_state || pipe->top_pipe || pipe->prev_odm_pipe) continue; if (!found && pipe->stream->mall_stream_config.type == SUBVP_NONE) { vblank_index = i; found = true; } if (!subvp_pipe && pipe->stream->mall_stream_config.type == SUBVP_MAIN) subvp_pipe = pipe; } if (found && context->res_ctx.pipe_ctx[vblank_index].stream->ignore_msa_timing_param) { schedulable = subvp_drr_schedulable(dc, context, &context->res_ctx.pipe_ctx[vblank_index]); <S2SV_StartVul> } else if (found) { <S2SV_EndVul> main_timing = &subvp_pipe->stream->timing; phantom_timing = &subvp_pipe->stream->mall_stream_config.paired_stream->timing; vblank_timing = &context->res_ctx.pipe_ctx[vblank_index].stream->timing; prefetch_us = (phantom_timing->v_total - phantom_timing->v_front_porch) * phantom_timing->h_total / (double)(phantom_timing->pix_clk_100hz * 100) * 1000000 + dc->caps.subvp_prefetch_end_to_mall_start_us; mall_region_us = phantom_timing->v_addressable * phantom_timing->h_total / (double)(phantom_timing->pix_clk_100hz * 100) * 1000000; vblank_frame_us = vblank_timing->v_total * vblank_timing->h_total / (double)(vblank_timing->pix_clk_100hz * 100) * 1000000; vblank_blank_us = (vblank_timing->v_total - vblank_timing->v_addressable) * vblank_timing->h_total / (double)(vblank_timing->pix_clk_100hz * 100) * 1000000; subvp_active_us = main_timing->v_addressable * main_timing->h_total / (double)(main_timing->pix_clk_100hz * 100) * 1000000; max_vblank_mallregion = vblank_blank_us > mall_region_us ? vblank_blank_us : mall_region_us; if (subvp_active_us - prefetch_us - vblank_frame_us - max_vblank_mallregion > 0) schedulable = true; } return schedulable; }","- } else if (found) {
+ } else if (found && subvp_pipe) {","static bool subvp_vblank_schedulable(struct dc *dc, struct dc_state *context) { struct pipe_ctx *pipe = NULL; struct pipe_ctx *subvp_pipe = NULL; bool found = false; bool schedulable = false; uint32_t i = 0; uint8_t vblank_index = 0; uint16_t prefetch_us = 0; uint16_t mall_region_us = 0; uint16_t vblank_frame_us = 0; uint16_t subvp_active_us = 0; uint16_t vblank_blank_us = 0; uint16_t max_vblank_mallregion = 0; struct dc_crtc_timing *main_timing = NULL; struct dc_crtc_timing *phantom_timing = NULL; struct dc_crtc_timing *vblank_timing = NULL; for (i = 0; i < dc->res_pool->pipe_count; i++) { pipe = &context->res_ctx.pipe_ctx[i]; if (!pipe->stream || !pipe->plane_state || pipe->top_pipe || pipe->prev_odm_pipe) continue; if (!found && pipe->stream->mall_stream_config.type == SUBVP_NONE) { vblank_index = i; found = true; } if (!subvp_pipe && pipe->stream->mall_stream_config.type == SUBVP_MAIN) subvp_pipe = pipe; } if (found && context->res_ctx.pipe_ctx[vblank_index].stream->ignore_msa_timing_param) { schedulable = subvp_drr_schedulable(dc, context, &context->res_ctx.pipe_ctx[vblank_index]); } else if (found && subvp_pipe) { main_timing = &subvp_pipe->stream->timing; phantom_timing = &subvp_pipe->stream->mall_stream_config.paired_stream->timing; vblank_timing = &context->res_ctx.pipe_ctx[vblank_index].stream->timing; prefetch_us = (phantom_timing->v_total - phantom_timing->v_front_porch) * phantom_timing->h_total / (double)(phantom_timing->pix_clk_100hz * 100) * 1000000 + dc->caps.subvp_prefetch_end_to_mall_start_us; mall_region_us = phantom_timing->v_addressable * phantom_timing->h_total / (double)(phantom_timing->pix_clk_100hz * 100) * 1000000; vblank_frame_us = vblank_timing->v_total * vblank_timing->h_total / (double)(vblank_timing->pix_clk_100hz * 100) * 1000000; vblank_blank_us = (vblank_timing->v_total - vblank_timing->v_addressable) * vblank_timing->h_total / (double)(vblank_timing->pix_clk_100hz * 100) * 1000000; subvp_active_us = main_timing->v_addressable * main_timing->h_total / (double)(main_timing->pix_clk_100hz * 100) * 1000000; max_vblank_mallregion = vblank_blank_us > mall_region_us ? vblank_blank_us : mall_region_us; if (subvp_active_us - prefetch_us - vblank_frame_us - max_vblank_mallregion > 0) schedulable = true; } return schedulable; }"
978----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50214/bad/drm_connector_test.c----drm_test_drm_hdmi_compute_mode_clock_rgb_double,"static void drm_test_drm_hdmi_compute_mode_clock_rgb_double(struct kunit *test) { struct drm_connector_init_priv *priv = test->priv; const struct drm_display_mode *mode; unsigned long long rate; struct drm_device *drm = &priv->drm; <S2SV_StartVul> mode = drm_display_mode_from_cea_vic(drm, 6); <S2SV_EndVul> KUNIT_ASSERT_NOT_NULL(test, mode); KUNIT_ASSERT_TRUE(test, mode->flags & DRM_MODE_FLAG_DBLCLK); rate = drm_hdmi_compute_mode_clock(mode, 8, HDMI_COLORSPACE_RGB); KUNIT_ASSERT_GT(test, rate, 0); KUNIT_EXPECT_EQ(test, (mode->clock * 1000ULL) * 2, rate); }","- mode = drm_display_mode_from_cea_vic(drm, 6);
+ mode = drm_kunit_display_mode_from_cea_vic(test, drm, 6);","static void drm_test_drm_hdmi_compute_mode_clock_rgb_double(struct kunit *test) { struct drm_connector_init_priv *priv = test->priv; const struct drm_display_mode *mode; unsigned long long rate; struct drm_device *drm = &priv->drm; mode = drm_kunit_display_mode_from_cea_vic(test, drm, 6); KUNIT_ASSERT_NOT_NULL(test, mode); KUNIT_ASSERT_TRUE(test, mode->flags & DRM_MODE_FLAG_DBLCLK); rate = drm_hdmi_compute_mode_clock(mode, 8, HDMI_COLORSPACE_RGB); KUNIT_ASSERT_GT(test, rate, 0); KUNIT_EXPECT_EQ(test, (mode->clock * 1000ULL) * 2, rate); }"
703----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49988/bad/oplock.c----free_opinfo,static void free_opinfo(struct oplock_info *opinfo) { if (opinfo->is_lease) free_lease(opinfo); kfree(opinfo); <S2SV_StartVul> } <S2SV_EndVul>,"- }
+ if (opinfo->conn && atomic_dec_and_test(&opinfo->conn->refcnt))
+ kfree(opinfo->conn);",static void free_opinfo(struct oplock_info *opinfo) { if (opinfo->is_lease) free_lease(opinfo); if (opinfo->conn && atomic_dec_and_test(&opinfo->conn->refcnt)) kfree(opinfo->conn); kfree(opinfo); }
1396----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56722/bad/hns_roce_hw_v2.c----hns_roce_v2_modify_qp,"static int hns_roce_v2_modify_qp(struct ib_qp *ibqp, const struct ib_qp_attr *attr, int attr_mask, enum ib_qp_state cur_state, enum ib_qp_state new_state, struct ib_udata *udata) { struct hns_roce_dev *hr_dev = to_hr_dev(ibqp->device); struct hns_roce_qp *hr_qp = to_hr_qp(ibqp); struct hns_roce_v2_qp_context ctx[2]; struct hns_roce_v2_qp_context *context = ctx; struct hns_roce_v2_qp_context *qpc_mask = ctx + 1; struct ib_device *ibdev = &hr_dev->ib_dev; int ret; if (attr_mask & ~IB_QP_ATTR_STANDARD_BITS) return -EOPNOTSUPP; memset(context, 0, hr_dev->caps.qpc_sz); memset(qpc_mask, 0xff, hr_dev->caps.qpc_sz); ret = hns_roce_v2_set_abs_fields(ibqp, attr, attr_mask, cur_state, new_state, context, qpc_mask, udata); if (ret) goto out; if (new_state == IB_QPS_ERR) v2_set_flushed_fields(ibqp, context, qpc_mask); ret = hns_roce_v2_set_opt_fields(ibqp, attr, attr_mask, context, qpc_mask); if (ret) goto out; hr_reg_write_bool(context, QPC_INV_CREDIT, to_hr_qp_type(hr_qp->ibqp.qp_type) == SERV_TYPE_XRC || ibqp->srq); hr_reg_clear(qpc_mask, QPC_INV_CREDIT); hr_reg_write(context, QPC_QP_ST, new_state); hr_reg_clear(qpc_mask, QPC_QP_ST); ret = hns_roce_v2_qp_modify(hr_dev, context, qpc_mask, hr_qp); if (ret) { <S2SV_StartVul> ibdev_err(ibdev, ""failed to modify QP, ret = %d.\n"", ret); <S2SV_EndVul> goto out; <S2SV_StartVul> } <S2SV_EndVul> hr_qp->state = new_state; hns_roce_v2_record_opt_fields(ibqp, attr, attr_mask); if (new_state == IB_QPS_RESET && !ibqp->uobject) clear_qp(hr_qp); out: return ret; }","- ibdev_err(ibdev, ""failed to modify QP, ret = %d.\n"", ret);
- }
+ ibdev_err_ratelimited(ibdev, ""failed to modify QP, ret = %d.\n"", ret);","static int hns_roce_v2_modify_qp(struct ib_qp *ibqp, const struct ib_qp_attr *attr, int attr_mask, enum ib_qp_state cur_state, enum ib_qp_state new_state, struct ib_udata *udata) { struct hns_roce_dev *hr_dev = to_hr_dev(ibqp->device); struct hns_roce_qp *hr_qp = to_hr_qp(ibqp); struct hns_roce_v2_qp_context ctx[2]; struct hns_roce_v2_qp_context *context = ctx; struct hns_roce_v2_qp_context *qpc_mask = ctx + 1; struct ib_device *ibdev = &hr_dev->ib_dev; int ret; if (attr_mask & ~IB_QP_ATTR_STANDARD_BITS) return -EOPNOTSUPP; memset(context, 0, hr_dev->caps.qpc_sz); memset(qpc_mask, 0xff, hr_dev->caps.qpc_sz); ret = hns_roce_v2_set_abs_fields(ibqp, attr, attr_mask, cur_state, new_state, context, qpc_mask, udata); if (ret) goto out; if (new_state == IB_QPS_ERR) v2_set_flushed_fields(ibqp, context, qpc_mask); ret = hns_roce_v2_set_opt_fields(ibqp, attr, attr_mask, context, qpc_mask); if (ret) goto out; hr_reg_write_bool(context, QPC_INV_CREDIT, to_hr_qp_type(hr_qp->ibqp.qp_type) == SERV_TYPE_XRC || ibqp->srq); hr_reg_clear(qpc_mask, QPC_INV_CREDIT); hr_reg_write(context, QPC_QP_ST, new_state); hr_reg_clear(qpc_mask, QPC_QP_ST); ret = hns_roce_v2_qp_modify(hr_dev, context, qpc_mask, hr_qp); if (ret) { ibdev_err_ratelimited(ibdev, ""failed to modify QP, ret = %d.\n"", ret); goto out; } hr_qp->state = new_state; hns_roce_v2_record_opt_fields(ibqp, attr, attr_mask); if (new_state == IB_QPS_RESET && !ibqp->uobject) clear_qp(hr_qp); out: return ret; }"
458----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47700/bad/super.c----__ext4_fill_super,"static int __ext4_fill_super(struct fs_context *fc, struct super_block *sb) { struct ext4_super_block *es = NULL; struct ext4_sb_info *sbi = EXT4_SB(sb); ext4_fsblk_t logical_sb_block; struct inode *root; int needs_recovery; int err; ext4_group_t first_not_zeroed; struct ext4_fs_context *ctx = fc->fs_private; int silent = fc->sb_flags & SB_SILENT; if (!(ctx->spec & EXT4_SPEC_JOURNAL_IOPRIO)) ctx->journal_ioprio = DEFAULT_JOURNAL_IOPRIO; sbi->s_inode_readahead_blks = EXT4_DEF_INODE_READAHEAD_BLKS; sbi->s_sectors_written_start = part_stat_read(sb->s_bdev, sectors[STAT_WRITE]); err = ext4_load_super(sb, &logical_sb_block, silent); if (err) goto out_fail; es = sbi->s_es; sbi->s_kbytes_written = le64_to_cpu(es->s_kbytes_written); err = ext4_init_metadata_csum(sb, es); if (err) goto failed_mount; ext4_set_def_opts(sb, es); sbi->s_resuid = make_kuid(&init_user_ns, le16_to_cpu(es->s_def_resuid)); sbi->s_resgid = make_kgid(&init_user_ns, le16_to_cpu(es->s_def_resgid)); sbi->s_commit_interval = JBD2_DEFAULT_MAX_COMMIT_AGE * HZ; sbi->s_min_batch_time = EXT4_DEF_MIN_BATCH_TIME; sbi->s_max_batch_time = EXT4_DEF_MAX_BATCH_TIME; sbi->s_li_wait_mult = EXT4_DEF_LI_WAIT_MULT; err = ext4_inode_info_init(sb, es); if (err) goto failed_mount; err = parse_apply_sb_mount_options(sb, ctx); if (err < 0) goto failed_mount; sbi->s_def_mount_opt = sbi->s_mount_opt; sbi->s_def_mount_opt2 = sbi->s_mount_opt2; err = ext4_check_opt_consistency(fc, sb); if (err < 0) goto failed_mount; ext4_apply_options(fc, sb); err = ext4_encoding_init(sb, es); if (err) goto failed_mount; err = ext4_check_journal_data_mode(sb); if (err) goto failed_mount; sb->s_flags = (sb->s_flags & ~SB_POSIXACL) | (test_opt(sb, POSIX_ACL) ? SB_POSIXACL : 0); sb->s_flags |= SB_I_VERSION; err = ext4_check_feature_compatibility(sb, es, silent); if (err) goto failed_mount; err = ext4_block_group_meta_init(sb, silent); if (err) goto failed_mount; err = ext4_hash_info_init(sb); if (err) goto failed_mount; err = ext4_handle_clustersize(sb); if (err) goto failed_mount; err = ext4_check_geometry(sb, es); if (err) goto failed_mount; timer_setup(&sbi->s_err_report, print_daily_error_info, 0); spin_lock_init(&sbi->s_error_lock); INIT_WORK(&sbi->s_sb_upd_work, update_super_work); err = ext4_group_desc_init(sb, es, logical_sb_block, &first_not_zeroed); if (err) goto failed_mount3; err = ext4_es_register_shrinker(sbi); if (err) goto failed_mount3; sbi->s_stripe = ext4_get_stripe_size(sbi); <S2SV_StartVul> if (sbi->s_stripe > 0 && sbi->s_cluster_ratio > 1 && <S2SV_EndVul> <S2SV_StartVul> sbi->s_stripe % sbi->s_cluster_ratio != 0) { <S2SV_EndVul> ext4_msg(sb, KERN_WARNING, ""stripe (%lu) is not aligned with cluster size (%u), "" ""stripe is disabled"", sbi->s_stripe, sbi->s_cluster_ratio); sbi->s_stripe = 0; } sbi->s_extent_max_zeroout_kb = 32; sb->s_op = &ext4_sops; sb->s_export_op = &ext4_export_ops; sb->s_xattr = ext4_xattr_handlers; #ifdef CONFIG_FS_ENCRYPTION sb->s_cop = &ext4_cryptops; #endif #ifdef CONFIG_FS_VERITY sb->s_vop = &ext4_verityops; #endif #ifdef CONFIG_QUOTA sb->dq_op = &ext4_quota_operations; if (ext4_has_feature_quota(sb)) sb->s_qcop = &dquot_quotactl_sysfile_ops; else sb->s_qcop = &ext4_qctl_operations; sb->s_quota_types = QTYPE_MASK_USR | QTYPE_MASK_GRP | QTYPE_MASK_PRJ; #endif super_set_uuid(sb, es->s_uuid, sizeof(es->s_uuid)); super_set_sysfs_name_bdev(sb); INIT_LIST_HEAD(&sbi->s_orphan); mutex_init(&sbi->s_orphan_lock); spin_lock_init(&sbi->s_bdev_wb_lock); ext4_fast_commit_init(sb); sb->s_root = NULL; needs_recovery = (es->s_last_orphan != 0 || ext4_has_feature_orphan_present(sb) || ext4_has_feature_journal_needs_recovery(sb)); if (ext4_has_feature_mmp(sb) && !sb_rdonly(sb)) { err = ext4_multi_mount_protect(sb, le64_to_cpu(es->s_mmp_block)); if (err) goto failed_mount3a; } err = -EINVAL; if (!test_opt(sb, NOLOAD) && ext4_has_feature_journal(sb)) { err = ext4_load_and_init_journal(sb, es, ctx); if (err) goto failed_mount3a; } else if (test_opt(sb, NOLOAD) && !sb_rdonly(sb) && ext4_has_feature_journal_needs_recovery(sb)) { ext4_msg(sb, KERN_ERR, ""required journal recovery "" ""suppressed and not mounted read-only""); goto failed_mount3a; } else { if (test_opt(sb, JOURNAL_ASYNC_COMMIT)) { ext4_msg(sb, KERN_ERR, ""can't mount with "" ""journal_async_commit, fs mounted w/o journal""); goto failed_mount3a; } if (test_opt2(sb, EXPLICIT_JOURNAL_CHECKSUM)) { ext4_msg(sb, KERN_ERR, ""can't mount with "" ""journal_checksum, fs mounted w/o journal""); goto failed_mount3a; } if (sbi->s_commit_interval != JBD2_DEFAULT_MAX_COMMIT_AGE*HZ) { ext4_msg(sb, KERN_ERR, ""can't mount with "" ""commit=%lu, fs mounted w/o journal"", sbi->s_commit_interval / HZ); goto failed_mount3a; } if (EXT4_MOUNT_DATA_FLAGS & (sbi->s_mount_opt ^ sbi->s_def_mount_opt)) { ext4_msg(sb, KERN_ERR, ""can't mount with "" ""data=, fs mounted w/o journal""); goto failed_mount3a; } sbi->s_def_mount_opt &= ~EXT4_MOUNT_JOURNAL_CHECKSUM; clear_opt(sb, JOURNAL_CHECKSUM); clear_opt(sb, DATA_FLAGS); clear_opt2(sb, JOURNAL_FAST_COMMIT); sbi->s_journal = NULL; needs_recovery = 0; } if (!test_opt(sb, NO_MBCACHE)) { sbi->s_ea_block_cache = ext4_xattr_create_cache(); if (!sbi->s_ea_block_cache) { ext4_msg(sb, KERN_ERR, ""Failed to create ea_block_cache""); err = -EINVAL; goto failed_mount_wq; } if (ext4_has_feature_ea_inode(sb)) { sbi->s_ea_inode_cache = ext4_xattr_create_cache(); if (!sbi->s_ea_inode_cache) { ext4_msg(sb, KERN_ERR, ""Failed to create ea_inode_cache""); err = -EINVAL; goto failed_mount_wq; } } } sbi->s_overhead = le32_to_cpu(es->s_overhead_clusters); if (sbi->s_overhead > ext4_blocks_count(es)) sbi->s_overhead = 0; if (!ext4_has_feature_bigalloc(sb)) sbi->s_overhead = 0; if (sbi->s_overhead == 0) { err = ext4_calculate_overhead(sb); if (err) goto failed_mount_wq; } EXT4_SB(sb)->rsv_conversion_wq = alloc_workqueue(""ext4-rsv-conversion"", WQ_MEM_RECLAIM | WQ_UNBOUND, 1); if (!EXT4_SB(sb)->rsv_conversion_wq) { printk(KERN_ERR ""EXT4-fs: failed to create workqueue\n""); err = -ENOMEM; goto failed_mount4; } root = ext4_iget(sb, EXT4_ROOT_INO, EXT4_IGET_SPECIAL); if (IS_ERR(root)) { ext4_msg(sb, KERN_ERR, ""get root inode failed""); err = PTR_ERR(root); root = NULL; goto failed_mount4; } if (!S_ISDIR(root->i_mode) || !root->i_blocks || !root->i_size) { ext4_msg(sb, KERN_ERR, ""corrupt root inode, run e2fsck""); iput(root); err = -EFSCORRUPTED; goto failed_mount4; } generic_set_sb_d_ops(sb); sb->s_root = d_make_root(root); if (!sb->s_root) { ext4_msg(sb, KERN_ERR, ""get root dentry failed""); err = -ENOMEM; goto failed_mount4; } err = ext4_setup_super(sb, es, sb_rdonly(sb)); if (err == -EROFS) { sb->s_flags |= SB_RDONLY; } else if (err) goto failed_mount4a; ext4_set_resv_clusters(sb); if (test_opt(sb, BLOCK_VALIDITY)) { err = ext4_setup_system_zone(sb); if (err) { ext4_msg(sb, KERN_ERR, ""failed to initialize system "" ""zone (%d)"", err); goto failed_mount4a; } } ext4_fc_replay_cleanup(sb); ext4_ext_init(sb); if (!(ctx->spec & EXT4_SPEC_mb_optimize_scan)) { if (sbi->s_groups_count >= MB_DEFAULT_LINEAR_SCAN_THRESHOLD) set_opt2(sb, MB_OPTIMIZE_SCAN); else clear_opt2(sb, MB_OPTIMIZE_SCAN); } err = ext4_mb_init(sb); if (err) { ext4_msg(sb, KERN_ERR, ""failed to initialize mballoc (%d)"", err); goto failed_mount5; } if (sbi->s_journal) sbi->s_journal->j_commit_callback = ext4_journal_commit_callback; err = ext4_percpu_param_init(sbi); if (err) goto failed_mount6; if (ext4_has_feature_flex_bg(sb)) if (!ext4_fill_flex_info(sb)) { ext4_msg(sb, KERN_ERR, ""unable to initialize "" ""flex_bg meta info!""); err = -ENOMEM; goto failed_mount6; } err = ext4_register_li_request(sb, first_not_zeroed); if (err) goto failed_mount6; err = ext4_init_orphan_info(sb); if (err) goto failed_mount7; #ifdef CONFIG_QUOTA if (ext4_has_feature_quota(sb) && !sb_rdonly(sb)) { err = ext4_enable_quotas(sb); if (err) goto failed_mount8; } #endif errseq_check_and_advance(&sb->s_bdev->bd_mapping->wb_err, &sbi->s_bdev_wb_err); EXT4_SB(sb)->s_mount_state |= EXT4_ORPHAN_FS; ext4_orphan_cleanup(sb, es); EXT4_SB(sb)->s_mount_state &= ~EXT4_ORPHAN_FS; ext4_superblock_csum_set(sb); if (needs_recovery) { ext4_msg(sb, KERN_INFO, ""recovery complete""); err = ext4_mark_recovery_complete(sb, es); if (err) goto failed_mount9; } if (test_opt(sb, DISCARD) && !bdev_max_discard_sectors(sb->s_bdev)) ext4_msg(sb, KERN_WARNING, ""mounting with \""discard\"" option, but the device does not support discard""); if (es->s_error_count) mod_timer(&sbi->s_err_report, jiffies + 300*HZ); ratelimit_state_init(&sbi->s_err_ratelimit_state, 5 * HZ, 10); ratelimit_state_init(&sbi->s_warning_ratelimit_state, 5 * HZ, 10); ratelimit_state_init(&sbi->s_msg_ratelimit_state, 5 * HZ, 10); atomic_set(&sbi->s_warning_count, 0); atomic_set(&sbi->s_msg_count, 0); err = ext4_register_sysfs(sb); if (err) goto failed_mount9; return 0; failed_mount9: ext4_quotas_off(sb, EXT4_MAXQUOTAS); failed_mount8: __maybe_unused ext4_release_orphan_info(sb); failed_mount7: ext4_unregister_li_request(sb); failed_mount6: ext4_mb_release(sb); ext4_flex_groups_free(sbi); ext4_percpu_param_destroy(sbi); failed_mount5: ext4_ext_release(sb); ext4_release_system_zone(sb); failed_mount4a: dput(sb->s_root); sb->s_root = NULL; failed_mount4: ext4_msg(sb, KERN_ERR, ""mount failed""); if (EXT4_SB(sb)->rsv_conversion_wq) destroy_workqueue(EXT4_SB(sb)->rsv_conversion_wq); failed_mount_wq: ext4_xattr_destroy_cache(sbi->s_ea_inode_cache); sbi->s_ea_inode_cache = NULL; ext4_xattr_destroy_cache(sbi->s_ea_block_cache); sbi->s_ea_block_cache = NULL; if (sbi->s_journal) { flush_work(&sbi->s_sb_upd_work); jbd2_journal_destroy(sbi->s_journal); sbi->s_journal = NULL; } failed_mount3a: ext4_es_unregister_shrinker(sbi); failed_mount3: flush_work(&sbi->s_sb_upd_work); ext4_stop_mmpd(sbi); del_timer_sync(&sbi->s_err_report); ext4_group_desc_free(sbi); failed_mount: if (sbi->s_chksum_driver) crypto_free_shash(sbi->s_chksum_driver); #if IS_ENABLED(CONFIG_UNICODE) utf8_unload(sb->s_encoding); #endif #ifdef CONFIG_QUOTA for (unsigned int i = 0; i < EXT4_MAXQUOTAS; i++) kfree(get_qf_name(sb, sbi, i)); #endif fscrypt_free_dummy_policy(&sbi->s_dummy_enc_policy); brelse(sbi->s_sbh); if (sbi->s_journal_bdev_file) { invalidate_bdev(file_bdev(sbi->s_journal_bdev_file)); bdev_fput(sbi->s_journal_bdev_file); } out_fail: invalidate_bdev(sb->s_bdev); sb->s_fs_info = NULL; return err; }","- if (sbi->s_stripe > 0 && sbi->s_cluster_ratio > 1 &&
- sbi->s_stripe % sbi->s_cluster_ratio != 0) {
+ if (ext4_is_stripe_incompatible(sb, sbi->s_stripe)) {
+ ext4_msg(sb, KERN_WARNING,
+ ""stripe (%lu) is not aligned with cluster size (%u), ""
+ ""stripe is disabled"",","static int __ext4_fill_super(struct fs_context *fc, struct super_block *sb) { struct ext4_super_block *es = NULL; struct ext4_sb_info *sbi = EXT4_SB(sb); ext4_fsblk_t logical_sb_block; struct inode *root; int needs_recovery; int err; ext4_group_t first_not_zeroed; struct ext4_fs_context *ctx = fc->fs_private; int silent = fc->sb_flags & SB_SILENT; if (!(ctx->spec & EXT4_SPEC_JOURNAL_IOPRIO)) ctx->journal_ioprio = DEFAULT_JOURNAL_IOPRIO; sbi->s_inode_readahead_blks = EXT4_DEF_INODE_READAHEAD_BLKS; sbi->s_sectors_written_start = part_stat_read(sb->s_bdev, sectors[STAT_WRITE]); err = ext4_load_super(sb, &logical_sb_block, silent); if (err) goto out_fail; es = sbi->s_es; sbi->s_kbytes_written = le64_to_cpu(es->s_kbytes_written); err = ext4_init_metadata_csum(sb, es); if (err) goto failed_mount; ext4_set_def_opts(sb, es); sbi->s_resuid = make_kuid(&init_user_ns, le16_to_cpu(es->s_def_resuid)); sbi->s_resgid = make_kgid(&init_user_ns, le16_to_cpu(es->s_def_resgid)); sbi->s_commit_interval = JBD2_DEFAULT_MAX_COMMIT_AGE * HZ; sbi->s_min_batch_time = EXT4_DEF_MIN_BATCH_TIME; sbi->s_max_batch_time = EXT4_DEF_MAX_BATCH_TIME; sbi->s_li_wait_mult = EXT4_DEF_LI_WAIT_MULT; err = ext4_inode_info_init(sb, es); if (err) goto failed_mount; err = parse_apply_sb_mount_options(sb, ctx); if (err < 0) goto failed_mount; sbi->s_def_mount_opt = sbi->s_mount_opt; sbi->s_def_mount_opt2 = sbi->s_mount_opt2; err = ext4_check_opt_consistency(fc, sb); if (err < 0) goto failed_mount; ext4_apply_options(fc, sb); err = ext4_encoding_init(sb, es); if (err) goto failed_mount; err = ext4_check_journal_data_mode(sb); if (err) goto failed_mount; sb->s_flags = (sb->s_flags & ~SB_POSIXACL) | (test_opt(sb, POSIX_ACL) ? SB_POSIXACL : 0); sb->s_flags |= SB_I_VERSION; err = ext4_check_feature_compatibility(sb, es, silent); if (err) goto failed_mount; err = ext4_block_group_meta_init(sb, silent); if (err) goto failed_mount; err = ext4_hash_info_init(sb); if (err) goto failed_mount; err = ext4_handle_clustersize(sb); if (err) goto failed_mount; err = ext4_check_geometry(sb, es); if (err) goto failed_mount; timer_setup(&sbi->s_err_report, print_daily_error_info, 0); spin_lock_init(&sbi->s_error_lock); INIT_WORK(&sbi->s_sb_upd_work, update_super_work); err = ext4_group_desc_init(sb, es, logical_sb_block, &first_not_zeroed); if (err) goto failed_mount3; err = ext4_es_register_shrinker(sbi); if (err) goto failed_mount3; sbi->s_stripe = ext4_get_stripe_size(sbi); if (ext4_is_stripe_incompatible(sb, sbi->s_stripe)) { ext4_msg(sb, KERN_WARNING, ""stripe (%lu) is not aligned with cluster size (%u), "" ""stripe is disabled"", sbi->s_stripe, sbi->s_cluster_ratio); sbi->s_stripe = 0; } sbi->s_extent_max_zeroout_kb = 32; sb->s_op = &ext4_sops; sb->s_export_op = &ext4_export_ops; sb->s_xattr = ext4_xattr_handlers; #ifdef CONFIG_FS_ENCRYPTION sb->s_cop = &ext4_cryptops; #endif #ifdef CONFIG_FS_VERITY sb->s_vop = &ext4_verityops; #endif #ifdef CONFIG_QUOTA sb->dq_op = &ext4_quota_operations; if (ext4_has_feature_quota(sb)) sb->s_qcop = &dquot_quotactl_sysfile_ops; else sb->s_qcop = &ext4_qctl_operations; sb->s_quota_types = QTYPE_MASK_USR | QTYPE_MASK_GRP | QTYPE_MASK_PRJ; #endif super_set_uuid(sb, es->s_uuid, sizeof(es->s_uuid)); super_set_sysfs_name_bdev(sb); INIT_LIST_HEAD(&sbi->s_orphan); mutex_init(&sbi->s_orphan_lock); spin_lock_init(&sbi->s_bdev_wb_lock); ext4_fast_commit_init(sb); sb->s_root = NULL; needs_recovery = (es->s_last_orphan != 0 || ext4_has_feature_orphan_present(sb) || ext4_has_feature_journal_needs_recovery(sb)); if (ext4_has_feature_mmp(sb) && !sb_rdonly(sb)) { err = ext4_multi_mount_protect(sb, le64_to_cpu(es->s_mmp_block)); if (err) goto failed_mount3a; } err = -EINVAL; if (!test_opt(sb, NOLOAD) && ext4_has_feature_journal(sb)) { err = ext4_load_and_init_journal(sb, es, ctx); if (err) goto failed_mount3a; } else if (test_opt(sb, NOLOAD) && !sb_rdonly(sb) && ext4_has_feature_journal_needs_recovery(sb)) { ext4_msg(sb, KERN_ERR, ""required journal recovery "" ""suppressed and not mounted read-only""); goto failed_mount3a; } else { if (test_opt(sb, JOURNAL_ASYNC_COMMIT)) { ext4_msg(sb, KERN_ERR, ""can't mount with "" ""journal_async_commit, fs mounted w/o journal""); goto failed_mount3a; } if (test_opt2(sb, EXPLICIT_JOURNAL_CHECKSUM)) { ext4_msg(sb, KERN_ERR, ""can't mount with "" ""journal_checksum, fs mounted w/o journal""); goto failed_mount3a; } if (sbi->s_commit_interval != JBD2_DEFAULT_MAX_COMMIT_AGE*HZ) { ext4_msg(sb, KERN_ERR, ""can't mount with "" ""commit=%lu, fs mounted w/o journal"", sbi->s_commit_interval / HZ); goto failed_mount3a; } if (EXT4_MOUNT_DATA_FLAGS & (sbi->s_mount_opt ^ sbi->s_def_mount_opt)) { ext4_msg(sb, KERN_ERR, ""can't mount with "" ""data=, fs mounted w/o journal""); goto failed_mount3a; } sbi->s_def_mount_opt &= ~EXT4_MOUNT_JOURNAL_CHECKSUM; clear_opt(sb, JOURNAL_CHECKSUM); clear_opt(sb, DATA_FLAGS); clear_opt2(sb, JOURNAL_FAST_COMMIT); sbi->s_journal = NULL; needs_recovery = 0; } if (!test_opt(sb, NO_MBCACHE)) { sbi->s_ea_block_cache = ext4_xattr_create_cache(); if (!sbi->s_ea_block_cache) { ext4_msg(sb, KERN_ERR, ""Failed to create ea_block_cache""); err = -EINVAL; goto failed_mount_wq; } if (ext4_has_feature_ea_inode(sb)) { sbi->s_ea_inode_cache = ext4_xattr_create_cache(); if (!sbi->s_ea_inode_cache) { ext4_msg(sb, KERN_ERR, ""Failed to create ea_inode_cache""); err = -EINVAL; goto failed_mount_wq; } } } sbi->s_overhead = le32_to_cpu(es->s_overhead_clusters); if (sbi->s_overhead > ext4_blocks_count(es)) sbi->s_overhead = 0; if (!ext4_has_feature_bigalloc(sb)) sbi->s_overhead = 0; if (sbi->s_overhead == 0) { err = ext4_calculate_overhead(sb); if (err) goto failed_mount_wq; } EXT4_SB(sb)->rsv_conversion_wq = alloc_workqueue(""ext4-rsv-conversion"", WQ_MEM_RECLAIM | WQ_UNBOUND, 1); if (!EXT4_SB(sb)->rsv_conversion_wq) { printk(KERN_ERR ""EXT4-fs: failed to create workqueue\n""); err = -ENOMEM; goto failed_mount4; } root = ext4_iget(sb, EXT4_ROOT_INO, EXT4_IGET_SPECIAL); if (IS_ERR(root)) { ext4_msg(sb, KERN_ERR, ""get root inode failed""); err = PTR_ERR(root); root = NULL; goto failed_mount4; } if (!S_ISDIR(root->i_mode) || !root->i_blocks || !root->i_size) { ext4_msg(sb, KERN_ERR, ""corrupt root inode, run e2fsck""); iput(root); err = -EFSCORRUPTED; goto failed_mount4; } generic_set_sb_d_ops(sb); sb->s_root = d_make_root(root); if (!sb->s_root) { ext4_msg(sb, KERN_ERR, ""get root dentry failed""); err = -ENOMEM; goto failed_mount4; } err = ext4_setup_super(sb, es, sb_rdonly(sb)); if (err == -EROFS) { sb->s_flags |= SB_RDONLY; } else if (err) goto failed_mount4a; ext4_set_resv_clusters(sb); if (test_opt(sb, BLOCK_VALIDITY)) { err = ext4_setup_system_zone(sb); if (err) { ext4_msg(sb, KERN_ERR, ""failed to initialize system "" ""zone (%d)"", err); goto failed_mount4a; } } ext4_fc_replay_cleanup(sb); ext4_ext_init(sb); if (!(ctx->spec & EXT4_SPEC_mb_optimize_scan)) { if (sbi->s_groups_count >= MB_DEFAULT_LINEAR_SCAN_THRESHOLD) set_opt2(sb, MB_OPTIMIZE_SCAN); else clear_opt2(sb, MB_OPTIMIZE_SCAN); } err = ext4_mb_init(sb); if (err) { ext4_msg(sb, KERN_ERR, ""failed to initialize mballoc (%d)"", err); goto failed_mount5; } if (sbi->s_journal) sbi->s_journal->j_commit_callback = ext4_journal_commit_callback; err = ext4_percpu_param_init(sbi); if (err) goto failed_mount6; if (ext4_has_feature_flex_bg(sb)) if (!ext4_fill_flex_info(sb)) { ext4_msg(sb, KERN_ERR, ""unable to initialize "" ""flex_bg meta info!""); err = -ENOMEM; goto failed_mount6; } err = ext4_register_li_request(sb, first_not_zeroed); if (err) goto failed_mount6; err = ext4_init_orphan_info(sb); if (err) goto failed_mount7; #ifdef CONFIG_QUOTA if (ext4_has_feature_quota(sb) && !sb_rdonly(sb)) { err = ext4_enable_quotas(sb); if (err) goto failed_mount8; } #endif errseq_check_and_advance(&sb->s_bdev->bd_mapping->wb_err, &sbi->s_bdev_wb_err); EXT4_SB(sb)->s_mount_state |= EXT4_ORPHAN_FS; ext4_orphan_cleanup(sb, es); EXT4_SB(sb)->s_mount_state &= ~EXT4_ORPHAN_FS; ext4_superblock_csum_set(sb); if (needs_recovery) { ext4_msg(sb, KERN_INFO, ""recovery complete""); err = ext4_mark_recovery_complete(sb, es); if (err) goto failed_mount9; } if (test_opt(sb, DISCARD) && !bdev_max_discard_sectors(sb->s_bdev)) ext4_msg(sb, KERN_WARNING, ""mounting with \""discard\"" option, but the device does not support discard""); if (es->s_error_count) mod_timer(&sbi->s_err_report, jiffies + 300*HZ); ratelimit_state_init(&sbi->s_err_ratelimit_state, 5 * HZ, 10); ratelimit_state_init(&sbi->s_warning_ratelimit_state, 5 * HZ, 10); ratelimit_state_init(&sbi->s_msg_ratelimit_state, 5 * HZ, 10); atomic_set(&sbi->s_warning_count, 0); atomic_set(&sbi->s_msg_count, 0); err = ext4_register_sysfs(sb); if (err) goto failed_mount9; return 0; failed_mount9: ext4_quotas_off(sb, EXT4_MAXQUOTAS); failed_mount8: __maybe_unused ext4_release_orphan_info(sb); failed_mount7: ext4_unregister_li_request(sb); failed_mount6: ext4_mb_release(sb); ext4_flex_groups_free(sbi); ext4_percpu_param_destroy(sbi); failed_mount5: ext4_ext_release(sb); ext4_release_system_zone(sb); failed_mount4a: dput(sb->s_root); sb->s_root = NULL; failed_mount4: ext4_msg(sb, KERN_ERR, ""mount failed""); if (EXT4_SB(sb)->rsv_conversion_wq) destroy_workqueue(EXT4_SB(sb)->rsv_conversion_wq); failed_mount_wq: ext4_xattr_destroy_cache(sbi->s_ea_inode_cache); sbi->s_ea_inode_cache = NULL; ext4_xattr_destroy_cache(sbi->s_ea_block_cache); sbi->s_ea_block_cache = NULL; if (sbi->s_journal) { flush_work(&sbi->s_sb_upd_work); jbd2_journal_destroy(sbi->s_journal); sbi->s_journal = NULL; } failed_mount3a: ext4_es_unregister_shrinker(sbi); failed_mount3: flush_work(&sbi->s_sb_upd_work); ext4_stop_mmpd(sbi); del_timer_sync(&sbi->s_err_report); ext4_group_desc_free(sbi); failed_mount: if (sbi->s_chksum_driver) crypto_free_shash(sbi->s_chksum_driver); #if IS_ENABLED(CONFIG_UNICODE) utf8_unload(sb->s_encoding); #endif #ifdef CONFIG_QUOTA for (unsigned int i = 0; i < EXT4_MAXQUOTAS; i++) kfree(get_qf_name(sb, sbi, i)); #endif fscrypt_free_dummy_policy(&sbi->s_dummy_enc_policy); brelse(sbi->s_sbh); if (sbi->s_journal_bdev_file) { invalidate_bdev(file_bdev(sbi->s_journal_bdev_file)); bdev_fput(sbi->s_journal_bdev_file); } out_fail: invalidate_bdev(sb->s_bdev); sb->s_fs_info = NULL; return err; }"
1256----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56540/bad/ivpu_jsm_msg.c----ivpu_jsm_pwr_d0i3_enter,"int ivpu_jsm_pwr_d0i3_enter(struct ivpu_device *vdev) { struct vpu_jsm_msg req = { .type = VPU_JSM_MSG_PWR_D0I3_ENTER }; struct vpu_jsm_msg resp; int ret; if (IVPU_WA(disable_d0i3_msg)) return 0; req.payload.pwr_d0i3_enter.send_response = 1; <S2SV_StartVul> ret = ivpu_ipc_send_receive_active(vdev, &req, VPU_JSM_MSG_PWR_D0I3_ENTER_DONE, <S2SV_EndVul> <S2SV_StartVul> &resp, VPU_IPC_CHAN_GEN_CMD, <S2SV_EndVul> <S2SV_StartVul> vdev->timeout.d0i3_entry_msg); <S2SV_EndVul> if (ret) return ret; return ivpu_hw_wait_for_idle(vdev); }","- ret = ivpu_ipc_send_receive_active(vdev, &req, VPU_JSM_MSG_PWR_D0I3_ENTER_DONE,
- &resp, VPU_IPC_CHAN_GEN_CMD,
- vdev->timeout.d0i3_entry_msg);
+ ret = ivpu_ipc_send_receive_internal(vdev, &req, VPU_JSM_MSG_PWR_D0I3_ENTER_DONE, &resp,
+ VPU_IPC_CHAN_GEN_CMD, vdev->timeout.d0i3_entry_msg);","int ivpu_jsm_pwr_d0i3_enter(struct ivpu_device *vdev) { struct vpu_jsm_msg req = { .type = VPU_JSM_MSG_PWR_D0I3_ENTER }; struct vpu_jsm_msg resp; int ret; if (IVPU_WA(disable_d0i3_msg)) return 0; req.payload.pwr_d0i3_enter.send_response = 1; ret = ivpu_ipc_send_receive_internal(vdev, &req, VPU_JSM_MSG_PWR_D0I3_ENTER_DONE, &resp, VPU_IPC_CHAN_GEN_CMD, vdev->timeout.d0i3_entry_msg); if (ret) return ret; return ivpu_hw_wait_for_idle(vdev); }"
1267----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56549/bad/ondemand.c----cachefiles_ondemand_fd_llseek,"static loff_t cachefiles_ondemand_fd_llseek(struct file *filp, loff_t pos, int whence) { struct cachefiles_object *object = filp->private_data; <S2SV_StartVul> struct file *file = object->file; <S2SV_EndVul> <S2SV_StartVul> if (!file) <S2SV_EndVul> return -ENOBUFS; <S2SV_StartVul> return vfs_llseek(file, pos, whence); <S2SV_EndVul> }","- struct file *file = object->file;
- if (!file)
- return vfs_llseek(file, pos, whence);
+ struct file *file;
+ loff_t ret;
+ spin_lock(&object->lock);
+ file = object->file;
+ if (!file) {
+ spin_unlock(&object->lock);
+ }
+ get_file(file);
+ spin_unlock(&object->lock);
+ ret = vfs_llseek(file, pos, whence);
+ fput(file);
+ return ret;
+ }","static loff_t cachefiles_ondemand_fd_llseek(struct file *filp, loff_t pos, int whence) { struct cachefiles_object *object = filp->private_data; struct file *file; loff_t ret; spin_lock(&object->lock); file = object->file; if (!file) { spin_unlock(&object->lock); return -ENOBUFS; } get_file(file); spin_unlock(&object->lock); ret = vfs_llseek(file, pos, whence); fput(file); return ret; }"
1398----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56723/bad/intel_soc_pmic_bxtwc.c----bxtwc_probe,"static int bxtwc_probe(struct platform_device *pdev) { struct device *dev = &pdev->dev; int ret; acpi_status status; unsigned long long hrv; struct intel_soc_pmic *pmic; status = acpi_evaluate_integer(ACPI_HANDLE(dev), ""_HRV"", NULL, &hrv); if (ACPI_FAILURE(status)) return dev_err_probe(dev, -ENODEV, ""Failed to get PMIC hardware revision\n""); if (hrv != BROXTON_PMIC_WC_HRV) return dev_err_probe(dev, -ENODEV, ""Invalid PMIC hardware revision: %llu\n"", hrv); pmic = devm_kzalloc(dev, sizeof(*pmic), GFP_KERNEL); if (!pmic) return -ENOMEM; ret = platform_get_irq(pdev, 0); if (ret < 0) return ret; pmic->irq = ret; platform_set_drvdata(pdev, pmic); pmic->dev = dev; pmic->scu = devm_intel_scu_ipc_dev_get(dev); if (!pmic->scu) return -EPROBE_DEFER; pmic->regmap = devm_regmap_init(dev, NULL, pmic, &bxtwc_regmap_config); if (IS_ERR(pmic->regmap)) return dev_err_probe(dev, PTR_ERR(pmic->regmap), ""Failed to initialise regmap\n""); ret = devm_regmap_add_irq_chip(dev, pmic->regmap, pmic->irq, IRQF_ONESHOT | IRQF_SHARED, 0, &bxtwc_regmap_irq_chip, &pmic->irq_chip_data); if (ret) return dev_err_probe(dev, ret, ""Failed to add IRQ chip\n""); ret = bxtwc_add_chained_devices(pmic, bxt_wc_tmu_dev, ARRAY_SIZE(bxt_wc_tmu_dev), pmic->irq_chip_data, BXTWC_TMU_LVL1_IRQ, IRQF_ONESHOT, &bxtwc_regmap_irq_chip_tmu, &pmic->irq_chip_data_tmu); if (ret) return ret; ret = bxtwc_add_chained_irq_chip(pmic, pmic->irq_chip_data, BXTWC_PWRBTN_LVL1_IRQ, IRQF_ONESHOT, &bxtwc_regmap_irq_chip_pwrbtn, &pmic->irq_chip_data_pwrbtn); if (ret) return dev_err_probe(dev, ret, ""Failed to add PWRBTN IRQ chip\n""); <S2SV_StartVul> ret = bxtwc_add_chained_irq_chip(pmic, pmic->irq_chip_data, <S2SV_EndVul> <S2SV_StartVul> BXTWC_BCU_LVL1_IRQ, <S2SV_EndVul> <S2SV_StartVul> IRQF_ONESHOT, <S2SV_EndVul> <S2SV_StartVul> &bxtwc_regmap_irq_chip_bcu, <S2SV_EndVul> <S2SV_StartVul> &pmic->irq_chip_data_bcu); <S2SV_EndVul> if (ret) <S2SV_StartVul> return dev_err_probe(dev, ret, ""Failed to add BUC IRQ chip\n""); <S2SV_EndVul> <S2SV_StartVul> ret = bxtwc_add_chained_irq_chip(pmic, pmic->irq_chip_data, <S2SV_EndVul> <S2SV_StartVul> BXTWC_ADC_LVL1_IRQ, <S2SV_EndVul> <S2SV_StartVul> IRQF_ONESHOT, <S2SV_EndVul> <S2SV_StartVul> &bxtwc_regmap_irq_chip_adc, <S2SV_EndVul> <S2SV_StartVul> &pmic->irq_chip_data_adc); <S2SV_EndVul> if (ret) <S2SV_StartVul> return dev_err_probe(dev, ret, ""Failed to add ADC IRQ chip\n""); <S2SV_EndVul> ret = bxtwc_add_chained_devices(pmic, bxt_wc_chgr_dev, ARRAY_SIZE(bxt_wc_chgr_dev), pmic->irq_chip_data, BXTWC_CHGR_LVL1_IRQ, IRQF_ONESHOT, &bxtwc_regmap_irq_chip_chgr, &pmic->irq_chip_data_chgr); if (ret) return ret; ret = bxtwc_add_chained_irq_chip(pmic, pmic->irq_chip_data, BXTWC_CRIT_LVL1_IRQ, IRQF_ONESHOT, &bxtwc_regmap_irq_chip_crit, &pmic->irq_chip_data_crit); if (ret) return dev_err_probe(dev, ret, ""Failed to add CRIT IRQ chip\n""); ret = devm_mfd_add_devices(dev, PLATFORM_DEVID_NONE, bxt_wc_dev, ARRAY_SIZE(bxt_wc_dev), NULL, 0, NULL); if (ret) return dev_err_probe(dev, ret, ""Failed to add devices\n""); regmap_update_bits(pmic->regmap, BXTWC_MIRQLVL1, BXTWC_MIRQLVL1_MCHGR, 0); return 0; }","- ret = bxtwc_add_chained_irq_chip(pmic, pmic->irq_chip_data,
- BXTWC_BCU_LVL1_IRQ,
- IRQF_ONESHOT,
- &bxtwc_regmap_irq_chip_bcu,
- &pmic->irq_chip_data_bcu);
- return dev_err_probe(dev, ret, ""Failed to add BUC IRQ chip\n"");
- ret = bxtwc_add_chained_irq_chip(pmic, pmic->irq_chip_data,
- BXTWC_ADC_LVL1_IRQ,
- IRQF_ONESHOT,
- &bxtwc_regmap_irq_chip_adc,
- &pmic->irq_chip_data_adc);
- return dev_err_probe(dev, ret, ""Failed to add ADC IRQ chip\n"");
+ ret = bxtwc_add_chained_devices(pmic, bxt_wc_bcu_dev, ARRAY_SIZE(bxt_wc_bcu_dev),
+ pmic->irq_chip_data,
+ BXTWC_BCU_LVL1_IRQ,
+ IRQF_ONESHOT,
+ &bxtwc_regmap_irq_chip_bcu,
+ &pmic->irq_chip_data_bcu);
+ return ret;
+ ret = bxtwc_add_chained_devices(pmic, bxt_wc_adc_dev, ARRAY_SIZE(bxt_wc_adc_dev),
+ pmic->irq_chip_data,
+ BXTWC_ADC_LVL1_IRQ,
+ IRQF_ONESHOT,
+ &bxtwc_regmap_irq_chip_adc,
+ &pmic->irq_chip_data_adc);
+ return ret;
+ pmic->irq_chip_data,","static int bxtwc_probe(struct platform_device *pdev) { struct device *dev = &pdev->dev; int ret; acpi_status status; unsigned long long hrv; struct intel_soc_pmic *pmic; status = acpi_evaluate_integer(ACPI_HANDLE(dev), ""_HRV"", NULL, &hrv); if (ACPI_FAILURE(status)) return dev_err_probe(dev, -ENODEV, ""Failed to get PMIC hardware revision\n""); if (hrv != BROXTON_PMIC_WC_HRV) return dev_err_probe(dev, -ENODEV, ""Invalid PMIC hardware revision: %llu\n"", hrv); pmic = devm_kzalloc(dev, sizeof(*pmic), GFP_KERNEL); if (!pmic) return -ENOMEM; ret = platform_get_irq(pdev, 0); if (ret < 0) return ret; pmic->irq = ret; platform_set_drvdata(pdev, pmic); pmic->dev = dev; pmic->scu = devm_intel_scu_ipc_dev_get(dev); if (!pmic->scu) return -EPROBE_DEFER; pmic->regmap = devm_regmap_init(dev, NULL, pmic, &bxtwc_regmap_config); if (IS_ERR(pmic->regmap)) return dev_err_probe(dev, PTR_ERR(pmic->regmap), ""Failed to initialise regmap\n""); ret = devm_regmap_add_irq_chip(dev, pmic->regmap, pmic->irq, IRQF_ONESHOT | IRQF_SHARED, 0, &bxtwc_regmap_irq_chip, &pmic->irq_chip_data); if (ret) return dev_err_probe(dev, ret, ""Failed to add IRQ chip\n""); ret = bxtwc_add_chained_devices(pmic, bxt_wc_tmu_dev, ARRAY_SIZE(bxt_wc_tmu_dev), pmic->irq_chip_data, BXTWC_TMU_LVL1_IRQ, IRQF_ONESHOT, &bxtwc_regmap_irq_chip_tmu, &pmic->irq_chip_data_tmu); if (ret) return ret; ret = bxtwc_add_chained_irq_chip(pmic, pmic->irq_chip_data, BXTWC_PWRBTN_LVL1_IRQ, IRQF_ONESHOT, &bxtwc_regmap_irq_chip_pwrbtn, &pmic->irq_chip_data_pwrbtn); if (ret) return dev_err_probe(dev, ret, ""Failed to add PWRBTN IRQ chip\n""); ret = bxtwc_add_chained_devices(pmic, bxt_wc_bcu_dev, ARRAY_SIZE(bxt_wc_bcu_dev), pmic->irq_chip_data, BXTWC_BCU_LVL1_IRQ, IRQF_ONESHOT, &bxtwc_regmap_irq_chip_bcu, &pmic->irq_chip_data_bcu); if (ret) return ret; ret = bxtwc_add_chained_devices(pmic, bxt_wc_adc_dev, ARRAY_SIZE(bxt_wc_adc_dev), pmic->irq_chip_data, BXTWC_ADC_LVL1_IRQ, IRQF_ONESHOT, &bxtwc_regmap_irq_chip_adc, &pmic->irq_chip_data_adc); if (ret) return ret; ret = bxtwc_add_chained_devices(pmic, bxt_wc_chgr_dev, ARRAY_SIZE(bxt_wc_chgr_dev), pmic->irq_chip_data, BXTWC_CHGR_LVL1_IRQ, IRQF_ONESHOT, &bxtwc_regmap_irq_chip_chgr, &pmic->irq_chip_data_chgr); if (ret) return ret; ret = bxtwc_add_chained_irq_chip(pmic, pmic->irq_chip_data, BXTWC_CRIT_LVL1_IRQ, IRQF_ONESHOT, &bxtwc_regmap_irq_chip_crit, &pmic->irq_chip_data_crit); if (ret) return dev_err_probe(dev, ret, ""Failed to add CRIT IRQ chip\n""); ret = devm_mfd_add_devices(dev, PLATFORM_DEVID_NONE, bxt_wc_dev, ARRAY_SIZE(bxt_wc_dev), NULL, 0, NULL); if (ret) return dev_err_probe(dev, ret, ""Failed to add devices\n""); regmap_update_bits(pmic->regmap, BXTWC_MIRQLVL1, BXTWC_MIRQLVL1_MCHGR, 0); return 0; }"
479----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47723/bad/jfs_imap.c----diAlloc,"int diAlloc(struct inode *pip, bool dir, struct inode *ip) { int rc, ino, iagno, addext, extno, bitno, sword; int nwords, rem, i, agno, dn_numag; u32 mask, inosmap, extsmap; struct inode *ipimap; struct metapage *mp; ino_t inum; struct iag *iagp; struct inomap *imap; ipimap = JFS_SBI(pip->i_sb)->ipimap; imap = JFS_IP(ipimap)->i_imap; JFS_IP(ip)->ipimap = ipimap; JFS_IP(ip)->fileset = FILESYSTEM_I; if (dir) { agno = dbNextAG(JFS_SBI(pip->i_sb)->ipbmap); AG_LOCK(imap, agno); goto tryag; } agno = BLKTOAG(JFS_IP(pip)->agstart, JFS_SBI(pip->i_sb)); dn_numag = JFS_SBI(pip->i_sb)->bmap->db_numag; <S2SV_StartVul> if (agno < 0 || agno > dn_numag) <S2SV_EndVul> return -EIO; if (atomic_read(&JFS_SBI(pip->i_sb)->bmap->db_active[agno])) { agno = dbNextAG(JFS_SBI(pip->i_sb)->ipbmap); AG_LOCK(imap, agno); goto tryag; } inum = pip->i_ino + 1; ino = inum & (INOSPERIAG - 1); if (ino == 0) inum = pip->i_ino; AG_LOCK(imap, agno); IREAD_LOCK(ipimap, RDWRLOCK_IMAP); iagno = INOTOIAG(inum); if ((rc = diIAGRead(imap, iagno, &mp))) { IREAD_UNLOCK(ipimap); AG_UNLOCK(imap, agno); return (rc); } iagp = (struct iag *) mp->data; addext = (imap->im_agctl[agno].numfree < 32 && iagp->nfreeexts); if (iagp->nfreeinos || addext) { extno = ino >> L2INOSPEREXT; if (addressPXD(&iagp->inoext[extno])) { bitno = ino & (INOSPEREXT - 1); if ((bitno = diFindFree(le32_to_cpu(iagp->wmap[extno]), bitno)) < INOSPEREXT) { ino = (extno << L2INOSPEREXT) + bitno; rc = diAllocBit(imap, iagp, ino); IREAD_UNLOCK(ipimap); if (rc) { assert(rc == -EIO); } else { diInitInode(ip, iagno, ino, extno, iagp); mark_metapage_dirty(mp); } release_metapage(mp); AG_UNLOCK(imap, agno); return (rc); } if (!addext) extno = (extno == EXTSPERIAG - 1) ? 0 : extno + 1; } bitno = extno & (EXTSPERSUM - 1); nwords = (bitno == 0) ? SMAPSZ : SMAPSZ + 1; sword = extno >> L2EXTSPERSUM; mask = (bitno == 0) ? 0 : (ONES << (EXTSPERSUM - bitno)); inosmap = le32_to_cpu(iagp->inosmap[sword]) | mask; extsmap = le32_to_cpu(iagp->extsmap[sword]) | mask; for (i = 0; i < nwords; i++) { if (~inosmap) { rem = diFindFree(inosmap, 0); extno = (sword << L2EXTSPERSUM) + rem; rem = diFindFree(le32_to_cpu(iagp->wmap[extno]), 0); if (rem >= INOSPEREXT) { IREAD_UNLOCK(ipimap); release_metapage(mp); AG_UNLOCK(imap, agno); jfs_error(ip->i_sb, ""can't find free bit in wmap\n""); return -EIO; } ino = (extno << L2INOSPEREXT) + rem; rc = diAllocBit(imap, iagp, ino); IREAD_UNLOCK(ipimap); if (rc) assert(rc == -EIO); else { diInitInode(ip, iagno, ino, extno, iagp); mark_metapage_dirty(mp); } release_metapage(mp); AG_UNLOCK(imap, agno); return (rc); } if (addext && ~extsmap) { rem = diFindFree(extsmap, 0); extno = (sword << L2EXTSPERSUM) + rem; if ((rc = diNewExt(imap, iagp, extno))) { if (rc == -ENOSPC) break; assert(rc == -EIO); } else { diInitInode(ip, iagno, extno << L2INOSPEREXT, extno, iagp); mark_metapage_dirty(mp); } release_metapage(mp); IREAD_UNLOCK(ipimap); AG_UNLOCK(imap, agno); return (rc); } sword = (sword == SMAPSZ - 1) ? 0 : sword + 1; inosmap = le32_to_cpu(iagp->inosmap[sword]); extsmap = le32_to_cpu(iagp->extsmap[sword]); } } IREAD_UNLOCK(ipimap); release_metapage(mp); tryag: rc = diAllocAG(imap, agno, dir, ip); AG_UNLOCK(imap, agno); if (rc != -ENOSPC) return (rc); return (diAllocAny(imap, agno, dir, ip)); }","- if (agno < 0 || agno > dn_numag)
+ if (agno < 0 || agno > dn_numag || agno >= MAXAG)","int diAlloc(struct inode *pip, bool dir, struct inode *ip) { int rc, ino, iagno, addext, extno, bitno, sword; int nwords, rem, i, agno, dn_numag; u32 mask, inosmap, extsmap; struct inode *ipimap; struct metapage *mp; ino_t inum; struct iag *iagp; struct inomap *imap; ipimap = JFS_SBI(pip->i_sb)->ipimap; imap = JFS_IP(ipimap)->i_imap; JFS_IP(ip)->ipimap = ipimap; JFS_IP(ip)->fileset = FILESYSTEM_I; if (dir) { agno = dbNextAG(JFS_SBI(pip->i_sb)->ipbmap); AG_LOCK(imap, agno); goto tryag; } agno = BLKTOAG(JFS_IP(pip)->agstart, JFS_SBI(pip->i_sb)); dn_numag = JFS_SBI(pip->i_sb)->bmap->db_numag; if (agno < 0 || agno > dn_numag || agno >= MAXAG) return -EIO; if (atomic_read(&JFS_SBI(pip->i_sb)->bmap->db_active[agno])) { agno = dbNextAG(JFS_SBI(pip->i_sb)->ipbmap); AG_LOCK(imap, agno); goto tryag; } inum = pip->i_ino + 1; ino = inum & (INOSPERIAG - 1); if (ino == 0) inum = pip->i_ino; AG_LOCK(imap, agno); IREAD_LOCK(ipimap, RDWRLOCK_IMAP); iagno = INOTOIAG(inum); if ((rc = diIAGRead(imap, iagno, &mp))) { IREAD_UNLOCK(ipimap); AG_UNLOCK(imap, agno); return (rc); } iagp = (struct iag *) mp->data; addext = (imap->im_agctl[agno].numfree < 32 && iagp->nfreeexts); if (iagp->nfreeinos || addext) { extno = ino >> L2INOSPEREXT; if (addressPXD(&iagp->inoext[extno])) { bitno = ino & (INOSPEREXT - 1); if ((bitno = diFindFree(le32_to_cpu(iagp->wmap[extno]), bitno)) < INOSPEREXT) { ino = (extno << L2INOSPEREXT) + bitno; rc = diAllocBit(imap, iagp, ino); IREAD_UNLOCK(ipimap); if (rc) { assert(rc == -EIO); } else { diInitInode(ip, iagno, ino, extno, iagp); mark_metapage_dirty(mp); } release_metapage(mp); AG_UNLOCK(imap, agno); return (rc); } if (!addext) extno = (extno == EXTSPERIAG - 1) ? 0 : extno + 1; } bitno = extno & (EXTSPERSUM - 1); nwords = (bitno == 0) ? SMAPSZ : SMAPSZ + 1; sword = extno >> L2EXTSPERSUM; mask = (bitno == 0) ? 0 : (ONES << (EXTSPERSUM - bitno)); inosmap = le32_to_cpu(iagp->inosmap[sword]) | mask; extsmap = le32_to_cpu(iagp->extsmap[sword]) | mask; for (i = 0; i < nwords; i++) { if (~inosmap) { rem = diFindFree(inosmap, 0); extno = (sword << L2EXTSPERSUM) + rem; rem = diFindFree(le32_to_cpu(iagp->wmap[extno]), 0); if (rem >= INOSPEREXT) { IREAD_UNLOCK(ipimap); release_metapage(mp); AG_UNLOCK(imap, agno); jfs_error(ip->i_sb, ""can't find free bit in wmap\n""); return -EIO; } ino = (extno << L2INOSPEREXT) + rem; rc = diAllocBit(imap, iagp, ino); IREAD_UNLOCK(ipimap); if (rc) assert(rc == -EIO); else { diInitInode(ip, iagno, ino, extno, iagp); mark_metapage_dirty(mp); } release_metapage(mp); AG_UNLOCK(imap, agno); return (rc); } if (addext && ~extsmap) { rem = diFindFree(extsmap, 0); extno = (sword << L2EXTSPERSUM) + rem; if ((rc = diNewExt(imap, iagp, extno))) { if (rc == -ENOSPC) break; assert(rc == -EIO); } else { diInitInode(ip, iagno, extno << L2INOSPEREXT, extno, iagp); mark_metapage_dirty(mp); } release_metapage(mp); IREAD_UNLOCK(ipimap); AG_UNLOCK(imap, agno); return (rc); } sword = (sword == SMAPSZ - 1) ? 0 : sword + 1; inosmap = le32_to_cpu(iagp->inosmap[sword]); extsmap = le32_to_cpu(iagp->extsmap[sword]); } } IREAD_UNLOCK(ipimap); release_metapage(mp); tryag: rc = diAllocAG(imap, agno, dir, ip); AG_UNLOCK(imap, agno); if (rc != -ENOSPC) return (rc); return (diAllocAny(imap, agno, dir, ip)); }"
413----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47141/bad/pinmux.c----pinmux_disable_setting,"void pinmux_disable_setting(const struct pinctrl_setting *setting) { struct pinctrl_dev *pctldev = setting->pctldev; const struct pinctrl_ops *pctlops = pctldev->desc->pctlops; int ret = 0; const unsigned int *pins = NULL; unsigned int num_pins = 0; int i; struct pin_desc *desc; if (pctlops->get_group_pins) ret = pctlops->get_group_pins(pctldev, setting->data.mux.group, &pins, &num_pins); if (ret) { const char *gname; gname = pctlops->get_group_name(pctldev, setting->data.mux.group); dev_warn(pctldev->dev, ""could not get pins for group %s\n"", gname); num_pins = 0; } for (i = 0; i < num_pins; i++) { desc = pin_desc_get(pctldev, pins[i]); if (desc == NULL) { dev_warn(pctldev->dev, ""could not get pin desc for pin %d\n"", pins[i]); continue; <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> if (desc->mux_setting == &(setting->data.mux)) { <S2SV_EndVul> pin_free(pctldev, pins[i], NULL); <S2SV_StartVul> } else { <S2SV_EndVul> const char *gname; gname = pctlops->get_group_name(pctldev, setting->data.mux.group); dev_warn(pctldev->dev, ""not freeing pin %d (%s) as part of deactivating group %s - it is already used for some other setting"", pins[i], desc->name, gname); } } }","- }
- if (desc->mux_setting == &(setting->data.mux)) {
- } else {
+ bool is_equal;
+ }
+ scoped_guard(mutex, &desc->mux_lock)
+ is_equal = (desc->mux_setting == &(setting->data.mux));
+ if (is_equal) {
+ } else {","void pinmux_disable_setting(const struct pinctrl_setting *setting) { struct pinctrl_dev *pctldev = setting->pctldev; const struct pinctrl_ops *pctlops = pctldev->desc->pctlops; int ret = 0; const unsigned int *pins = NULL; unsigned int num_pins = 0; int i; struct pin_desc *desc; bool is_equal; if (pctlops->get_group_pins) ret = pctlops->get_group_pins(pctldev, setting->data.mux.group, &pins, &num_pins); if (ret) { const char *gname; gname = pctlops->get_group_name(pctldev, setting->data.mux.group); dev_warn(pctldev->dev, ""could not get pins for group %s\n"", gname); num_pins = 0; } for (i = 0; i < num_pins; i++) { desc = pin_desc_get(pctldev, pins[i]); if (desc == NULL) { dev_warn(pctldev->dev, ""could not get pin desc for pin %d\n"", pins[i]); continue; } scoped_guard(mutex, &desc->mux_lock) is_equal = (desc->mux_setting == &(setting->data.mux)); if (is_equal) { pin_free(pctldev, pins[i], NULL); } else { const char *gname; gname = pctlops->get_group_name(pctldev, setting->data.mux.group); dev_warn(pctldev->dev, ""not freeing pin %d (%s) as part of deactivating group %s - it is already used for some other setting"", pins[i], desc->name, gname); } } }"
213----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46694/bad/amdgpu_dm_plane.c----dm_plane_helper_prepare_fb,"static int dm_plane_helper_prepare_fb(struct drm_plane *plane, struct drm_plane_state *new_state) { struct amdgpu_framebuffer *afb; struct drm_gem_object *obj; struct amdgpu_device *adev; struct amdgpu_bo *rbo; struct dm_plane_state *dm_plane_state_new, *dm_plane_state_old; uint32_t domain; int r; if (!new_state->fb) { DRM_DEBUG_KMS(""No FB bound\n""); return 0; } afb = to_amdgpu_framebuffer(new_state->fb); <S2SV_StartVul> obj = new_state->fb->obj[0]; <S2SV_EndVul> rbo = gem_to_amdgpu_bo(obj); adev = amdgpu_ttm_adev(rbo->tbo.bdev); r = amdgpu_bo_reserve(rbo, true); if (r) { dev_err(adev->dev, ""fail to reserve bo (%d)\n"", r); return r; } r = dma_resv_reserve_fences(rbo->tbo.base.resv, 1); if (r) { dev_err(adev->dev, ""reserving fence slot failed (%d)\n"", r); goto error_unlock; } if (plane->type != DRM_PLANE_TYPE_CURSOR) domain = amdgpu_display_supported_domains(adev, rbo->flags); else domain = AMDGPU_GEM_DOMAIN_VRAM; r = amdgpu_bo_pin(rbo, domain); if (unlikely(r != 0)) { if (r != -ERESTARTSYS) DRM_ERROR(""Failed to pin framebuffer with error %d\n"", r); goto error_unlock; } r = amdgpu_ttm_alloc_gart(&rbo->tbo); if (unlikely(r != 0)) { DRM_ERROR(""%p bind failed\n"", rbo); goto error_unpin; } r = drm_gem_plane_helper_prepare_fb(plane, new_state); if (unlikely(r != 0)) goto error_unpin; amdgpu_bo_unreserve(rbo); afb->address = amdgpu_bo_gpu_offset(rbo); amdgpu_bo_ref(rbo); dm_plane_state_old = to_dm_plane_state(plane->state); dm_plane_state_new = to_dm_plane_state(new_state); if (dm_plane_state_new->dc_state && dm_plane_state_old->dc_state != dm_plane_state_new->dc_state) { struct dc_plane_state *plane_state = dm_plane_state_new->dc_state; bool force_disable_dcc = !plane_state->dcc.enable; fill_plane_buffer_attributes( adev, afb, plane_state->format, plane_state->rotation, afb->tiling_flags, &plane_state->tiling_info, &plane_state->plane_size, &plane_state->dcc, &plane_state->address, afb->tmz_surface, force_disable_dcc); } return 0; error_unpin: amdgpu_bo_unpin(rbo); error_unlock: amdgpu_bo_unreserve(rbo); return r; }","- obj = new_state->fb->obj[0];
+ obj = drm_gem_fb_get_obj(new_state->fb, 0);
+ if (!obj) {
+ DRM_ERROR(""Failed to get obj from framebuffer\n"");
+ return -EINVAL;
+ }","static int dm_plane_helper_prepare_fb(struct drm_plane *plane, struct drm_plane_state *new_state) { struct amdgpu_framebuffer *afb; struct drm_gem_object *obj; struct amdgpu_device *adev; struct amdgpu_bo *rbo; struct dm_plane_state *dm_plane_state_new, *dm_plane_state_old; uint32_t domain; int r; if (!new_state->fb) { DRM_DEBUG_KMS(""No FB bound\n""); return 0; } afb = to_amdgpu_framebuffer(new_state->fb); obj = drm_gem_fb_get_obj(new_state->fb, 0); if (!obj) { DRM_ERROR(""Failed to get obj from framebuffer\n""); return -EINVAL; } rbo = gem_to_amdgpu_bo(obj); adev = amdgpu_ttm_adev(rbo->tbo.bdev); r = amdgpu_bo_reserve(rbo, true); if (r) { dev_err(adev->dev, ""fail to reserve bo (%d)\n"", r); return r; } r = dma_resv_reserve_fences(rbo->tbo.base.resv, 1); if (r) { dev_err(adev->dev, ""reserving fence slot failed (%d)\n"", r); goto error_unlock; } if (plane->type != DRM_PLANE_TYPE_CURSOR) domain = amdgpu_display_supported_domains(adev, rbo->flags); else domain = AMDGPU_GEM_DOMAIN_VRAM; r = amdgpu_bo_pin(rbo, domain); if (unlikely(r != 0)) { if (r != -ERESTARTSYS) DRM_ERROR(""Failed to pin framebuffer with error %d\n"", r); goto error_unlock; } r = amdgpu_ttm_alloc_gart(&rbo->tbo); if (unlikely(r != 0)) { DRM_ERROR(""%p bind failed\n"", rbo); goto error_unpin; } r = drm_gem_plane_helper_prepare_fb(plane, new_state); if (unlikely(r != 0)) goto error_unpin; amdgpu_bo_unreserve(rbo); afb->address = amdgpu_bo_gpu_offset(rbo); amdgpu_bo_ref(rbo); dm_plane_state_old = to_dm_plane_state(plane->state); dm_plane_state_new = to_dm_plane_state(new_state); if (dm_plane_state_new->dc_state && dm_plane_state_old->dc_state != dm_plane_state_new->dc_state) { struct dc_plane_state *plane_state = dm_plane_state_new->dc_state; bool force_disable_dcc = !plane_state->dcc.enable; fill_plane_buffer_attributes( adev, afb, plane_state->format, plane_state->rotation, afb->tiling_flags, &plane_state->tiling_info, &plane_state->plane_size, &plane_state->dcc, &plane_state->address, afb->tmz_surface, force_disable_dcc); } return 0; error_unpin: amdgpu_bo_unpin(rbo); error_unlock: amdgpu_bo_unreserve(rbo); return r; }"
394----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46855/bad/nft_socket.c----nft_socket_eval,"static void nft_socket_eval(const struct nft_expr *expr, struct nft_regs *regs, const struct nft_pktinfo *pkt) { const struct nft_socket *priv = nft_expr_priv(expr); struct sk_buff *skb = pkt->skb; struct sock *sk = skb->sk; u32 *dest = &regs->data[priv->dreg]; if (sk && !net_eq(nft_net(pkt), sock_net(sk))) sk = NULL; if (!sk) sk = nft_socket_do_lookup(pkt); if (!sk) { regs->verdict.code = NFT_BREAK; return; } switch(priv->key) { case NFT_SOCKET_TRANSPARENT: nft_reg_store8(dest, inet_sk_transparent(sk)); break; case NFT_SOCKET_MARK: if (sk_fullsock(sk)) { *dest = sk->sk_mark; } else { regs->verdict.code = NFT_BREAK; <S2SV_StartVul> return; <S2SV_EndVul> } break; case NFT_SOCKET_WILDCARD: if (!sk_fullsock(sk)) { regs->verdict.code = NFT_BREAK; <S2SV_StartVul> return; <S2SV_EndVul> } nft_socket_wildcard(pkt, regs, sk, dest); break; #ifdef CONFIG_SOCK_CGROUP_DATA case NFT_SOCKET_CGROUPV2: if (!nft_sock_get_eval_cgroupv2(dest, sk, pkt, priv->level)) { regs->verdict.code = NFT_BREAK; <S2SV_StartVul> return; <S2SV_EndVul> } break; #endif default: WARN_ON(1); regs->verdict.code = NFT_BREAK; } if (sk != skb->sk) sock_gen_put(sk); }","- return;
- return;
- return;
+ goto out_put_sk;
+ goto out_put_sk;
+ goto out_put_sk;
+ out_put_sk:","static void nft_socket_eval(const struct nft_expr *expr, struct nft_regs *regs, const struct nft_pktinfo *pkt) { const struct nft_socket *priv = nft_expr_priv(expr); struct sk_buff *skb = pkt->skb; struct sock *sk = skb->sk; u32 *dest = &regs->data[priv->dreg]; if (sk && !net_eq(nft_net(pkt), sock_net(sk))) sk = NULL; if (!sk) sk = nft_socket_do_lookup(pkt); if (!sk) { regs->verdict.code = NFT_BREAK; return; } switch(priv->key) { case NFT_SOCKET_TRANSPARENT: nft_reg_store8(dest, inet_sk_transparent(sk)); break; case NFT_SOCKET_MARK: if (sk_fullsock(sk)) { *dest = sk->sk_mark; } else { regs->verdict.code = NFT_BREAK; goto out_put_sk; } break; case NFT_SOCKET_WILDCARD: if (!sk_fullsock(sk)) { regs->verdict.code = NFT_BREAK; goto out_put_sk; } nft_socket_wildcard(pkt, regs, sk, dest); break; #ifdef CONFIG_SOCK_CGROUP_DATA case NFT_SOCKET_CGROUPV2: if (!nft_sock_get_eval_cgroupv2(dest, sk, pkt, priv->level)) { regs->verdict.code = NFT_BREAK; goto out_put_sk; } break; #endif default: WARN_ON(1); regs->verdict.code = NFT_BREAK; } out_put_sk: if (sk != skb->sk) sock_gen_put(sk); }"
354----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46818/bad/gpio_service.c----dal_gpio_service_unlock,"enum gpio_result dal_gpio_service_unlock( struct gpio_service *service, enum gpio_id id, uint32_t en) { <S2SV_StartVul> if (!service->busyness[id]) { <S2SV_EndVul> ASSERT_CRITICAL(false); return GPIO_RESULT_OPEN_FAILED; } set_pin_free(service, id, en); return GPIO_RESULT_OK; }","- if (!service->busyness[id]) {
+ if (id != GPIO_ID_UNKNOWN && !service->busyness[id]) {","enum gpio_result dal_gpio_service_unlock( struct gpio_service *service, enum gpio_id id, uint32_t en) { if (id != GPIO_ID_UNKNOWN && !service->busyness[id]) { ASSERT_CRITICAL(false); return GPIO_RESULT_OPEN_FAILED; } set_pin_free(service, id, en); return GPIO_RESULT_OK; }"
86----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44956/bad/xe_preempt_fence.c----preempt_fence_work_func,"static void preempt_fence_work_func(struct work_struct *w) { bool cookie = dma_fence_begin_signalling(); struct xe_preempt_fence *pfence = container_of(w, typeof(*pfence), preempt_work); struct xe_exec_queue *q = pfence->q; if (pfence->error) dma_fence_set_error(&pfence->base, pfence->error); else q->ops->suspend_wait(q); dma_fence_signal(&pfence->base); <S2SV_StartVul> dma_fence_end_signalling(cookie); <S2SV_EndVul> xe_vm_queue_rebind_worker(q->vm); xe_exec_queue_put(q); }","- dma_fence_end_signalling(cookie);
+ dma_fence_end_signalling(cookie);","static void preempt_fence_work_func(struct work_struct *w) { bool cookie = dma_fence_begin_signalling(); struct xe_preempt_fence *pfence = container_of(w, typeof(*pfence), preempt_work); struct xe_exec_queue *q = pfence->q; if (pfence->error) dma_fence_set_error(&pfence->base, pfence->error); else q->ops->suspend_wait(q); dma_fence_signal(&pfence->base); xe_vm_queue_rebind_worker(q->vm); xe_exec_queue_put(q); dma_fence_end_signalling(cookie); }"
721----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49994/bad/ioctl.c----blk_ioctl_secure_erase,"static int blk_ioctl_secure_erase(struct block_device *bdev, blk_mode_t mode, void __user *argp) { <S2SV_StartVul> uint64_t start, len; <S2SV_EndVul> uint64_t range[2]; int err; if (!(mode & BLK_OPEN_WRITE)) return -EBADF; if (!bdev_max_secure_erase_sectors(bdev)) return -EOPNOTSUPP; if (copy_from_user(range, argp, sizeof(range))) return -EFAULT; start = range[0]; len = range[1]; if ((start & 511) || (len & 511)) return -EINVAL; <S2SV_StartVul> if (start + len > bdev_nr_bytes(bdev)) <S2SV_EndVul> return -EINVAL; filemap_invalidate_lock(bdev->bd_inode->i_mapping); <S2SV_StartVul> err = truncate_bdev_range(bdev, mode, start, start + len - 1); <S2SV_EndVul> if (!err) err = blkdev_issue_secure_erase(bdev, start >> 9, len >> 9, GFP_KERNEL); filemap_invalidate_unlock(bdev->bd_inode->i_mapping); return err; }","- uint64_t start, len;
- if (start + len > bdev_nr_bytes(bdev))
- err = truncate_bdev_range(bdev, mode, start, start + len - 1);
+ uint64_t start, len, end;
+ if (check_add_overflow(start, len, &end) ||
+ end > bdev_nr_bytes(bdev))
+ err = truncate_bdev_range(bdev, mode, start, end - 1);","static int blk_ioctl_secure_erase(struct block_device *bdev, blk_mode_t mode, void __user *argp) { uint64_t start, len, end; uint64_t range[2]; int err; if (!(mode & BLK_OPEN_WRITE)) return -EBADF; if (!bdev_max_secure_erase_sectors(bdev)) return -EOPNOTSUPP; if (copy_from_user(range, argp, sizeof(range))) return -EFAULT; start = range[0]; len = range[1]; if ((start & 511) || (len & 511)) return -EINVAL; if (check_add_overflow(start, len, &end) || end > bdev_nr_bytes(bdev)) return -EINVAL; filemap_invalidate_lock(bdev->bd_inode->i_mapping); err = truncate_bdev_range(bdev, mode, start, end - 1); if (!err) err = blkdev_issue_secure_erase(bdev, start >> 9, len >> 9, GFP_KERNEL); filemap_invalidate_unlock(bdev->bd_inode->i_mapping); return err; }"
255----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46730/bad/dc_resource.c----acquire_otg_master_pipe_for_stream,"static bool acquire_otg_master_pipe_for_stream( const struct dc_state *cur_ctx, struct dc_state *new_ctx, const struct resource_pool *pool, struct dc_stream_state *stream) { int pipe_idx; struct pipe_ctx *pipe_ctx = NULL; pipe_idx = recource_find_free_pipe_used_as_otg_master_in_cur_res_ctx( &cur_ctx->res_ctx, &new_ctx->res_ctx, pool); if (pipe_idx == FREE_PIPE_INDEX_NOT_FOUND) pipe_idx = recource_find_free_pipe_not_used_in_cur_res_ctx( &cur_ctx->res_ctx, &new_ctx->res_ctx, pool); if (pipe_idx == FREE_PIPE_INDEX_NOT_FOUND) pipe_idx = resource_find_free_pipe_used_as_cur_sec_dpp( &cur_ctx->res_ctx, &new_ctx->res_ctx, pool); if (pipe_idx == FREE_PIPE_INDEX_NOT_FOUND) pipe_idx = resource_find_any_free_pipe(&new_ctx->res_ctx, pool); if (pipe_idx != FREE_PIPE_INDEX_NOT_FOUND) { pipe_ctx = &new_ctx->res_ctx.pipe_ctx[pipe_idx]; memset(pipe_ctx, 0, sizeof(*pipe_ctx)); pipe_ctx->pipe_idx = pipe_idx; pipe_ctx->stream_res.tg = pool->timing_generators[pipe_idx]; pipe_ctx->plane_res.mi = pool->mis[pipe_idx]; pipe_ctx->plane_res.hubp = pool->hubps[pipe_idx]; pipe_ctx->plane_res.ipp = pool->ipps[pipe_idx]; pipe_ctx->plane_res.xfm = pool->transforms[pipe_idx]; pipe_ctx->plane_res.dpp = pool->dpps[pipe_idx]; pipe_ctx->stream_res.opp = pool->opps[pipe_idx]; if (pool->dpps[pipe_idx]) pipe_ctx->plane_res.mpcc_inst = pool->dpps[pipe_idx]->inst; <S2SV_StartVul> if (pipe_idx >= pool->timing_generator_count) { <S2SV_EndVul> int tg_inst = pool->timing_generator_count - 1; pipe_ctx->stream_res.tg = pool->timing_generators[tg_inst]; pipe_ctx->stream_res.opp = pool->opps[tg_inst]; } pipe_ctx->stream = stream; } else { pipe_idx = acquire_first_split_pipe(&new_ctx->res_ctx, pool, stream); } return pipe_idx != FREE_PIPE_INDEX_NOT_FOUND; }","- if (pipe_idx >= pool->timing_generator_count) {
+ if (pipe_idx >= pool->timing_generator_count && pool->timing_generator_count != 0) {","static bool acquire_otg_master_pipe_for_stream( const struct dc_state *cur_ctx, struct dc_state *new_ctx, const struct resource_pool *pool, struct dc_stream_state *stream) { int pipe_idx; struct pipe_ctx *pipe_ctx = NULL; pipe_idx = recource_find_free_pipe_used_as_otg_master_in_cur_res_ctx( &cur_ctx->res_ctx, &new_ctx->res_ctx, pool); if (pipe_idx == FREE_PIPE_INDEX_NOT_FOUND) pipe_idx = recource_find_free_pipe_not_used_in_cur_res_ctx( &cur_ctx->res_ctx, &new_ctx->res_ctx, pool); if (pipe_idx == FREE_PIPE_INDEX_NOT_FOUND) pipe_idx = resource_find_free_pipe_used_as_cur_sec_dpp( &cur_ctx->res_ctx, &new_ctx->res_ctx, pool); if (pipe_idx == FREE_PIPE_INDEX_NOT_FOUND) pipe_idx = resource_find_any_free_pipe(&new_ctx->res_ctx, pool); if (pipe_idx != FREE_PIPE_INDEX_NOT_FOUND) { pipe_ctx = &new_ctx->res_ctx.pipe_ctx[pipe_idx]; memset(pipe_ctx, 0, sizeof(*pipe_ctx)); pipe_ctx->pipe_idx = pipe_idx; pipe_ctx->stream_res.tg = pool->timing_generators[pipe_idx]; pipe_ctx->plane_res.mi = pool->mis[pipe_idx]; pipe_ctx->plane_res.hubp = pool->hubps[pipe_idx]; pipe_ctx->plane_res.ipp = pool->ipps[pipe_idx]; pipe_ctx->plane_res.xfm = pool->transforms[pipe_idx]; pipe_ctx->plane_res.dpp = pool->dpps[pipe_idx]; pipe_ctx->stream_res.opp = pool->opps[pipe_idx]; if (pool->dpps[pipe_idx]) pipe_ctx->plane_res.mpcc_inst = pool->dpps[pipe_idx]->inst; if (pipe_idx >= pool->timing_generator_count && pool->timing_generator_count != 0) { int tg_inst = pool->timing_generator_count - 1; pipe_ctx->stream_res.tg = pool->timing_generators[tg_inst]; pipe_ctx->stream_res.opp = pool->opps[tg_inst]; } pipe_ctx->stream = stream; } else { pipe_idx = acquire_first_split_pipe(&new_ctx->res_ctx, pool, stream); } return pipe_idx != FREE_PIPE_INDEX_NOT_FOUND; }"
1071----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53055/bad/scan.c----iwl_mvm_umac_scan_cfg_channels_v7_6g,"iwl_mvm_umac_scan_cfg_channels_v7_6g(struct iwl_mvm *mvm, struct iwl_mvm_scan_params *params, u32 n_channels, struct iwl_scan_probe_params_v4 *pp, struct iwl_scan_channel_params_v7 *cp, enum nl80211_iftype vif_type, u32 version) { int i; struct cfg80211_scan_6ghz_params *scan_6ghz_params = params->scan_6ghz_params; u32 ch_cnt; for (i = 0, ch_cnt = 0; i < params->n_channels; i++) { struct iwl_scan_channel_cfg_umac *cfg = &cp->channel_config[ch_cnt]; u32 s_ssid_bitmap = 0, bssid_bitmap = 0, flags = 0; <S2SV_StartVul> u8 j, k, n_s_ssids = 0, n_bssids = 0; <S2SV_EndVul> u8 max_s_ssids, max_bssids; bool force_passive = false, found = false, allow_passive = true, unsolicited_probe_on_chan = false, psc_no_listen = false; s8 psd_20 = IEEE80211_RNR_TBTT_PARAMS_PSD_RESERVED; if (!cfg80211_channel_is_psc(params->channels[i]) && !params->n_6ghz_params && params->n_ssids) continue; cfg->channel_num = params->channels[i]->hw_value; if (version < 17) cfg->v2.band = PHY_BAND_6; else cfg->flags |= cpu_to_le32(PHY_BAND_6 << IWL_CHAN_CFG_FLAGS_BAND_POS); cfg->v5.iter_count = 1; cfg->v5.iter_interval = 0; <S2SV_StartVul> for (j = 0; j < params->n_6ghz_params; j++) { <S2SV_EndVul> s8 tmp_psd_20; if (!(scan_6ghz_params[j].channel_idx == i)) continue; unsolicited_probe_on_chan |= scan_6ghz_params[j].unsolicited_probe; tmp_psd_20 = scan_6ghz_params[j].psd_20; if (tmp_psd_20 != IEEE80211_RNR_TBTT_PARAMS_PSD_RESERVED && (psd_20 == IEEE80211_RNR_TBTT_PARAMS_PSD_RESERVED || psd_20 < tmp_psd_20)) psd_20 = tmp_psd_20; psc_no_listen |= scan_6ghz_params[j].psc_no_listen; } if (!iwl_mvm_is_scan_fragmented(params->type)) { if (!cfg80211_channel_is_psc(params->channels[i]) || psc_no_listen) { if (unsolicited_probe_on_chan) { max_s_ssids = 2; max_bssids = 6; } else { max_s_ssids = 3; max_bssids = 9; } } else { max_s_ssids = 2; max_bssids = 6; } } else if (cfg80211_channel_is_psc(params->channels[i])) { max_s_ssids = 1; max_bssids = 3; } else { if (unsolicited_probe_on_chan) { max_s_ssids = 1; max_bssids = 3; } else { max_s_ssids = 2; max_bssids = 6; } } <S2SV_StartVul> for (j = 0; j < params->n_6ghz_params; j++) { <S2SV_EndVul> if (!(scan_6ghz_params[j].channel_idx == i)) continue; found = false; for (k = 0; k < pp->short_ssid_num && n_s_ssids < max_s_ssids; k++) { if (!scan_6ghz_params[j].unsolicited_probe && le32_to_cpu(pp->short_ssid[k]) == scan_6ghz_params[j].short_ssid) { if (s_ssid_bitmap & BIT(k)) { found = true; break; } if (3 * n_s_ssids > n_bssids && !pp->direct_scan[k].len) break; if (pp->direct_scan[k].len) allow_passive = false; s_ssid_bitmap |= BIT(k); n_s_ssids++; found = true; break; } } if (found) continue; for (k = 0; k < pp->bssid_num; k++) { if (!memcmp(&pp->bssid_array[k], scan_6ghz_params[j].bssid, ETH_ALEN)) { if (!(bssid_bitmap & BIT(k))) { if (n_bssids < max_bssids) { bssid_bitmap |= BIT(k); n_bssids++; } else { force_passive = TRUE; } } break; } } } if (cfg80211_channel_is_psc(params->channels[i]) && psc_no_listen) flags |= IWL_UHB_CHAN_CFG_FLAG_PSC_CHAN_NO_LISTEN; if (unsolicited_probe_on_chan) flags |= IWL_UHB_CHAN_CFG_FLAG_UNSOLICITED_PROBE_RES; if ((allow_passive && force_passive) || (!(bssid_bitmap | s_ssid_bitmap) && !cfg80211_channel_is_psc(params->channels[i]))) flags |= IWL_UHB_CHAN_CFG_FLAG_FORCE_PASSIVE; else flags |= bssid_bitmap | (s_ssid_bitmap << 16); cfg->flags |= cpu_to_le32(flags); if (version >= 17) cfg->v5.psd_20 = psd_20; ch_cnt++; } if (params->n_channels > ch_cnt) IWL_DEBUG_SCAN(mvm, ""6GHz: reducing number channels: (%u->%u)\n"", params->n_channels, ch_cnt); return ch_cnt; }","- u8 j, k, n_s_ssids = 0, n_bssids = 0;
- for (j = 0; j < params->n_6ghz_params; j++) {
- for (j = 0; j < params->n_6ghz_params; j++) {
+ u8 k, n_s_ssids = 0, n_bssids = 0;
+ for (u32 j = 0; j < params->n_6ghz_params; j++) {
+ for (u32 j = 0; j < params->n_6ghz_params; j++) {","iwl_mvm_umac_scan_cfg_channels_v7_6g(struct iwl_mvm *mvm, struct iwl_mvm_scan_params *params, u32 n_channels, struct iwl_scan_probe_params_v4 *pp, struct iwl_scan_channel_params_v7 *cp, enum nl80211_iftype vif_type, u32 version) { int i; struct cfg80211_scan_6ghz_params *scan_6ghz_params = params->scan_6ghz_params; u32 ch_cnt; for (i = 0, ch_cnt = 0; i < params->n_channels; i++) { struct iwl_scan_channel_cfg_umac *cfg = &cp->channel_config[ch_cnt]; u32 s_ssid_bitmap = 0, bssid_bitmap = 0, flags = 0; u8 k, n_s_ssids = 0, n_bssids = 0; u8 max_s_ssids, max_bssids; bool force_passive = false, found = false, allow_passive = true, unsolicited_probe_on_chan = false, psc_no_listen = false; s8 psd_20 = IEEE80211_RNR_TBTT_PARAMS_PSD_RESERVED; if (!cfg80211_channel_is_psc(params->channels[i]) && !params->n_6ghz_params && params->n_ssids) continue; cfg->channel_num = params->channels[i]->hw_value; if (version < 17) cfg->v2.band = PHY_BAND_6; else cfg->flags |= cpu_to_le32(PHY_BAND_6 << IWL_CHAN_CFG_FLAGS_BAND_POS); cfg->v5.iter_count = 1; cfg->v5.iter_interval = 0; for (u32 j = 0; j < params->n_6ghz_params; j++) { s8 tmp_psd_20; if (!(scan_6ghz_params[j].channel_idx == i)) continue; unsolicited_probe_on_chan |= scan_6ghz_params[j].unsolicited_probe; tmp_psd_20 = scan_6ghz_params[j].psd_20; if (tmp_psd_20 != IEEE80211_RNR_TBTT_PARAMS_PSD_RESERVED && (psd_20 == IEEE80211_RNR_TBTT_PARAMS_PSD_RESERVED || psd_20 < tmp_psd_20)) psd_20 = tmp_psd_20; psc_no_listen |= scan_6ghz_params[j].psc_no_listen; } if (!iwl_mvm_is_scan_fragmented(params->type)) { if (!cfg80211_channel_is_psc(params->channels[i]) || psc_no_listen) { if (unsolicited_probe_on_chan) { max_s_ssids = 2; max_bssids = 6; } else { max_s_ssids = 3; max_bssids = 9; } } else { max_s_ssids = 2; max_bssids = 6; } } else if (cfg80211_channel_is_psc(params->channels[i])) { max_s_ssids = 1; max_bssids = 3; } else { if (unsolicited_probe_on_chan) { max_s_ssids = 1; max_bssids = 3; } else { max_s_ssids = 2; max_bssids = 6; } } for (u32 j = 0; j < params->n_6ghz_params; j++) { if (!(scan_6ghz_params[j].channel_idx == i)) continue; found = false; for (k = 0; k < pp->short_ssid_num && n_s_ssids < max_s_ssids; k++) { if (!scan_6ghz_params[j].unsolicited_probe && le32_to_cpu(pp->short_ssid[k]) == scan_6ghz_params[j].short_ssid) { if (s_ssid_bitmap & BIT(k)) { found = true; break; } if (3 * n_s_ssids > n_bssids && !pp->direct_scan[k].len) break; if (pp->direct_scan[k].len) allow_passive = false; s_ssid_bitmap |= BIT(k); n_s_ssids++; found = true; break; } } if (found) continue; for (k = 0; k < pp->bssid_num; k++) { if (!memcmp(&pp->bssid_array[k], scan_6ghz_params[j].bssid, ETH_ALEN)) { if (!(bssid_bitmap & BIT(k))) { if (n_bssids < max_bssids) { bssid_bitmap |= BIT(k); n_bssids++; } else { force_passive = TRUE; } } break; } } } if (cfg80211_channel_is_psc(params->channels[i]) && psc_no_listen) flags |= IWL_UHB_CHAN_CFG_FLAG_PSC_CHAN_NO_LISTEN; if (unsolicited_probe_on_chan) flags |= IWL_UHB_CHAN_CFG_FLAG_UNSOLICITED_PROBE_RES; if ((allow_passive && force_passive) || (!(bssid_bitmap | s_ssid_bitmap) && !cfg80211_channel_is_psc(params->channels[i]))) flags |= IWL_UHB_CHAN_CFG_FLAG_FORCE_PASSIVE; else flags |= bssid_bitmap | (s_ssid_bitmap << 16); cfg->flags |= cpu_to_le32(flags); if (version >= 17) cfg->v5.psd_20 = psd_20; ch_cnt++; } if (params->n_channels > ch_cnt) IWL_DEBUG_SCAN(mvm, ""6GHz: reducing number channels: (%u->%u)\n"", params->n_channels, ch_cnt); return ch_cnt; }"
1329----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56646/bad/addrconf.c----inet6_addr_modify,"static int inet6_addr_modify(struct net *net, struct inet6_ifaddr *ifp, struct ifa6_config *cfg) { u32 flags; clock_t expires; unsigned long timeout; bool was_managetempaddr; bool had_prefixroute; bool new_peer = false; ASSERT_RTNL(); if (!cfg->valid_lft || cfg->preferred_lft > cfg->valid_lft) return -EINVAL; if (cfg->ifa_flags & IFA_F_MANAGETEMPADDR && (ifp->flags & IFA_F_TEMPORARY || ifp->prefix_len != 64)) return -EINVAL; if (!(ifp->flags & IFA_F_TENTATIVE) || ifp->flags & IFA_F_DADFAILED) cfg->ifa_flags &= ~IFA_F_OPTIMISTIC; timeout = addrconf_timeout_fixup(cfg->valid_lft, HZ); if (addrconf_finite_timeout(timeout)) { expires = jiffies_to_clock_t(timeout * HZ); cfg->valid_lft = timeout; flags = RTF_EXPIRES; } else { expires = 0; flags = 0; cfg->ifa_flags |= IFA_F_PERMANENT; } timeout = addrconf_timeout_fixup(cfg->preferred_lft, HZ); if (addrconf_finite_timeout(timeout)) { if (timeout == 0) cfg->ifa_flags |= IFA_F_DEPRECATED; cfg->preferred_lft = timeout; } if (cfg->peer_pfx && memcmp(&ifp->peer_addr, cfg->peer_pfx, sizeof(struct in6_addr))) { if (!ipv6_addr_any(&ifp->peer_addr)) cleanup_prefix_route(ifp, expires, true, true); new_peer = true; } spin_lock_bh(&ifp->lock); was_managetempaddr = ifp->flags & IFA_F_MANAGETEMPADDR; had_prefixroute = ifp->flags & IFA_F_PERMANENT && !(ifp->flags & IFA_F_NOPREFIXROUTE); ifp->flags &= ~(IFA_F_DEPRECATED | IFA_F_PERMANENT | IFA_F_NODAD | IFA_F_HOMEADDRESS | IFA_F_MANAGETEMPADDR | IFA_F_NOPREFIXROUTE); ifp->flags |= cfg->ifa_flags; WRITE_ONCE(ifp->tstamp, jiffies); WRITE_ONCE(ifp->valid_lft, cfg->valid_lft); WRITE_ONCE(ifp->prefered_lft, cfg->preferred_lft); WRITE_ONCE(ifp->ifa_proto, cfg->ifa_proto); if (cfg->rt_priority && cfg->rt_priority != ifp->rt_priority) WRITE_ONCE(ifp->rt_priority, cfg->rt_priority); if (new_peer) ifp->peer_addr = *cfg->peer_pfx; spin_unlock_bh(&ifp->lock); if (!(ifp->flags&IFA_F_TENTATIVE)) ipv6_ifa_notify(0, ifp); if (!(cfg->ifa_flags & IFA_F_NOPREFIXROUTE)) { int rc = -ENOENT; if (had_prefixroute) <S2SV_StartVul> rc = modify_prefix_route(ifp, expires, flags, false); <S2SV_EndVul> if (rc == -ENOENT) { addrconf_prefix_route(&ifp->addr, ifp->prefix_len, ifp->rt_priority, ifp->idev->dev, expires, flags, GFP_KERNEL); } if (had_prefixroute && !ipv6_addr_any(&ifp->peer_addr)) <S2SV_StartVul> rc = modify_prefix_route(ifp, expires, flags, true); <S2SV_EndVul> if (rc == -ENOENT && !ipv6_addr_any(&ifp->peer_addr)) { addrconf_prefix_route(&ifp->peer_addr, ifp->prefix_len, ifp->rt_priority, ifp->idev->dev, expires, flags, GFP_KERNEL); } } else if (had_prefixroute) { enum cleanup_prefix_rt_t action; unsigned long rt_expires; write_lock_bh(&ifp->idev->lock); action = check_cleanup_prefix_route(ifp, &rt_expires); write_unlock_bh(&ifp->idev->lock); if (action != CLEANUP_PREFIX_RT_NOP) { cleanup_prefix_route(ifp, rt_expires, action == CLEANUP_PREFIX_RT_DEL, false); } } if (was_managetempaddr || ifp->flags & IFA_F_MANAGETEMPADDR) { if (was_managetempaddr && !(ifp->flags & IFA_F_MANAGETEMPADDR)) delete_tempaddrs(ifp->idev, ifp); else manage_tempaddrs(ifp->idev, ifp, cfg->valid_lft, cfg->preferred_lft, !was_managetempaddr, jiffies); } addrconf_verify_rtnl(net); return 0; }","- rc = modify_prefix_route(ifp, expires, flags, false);
- rc = modify_prefix_route(ifp, expires, flags, true);
+ rc = modify_prefix_route(net, ifp, expires, flags, false);
+ rc = modify_prefix_route(net, ifp, expires, flags, true);","static int inet6_addr_modify(struct net *net, struct inet6_ifaddr *ifp, struct ifa6_config *cfg) { u32 flags; clock_t expires; unsigned long timeout; bool was_managetempaddr; bool had_prefixroute; bool new_peer = false; ASSERT_RTNL(); if (!cfg->valid_lft || cfg->preferred_lft > cfg->valid_lft) return -EINVAL; if (cfg->ifa_flags & IFA_F_MANAGETEMPADDR && (ifp->flags & IFA_F_TEMPORARY || ifp->prefix_len != 64)) return -EINVAL; if (!(ifp->flags & IFA_F_TENTATIVE) || ifp->flags & IFA_F_DADFAILED) cfg->ifa_flags &= ~IFA_F_OPTIMISTIC; timeout = addrconf_timeout_fixup(cfg->valid_lft, HZ); if (addrconf_finite_timeout(timeout)) { expires = jiffies_to_clock_t(timeout * HZ); cfg->valid_lft = timeout; flags = RTF_EXPIRES; } else { expires = 0; flags = 0; cfg->ifa_flags |= IFA_F_PERMANENT; } timeout = addrconf_timeout_fixup(cfg->preferred_lft, HZ); if (addrconf_finite_timeout(timeout)) { if (timeout == 0) cfg->ifa_flags |= IFA_F_DEPRECATED; cfg->preferred_lft = timeout; } if (cfg->peer_pfx && memcmp(&ifp->peer_addr, cfg->peer_pfx, sizeof(struct in6_addr))) { if (!ipv6_addr_any(&ifp->peer_addr)) cleanup_prefix_route(ifp, expires, true, true); new_peer = true; } spin_lock_bh(&ifp->lock); was_managetempaddr = ifp->flags & IFA_F_MANAGETEMPADDR; had_prefixroute = ifp->flags & IFA_F_PERMANENT && !(ifp->flags & IFA_F_NOPREFIXROUTE); ifp->flags &= ~(IFA_F_DEPRECATED | IFA_F_PERMANENT | IFA_F_NODAD | IFA_F_HOMEADDRESS | IFA_F_MANAGETEMPADDR | IFA_F_NOPREFIXROUTE); ifp->flags |= cfg->ifa_flags; WRITE_ONCE(ifp->tstamp, jiffies); WRITE_ONCE(ifp->valid_lft, cfg->valid_lft); WRITE_ONCE(ifp->prefered_lft, cfg->preferred_lft); WRITE_ONCE(ifp->ifa_proto, cfg->ifa_proto); if (cfg->rt_priority && cfg->rt_priority != ifp->rt_priority) WRITE_ONCE(ifp->rt_priority, cfg->rt_priority); if (new_peer) ifp->peer_addr = *cfg->peer_pfx; spin_unlock_bh(&ifp->lock); if (!(ifp->flags&IFA_F_TENTATIVE)) ipv6_ifa_notify(0, ifp); if (!(cfg->ifa_flags & IFA_F_NOPREFIXROUTE)) { int rc = -ENOENT; if (had_prefixroute) rc = modify_prefix_route(net, ifp, expires, flags, false); if (rc == -ENOENT) { addrconf_prefix_route(&ifp->addr, ifp->prefix_len, ifp->rt_priority, ifp->idev->dev, expires, flags, GFP_KERNEL); } if (had_prefixroute && !ipv6_addr_any(&ifp->peer_addr)) rc = modify_prefix_route(net, ifp, expires, flags, true); if (rc == -ENOENT && !ipv6_addr_any(&ifp->peer_addr)) { addrconf_prefix_route(&ifp->peer_addr, ifp->prefix_len, ifp->rt_priority, ifp->idev->dev, expires, flags, GFP_KERNEL); } } else if (had_prefixroute) { enum cleanup_prefix_rt_t action; unsigned long rt_expires; write_lock_bh(&ifp->idev->lock); action = check_cleanup_prefix_route(ifp, &rt_expires); write_unlock_bh(&ifp->idev->lock); if (action != CLEANUP_PREFIX_RT_NOP) { cleanup_prefix_route(ifp, rt_expires, action == CLEANUP_PREFIX_RT_DEL, false); } } if (was_managetempaddr || ifp->flags & IFA_F_MANAGETEMPADDR) { if (was_managetempaddr && !(ifp->flags & IFA_F_MANAGETEMPADDR)) delete_tempaddrs(ifp->idev, ifp); else manage_tempaddrs(ifp->idev, ifp, cfg->valid_lft, cfg->preferred_lft, !was_managetempaddr, jiffies); } addrconf_verify_rtnl(net); return 0; }"
152----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-45010/bad/pm_netlink.c----mptcp_pm_nl_rm_addr_or_subflow,"static void mptcp_pm_nl_rm_addr_or_subflow(struct mptcp_sock *msk, const struct mptcp_rm_list *rm_list, enum linux_mptcp_mib_field rm_type) { struct mptcp_subflow_context *subflow, *tmp; struct sock *sk = (struct sock *)msk; u8 i; pr_debug(""%s rm_list_nr %d"", rm_type == MPTCP_MIB_RMADDR ? ""address"" : ""subflow"", rm_list->nr); msk_owned_by_me(msk); if (sk->sk_state == TCP_LISTEN) return; if (!rm_list->nr) return; if (list_empty(&msk->conn_list)) return; for (i = 0; i < rm_list->nr; i++) { u8 rm_id = rm_list->ids[i]; bool removed = false; mptcp_for_each_subflow_safe(msk, subflow, tmp) { struct sock *ssk = mptcp_subflow_tcp_sock(subflow); u8 remote_id = READ_ONCE(subflow->remote_id); int how = RCV_SHUTDOWN | SEND_SHUTDOWN; u8 id = subflow_get_local_id(subflow); if (rm_type == MPTCP_MIB_RMADDR && remote_id != rm_id) continue; if (rm_type == MPTCP_MIB_RMSUBFLOW && !mptcp_local_id_match(msk, id, rm_id)) continue; pr_debug("" -> %s rm_list_ids[%d]=%u local_id=%u remote_id=%u mpc_id=%u"", rm_type == MPTCP_MIB_RMADDR ? ""address"" : ""subflow"", i, rm_id, id, remote_id, msk->mpc_endpoint_id); spin_unlock_bh(&msk->pm.lock); mptcp_subflow_shutdown(sk, ssk, how); mptcp_close_ssk(sk, ssk, subflow); spin_lock_bh(&msk->pm.lock); removed = true; if (rm_type == MPTCP_MIB_RMSUBFLOW) __MPTCP_INC_STATS(sock_net(sk), rm_type); } <S2SV_StartVul> if (rm_type == MPTCP_MIB_RMSUBFLOW) <S2SV_EndVul> <S2SV_StartVul> __set_bit(rm_id ? rm_id : msk->mpc_endpoint_id, msk->pm.id_avail_bitmap); <S2SV_EndVul> <S2SV_StartVul> else if (rm_type == MPTCP_MIB_RMADDR) <S2SV_EndVul> __MPTCP_INC_STATS(sock_net(sk), rm_type); if (!removed) continue; if (!mptcp_pm_is_kernel(msk)) continue; if (rm_type == MPTCP_MIB_RMADDR) { msk->pm.add_addr_accepted--; WRITE_ONCE(msk->pm.accept_addr, true); <S2SV_StartVul> } else if (rm_type == MPTCP_MIB_RMSUBFLOW) { <S2SV_EndVul> <S2SV_StartVul> msk->pm.local_addr_used--; <S2SV_EndVul> } } }","- if (rm_type == MPTCP_MIB_RMSUBFLOW)
- __set_bit(rm_id ? rm_id : msk->mpc_endpoint_id, msk->pm.id_avail_bitmap);
- else if (rm_type == MPTCP_MIB_RMADDR)
- } else if (rm_type == MPTCP_MIB_RMSUBFLOW) {
- msk->pm.local_addr_used--;
+ }
+ if (rm_type == MPTCP_MIB_RMADDR)
+ }
+ }
+ }","static void mptcp_pm_nl_rm_addr_or_subflow(struct mptcp_sock *msk, const struct mptcp_rm_list *rm_list, enum linux_mptcp_mib_field rm_type) { struct mptcp_subflow_context *subflow, *tmp; struct sock *sk = (struct sock *)msk; u8 i; pr_debug(""%s rm_list_nr %d"", rm_type == MPTCP_MIB_RMADDR ? ""address"" : ""subflow"", rm_list->nr); msk_owned_by_me(msk); if (sk->sk_state == TCP_LISTEN) return; if (!rm_list->nr) return; if (list_empty(&msk->conn_list)) return; for (i = 0; i < rm_list->nr; i++) { u8 rm_id = rm_list->ids[i]; bool removed = false; mptcp_for_each_subflow_safe(msk, subflow, tmp) { struct sock *ssk = mptcp_subflow_tcp_sock(subflow); u8 remote_id = READ_ONCE(subflow->remote_id); int how = RCV_SHUTDOWN | SEND_SHUTDOWN; u8 id = subflow_get_local_id(subflow); if (rm_type == MPTCP_MIB_RMADDR && remote_id != rm_id) continue; if (rm_type == MPTCP_MIB_RMSUBFLOW && !mptcp_local_id_match(msk, id, rm_id)) continue; pr_debug("" -> %s rm_list_ids[%d]=%u local_id=%u remote_id=%u mpc_id=%u"", rm_type == MPTCP_MIB_RMADDR ? ""address"" : ""subflow"", i, rm_id, id, remote_id, msk->mpc_endpoint_id); spin_unlock_bh(&msk->pm.lock); mptcp_subflow_shutdown(sk, ssk, how); mptcp_close_ssk(sk, ssk, subflow); spin_lock_bh(&msk->pm.lock); removed = true; if (rm_type == MPTCP_MIB_RMSUBFLOW) __MPTCP_INC_STATS(sock_net(sk), rm_type); } if (rm_type == MPTCP_MIB_RMADDR) __MPTCP_INC_STATS(sock_net(sk), rm_type); if (!removed) continue; if (!mptcp_pm_is_kernel(msk)) continue; if (rm_type == MPTCP_MIB_RMADDR) { msk->pm.add_addr_accepted--; WRITE_ONCE(msk->pm.accept_addr, true); } } }"
252----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46726/bad/dcn_calcs.c----dcn_bw_update_from_pplib_fclks,"void dcn_bw_update_from_pplib_fclks( struct dc *dc, struct dm_pp_clock_levels_with_voltage *fclks) { unsigned vmin0p65_idx, vmid0p72_idx, vnom0p8_idx, vmax0p9_idx; ASSERT(fclks->num_levels); vmin0p65_idx = 0; vmid0p72_idx = fclks->num_levels - <S2SV_StartVul> (fclks->num_levels > 2 ? 3 : (fclks->num_levels > 1 ? 2 : 1)); <S2SV_EndVul> <S2SV_StartVul> vnom0p8_idx = fclks->num_levels - (fclks->num_levels > 1 ? 2 : 1); <S2SV_EndVul> <S2SV_StartVul> vmax0p9_idx = fclks->num_levels - 1; <S2SV_EndVul> dc->dcn_soc->fabric_and_dram_bandwidth_vmin0p65 = 32 * (fclks->data[vmin0p65_idx].clocks_in_khz / 1000.0) / 1000.0; dc->dcn_soc->fabric_and_dram_bandwidth_vmid0p72 = dc->dcn_soc->number_of_channels * (fclks->data[vmid0p72_idx].clocks_in_khz / 1000.0) * ddr4_dram_factor_single_Channel / 1000.0; dc->dcn_soc->fabric_and_dram_bandwidth_vnom0p8 = dc->dcn_soc->number_of_channels * (fclks->data[vnom0p8_idx].clocks_in_khz / 1000.0) * ddr4_dram_factor_single_Channel / 1000.0; dc->dcn_soc->fabric_and_dram_bandwidth_vmax0p9 = dc->dcn_soc->number_of_channels * (fclks->data[vmax0p9_idx].clocks_in_khz / 1000.0) * ddr4_dram_factor_single_Channel / 1000.0; }","- (fclks->num_levels > 2 ? 3 : (fclks->num_levels > 1 ? 2 : 1));
- vnom0p8_idx = fclks->num_levels - (fclks->num_levels > 1 ? 2 : 1);
- vmax0p9_idx = fclks->num_levels - 1;
+ vmid0p72_idx = fclks->num_levels > 2 ? fclks->num_levels - 3 : 0;
+ vnom0p8_idx = fclks->num_levels > 1 ? fclks->num_levels - 2 : 0;
+ vmax0p9_idx = fclks->num_levels > 0 ? fclks->num_levels - 1 : 0;","void dcn_bw_update_from_pplib_fclks( struct dc *dc, struct dm_pp_clock_levels_with_voltage *fclks) { unsigned vmin0p65_idx, vmid0p72_idx, vnom0p8_idx, vmax0p9_idx; ASSERT(fclks->num_levels); vmin0p65_idx = 0; vmid0p72_idx = fclks->num_levels > 2 ? fclks->num_levels - 3 : 0; vnom0p8_idx = fclks->num_levels > 1 ? fclks->num_levels - 2 : 0; vmax0p9_idx = fclks->num_levels > 0 ? fclks->num_levels - 1 : 0; dc->dcn_soc->fabric_and_dram_bandwidth_vmin0p65 = 32 * (fclks->data[vmin0p65_idx].clocks_in_khz / 1000.0) / 1000.0; dc->dcn_soc->fabric_and_dram_bandwidth_vmid0p72 = dc->dcn_soc->number_of_channels * (fclks->data[vmid0p72_idx].clocks_in_khz / 1000.0) * ddr4_dram_factor_single_Channel / 1000.0; dc->dcn_soc->fabric_and_dram_bandwidth_vnom0p8 = dc->dcn_soc->number_of_channels * (fclks->data[vnom0p8_idx].clocks_in_khz / 1000.0) * ddr4_dram_factor_single_Channel / 1000.0; dc->dcn_soc->fabric_and_dram_bandwidth_vmax0p9 = dc->dcn_soc->number_of_channels * (fclks->data[vmax0p9_idx].clocks_in_khz / 1000.0) * ddr4_dram_factor_single_Channel / 1000.0; }"
845----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50105/bad/sc7280.c----sc7280_snd_startup,"static int sc7280_snd_startup(struct snd_pcm_substream *substream) { unsigned int fmt = SND_SOC_DAIFMT_CBS_CFS; unsigned int codec_dai_fmt = SND_SOC_DAIFMT_CBS_CFS; struct snd_soc_pcm_runtime *rtd = snd_soc_substream_to_rtd(substream); struct snd_soc_dai *cpu_dai = snd_soc_rtd_to_cpu(rtd, 0); struct snd_soc_dai *codec_dai = snd_soc_rtd_to_codec(rtd, 0); int ret = 0; switch (cpu_dai->id) { case MI2S_PRIMARY: ret = sc7280_rt5682_init(rtd); break; case SECONDARY_MI2S_RX: codec_dai_fmt |= SND_SOC_DAIFMT_NB_NF | SND_SOC_DAIFMT_I2S; snd_soc_dai_set_sysclk(cpu_dai, Q6AFE_LPASS_CLK_ID_SEC_MI2S_IBIT, MI2S_BCLK_RATE, SNDRV_PCM_STREAM_PLAYBACK); snd_soc_dai_set_fmt(cpu_dai, fmt); snd_soc_dai_set_fmt(codec_dai, codec_dai_fmt); break; default: break; } <S2SV_StartVul> return ret; <S2SV_EndVul> }","- return ret;
+ if (ret)
+ return ret;
+ return qcom_snd_sdw_startup(substream);","static int sc7280_snd_startup(struct snd_pcm_substream *substream) { unsigned int fmt = SND_SOC_DAIFMT_CBS_CFS; unsigned int codec_dai_fmt = SND_SOC_DAIFMT_CBS_CFS; struct snd_soc_pcm_runtime *rtd = snd_soc_substream_to_rtd(substream); struct snd_soc_dai *cpu_dai = snd_soc_rtd_to_cpu(rtd, 0); struct snd_soc_dai *codec_dai = snd_soc_rtd_to_codec(rtd, 0); int ret = 0; switch (cpu_dai->id) { case MI2S_PRIMARY: ret = sc7280_rt5682_init(rtd); if (ret) return ret; break; case SECONDARY_MI2S_RX: codec_dai_fmt |= SND_SOC_DAIFMT_NB_NF | SND_SOC_DAIFMT_I2S; snd_soc_dai_set_sysclk(cpu_dai, Q6AFE_LPASS_CLK_ID_SEC_MI2S_IBIT, MI2S_BCLK_RATE, SNDRV_PCM_STREAM_PLAYBACK); snd_soc_dai_set_fmt(cpu_dai, fmt); snd_soc_dai_set_fmt(codec_dai, codec_dai_fmt); break; default: break; } return qcom_snd_sdw_startup(substream); }"
1493----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-57933/bad/gve_main.c----gve_xsk_pool_enable,"static int gve_xsk_pool_enable(struct net_device *dev, struct xsk_buff_pool *pool, u16 qid) { struct gve_priv *priv = netdev_priv(dev); struct napi_struct *napi; struct gve_rx_ring *rx; int tx_qid; int err; if (qid >= priv->rx_cfg.num_queues) { dev_err(&priv->pdev->dev, ""xsk pool invalid qid %d"", qid); return -EINVAL; } if (xsk_pool_get_rx_frame_size(pool) < priv->dev->max_mtu + sizeof(struct ethhdr)) { dev_err(&priv->pdev->dev, ""xsk pool frame_len too small""); return -EINVAL; } err = xsk_pool_dma_map(pool, &priv->pdev->dev, DMA_ATTR_SKIP_CPU_SYNC | DMA_ATTR_WEAK_ORDERING); if (err) return err; <S2SV_StartVul> if (!priv->xdp_prog) <S2SV_EndVul> return 0; rx = &priv->rx[qid]; napi = &priv->ntfy_blocks[rx->ntfy_id].napi; err = xdp_rxq_info_reg(&rx->xsk_rxq, dev, qid, napi->napi_id); if (err) goto err; err = xdp_rxq_info_reg_mem_model(&rx->xsk_rxq, MEM_TYPE_XSK_BUFF_POOL, NULL); if (err) goto err; xsk_pool_set_rxq_info(pool, &rx->xsk_rxq); rx->xsk_pool = pool; tx_qid = gve_xdp_tx_queue_id(priv, qid); priv->tx[tx_qid].xsk_pool = pool; return 0; err: if (xdp_rxq_info_is_reg(&rx->xsk_rxq)) xdp_rxq_info_unreg(&rx->xsk_rxq); xsk_pool_dma_unmap(pool, DMA_ATTR_SKIP_CPU_SYNC | DMA_ATTR_WEAK_ORDERING); return err; }","- if (!priv->xdp_prog)
+ if (!priv->xdp_prog || !netif_running(dev))","static int gve_xsk_pool_enable(struct net_device *dev, struct xsk_buff_pool *pool, u16 qid) { struct gve_priv *priv = netdev_priv(dev); struct napi_struct *napi; struct gve_rx_ring *rx; int tx_qid; int err; if (qid >= priv->rx_cfg.num_queues) { dev_err(&priv->pdev->dev, ""xsk pool invalid qid %d"", qid); return -EINVAL; } if (xsk_pool_get_rx_frame_size(pool) < priv->dev->max_mtu + sizeof(struct ethhdr)) { dev_err(&priv->pdev->dev, ""xsk pool frame_len too small""); return -EINVAL; } err = xsk_pool_dma_map(pool, &priv->pdev->dev, DMA_ATTR_SKIP_CPU_SYNC | DMA_ATTR_WEAK_ORDERING); if (err) return err; if (!priv->xdp_prog || !netif_running(dev)) return 0; rx = &priv->rx[qid]; napi = &priv->ntfy_blocks[rx->ntfy_id].napi; err = xdp_rxq_info_reg(&rx->xsk_rxq, dev, qid, napi->napi_id); if (err) goto err; err = xdp_rxq_info_reg_mem_model(&rx->xsk_rxq, MEM_TYPE_XSK_BUFF_POOL, NULL); if (err) goto err; xsk_pool_set_rxq_info(pool, &rx->xsk_rxq); rx->xsk_pool = pool; tx_qid = gve_xdp_tx_queue_id(priv, qid); priv->tx[tx_qid].xsk_pool = pool; return 0; err: if (xdp_rxq_info_is_reg(&rx->xsk_rxq)) xdp_rxq_info_unreg(&rx->xsk_rxq); xsk_pool_dma_unmap(pool, DMA_ATTR_SKIP_CPU_SYNC | DMA_ATTR_WEAK_ORDERING); return err; }"
1049----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50288/bad/vivid-ctrls.c----vivid_vid_cap_s_ctrl,"static int vivid_vid_cap_s_ctrl(struct v4l2_ctrl *ctrl) { static const u32 colorspaces[] = { V4L2_COLORSPACE_SMPTE170M, V4L2_COLORSPACE_REC709, V4L2_COLORSPACE_SRGB, V4L2_COLORSPACE_OPRGB, V4L2_COLORSPACE_BT2020, V4L2_COLORSPACE_DCI_P3, V4L2_COLORSPACE_SMPTE240M, V4L2_COLORSPACE_470_SYSTEM_M, V4L2_COLORSPACE_470_SYSTEM_BG, }; struct vivid_dev *dev = container_of(ctrl->handler, struct vivid_dev, ctrl_hdl_vid_cap); unsigned int i; struct vivid_dev *output_inst = NULL; int index = 0; int hdmi_index, svid_index; s32 input_index = 0; switch (ctrl->id) { case VIVID_CID_TEST_PATTERN: vivid_update_quality(dev); tpg_s_pattern(&dev->tpg, ctrl->val); break; case VIVID_CID_COLORSPACE: tpg_s_colorspace(&dev->tpg, colorspaces[ctrl->val]); vivid_send_source_change(dev, TV); vivid_send_source_change(dev, SVID); vivid_send_source_change(dev, HDMI); vivid_send_source_change(dev, WEBCAM); break; case VIVID_CID_XFER_FUNC: tpg_s_xfer_func(&dev->tpg, ctrl->val); vivid_send_source_change(dev, TV); vivid_send_source_change(dev, SVID); vivid_send_source_change(dev, HDMI); vivid_send_source_change(dev, WEBCAM); break; case VIVID_CID_YCBCR_ENC: tpg_s_ycbcr_enc(&dev->tpg, ctrl->val); vivid_send_source_change(dev, TV); vivid_send_source_change(dev, SVID); vivid_send_source_change(dev, HDMI); vivid_send_source_change(dev, WEBCAM); break; case VIVID_CID_HSV_ENC: tpg_s_hsv_enc(&dev->tpg, ctrl->val ? V4L2_HSV_ENC_256 : V4L2_HSV_ENC_180); vivid_send_source_change(dev, TV); vivid_send_source_change(dev, SVID); vivid_send_source_change(dev, HDMI); vivid_send_source_change(dev, WEBCAM); break; case VIVID_CID_QUANTIZATION: tpg_s_quantization(&dev->tpg, ctrl->val); vivid_send_source_change(dev, TV); vivid_send_source_change(dev, SVID); vivid_send_source_change(dev, HDMI); vivid_send_source_change(dev, WEBCAM); break; case V4L2_CID_DV_RX_RGB_RANGE: if (!vivid_is_hdmi_cap(dev)) break; tpg_s_rgb_range(&dev->tpg, ctrl->val); break; case VIVID_CID_LIMITED_RGB_RANGE: tpg_s_real_rgb_range(&dev->tpg, ctrl->val ? V4L2_DV_RGB_RANGE_LIMITED : V4L2_DV_RGB_RANGE_FULL); break; case VIVID_CID_ALPHA_MODE: tpg_s_alpha_mode(&dev->tpg, ctrl->val); break; case VIVID_CID_HOR_MOVEMENT: tpg_s_mv_hor_mode(&dev->tpg, ctrl->val); break; case VIVID_CID_VERT_MOVEMENT: tpg_s_mv_vert_mode(&dev->tpg, ctrl->val); break; case VIVID_CID_OSD_TEXT_MODE: dev->osd_mode = ctrl->val; break; case VIVID_CID_PERCENTAGE_FILL: tpg_s_perc_fill(&dev->tpg, ctrl->val); <S2SV_StartVul> for (i = 0; i < VIDEO_MAX_FRAME; i++) <S2SV_EndVul> dev->must_blank[i] = ctrl->val < 100; break; case VIVID_CID_INSERT_SAV: tpg_s_insert_sav(&dev->tpg, ctrl->val); break; case VIVID_CID_INSERT_EAV: tpg_s_insert_eav(&dev->tpg, ctrl->val); break; case VIVID_CID_INSERT_HDMI_VIDEO_GUARD_BAND: tpg_s_insert_hdmi_video_guard_band(&dev->tpg, ctrl->val); break; case VIVID_CID_HFLIP: dev->sensor_hflip = ctrl->val; tpg_s_hflip(&dev->tpg, dev->sensor_hflip ^ dev->hflip); break; case VIVID_CID_VFLIP: dev->sensor_vflip = ctrl->val; tpg_s_vflip(&dev->tpg, dev->sensor_vflip ^ dev->vflip); break; case VIVID_CID_REDUCED_FPS: dev->reduced_fps = ctrl->val; vivid_update_format_cap(dev, true); break; case VIVID_CID_HAS_CROP_CAP: dev->has_crop_cap = ctrl->val; vivid_update_format_cap(dev, true); break; case VIVID_CID_HAS_COMPOSE_CAP: dev->has_compose_cap = ctrl->val; vivid_update_format_cap(dev, true); break; case VIVID_CID_HAS_SCALER_CAP: dev->has_scaler_cap = ctrl->val; vivid_update_format_cap(dev, true); break; case VIVID_CID_SHOW_BORDER: tpg_s_show_border(&dev->tpg, ctrl->val); break; case VIVID_CID_SHOW_SQUARE: tpg_s_show_square(&dev->tpg, ctrl->val); break; case VIVID_CID_STD_ASPECT_RATIO: dev->std_aspect_ratio[dev->input] = ctrl->val; tpg_s_video_aspect(&dev->tpg, vivid_get_video_aspect(dev)); break; case VIVID_CID_DV_TIMINGS_SIGNAL_MODE: dev->dv_timings_signal_mode[dev->input] = dev->ctrl_dv_timings_signal_mode->val; dev->query_dv_timings[dev->input] = dev->ctrl_dv_timings->val; vivid_update_power_present(dev); vivid_update_quality(dev); vivid_send_input_source_change(dev, dev->input); break; case VIVID_CID_DV_TIMINGS_ASPECT_RATIO: dev->dv_timings_aspect_ratio[dev->input] = ctrl->val; tpg_s_video_aspect(&dev->tpg, vivid_get_video_aspect(dev)); break; case VIVID_CID_TSTAMP_SRC: dev->tstamp_src_is_soe = ctrl->val; dev->vb_vid_cap_q.timestamp_flags &= ~V4L2_BUF_FLAG_TSTAMP_SRC_MASK; if (dev->tstamp_src_is_soe) dev->vb_vid_cap_q.timestamp_flags |= V4L2_BUF_FLAG_TSTAMP_SRC_SOE; break; case VIVID_CID_MAX_EDID_BLOCKS: dev->edid_max_blocks = ctrl->val; if (dev->edid_blocks > dev->edid_max_blocks) dev->edid_blocks = dev->edid_max_blocks; break; case VIVID_CID_HDMI_IS_CONNECTED_TO_OUTPUT(0) ... VIVID_CID_HDMI_IS_CONNECTED_TO_OUTPUT(14): hdmi_index = ctrl->id - VIVID_CID_HDMI_IS_CONNECTED_TO_OUTPUT(0); output_inst = vivid_ctrl_hdmi_to_output_instance[ctrl->cur.val]; index = vivid_ctrl_hdmi_to_output_index[ctrl->cur.val]; input_index = dev->hdmi_index_to_input_index[hdmi_index]; dev->input_is_connected_to_output[input_index] = ctrl->val; if (output_inst) { output_inst->output_to_input_instance[index] = NULL; vivid_update_outputs(output_inst); cec_phys_addr_invalidate(output_inst->cec_tx_adap[index]); } if (ctrl->val >= FIXED_MENU_ITEMS) { output_inst = vivid_ctrl_hdmi_to_output_instance[ctrl->val]; index = vivid_ctrl_hdmi_to_output_index[ctrl->val]; output_inst->output_to_input_instance[index] = dev; output_inst->output_to_input_index[index] = dev->hdmi_index_to_input_index[hdmi_index]; } spin_lock(&hdmi_output_skip_mask_lock); hdmi_to_output_menu_skip_mask &= ~(1ULL << ctrl->cur.val); if (ctrl->val >= FIXED_MENU_ITEMS) hdmi_to_output_menu_skip_mask |= 1ULL << ctrl->val; spin_unlock(&hdmi_output_skip_mask_lock); vivid_update_power_present(dev); vivid_update_quality(dev); vivid_send_input_source_change(dev, dev->hdmi_index_to_input_index[hdmi_index]); if (ctrl->val < FIXED_MENU_ITEMS && ctrl->cur.val < FIXED_MENU_ITEMS) break; spin_lock(&hdmi_output_skip_mask_lock); hdmi_input_update_outputs_mask |= 1 << dev->inst; spin_unlock(&hdmi_output_skip_mask_lock); queue_work(update_hdmi_ctrls_workqueue, &dev->update_hdmi_ctrl_work); break; case VIVID_CID_SVID_IS_CONNECTED_TO_OUTPUT(0) ... VIVID_CID_SVID_IS_CONNECTED_TO_OUTPUT(15): svid_index = ctrl->id - VIVID_CID_SVID_IS_CONNECTED_TO_OUTPUT(0); output_inst = vivid_ctrl_svid_to_output_instance[ctrl->cur.val]; index = vivid_ctrl_svid_to_output_index[ctrl->cur.val]; input_index = dev->svid_index_to_input_index[svid_index]; dev->input_is_connected_to_output[input_index] = ctrl->val; if (output_inst) output_inst->output_to_input_instance[index] = NULL; if (ctrl->val >= FIXED_MENU_ITEMS) { output_inst = vivid_ctrl_svid_to_output_instance[ctrl->val]; index = vivid_ctrl_svid_to_output_index[ctrl->val]; output_inst->output_to_input_instance[index] = dev; output_inst->output_to_input_index[index] = dev->svid_index_to_input_index[svid_index]; } spin_lock(&svid_output_skip_mask_lock); svid_to_output_menu_skip_mask &= ~(1ULL << ctrl->cur.val); if (ctrl->val >= FIXED_MENU_ITEMS) svid_to_output_menu_skip_mask |= 1ULL << ctrl->val; spin_unlock(&svid_output_skip_mask_lock); vivid_update_quality(dev); vivid_send_input_source_change(dev, dev->svid_index_to_input_index[svid_index]); if (ctrl->val < FIXED_MENU_ITEMS && ctrl->cur.val < FIXED_MENU_ITEMS) break; queue_work(update_svid_ctrls_workqueue, &dev->update_svid_ctrl_work); break; } return 0; }","- for (i = 0; i < VIDEO_MAX_FRAME; i++)
+ for (i = 0; i < MAX_VID_CAP_BUFFERS; i++)","static int vivid_vid_cap_s_ctrl(struct v4l2_ctrl *ctrl) { static const u32 colorspaces[] = { V4L2_COLORSPACE_SMPTE170M, V4L2_COLORSPACE_REC709, V4L2_COLORSPACE_SRGB, V4L2_COLORSPACE_OPRGB, V4L2_COLORSPACE_BT2020, V4L2_COLORSPACE_DCI_P3, V4L2_COLORSPACE_SMPTE240M, V4L2_COLORSPACE_470_SYSTEM_M, V4L2_COLORSPACE_470_SYSTEM_BG, }; struct vivid_dev *dev = container_of(ctrl->handler, struct vivid_dev, ctrl_hdl_vid_cap); unsigned int i; struct vivid_dev *output_inst = NULL; int index = 0; int hdmi_index, svid_index; s32 input_index = 0; switch (ctrl->id) { case VIVID_CID_TEST_PATTERN: vivid_update_quality(dev); tpg_s_pattern(&dev->tpg, ctrl->val); break; case VIVID_CID_COLORSPACE: tpg_s_colorspace(&dev->tpg, colorspaces[ctrl->val]); vivid_send_source_change(dev, TV); vivid_send_source_change(dev, SVID); vivid_send_source_change(dev, HDMI); vivid_send_source_change(dev, WEBCAM); break; case VIVID_CID_XFER_FUNC: tpg_s_xfer_func(&dev->tpg, ctrl->val); vivid_send_source_change(dev, TV); vivid_send_source_change(dev, SVID); vivid_send_source_change(dev, HDMI); vivid_send_source_change(dev, WEBCAM); break; case VIVID_CID_YCBCR_ENC: tpg_s_ycbcr_enc(&dev->tpg, ctrl->val); vivid_send_source_change(dev, TV); vivid_send_source_change(dev, SVID); vivid_send_source_change(dev, HDMI); vivid_send_source_change(dev, WEBCAM); break; case VIVID_CID_HSV_ENC: tpg_s_hsv_enc(&dev->tpg, ctrl->val ? V4L2_HSV_ENC_256 : V4L2_HSV_ENC_180); vivid_send_source_change(dev, TV); vivid_send_source_change(dev, SVID); vivid_send_source_change(dev, HDMI); vivid_send_source_change(dev, WEBCAM); break; case VIVID_CID_QUANTIZATION: tpg_s_quantization(&dev->tpg, ctrl->val); vivid_send_source_change(dev, TV); vivid_send_source_change(dev, SVID); vivid_send_source_change(dev, HDMI); vivid_send_source_change(dev, WEBCAM); break; case V4L2_CID_DV_RX_RGB_RANGE: if (!vivid_is_hdmi_cap(dev)) break; tpg_s_rgb_range(&dev->tpg, ctrl->val); break; case VIVID_CID_LIMITED_RGB_RANGE: tpg_s_real_rgb_range(&dev->tpg, ctrl->val ? V4L2_DV_RGB_RANGE_LIMITED : V4L2_DV_RGB_RANGE_FULL); break; case VIVID_CID_ALPHA_MODE: tpg_s_alpha_mode(&dev->tpg, ctrl->val); break; case VIVID_CID_HOR_MOVEMENT: tpg_s_mv_hor_mode(&dev->tpg, ctrl->val); break; case VIVID_CID_VERT_MOVEMENT: tpg_s_mv_vert_mode(&dev->tpg, ctrl->val); break; case VIVID_CID_OSD_TEXT_MODE: dev->osd_mode = ctrl->val; break; case VIVID_CID_PERCENTAGE_FILL: tpg_s_perc_fill(&dev->tpg, ctrl->val); for (i = 0; i < MAX_VID_CAP_BUFFERS; i++) dev->must_blank[i] = ctrl->val < 100; break; case VIVID_CID_INSERT_SAV: tpg_s_insert_sav(&dev->tpg, ctrl->val); break; case VIVID_CID_INSERT_EAV: tpg_s_insert_eav(&dev->tpg, ctrl->val); break; case VIVID_CID_INSERT_HDMI_VIDEO_GUARD_BAND: tpg_s_insert_hdmi_video_guard_band(&dev->tpg, ctrl->val); break; case VIVID_CID_HFLIP: dev->sensor_hflip = ctrl->val; tpg_s_hflip(&dev->tpg, dev->sensor_hflip ^ dev->hflip); break; case VIVID_CID_VFLIP: dev->sensor_vflip = ctrl->val; tpg_s_vflip(&dev->tpg, dev->sensor_vflip ^ dev->vflip); break; case VIVID_CID_REDUCED_FPS: dev->reduced_fps = ctrl->val; vivid_update_format_cap(dev, true); break; case VIVID_CID_HAS_CROP_CAP: dev->has_crop_cap = ctrl->val; vivid_update_format_cap(dev, true); break; case VIVID_CID_HAS_COMPOSE_CAP: dev->has_compose_cap = ctrl->val; vivid_update_format_cap(dev, true); break; case VIVID_CID_HAS_SCALER_CAP: dev->has_scaler_cap = ctrl->val; vivid_update_format_cap(dev, true); break; case VIVID_CID_SHOW_BORDER: tpg_s_show_border(&dev->tpg, ctrl->val); break; case VIVID_CID_SHOW_SQUARE: tpg_s_show_square(&dev->tpg, ctrl->val); break; case VIVID_CID_STD_ASPECT_RATIO: dev->std_aspect_ratio[dev->input] = ctrl->val; tpg_s_video_aspect(&dev->tpg, vivid_get_video_aspect(dev)); break; case VIVID_CID_DV_TIMINGS_SIGNAL_MODE: dev->dv_timings_signal_mode[dev->input] = dev->ctrl_dv_timings_signal_mode->val; dev->query_dv_timings[dev->input] = dev->ctrl_dv_timings->val; vivid_update_power_present(dev); vivid_update_quality(dev); vivid_send_input_source_change(dev, dev->input); break; case VIVID_CID_DV_TIMINGS_ASPECT_RATIO: dev->dv_timings_aspect_ratio[dev->input] = ctrl->val; tpg_s_video_aspect(&dev->tpg, vivid_get_video_aspect(dev)); break; case VIVID_CID_TSTAMP_SRC: dev->tstamp_src_is_soe = ctrl->val; dev->vb_vid_cap_q.timestamp_flags &= ~V4L2_BUF_FLAG_TSTAMP_SRC_MASK; if (dev->tstamp_src_is_soe) dev->vb_vid_cap_q.timestamp_flags |= V4L2_BUF_FLAG_TSTAMP_SRC_SOE; break; case VIVID_CID_MAX_EDID_BLOCKS: dev->edid_max_blocks = ctrl->val; if (dev->edid_blocks > dev->edid_max_blocks) dev->edid_blocks = dev->edid_max_blocks; break; case VIVID_CID_HDMI_IS_CONNECTED_TO_OUTPUT(0) ... VIVID_CID_HDMI_IS_CONNECTED_TO_OUTPUT(14): hdmi_index = ctrl->id - VIVID_CID_HDMI_IS_CONNECTED_TO_OUTPUT(0); output_inst = vivid_ctrl_hdmi_to_output_instance[ctrl->cur.val]; index = vivid_ctrl_hdmi_to_output_index[ctrl->cur.val]; input_index = dev->hdmi_index_to_input_index[hdmi_index]; dev->input_is_connected_to_output[input_index] = ctrl->val; if (output_inst) { output_inst->output_to_input_instance[index] = NULL; vivid_update_outputs(output_inst); cec_phys_addr_invalidate(output_inst->cec_tx_adap[index]); } if (ctrl->val >= FIXED_MENU_ITEMS) { output_inst = vivid_ctrl_hdmi_to_output_instance[ctrl->val]; index = vivid_ctrl_hdmi_to_output_index[ctrl->val]; output_inst->output_to_input_instance[index] = dev; output_inst->output_to_input_index[index] = dev->hdmi_index_to_input_index[hdmi_index]; } spin_lock(&hdmi_output_skip_mask_lock); hdmi_to_output_menu_skip_mask &= ~(1ULL << ctrl->cur.val); if (ctrl->val >= FIXED_MENU_ITEMS) hdmi_to_output_menu_skip_mask |= 1ULL << ctrl->val; spin_unlock(&hdmi_output_skip_mask_lock); vivid_update_power_present(dev); vivid_update_quality(dev); vivid_send_input_source_change(dev, dev->hdmi_index_to_input_index[hdmi_index]); if (ctrl->val < FIXED_MENU_ITEMS && ctrl->cur.val < FIXED_MENU_ITEMS) break; spin_lock(&hdmi_output_skip_mask_lock); hdmi_input_update_outputs_mask |= 1 << dev->inst; spin_unlock(&hdmi_output_skip_mask_lock); queue_work(update_hdmi_ctrls_workqueue, &dev->update_hdmi_ctrl_work); break; case VIVID_CID_SVID_IS_CONNECTED_TO_OUTPUT(0) ... VIVID_CID_SVID_IS_CONNECTED_TO_OUTPUT(15): svid_index = ctrl->id - VIVID_CID_SVID_IS_CONNECTED_TO_OUTPUT(0); output_inst = vivid_ctrl_svid_to_output_instance[ctrl->cur.val]; index = vivid_ctrl_svid_to_output_index[ctrl->cur.val]; input_index = dev->svid_index_to_input_index[svid_index]; dev->input_is_connected_to_output[input_index] = ctrl->val; if (output_inst) output_inst->output_to_input_instance[index] = NULL; if (ctrl->val >= FIXED_MENU_ITEMS) { output_inst = vivid_ctrl_svid_to_output_instance[ctrl->val]; index = vivid_ctrl_svid_to_output_index[ctrl->val]; output_inst->output_to_input_instance[index] = dev; output_inst->output_to_input_index[index] = dev->svid_index_to_input_index[svid_index]; } spin_lock(&svid_output_skip_mask_lock); svid_to_output_menu_skip_mask &= ~(1ULL << ctrl->cur.val); if (ctrl->val >= FIXED_MENU_ITEMS) svid_to_output_menu_skip_mask |= 1ULL << ctrl->val; spin_unlock(&svid_output_skip_mask_lock); vivid_update_quality(dev); vivid_send_input_source_change(dev, dev->svid_index_to_input_index[svid_index]); if (ctrl->val < FIXED_MENU_ITEMS && ctrl->cur.val < FIXED_MENU_ITEMS) break; queue_work(update_svid_ctrls_workqueue, &dev->update_svid_ctrl_work); break; } return 0; }"
870----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50127/bad/sch_taprio.c----taprio_change,"static int taprio_change(struct Qdisc *sch, struct nlattr *opt, struct netlink_ext_ack *extack) { struct nlattr *tb[TCA_TAPRIO_ATTR_MAX + 1] = { }; struct sched_gate_list *oper, *admin, *new_admin; struct taprio_sched *q = qdisc_priv(sch); struct net_device *dev = qdisc_dev(sch); struct tc_mqprio_qopt *mqprio = NULL; unsigned long flags; ktime_t start; int i, err; err = nla_parse_nested_deprecated(tb, TCA_TAPRIO_ATTR_MAX, opt, taprio_policy, extack); if (err < 0) return err; if (tb[TCA_TAPRIO_ATTR_PRIOMAP]) mqprio = nla_data(tb[TCA_TAPRIO_ATTR_PRIOMAP]); err = taprio_new_flags(tb[TCA_TAPRIO_ATTR_FLAGS], q->flags, extack); if (err < 0) return err; q->flags = err; err = taprio_parse_mqprio_opt(dev, mqprio, extack, q->flags); if (err < 0) return err; new_admin = kzalloc(sizeof(*new_admin), GFP_KERNEL); if (!new_admin) { NL_SET_ERR_MSG(extack, ""Not enough memory for a new schedule""); return -ENOMEM; } INIT_LIST_HEAD(&new_admin->entries); rcu_read_lock(); oper = rcu_dereference(q->oper_sched); admin = rcu_dereference(q->admin_sched); rcu_read_unlock(); if (!taprio_mqprio_cmp(dev, mqprio)) mqprio = NULL; if (mqprio && (oper || admin)) { NL_SET_ERR_MSG(extack, ""Changing the traffic mapping of a running schedule is not supported""); err = -ENOTSUPP; goto free_sched; } err = parse_taprio_schedule(q, tb, new_admin, extack); if (err < 0) goto free_sched; if (new_admin->num_entries == 0) { NL_SET_ERR_MSG(extack, ""There should be at least one entry in the schedule""); err = -EINVAL; goto free_sched; } err = taprio_parse_clockid(sch, tb, extack); if (err < 0) goto free_sched; taprio_set_picos_per_byte(dev, q); if (mqprio) { err = netdev_set_num_tc(dev, mqprio->num_tc); if (err) goto free_sched; for (i = 0; i < mqprio->num_tc; i++) netdev_set_tc_queue(dev, i, mqprio->count[i], mqprio->offset[i]); for (i = 0; i <= TC_BITMASK; i++) netdev_set_prio_tc_map(dev, i, mqprio->prio_tc_map[i]); } if (FULL_OFFLOAD_IS_ENABLED(q->flags)) err = taprio_enable_offload(dev, q, new_admin, extack); else err = taprio_disable_offload(dev, q, extack); if (err) goto free_sched; spin_lock_bh(qdisc_lock(sch)); if (tb[TCA_TAPRIO_ATTR_TXTIME_DELAY]) { if (!TXTIME_ASSIST_IS_ENABLED(q->flags)) { NL_SET_ERR_MSG_MOD(extack, ""txtime-delay can only be set when txtime-assist mode is enabled""); err = -EINVAL; goto unlock; } q->txtime_delay = nla_get_u32(tb[TCA_TAPRIO_ATTR_TXTIME_DELAY]); } if (!TXTIME_ASSIST_IS_ENABLED(q->flags) && !FULL_OFFLOAD_IS_ENABLED(q->flags) && !hrtimer_active(&q->advance_timer)) { hrtimer_init(&q->advance_timer, q->clockid, HRTIMER_MODE_ABS); q->advance_timer.function = advance_sched; } if (FULL_OFFLOAD_IS_ENABLED(q->flags)) { q->dequeue = taprio_dequeue_offload; q->peek = taprio_peek_offload; } else { q->dequeue = taprio_dequeue_soft; q->peek = taprio_peek_soft; } err = taprio_get_start_time(sch, new_admin, &start); if (err < 0) { NL_SET_ERR_MSG(extack, ""Internal error: failed get start time""); goto unlock; } setup_txtime(q, new_admin, start); if (TXTIME_ASSIST_IS_ENABLED(q->flags)) { if (!oper) { rcu_assign_pointer(q->oper_sched, new_admin); err = 0; new_admin = NULL; goto unlock; } admin = rcu_replace_pointer(q->admin_sched, new_admin, lockdep_rtnl_is_held()); if (admin) call_rcu(&admin->rcu, taprio_free_sched_cb); } else { setup_first_close_time(q, new_admin, start); spin_lock_irqsave(&q->current_entry_lock, flags); taprio_start_sched(sch, start, new_admin); <S2SV_StartVul> rcu_assign_pointer(q->admin_sched, new_admin); <S2SV_EndVul> if (admin) call_rcu(&admin->rcu, taprio_free_sched_cb); spin_unlock_irqrestore(&q->current_entry_lock, flags); if (FULL_OFFLOAD_IS_ENABLED(q->flags)) taprio_offload_config_changed(q); } new_admin = NULL; err = 0; unlock: spin_unlock_bh(qdisc_lock(sch)); free_sched: if (new_admin) call_rcu(&new_admin->rcu, taprio_free_sched_cb); return err; }","- rcu_assign_pointer(q->admin_sched, new_admin);
+ admin = rcu_replace_pointer(q->admin_sched, new_admin,
+ lockdep_rtnl_is_held());","static int taprio_change(struct Qdisc *sch, struct nlattr *opt, struct netlink_ext_ack *extack) { struct nlattr *tb[TCA_TAPRIO_ATTR_MAX + 1] = { }; struct sched_gate_list *oper, *admin, *new_admin; struct taprio_sched *q = qdisc_priv(sch); struct net_device *dev = qdisc_dev(sch); struct tc_mqprio_qopt *mqprio = NULL; unsigned long flags; ktime_t start; int i, err; err = nla_parse_nested_deprecated(tb, TCA_TAPRIO_ATTR_MAX, opt, taprio_policy, extack); if (err < 0) return err; if (tb[TCA_TAPRIO_ATTR_PRIOMAP]) mqprio = nla_data(tb[TCA_TAPRIO_ATTR_PRIOMAP]); err = taprio_new_flags(tb[TCA_TAPRIO_ATTR_FLAGS], q->flags, extack); if (err < 0) return err; q->flags = err; err = taprio_parse_mqprio_opt(dev, mqprio, extack, q->flags); if (err < 0) return err; new_admin = kzalloc(sizeof(*new_admin), GFP_KERNEL); if (!new_admin) { NL_SET_ERR_MSG(extack, ""Not enough memory for a new schedule""); return -ENOMEM; } INIT_LIST_HEAD(&new_admin->entries); rcu_read_lock(); oper = rcu_dereference(q->oper_sched); admin = rcu_dereference(q->admin_sched); rcu_read_unlock(); if (!taprio_mqprio_cmp(dev, mqprio)) mqprio = NULL; if (mqprio && (oper || admin)) { NL_SET_ERR_MSG(extack, ""Changing the traffic mapping of a running schedule is not supported""); err = -ENOTSUPP; goto free_sched; } err = parse_taprio_schedule(q, tb, new_admin, extack); if (err < 0) goto free_sched; if (new_admin->num_entries == 0) { NL_SET_ERR_MSG(extack, ""There should be at least one entry in the schedule""); err = -EINVAL; goto free_sched; } err = taprio_parse_clockid(sch, tb, extack); if (err < 0) goto free_sched; taprio_set_picos_per_byte(dev, q); if (mqprio) { err = netdev_set_num_tc(dev, mqprio->num_tc); if (err) goto free_sched; for (i = 0; i < mqprio->num_tc; i++) netdev_set_tc_queue(dev, i, mqprio->count[i], mqprio->offset[i]); for (i = 0; i <= TC_BITMASK; i++) netdev_set_prio_tc_map(dev, i, mqprio->prio_tc_map[i]); } if (FULL_OFFLOAD_IS_ENABLED(q->flags)) err = taprio_enable_offload(dev, q, new_admin, extack); else err = taprio_disable_offload(dev, q, extack); if (err) goto free_sched; spin_lock_bh(qdisc_lock(sch)); if (tb[TCA_TAPRIO_ATTR_TXTIME_DELAY]) { if (!TXTIME_ASSIST_IS_ENABLED(q->flags)) { NL_SET_ERR_MSG_MOD(extack, ""txtime-delay can only be set when txtime-assist mode is enabled""); err = -EINVAL; goto unlock; } q->txtime_delay = nla_get_u32(tb[TCA_TAPRIO_ATTR_TXTIME_DELAY]); } if (!TXTIME_ASSIST_IS_ENABLED(q->flags) && !FULL_OFFLOAD_IS_ENABLED(q->flags) && !hrtimer_active(&q->advance_timer)) { hrtimer_init(&q->advance_timer, q->clockid, HRTIMER_MODE_ABS); q->advance_timer.function = advance_sched; } if (FULL_OFFLOAD_IS_ENABLED(q->flags)) { q->dequeue = taprio_dequeue_offload; q->peek = taprio_peek_offload; } else { q->dequeue = taprio_dequeue_soft; q->peek = taprio_peek_soft; } err = taprio_get_start_time(sch, new_admin, &start); if (err < 0) { NL_SET_ERR_MSG(extack, ""Internal error: failed get start time""); goto unlock; } setup_txtime(q, new_admin, start); if (TXTIME_ASSIST_IS_ENABLED(q->flags)) { if (!oper) { rcu_assign_pointer(q->oper_sched, new_admin); err = 0; new_admin = NULL; goto unlock; } admin = rcu_replace_pointer(q->admin_sched, new_admin, lockdep_rtnl_is_held()); if (admin) call_rcu(&admin->rcu, taprio_free_sched_cb); } else { setup_first_close_time(q, new_admin, start); spin_lock_irqsave(&q->current_entry_lock, flags); taprio_start_sched(sch, start, new_admin); admin = rcu_replace_pointer(q->admin_sched, new_admin, lockdep_rtnl_is_held()); if (admin) call_rcu(&admin->rcu, taprio_free_sched_cb); spin_unlock_irqrestore(&q->current_entry_lock, flags); if (FULL_OFFLOAD_IS_ENABLED(q->flags)) taprio_offload_config_changed(q); } new_admin = NULL; err = 0; unlock: spin_unlock_bh(qdisc_lock(sch)); free_sched: if (new_admin) call_rcu(&new_admin->rcu, taprio_free_sched_cb); return err; }"
599----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49920/bad/dcn20_hwseq.c----dcn20_post_unlock_program_front_end,"void dcn20_post_unlock_program_front_end( struct dc *dc, struct dc_state *context) { int i; const unsigned int TIMEOUT_FOR_PIPE_ENABLE_US = 100000; unsigned int polling_interval_us = 1; struct dce_hwseq *hwseq = dc->hwseq; for (i = 0; i < dc->res_pool->pipe_count; i++) if (resource_is_pipe_type(&dc->current_state->res_ctx.pipe_ctx[i], OPP_HEAD) && !resource_is_pipe_type(&context->res_ctx.pipe_ctx[i], OPP_HEAD)) post_unlock_reset_opp(dc, &dc->current_state->res_ctx.pipe_ctx[i]); for (i = 0; i < dc->res_pool->pipe_count; i++) if (context->res_ctx.pipe_ctx[i].update_flags.bits.disable) dc->hwss.disable_plane(dc, dc->current_state, &dc->current_state->res_ctx.pipe_ctx[i]); for (i = 0; i < dc->res_pool->pipe_count; i++) { struct pipe_ctx *pipe = &context->res_ctx.pipe_ctx[i]; if (pipe->plane_state && !pipe->top_pipe && pipe->update_flags.bits.enable && dc_state_get_pipe_subvp_type(context, pipe) != SUBVP_PHANTOM) { struct hubp *hubp = pipe->plane_res.hubp; int j = 0; for (j = 0; j < TIMEOUT_FOR_PIPE_ENABLE_US / polling_interval_us && hubp->funcs->hubp_is_flip_pending(hubp); j++) udelay(polling_interval_us); } } for (i = 0; i < dc->res_pool->pipe_count; i++) { struct pipe_ctx *pipe = &context->res_ctx.pipe_ctx[i]; struct pipe_ctx *old_pipe = &dc->current_state->res_ctx.pipe_ctx[i]; if (resource_is_pipe_type(old_pipe, OTG_MASTER) && resource_is_pipe_type(pipe, OTG_MASTER) && resource_get_odm_slice_count(old_pipe) < resource_get_odm_slice_count(pipe) && dc_state_get_pipe_subvp_type(context, pipe) != SUBVP_PHANTOM) { int j = 0; struct timing_generator *tg = pipe->stream_res.tg; if (tg->funcs->get_double_buffer_pending) { for (j = 0; j < TIMEOUT_FOR_PIPE_ENABLE_US / polling_interval_us && tg->funcs->get_double_buffer_pending(tg); j++) udelay(polling_interval_us); } } } if (dc->res_pool->hubbub->funcs->force_pstate_change_control) dc->res_pool->hubbub->funcs->force_pstate_change_control( dc->res_pool->hubbub, false, false); for (i = 0; i < dc->res_pool->pipe_count; i++) { struct pipe_ctx *pipe = &context->res_ctx.pipe_ctx[i]; if (pipe->plane_state && !pipe->top_pipe) { while (pipe) { if (pipe->stream && dc_state_get_pipe_subvp_type(context, pipe) == SUBVP_PHANTOM) { if (dc->hwss.apply_update_flags_for_phantom) dc->hwss.apply_update_flags_for_phantom(pipe); if (dc->hwss.update_phantom_vp_position) dc->hwss.update_phantom_vp_position(dc, context, pipe); dcn20_program_pipe(dc, pipe, context); } pipe = pipe->bottom_pipe; } } } <S2SV_StartVul> if (hwseq && hwseq->funcs.update_force_pstate) <S2SV_EndVul> dc->hwseq->funcs.update_force_pstate(dc, context); if (hwseq->funcs.program_mall_pipe_config) hwseq->funcs.program_mall_pipe_config(dc, context); if (hwseq->wa.DEGVIDCN21) dc->res_pool->hubbub->funcs->apply_DEDCN21_147_wa(dc->res_pool->hubbub); if (hwseq->wa.disallow_self_refresh_during_multi_plane_transition) { if (dc->current_state->stream_status[0].plane_count == 1 && context->stream_status[0].plane_count > 1) { struct timing_generator *tg = dc->res_pool->timing_generators[0]; dc->res_pool->hubbub->funcs->allow_self_refresh_control(dc->res_pool->hubbub, false); hwseq->wa_state.disallow_self_refresh_during_multi_plane_transition_applied = true; hwseq->wa_state.disallow_self_refresh_during_multi_plane_transition_applied_on_frame = tg->funcs->get_frame_count(tg); } } }","- if (hwseq && hwseq->funcs.update_force_pstate)
+ if (!hwseq)
+ return;
+ if (hwseq->funcs.update_force_pstate)","void dcn20_post_unlock_program_front_end( struct dc *dc, struct dc_state *context) { int i; const unsigned int TIMEOUT_FOR_PIPE_ENABLE_US = 100000; unsigned int polling_interval_us = 1; struct dce_hwseq *hwseq = dc->hwseq; for (i = 0; i < dc->res_pool->pipe_count; i++) if (resource_is_pipe_type(&dc->current_state->res_ctx.pipe_ctx[i], OPP_HEAD) && !resource_is_pipe_type(&context->res_ctx.pipe_ctx[i], OPP_HEAD)) post_unlock_reset_opp(dc, &dc->current_state->res_ctx.pipe_ctx[i]); for (i = 0; i < dc->res_pool->pipe_count; i++) if (context->res_ctx.pipe_ctx[i].update_flags.bits.disable) dc->hwss.disable_plane(dc, dc->current_state, &dc->current_state->res_ctx.pipe_ctx[i]); for (i = 0; i < dc->res_pool->pipe_count; i++) { struct pipe_ctx *pipe = &context->res_ctx.pipe_ctx[i]; if (pipe->plane_state && !pipe->top_pipe && pipe->update_flags.bits.enable && dc_state_get_pipe_subvp_type(context, pipe) != SUBVP_PHANTOM) { struct hubp *hubp = pipe->plane_res.hubp; int j = 0; for (j = 0; j < TIMEOUT_FOR_PIPE_ENABLE_US / polling_interval_us && hubp->funcs->hubp_is_flip_pending(hubp); j++) udelay(polling_interval_us); } } for (i = 0; i < dc->res_pool->pipe_count; i++) { struct pipe_ctx *pipe = &context->res_ctx.pipe_ctx[i]; struct pipe_ctx *old_pipe = &dc->current_state->res_ctx.pipe_ctx[i]; if (resource_is_pipe_type(old_pipe, OTG_MASTER) && resource_is_pipe_type(pipe, OTG_MASTER) && resource_get_odm_slice_count(old_pipe) < resource_get_odm_slice_count(pipe) && dc_state_get_pipe_subvp_type(context, pipe) != SUBVP_PHANTOM) { int j = 0; struct timing_generator *tg = pipe->stream_res.tg; if (tg->funcs->get_double_buffer_pending) { for (j = 0; j < TIMEOUT_FOR_PIPE_ENABLE_US / polling_interval_us && tg->funcs->get_double_buffer_pending(tg); j++) udelay(polling_interval_us); } } } if (dc->res_pool->hubbub->funcs->force_pstate_change_control) dc->res_pool->hubbub->funcs->force_pstate_change_control( dc->res_pool->hubbub, false, false); for (i = 0; i < dc->res_pool->pipe_count; i++) { struct pipe_ctx *pipe = &context->res_ctx.pipe_ctx[i]; if (pipe->plane_state && !pipe->top_pipe) { while (pipe) { if (pipe->stream && dc_state_get_pipe_subvp_type(context, pipe) == SUBVP_PHANTOM) { if (dc->hwss.apply_update_flags_for_phantom) dc->hwss.apply_update_flags_for_phantom(pipe); if (dc->hwss.update_phantom_vp_position) dc->hwss.update_phantom_vp_position(dc, context, pipe); dcn20_program_pipe(dc, pipe, context); } pipe = pipe->bottom_pipe; } } } if (!hwseq) return; if (hwseq->funcs.update_force_pstate) dc->hwseq->funcs.update_force_pstate(dc, context); if (hwseq->funcs.program_mall_pipe_config) hwseq->funcs.program_mall_pipe_config(dc, context); if (hwseq->wa.DEGVIDCN21) dc->res_pool->hubbub->funcs->apply_DEDCN21_147_wa(dc->res_pool->hubbub); if (hwseq->wa.disallow_self_refresh_during_multi_plane_transition) { if (dc->current_state->stream_status[0].plane_count == 1 && context->stream_status[0].plane_count > 1) { struct timing_generator *tg = dc->res_pool->timing_generators[0]; dc->res_pool->hubbub->funcs->allow_self_refresh_control(dc->res_pool->hubbub, false); hwseq->wa_state.disallow_self_refresh_during_multi_plane_transition_applied = true; hwseq->wa_state.disallow_self_refresh_during_multi_plane_transition_applied_on_frame = tg->funcs->get_frame_count(tg); } } }"
1543----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2025-21696/bad/hugetlb.c----move_huge_pte,"static void move_huge_pte(struct vm_area_struct *vma, unsigned long old_addr, unsigned long new_addr, pte_t *src_pte, pte_t *dst_pte, unsigned long sz) { struct hstate *h = hstate_vma(vma); struct mm_struct *mm = vma->vm_mm; spinlock_t *src_ptl, *dst_ptl; pte_t pte; dst_ptl = huge_pte_lock(h, mm, dst_pte); src_ptl = huge_pte_lockptr(h, mm, src_pte); if (src_ptl != dst_ptl) spin_lock_nested(src_ptl, SINGLE_DEPTH_NESTING); pte = huge_ptep_get_and_clear(mm, old_addr, src_pte); <S2SV_StartVul> set_huge_pte_at(mm, new_addr, dst_pte, pte, sz); <S2SV_EndVul> if (src_ptl != dst_ptl) spin_unlock(src_ptl); spin_unlock(dst_ptl); }","- set_huge_pte_at(mm, new_addr, dst_pte, pte, sz);
+ bool need_clear_uffd_wp = vma_has_uffd_without_event_remap(vma);
+ if (need_clear_uffd_wp && pte_marker_uffd_wp(pte))
+ huge_pte_clear(mm, new_addr, dst_pte, sz);
+ else {
+ if (need_clear_uffd_wp) {
+ if (pte_present(pte))
+ pte = huge_pte_clear_uffd_wp(pte);
+ else if (is_swap_pte(pte))
+ pte = pte_swp_clear_uffd_wp(pte);
+ }
+ set_huge_pte_at(mm, new_addr, dst_pte, pte, sz);
+ }","static void move_huge_pte(struct vm_area_struct *vma, unsigned long old_addr, unsigned long new_addr, pte_t *src_pte, pte_t *dst_pte, unsigned long sz) { bool need_clear_uffd_wp = vma_has_uffd_without_event_remap(vma); struct hstate *h = hstate_vma(vma); struct mm_struct *mm = vma->vm_mm; spinlock_t *src_ptl, *dst_ptl; pte_t pte; dst_ptl = huge_pte_lock(h, mm, dst_pte); src_ptl = huge_pte_lockptr(h, mm, src_pte); if (src_ptl != dst_ptl) spin_lock_nested(src_ptl, SINGLE_DEPTH_NESTING); pte = huge_ptep_get_and_clear(mm, old_addr, src_pte); if (need_clear_uffd_wp && pte_marker_uffd_wp(pte)) huge_pte_clear(mm, new_addr, dst_pte, sz); else { if (need_clear_uffd_wp) { if (pte_present(pte)) pte = huge_pte_clear_uffd_wp(pte); else if (is_swap_pte(pte)) pte = pte_swp_clear_uffd_wp(pte); } set_huge_pte_at(mm, new_addr, dst_pte, pte, sz); } if (src_ptl != dst_ptl) spin_unlock(src_ptl); spin_unlock(dst_ptl); }"
1149----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53126/bad/snet_main.c----psnet_open_pf_bar,"static int psnet_open_pf_bar(struct pci_dev *pdev, struct psnet *psnet) { char name[50]; int ret, i, mask = 0; for (i = 0; i < PCI_STD_NUM_BARS; i++) { if (pci_resource_len(pdev, i)) mask |= (1 << i); } if (!mask) { SNET_ERR(pdev, ""Failed to find a PCI BAR\n""); return -ENODEV; } <S2SV_StartVul> snprintf(name, sizeof(name), ""psnet[%s]-bars"", pci_name(pdev)); <S2SV_EndVul> ret = pcim_iomap_regions(pdev, mask, name); if (ret) { SNET_ERR(pdev, ""Failed to request and map PCI BARs\n""); return ret; } for (i = 0; i < PCI_STD_NUM_BARS; i++) { if (mask & (1 << i)) psnet->bars[i] = pcim_iomap_table(pdev)[i]; } return 0; }","- snprintf(name, sizeof(name), ""psnet[%s]-bars"", pci_name(pdev));
+ name = devm_kasprintf(&pdev->dev, GFP_KERNEL, ""psnet[%s]-bars"", pci_name(pdev));
+ if (!name)
+ return -ENOMEM;","static int psnet_open_pf_bar(struct pci_dev *pdev, struct psnet *psnet) { char *name; int ret, i, mask = 0; for (i = 0; i < PCI_STD_NUM_BARS; i++) { if (pci_resource_len(pdev, i)) mask |= (1 << i); } if (!mask) { SNET_ERR(pdev, ""Failed to find a PCI BAR\n""); return -ENODEV; } name = devm_kasprintf(&pdev->dev, GFP_KERNEL, ""psnet[%s]-bars"", pci_name(pdev)); if (!name) return -ENOMEM; ret = pcim_iomap_regions(pdev, mask, name); if (ret) { SNET_ERR(pdev, ""Failed to request and map PCI BARs\n""); return ret; } for (i = 0; i < PCI_STD_NUM_BARS; i++) { if (mask & (1 << i)) psnet->bars[i] = pcim_iomap_table(pdev)[i]; } return 0; }"
372----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46835/bad/gfx_v11_0.c----gfx_v11_0_hw_init,"static int gfx_v11_0_hw_init(void *handle) { int r; struct amdgpu_device *adev = (struct amdgpu_device *)handle; if (adev->firmware.load_type == AMDGPU_FW_LOAD_RLC_BACKDOOR_AUTO) { if (adev->gfx.imu.funcs) { if (adev->gfx.imu.funcs->program_rlc_ram) adev->gfx.imu.funcs->program_rlc_ram(adev); } <S2SV_StartVul> r = gfx_v11_0_rlc_backdoor_autoload_enable(adev); <S2SV_EndVul> <S2SV_StartVul> if (r) <S2SV_EndVul> <S2SV_StartVul> return r; <S2SV_EndVul> } else { if (adev->firmware.load_type == AMDGPU_FW_LOAD_DIRECT) { if (adev->gfx.imu.funcs && (amdgpu_dpm > 0)) { if (adev->gfx.imu.funcs->load_microcode) adev->gfx.imu.funcs->load_microcode(adev); if (adev->gfx.imu.funcs->setup_imu) adev->gfx.imu.funcs->setup_imu(adev); if (adev->gfx.imu.funcs->start_imu) adev->gfx.imu.funcs->start_imu(adev); } gfx_v11_0_disable_gpa_mode(adev); } } if ((adev->firmware.load_type == AMDGPU_FW_LOAD_RLC_BACKDOOR_AUTO) || (adev->firmware.load_type == AMDGPU_FW_LOAD_PSP)) { r = gfx_v11_0_wait_for_rlc_autoload_complete(adev); if (r) { dev_err(adev->dev, ""(%d) failed to wait rlc autoload complete\n"", r); return r; } } adev->gfx.is_poweron = true; if(get_gb_addr_config(adev)) DRM_WARN(""Invalid gb_addr_config !\n""); if (adev->firmware.load_type == AMDGPU_FW_LOAD_PSP && adev->gfx.rs64_enable) gfx_v11_0_config_gfx_rs64(adev); r = gfx_v11_0_gfxhub_enable(adev); if (r) return r; if (!amdgpu_emu_mode) gfx_v11_0_init_golden_registers(adev); if ((adev->firmware.load_type == AMDGPU_FW_LOAD_DIRECT) || (adev->firmware.load_type == AMDGPU_FW_LOAD_RLC_BACKDOOR_AUTO && amdgpu_dpm == 1)) { if (!(adev->flags & AMD_IS_APU)) { r = amdgpu_pm_load_smu_firmware(adev, NULL); if (r) return r; } } gfx_v11_0_constants_init(adev); if (adev->firmware.load_type != AMDGPU_FW_LOAD_PSP) gfx_v11_0_select_cp_fw_arch(adev); if (adev->nbio.funcs->gc_doorbell_init) adev->nbio.funcs->gc_doorbell_init(adev); r = gfx_v11_0_rlc_resume(adev); if (r) return r; gfx_v11_0_tcp_harvest(adev); r = gfx_v11_0_cp_resume(adev); if (r) return r; if (!adev->gfx.imu_fw_version) adev->gfx.imu_fw_version = RREG32_SOC15(GC, 0, regGFX_IMU_SCRATCH_0); return r; }","- r = gfx_v11_0_rlc_backdoor_autoload_enable(adev);
- if (r)
- return r;
+ r = gfx_v11_0_rlc_backdoor_autoload_enable(adev);
+ if (r)
+ return r;","static int gfx_v11_0_hw_init(void *handle) { int r; struct amdgpu_device *adev = (struct amdgpu_device *)handle; if (adev->firmware.load_type == AMDGPU_FW_LOAD_RLC_BACKDOOR_AUTO) { if (adev->gfx.imu.funcs) { if (adev->gfx.imu.funcs->program_rlc_ram) adev->gfx.imu.funcs->program_rlc_ram(adev); r = gfx_v11_0_rlc_backdoor_autoload_enable(adev); if (r) return r; } } else { if (adev->firmware.load_type == AMDGPU_FW_LOAD_DIRECT) { if (adev->gfx.imu.funcs && (amdgpu_dpm > 0)) { if (adev->gfx.imu.funcs->load_microcode) adev->gfx.imu.funcs->load_microcode(adev); if (adev->gfx.imu.funcs->setup_imu) adev->gfx.imu.funcs->setup_imu(adev); if (adev->gfx.imu.funcs->start_imu) adev->gfx.imu.funcs->start_imu(adev); } gfx_v11_0_disable_gpa_mode(adev); } } if ((adev->firmware.load_type == AMDGPU_FW_LOAD_RLC_BACKDOOR_AUTO) || (adev->firmware.load_type == AMDGPU_FW_LOAD_PSP)) { r = gfx_v11_0_wait_for_rlc_autoload_complete(adev); if (r) { dev_err(adev->dev, ""(%d) failed to wait rlc autoload complete\n"", r); return r; } } adev->gfx.is_poweron = true; if(get_gb_addr_config(adev)) DRM_WARN(""Invalid gb_addr_config !\n""); if (adev->firmware.load_type == AMDGPU_FW_LOAD_PSP && adev->gfx.rs64_enable) gfx_v11_0_config_gfx_rs64(adev); r = gfx_v11_0_gfxhub_enable(adev); if (r) return r; if (!amdgpu_emu_mode) gfx_v11_0_init_golden_registers(adev); if ((adev->firmware.load_type == AMDGPU_FW_LOAD_DIRECT) || (adev->firmware.load_type == AMDGPU_FW_LOAD_RLC_BACKDOOR_AUTO && amdgpu_dpm == 1)) { if (!(adev->flags & AMD_IS_APU)) { r = amdgpu_pm_load_smu_firmware(adev, NULL); if (r) return r; } } gfx_v11_0_constants_init(adev); if (adev->firmware.load_type != AMDGPU_FW_LOAD_PSP) gfx_v11_0_select_cp_fw_arch(adev); if (adev->nbio.funcs->gc_doorbell_init) adev->nbio.funcs->gc_doorbell_init(adev); r = gfx_v11_0_rlc_resume(adev); if (r) return r; gfx_v11_0_tcp_harvest(adev); r = gfx_v11_0_cp_resume(adev); if (r) return r; if (!adev->gfx.imu_fw_version) adev->gfx.imu_fw_version = RREG32_SOC15(GC, 0, regGFX_IMU_SCRATCH_0); return r; }"
311----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46780/bad/sysfs.c----nilfs_dev_revision_show,"ssize_t nilfs_dev_revision_show(struct nilfs_dev_attr *attr, struct the_nilfs *nilfs, char *buf) { <S2SV_StartVul> struct nilfs_super_block **sbp = nilfs->ns_sbp; <S2SV_EndVul> <S2SV_StartVul> u32 major = le32_to_cpu(sbp[0]->s_rev_level); <S2SV_EndVul> <S2SV_StartVul> u16 minor = le16_to_cpu(sbp[0]->s_minor_rev_level); <S2SV_EndVul> return sysfs_emit(buf, ""%d.%d\n"", major, minor); }","- struct nilfs_super_block **sbp = nilfs->ns_sbp;
- u32 major = le32_to_cpu(sbp[0]->s_rev_level);
- u16 minor = le16_to_cpu(sbp[0]->s_minor_rev_level);
+ struct nilfs_super_block *raw_sb;
+ u32 major;
+ u16 minor;
+ down_read(&nilfs->ns_sem);
+ raw_sb = nilfs->ns_sbp[0];
+ major = le32_to_cpu(raw_sb->s_rev_level);
+ minor = le16_to_cpu(raw_sb->s_minor_rev_level);
+ up_read(&nilfs->ns_sem);","ssize_t nilfs_dev_revision_show(struct nilfs_dev_attr *attr, struct the_nilfs *nilfs, char *buf) { struct nilfs_super_block *raw_sb; u32 major; u16 minor; down_read(&nilfs->ns_sem); raw_sb = nilfs->ns_sbp[0]; major = le32_to_cpu(raw_sb->s_rev_level); minor = le16_to_cpu(raw_sb->s_minor_rev_level); up_read(&nilfs->ns_sem); return sysfs_emit(buf, ""%d.%d\n"", major, minor); }"
505----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47744/bad/kvm_main.c----kvm_online_cpu,static int kvm_online_cpu(unsigned int cpu) { int ret = 0; <S2SV_StartVul> mutex_lock(&kvm_lock); <S2SV_EndVul> if (kvm_usage_count) ret = __hardware_enable_nolock(); <S2SV_StartVul> mutex_unlock(&kvm_lock); <S2SV_EndVul> return ret; },"- mutex_lock(&kvm_lock);
- mutex_unlock(&kvm_lock);
+ mutex_lock(&kvm_usage_lock);
+ mutex_unlock(&kvm_usage_lock);",static int kvm_online_cpu(unsigned int cpu) { int ret = 0; mutex_lock(&kvm_usage_lock); if (kvm_usage_count) ret = __hardware_enable_nolock(); mutex_unlock(&kvm_usage_lock); return ret; }
900----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50147/bad/cmd.c----mlx5_cmd_trigger_completions,"static void mlx5_cmd_trigger_completions(struct mlx5_core_dev *dev) { struct mlx5_cmd *cmd = &dev->cmd; unsigned long bitmask; unsigned long flags; u64 vector; int i; mlx5_eq_synchronize_cmd_irq(dev); spin_lock_irqsave(&dev->cmd.alloc_lock, flags); <S2SV_StartVul> vector = ~dev->cmd.vars.bitmask & ((1ul << (1 << dev->cmd.vars.log_sz)) - 1); <S2SV_EndVul> if (!vector) goto no_trig; bitmask = vector; for_each_set_bit(i, &bitmask, (1 << cmd->vars.log_sz)) cmd_ent_get(cmd->ent_arr[i]); vector |= MLX5_TRIGGERED_CMD_COMP; spin_unlock_irqrestore(&dev->cmd.alloc_lock, flags); mlx5_core_dbg(dev, ""vector 0x%llx\n"", vector); mlx5_cmd_comp_handler(dev, vector, true); for_each_set_bit(i, &bitmask, (1 << cmd->vars.log_sz)) cmd_ent_put(cmd->ent_arr[i]); return; no_trig: spin_unlock_irqrestore(&dev->cmd.alloc_lock, flags); }","- vector = ~dev->cmd.vars.bitmask & ((1ul << (1 << dev->cmd.vars.log_sz)) - 1);
+ vector = ~dev->cmd.vars.bitmask & MLX5_CMD_MASK;","static void mlx5_cmd_trigger_completions(struct mlx5_core_dev *dev) { struct mlx5_cmd *cmd = &dev->cmd; unsigned long bitmask; unsigned long flags; u64 vector; int i; mlx5_eq_synchronize_cmd_irq(dev); spin_lock_irqsave(&dev->cmd.alloc_lock, flags); vector = ~dev->cmd.vars.bitmask & MLX5_CMD_MASK; if (!vector) goto no_trig; bitmask = vector; for_each_set_bit(i, &bitmask, (1 << cmd->vars.log_sz)) cmd_ent_get(cmd->ent_arr[i]); vector |= MLX5_TRIGGERED_CMD_COMP; spin_unlock_irqrestore(&dev->cmd.alloc_lock, flags); mlx5_core_dbg(dev, ""vector 0x%llx\n"", vector); mlx5_cmd_comp_handler(dev, vector, true); for_each_set_bit(i, &bitmask, (1 << cmd->vars.log_sz)) cmd_ent_put(cmd->ent_arr[i]); return; no_trig: spin_unlock_irqrestore(&dev->cmd.alloc_lock, flags); }"
637----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49940/bad/l2tp_core.c----l2tp_session_set_header_len,"void l2tp_session_set_header_len(struct l2tp_session *session, int version) { if (version == L2TP_HDR_VER_2) { session->hdr_len = 6; if (session->send_seq) session->hdr_len += 4; } else { session->hdr_len = 4 + session->cookie_len; session->hdr_len += l2tp_get_l2specific_len(session); <S2SV_StartVul> if (session->tunnel->encap == L2TP_ENCAPTYPE_UDP) <S2SV_EndVul> session->hdr_len += 4; } }",- if (session->tunnel->encap == L2TP_ENCAPTYPE_UDP),"#define pr_fmt(fmt) KBUILD_MODNAME "": "" fmt #include <linux/module.h> #include <linux/string.h> #include <linux/list.h> #include <linux/rculist.h> #include <linux/uaccess.h> #include <linux/kernel.h> #include <linux/spinlock.h> #include <linux/kthread.h> #include <linux/sched.h> #include <linux/slab.h> #include <linux/errno.h> #include <linux/jiffies.h> #include <linux/netdevice.h> #include <linux/net.h> #include <linux/inetdevice.h> #include <linux/skbuff.h> #include <linux/init.h> #include <linux/in.h> #include <linux/ip.h> #include <linux/udp.h> #include <linux/l2tp.h> #include <linux/sort.h> #include <linux/file.h> #include <linux/nsproxy.h> #include <net/net_namespace.h> #include <net/netns/generic.h> #include <net/dst.h> #include <net/ip.h> #include <net/udp.h> #include <net/udp_tunnel.h> #include <net/inet_common.h> #include <net/xfrm.h> #include <net/protocol.h> #include <net/inet6_connection_sock.h> #include <net/inet_ecn.h> #include <net/ip6_route.h> #include <net/ip6_checksum.h> #include <asm/byteorder.h> #include <linux/atomic.h> #include ""l2tp_core.h"" #define CREATE_TRACE_POINTS #include ""trace.h"" #define L2TP_DRV_VERSION ""V2.0"" #define L2TP_HDRFLAG_T 0x8000 #define L2TP_HDRFLAG_L 0x4000 #define L2TP_HDRFLAG_S 0x0800 #define L2TP_HDRFLAG_O 0x0200 #define L2TP_HDRFLAG_P 0x0100 #define L2TP_HDR_VER_MASK 0x000F #define L2TP_HDR_VER_2 0x0002 #define L2TP_HDR_VER_3 0x0003 #define L2TP_SLFLAG_S 0x40000000 #define L2TP_SL_SEQ_MASK 0x00ffffff #define L2TP_HDR_SIZE_MAX 14 #define L2TP_DEFAULT_DEBUG_FLAGS 0 struct l2tp_skb_cb { u32 ns; u16 has_seq; u16 length; unsigned long expires; };"
975----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50213/bad/drm_hdmi_state_helper_test.c----drm_test_check_broadcast_rgb_full_cea_mode_vic_1,"static void drm_test_check_broadcast_rgb_full_cea_mode_vic_1(struct kunit *test) { struct drm_atomic_helper_connector_hdmi_priv *priv; struct drm_modeset_acquire_ctx *ctx; struct drm_connector_state *conn_state; struct drm_atomic_state *state; struct drm_display_mode *mode; struct drm_connector *conn; struct drm_device *drm; struct drm_crtc *crtc; int ret; priv = drm_atomic_helper_connector_hdmi_init(test, BIT(HDMI_COLORSPACE_RGB), 8); KUNIT_ASSERT_NOT_NULL(test, priv); drm = &priv->drm; conn = &priv->connector; KUNIT_ASSERT_TRUE(test, conn->display_info.is_hdmi); ctx = drm_kunit_helper_acquire_ctx_alloc(test); KUNIT_ASSERT_NOT_ERR_OR_NULL(test, ctx); <S2SV_StartVul> mode = drm_display_mode_from_cea_vic(drm, 1); <S2SV_EndVul> KUNIT_ASSERT_NOT_NULL(test, mode); drm = &priv->drm; crtc = priv->crtc; ret = light_up_connector(test, drm, crtc, conn, mode, ctx); KUNIT_ASSERT_EQ(test, ret, 0); state = drm_kunit_helper_atomic_state_alloc(test, drm, ctx); KUNIT_ASSERT_NOT_ERR_OR_NULL(test, state); conn_state = drm_atomic_get_connector_state(state, conn); KUNIT_ASSERT_NOT_ERR_OR_NULL(test, conn_state); conn_state->hdmi.broadcast_rgb = DRM_HDMI_BROADCAST_RGB_FULL; ret = drm_atomic_check_only(state); KUNIT_ASSERT_EQ(test, ret, 0); conn_state = drm_atomic_get_connector_state(state, conn); KUNIT_ASSERT_NOT_ERR_OR_NULL(test, conn_state); KUNIT_ASSERT_EQ(test, conn_state->hdmi.broadcast_rgb, DRM_HDMI_BROADCAST_RGB_FULL); KUNIT_EXPECT_FALSE(test, conn_state->hdmi.is_limited_range); }","- mode = drm_display_mode_from_cea_vic(drm, 1);
+ mode = drm_kunit_display_mode_from_cea_vic(test, drm, 1);","static void drm_test_check_broadcast_rgb_full_cea_mode_vic_1(struct kunit *test) { struct drm_atomic_helper_connector_hdmi_priv *priv; struct drm_modeset_acquire_ctx *ctx; struct drm_connector_state *conn_state; struct drm_atomic_state *state; struct drm_display_mode *mode; struct drm_connector *conn; struct drm_device *drm; struct drm_crtc *crtc; int ret; priv = drm_atomic_helper_connector_hdmi_init(test, BIT(HDMI_COLORSPACE_RGB), 8); KUNIT_ASSERT_NOT_NULL(test, priv); drm = &priv->drm; conn = &priv->connector; KUNIT_ASSERT_TRUE(test, conn->display_info.is_hdmi); ctx = drm_kunit_helper_acquire_ctx_alloc(test); KUNIT_ASSERT_NOT_ERR_OR_NULL(test, ctx); mode = drm_kunit_display_mode_from_cea_vic(test, drm, 1); KUNIT_ASSERT_NOT_NULL(test, mode); drm = &priv->drm; crtc = priv->crtc; ret = light_up_connector(test, drm, crtc, conn, mode, ctx); KUNIT_ASSERT_EQ(test, ret, 0); state = drm_kunit_helper_atomic_state_alloc(test, drm, ctx); KUNIT_ASSERT_NOT_ERR_OR_NULL(test, state); conn_state = drm_atomic_get_connector_state(state, conn); KUNIT_ASSERT_NOT_ERR_OR_NULL(test, conn_state); conn_state->hdmi.broadcast_rgb = DRM_HDMI_BROADCAST_RGB_FULL; ret = drm_atomic_check_only(state); KUNIT_ASSERT_EQ(test, ret, 0); conn_state = drm_atomic_get_connector_state(state, conn); KUNIT_ASSERT_NOT_ERR_OR_NULL(test, conn_state); KUNIT_ASSERT_EQ(test, conn_state->hdmi.broadcast_rgb, DRM_HDMI_BROADCAST_RGB_FULL); KUNIT_EXPECT_FALSE(test, conn_state->hdmi.is_limited_range); }"
746----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50024/bad/af_netlink.c----__netlink_clear_multicast_users,"void __netlink_clear_multicast_users(struct sock *ksk, unsigned int group) { struct sock *sk; struct netlink_table *tbl = &nl_table[ksk->sk_protocol]; <S2SV_StartVul> sk_for_each_bound(sk, &tbl->mc_list) <S2SV_EndVul> netlink_update_socket_mc(nlk_sk(sk), group, 0); }","- sk_for_each_bound(sk, &tbl->mc_list)
+ struct hlist_node *tmp;
+ sk_for_each_bound_safe(sk, tmp, &tbl->mc_list)","void __netlink_clear_multicast_users(struct sock *ksk, unsigned int group) { struct sock *sk; struct netlink_table *tbl = &nl_table[ksk->sk_protocol]; struct hlist_node *tmp; sk_for_each_bound_safe(sk, tmp, &tbl->mc_list) netlink_update_socket_mc(nlk_sk(sk), group, 0); }"
212----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46693/bad/ucsi_glink.c----pmic_glink_ucsi_probe,"static int pmic_glink_ucsi_probe(struct auxiliary_device *adev, const struct auxiliary_device_id *id) { struct pmic_glink_ucsi *ucsi; struct device *dev = &adev->dev; const struct of_device_id *match; struct fwnode_handle *fwnode; int ret; ucsi = devm_kzalloc(dev, sizeof(*ucsi), GFP_KERNEL); if (!ucsi) return -ENOMEM; ucsi->dev = dev; dev_set_drvdata(dev, ucsi); INIT_WORK(&ucsi->notify_work, pmic_glink_ucsi_notify); INIT_WORK(&ucsi->register_work, pmic_glink_ucsi_register); init_completion(&ucsi->read_ack); init_completion(&ucsi->write_ack); init_completion(&ucsi->sync_ack); mutex_init(&ucsi->lock); ucsi->ucsi = ucsi_create(dev, &pmic_glink_ucsi_ops); if (IS_ERR(ucsi->ucsi)) return PTR_ERR(ucsi->ucsi); ret = devm_add_action_or_reset(dev, pmic_glink_ucsi_destroy, ucsi); if (ret) return ret; match = of_match_device(pmic_glink_ucsi_of_quirks, dev->parent); if (match) ucsi->ucsi->quirks = *(unsigned long *)match->data; ucsi_set_drvdata(ucsi->ucsi, ucsi); device_for_each_child_node(dev, fwnode) { struct gpio_desc *desc; u32 port; ret = fwnode_property_read_u32(fwnode, ""reg"", &port); if (ret < 0) { dev_err(dev, ""missing reg property of %pOFn\n"", fwnode); fwnode_handle_put(fwnode); return ret; } if (port >= PMIC_GLINK_MAX_PORTS) { dev_warn(dev, ""invalid connector number, ignoring\n""); continue; } desc = devm_gpiod_get_index_optional(&adev->dev, ""orientation"", port, GPIOD_IN); if (!desc) continue; if (IS_ERR(desc)) { fwnode_handle_put(fwnode); return dev_err_probe(dev, PTR_ERR(desc), ""unable to acquire orientation gpio\n""); } ucsi->port_orientation[port] = desc; } <S2SV_StartVul> ucsi->client = devm_pmic_glink_register_client(dev, <S2SV_EndVul> <S2SV_StartVul> PMIC_GLINK_OWNER_USBC, <S2SV_EndVul> <S2SV_StartVul> pmic_glink_ucsi_callback, <S2SV_EndVul> <S2SV_StartVul> pmic_glink_ucsi_pdr_notify, <S2SV_EndVul> <S2SV_StartVul> ucsi); <S2SV_EndVul> <S2SV_StartVul> return PTR_ERR_OR_ZERO(ucsi->client); <S2SV_EndVul> }","- ucsi->client = devm_pmic_glink_register_client(dev,
- PMIC_GLINK_OWNER_USBC,
- pmic_glink_ucsi_callback,
- pmic_glink_ucsi_pdr_notify,
- ucsi);
- return PTR_ERR_OR_ZERO(ucsi->client);
+ ucsi->client = devm_pmic_glink_client_alloc(dev, PMIC_GLINK_OWNER_USBC,
+ pmic_glink_ucsi_callback,
+ pmic_glink_ucsi_pdr_notify,
+ ucsi);
+ if (IS_ERR(ucsi->client))
+ return PTR_ERR(ucsi->client);
+ pmic_glink_client_register(ucsi->client);
+ return 0;","static int pmic_glink_ucsi_probe(struct auxiliary_device *adev, const struct auxiliary_device_id *id) { struct pmic_glink_ucsi *ucsi; struct device *dev = &adev->dev; const struct of_device_id *match; struct fwnode_handle *fwnode; int ret; ucsi = devm_kzalloc(dev, sizeof(*ucsi), GFP_KERNEL); if (!ucsi) return -ENOMEM; ucsi->dev = dev; dev_set_drvdata(dev, ucsi); INIT_WORK(&ucsi->notify_work, pmic_glink_ucsi_notify); INIT_WORK(&ucsi->register_work, pmic_glink_ucsi_register); init_completion(&ucsi->read_ack); init_completion(&ucsi->write_ack); init_completion(&ucsi->sync_ack); mutex_init(&ucsi->lock); ucsi->ucsi = ucsi_create(dev, &pmic_glink_ucsi_ops); if (IS_ERR(ucsi->ucsi)) return PTR_ERR(ucsi->ucsi); ret = devm_add_action_or_reset(dev, pmic_glink_ucsi_destroy, ucsi); if (ret) return ret; match = of_match_device(pmic_glink_ucsi_of_quirks, dev->parent); if (match) ucsi->ucsi->quirks = *(unsigned long *)match->data; ucsi_set_drvdata(ucsi->ucsi, ucsi); device_for_each_child_node(dev, fwnode) { struct gpio_desc *desc; u32 port; ret = fwnode_property_read_u32(fwnode, ""reg"", &port); if (ret < 0) { dev_err(dev, ""missing reg property of %pOFn\n"", fwnode); fwnode_handle_put(fwnode); return ret; } if (port >= PMIC_GLINK_MAX_PORTS) { dev_warn(dev, ""invalid connector number, ignoring\n""); continue; } desc = devm_gpiod_get_index_optional(&adev->dev, ""orientation"", port, GPIOD_IN); if (!desc) continue; if (IS_ERR(desc)) { fwnode_handle_put(fwnode); return dev_err_probe(dev, PTR_ERR(desc), ""unable to acquire orientation gpio\n""); } ucsi->port_orientation[port] = desc; } ucsi->client = devm_pmic_glink_client_alloc(dev, PMIC_GLINK_OWNER_USBC, pmic_glink_ucsi_callback, pmic_glink_ucsi_pdr_notify, ucsi); if (IS_ERR(ucsi->client)) return PTR_ERR(ucsi->client); pmic_glink_client_register(ucsi->client); return 0; }"
438----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47676/bad/hugetlb.c----hugetlb_no_page,"static vm_fault_t hugetlb_no_page(struct address_space *mapping, struct vm_fault *vmf) { struct vm_area_struct *vma = vmf->vma; struct mm_struct *mm = vma->vm_mm; struct hstate *h = hstate_vma(vma); vm_fault_t ret = VM_FAULT_SIGBUS; int anon_rmap = 0; unsigned long size; struct folio *folio; pte_t new_pte; bool new_folio, new_pagecache_folio = false; u32 hash = hugetlb_fault_mutex_hash(mapping, vmf->pgoff); if (is_vma_resv_set(vma, HPAGE_RESV_UNMAPPED)) { pr_warn_ratelimited(""PID %d killed due to inadequate hugepage pool\n"", current->pid); goto out; } new_folio = false; folio = filemap_lock_hugetlb_folio(h, mapping, vmf->pgoff); if (IS_ERR(folio)) { size = i_size_read(mapping->host) >> huge_page_shift(h); if (vmf->pgoff >= size) goto out; if (userfaultfd_missing(vma)) { if (!hugetlb_pte_stable(h, mm, vmf->address, vmf->pte, vmf->orig_pte)) { ret = 0; goto out; } return hugetlb_handle_userfault(vmf, mapping, VM_UFFD_MISSING); } if (!(vma->vm_flags & VM_MAYSHARE)) { <S2SV_StartVul> ret = vmf_anon_prepare(vmf); <S2SV_EndVul> if (unlikely(ret)) goto out; } folio = alloc_hugetlb_folio(vma, vmf->address, 0); if (IS_ERR(folio)) { if (hugetlb_pte_stable(h, mm, vmf->address, vmf->pte, vmf->orig_pte)) ret = vmf_error(PTR_ERR(folio)); else ret = 0; goto out; } folio_zero_user(folio, vmf->real_address); __folio_mark_uptodate(folio); new_folio = true; if (vma->vm_flags & VM_MAYSHARE) { int err = hugetlb_add_to_page_cache(folio, mapping, vmf->pgoff); if (err) { restore_reserve_on_error(h, vma, vmf->address, folio); folio_put(folio); ret = VM_FAULT_SIGBUS; goto out; } new_pagecache_folio = true; } else { folio_lock(folio); anon_rmap = 1; } } else { if (unlikely(folio_test_hwpoison(folio))) { ret = VM_FAULT_HWPOISON_LARGE | VM_FAULT_SET_HINDEX(hstate_index(h)); goto backout_unlocked; } if (userfaultfd_minor(vma)) { folio_unlock(folio); folio_put(folio); if (!hugetlb_pte_stable(h, mm, vmf->address, vmf->pte, vmf->orig_pte)) { ret = 0; goto out; } return hugetlb_handle_userfault(vmf, mapping, VM_UFFD_MINOR); } } if ((vmf->flags & FAULT_FLAG_WRITE) && !(vma->vm_flags & VM_SHARED)) { if (vma_needs_reservation(h, vma, vmf->address) < 0) { ret = VM_FAULT_OOM; goto backout_unlocked; } vma_end_reservation(h, vma, vmf->address); } vmf->ptl = huge_pte_lock(h, mm, vmf->pte); ret = 0; if (!pte_same(huge_ptep_get(mm, vmf->address, vmf->pte), vmf->orig_pte)) goto backout; if (anon_rmap) hugetlb_add_new_anon_rmap(folio, vma, vmf->address); else hugetlb_add_file_rmap(folio); new_pte = make_huge_pte(vma, &folio->page, ((vma->vm_flags & VM_WRITE) && (vma->vm_flags & VM_SHARED))); if (unlikely(pte_marker_uffd_wp(vmf->orig_pte))) new_pte = huge_pte_mkuffd_wp(new_pte); set_huge_pte_at(mm, vmf->address, vmf->pte, new_pte, huge_page_size(h)); hugetlb_count_add(pages_per_huge_page(h), mm); if ((vmf->flags & FAULT_FLAG_WRITE) && !(vma->vm_flags & VM_SHARED)) { ret = hugetlb_wp(folio, vmf); } spin_unlock(vmf->ptl); if (new_folio) folio_set_hugetlb_migratable(folio); folio_unlock(folio); out: hugetlb_vma_unlock_read(vma); mutex_unlock(&hugetlb_fault_mutex_table[hash]); return ret; backout: spin_unlock(vmf->ptl); backout_unlocked: if (new_folio && !new_pagecache_folio) restore_reserve_on_error(h, vma, vmf->address, folio); folio_unlock(folio); folio_put(folio); goto out; }","- ret = vmf_anon_prepare(vmf);
+ ret = __vmf_anon_prepare(vmf);
+ if (unlikely(ret & VM_FAULT_RETRY))
+ vma_end_read(vma);","static vm_fault_t hugetlb_no_page(struct address_space *mapping, struct vm_fault *vmf) { struct vm_area_struct *vma = vmf->vma; struct mm_struct *mm = vma->vm_mm; struct hstate *h = hstate_vma(vma); vm_fault_t ret = VM_FAULT_SIGBUS; int anon_rmap = 0; unsigned long size; struct folio *folio; pte_t new_pte; bool new_folio, new_pagecache_folio = false; u32 hash = hugetlb_fault_mutex_hash(mapping, vmf->pgoff); if (is_vma_resv_set(vma, HPAGE_RESV_UNMAPPED)) { pr_warn_ratelimited(""PID %d killed due to inadequate hugepage pool\n"", current->pid); goto out; } new_folio = false; folio = filemap_lock_hugetlb_folio(h, mapping, vmf->pgoff); if (IS_ERR(folio)) { size = i_size_read(mapping->host) >> huge_page_shift(h); if (vmf->pgoff >= size) goto out; if (userfaultfd_missing(vma)) { if (!hugetlb_pte_stable(h, mm, vmf->address, vmf->pte, vmf->orig_pte)) { ret = 0; goto out; } return hugetlb_handle_userfault(vmf, mapping, VM_UFFD_MISSING); } if (!(vma->vm_flags & VM_MAYSHARE)) { ret = __vmf_anon_prepare(vmf); if (unlikely(ret)) goto out; } folio = alloc_hugetlb_folio(vma, vmf->address, 0); if (IS_ERR(folio)) { if (hugetlb_pte_stable(h, mm, vmf->address, vmf->pte, vmf->orig_pte)) ret = vmf_error(PTR_ERR(folio)); else ret = 0; goto out; } folio_zero_user(folio, vmf->real_address); __folio_mark_uptodate(folio); new_folio = true; if (vma->vm_flags & VM_MAYSHARE) { int err = hugetlb_add_to_page_cache(folio, mapping, vmf->pgoff); if (err) { restore_reserve_on_error(h, vma, vmf->address, folio); folio_put(folio); ret = VM_FAULT_SIGBUS; goto out; } new_pagecache_folio = true; } else { folio_lock(folio); anon_rmap = 1; } } else { if (unlikely(folio_test_hwpoison(folio))) { ret = VM_FAULT_HWPOISON_LARGE | VM_FAULT_SET_HINDEX(hstate_index(h)); goto backout_unlocked; } if (userfaultfd_minor(vma)) { folio_unlock(folio); folio_put(folio); if (!hugetlb_pte_stable(h, mm, vmf->address, vmf->pte, vmf->orig_pte)) { ret = 0; goto out; } return hugetlb_handle_userfault(vmf, mapping, VM_UFFD_MINOR); } } if ((vmf->flags & FAULT_FLAG_WRITE) && !(vma->vm_flags & VM_SHARED)) { if (vma_needs_reservation(h, vma, vmf->address) < 0) { ret = VM_FAULT_OOM; goto backout_unlocked; } vma_end_reservation(h, vma, vmf->address); } vmf->ptl = huge_pte_lock(h, mm, vmf->pte); ret = 0; if (!pte_same(huge_ptep_get(mm, vmf->address, vmf->pte), vmf->orig_pte)) goto backout; if (anon_rmap) hugetlb_add_new_anon_rmap(folio, vma, vmf->address); else hugetlb_add_file_rmap(folio); new_pte = make_huge_pte(vma, &folio->page, ((vma->vm_flags & VM_WRITE) && (vma->vm_flags & VM_SHARED))); if (unlikely(pte_marker_uffd_wp(vmf->orig_pte))) new_pte = huge_pte_mkuffd_wp(new_pte); set_huge_pte_at(mm, vmf->address, vmf->pte, new_pte, huge_page_size(h)); hugetlb_count_add(pages_per_huge_page(h), mm); if ((vmf->flags & FAULT_FLAG_WRITE) && !(vma->vm_flags & VM_SHARED)) { ret = hugetlb_wp(folio, vmf); } spin_unlock(vmf->ptl); if (new_folio) folio_set_hugetlb_migratable(folio); folio_unlock(folio); out: hugetlb_vma_unlock_read(vma); if (unlikely(ret & VM_FAULT_RETRY)) vma_end_read(vma); mutex_unlock(&hugetlb_fault_mutex_table[hash]); return ret; backout: spin_unlock(vmf->ptl); backout_unlocked: if (new_folio && !new_pagecache_folio) restore_reserve_on_error(h, vma, vmf->address, folio); folio_unlock(folio); folio_put(folio); goto out; }"
1043----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50284/bad/user_session.c----ksmbd_session_rpc_open,"int ksmbd_session_rpc_open(struct ksmbd_session *sess, char *rpc_name) { struct ksmbd_session_rpc *entry; struct ksmbd_rpc_command *resp; int method; method = __rpc_method(rpc_name); if (!method) return -EINVAL; entry = kzalloc(sizeof(struct ksmbd_session_rpc), GFP_KERNEL); if (!entry) return -ENOMEM; entry->method = method; entry->id = ksmbd_ipc_id_alloc(); if (entry->id < 0) goto free_entry; <S2SV_StartVul> xa_store(&sess->rpc_handle_list, entry->id, entry, GFP_KERNEL); <S2SV_EndVul> resp = ksmbd_rpc_open(sess, entry->id); if (!resp) <S2SV_StartVul> goto free_id; <S2SV_EndVul> kvfree(resp); return entry->id; <S2SV_StartVul> free_id: <S2SV_EndVul> xa_erase(&sess->rpc_handle_list, entry->id); ksmbd_rpc_id_free(entry->id); free_entry: kfree(entry); return -EINVAL; }","- xa_store(&sess->rpc_handle_list, entry->id, entry, GFP_KERNEL);
- goto free_id;
- free_id:
+ old = xa_store(&sess->rpc_handle_list, entry->id, entry, GFP_KERNEL);
+ if (xa_is_err(old))
+ goto free_id;
+ goto erase_xa;
+ erase_xa:
+ free_id:","int ksmbd_session_rpc_open(struct ksmbd_session *sess, char *rpc_name) { struct ksmbd_session_rpc *entry, *old; struct ksmbd_rpc_command *resp; int method; method = __rpc_method(rpc_name); if (!method) return -EINVAL; entry = kzalloc(sizeof(struct ksmbd_session_rpc), GFP_KERNEL); if (!entry) return -ENOMEM; entry->method = method; entry->id = ksmbd_ipc_id_alloc(); if (entry->id < 0) goto free_entry; old = xa_store(&sess->rpc_handle_list, entry->id, entry, GFP_KERNEL); if (xa_is_err(old)) goto free_id; resp = ksmbd_rpc_open(sess, entry->id); if (!resp) goto erase_xa; kvfree(resp); return entry->id; erase_xa: xa_erase(&sess->rpc_handle_list, entry->id); free_id: ksmbd_rpc_id_free(entry->id); free_entry: kfree(entry); return -EINVAL; }"
333----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46793/bad/bytcr_rt5651.c----snd_byt_rt5651_mc_probe,"static int snd_byt_rt5651_mc_probe(struct platform_device *pdev) { struct device *dev = &pdev->dev; static const char * const mic_name[] = { ""dmic"", ""in1"", ""in2"", ""in12"" }; struct snd_soc_acpi_mach *mach = dev_get_platdata(dev); struct byt_rt5651_private *priv; const char *platform_name; struct acpi_device *adev; struct device *codec_dev; bool sof_parent; bool is_bytcr = false; int ret_val = 0; int dai_index = 0; int i; priv = devm_kzalloc(dev, sizeof(*priv), GFP_KERNEL); if (!priv) return -ENOMEM; byt_rt5651_card.dev = dev; snd_soc_card_set_drvdata(&byt_rt5651_card, priv); for (i = 0; i < ARRAY_SIZE(byt_rt5651_dais); i++) { <S2SV_StartVul> if (byt_rt5651_dais[i].codecs->name && <S2SV_EndVul> !strcmp(byt_rt5651_dais[i].codecs->name, ""i2c-10EC5651:00"")) { dai_index = i; break; } } adev = acpi_dev_get_first_match_dev(mach->id, NULL, -1); if (adev) { snprintf(byt_rt5651_codec_name, sizeof(byt_rt5651_codec_name), ""i2c-%s"", acpi_dev_name(adev)); byt_rt5651_dais[dai_index].codecs->name = byt_rt5651_codec_name; } else { dev_err(dev, ""Error cannot find '%s' dev\n"", mach->id); return -ENXIO; } codec_dev = acpi_get_first_physical_node(adev); acpi_dev_put(adev); if (!codec_dev) return -EPROBE_DEFER; priv->codec_dev = get_device(codec_dev); if (soc_intel_is_byt()) { if (mach->mach_params.acpi_ipc_irq_index == 0) is_bytcr = true; } if (is_bytcr) { struct acpi_chan_package chan_package = { 0 }; struct acpi_buffer format = {sizeof(""NN""), ""NN""}; struct acpi_buffer state = {0, NULL}; struct snd_soc_acpi_package_context pkg_ctx; bool pkg_found = false; state.length = sizeof(chan_package); state.pointer = &chan_package; pkg_ctx.name = ""CHAN""; pkg_ctx.length = 2; pkg_ctx.format = &format; pkg_ctx.state = &state; pkg_ctx.data_valid = false; pkg_found = snd_soc_acpi_find_package_from_hid(mach->id, &pkg_ctx); if (pkg_found) { if (chan_package.aif_value == 1) { dev_info(dev, ""BIOS Routing: AIF1 connected\n""); byt_rt5651_quirk |= BYT_RT5651_SSP0_AIF1; } else if (chan_package.aif_value == 2) { dev_info(dev, ""BIOS Routing: AIF2 connected\n""); byt_rt5651_quirk |= BYT_RT5651_SSP0_AIF2; } else { dev_info(dev, ""BIOS Routing isn't valid, ignored\n""); pkg_found = false; } } if (!pkg_found) { byt_rt5651_quirk |= BYT_RT5651_SSP0_AIF2; } } dmi_check_system(byt_rt5651_quirk_table); if (quirk_override != -1) { dev_info(dev, ""Overriding quirk 0x%lx => 0x%x\n"", byt_rt5651_quirk, quirk_override); byt_rt5651_quirk = quirk_override; } ret_val = byt_rt5651_add_codec_device_props(codec_dev, priv); if (ret_val) goto err_device; if (soc_intel_is_cht() && !byt_rt5651_gpios) byt_rt5651_gpios = cht_rt5651_gpios; if (byt_rt5651_gpios) { devm_acpi_dev_add_driver_gpios(codec_dev, byt_rt5651_gpios); priv->ext_amp_gpio = devm_fwnode_gpiod_get(dev, codec_dev->fwnode, ""ext-amp-enable"", GPIOD_OUT_LOW, ""speaker-amp""); if (IS_ERR(priv->ext_amp_gpio)) { ret_val = PTR_ERR(priv->ext_amp_gpio); switch (ret_val) { case -ENOENT: priv->ext_amp_gpio = NULL; break; default: dev_err(dev, ""Failed to get ext-amp-enable GPIO: %d\n"", ret_val); fallthrough; case -EPROBE_DEFER: goto err; } } priv->hp_detect = devm_fwnode_gpiod_get(dev, codec_dev->fwnode, ""hp-detect"", GPIOD_IN, ""hp-detect""); if (IS_ERR(priv->hp_detect)) { ret_val = PTR_ERR(priv->hp_detect); switch (ret_val) { case -ENOENT: priv->hp_detect = NULL; break; default: dev_err(dev, ""Failed to get hp-detect GPIO: %d\n"", ret_val); fallthrough; case -EPROBE_DEFER: goto err; } } } log_quirks(dev); if ((byt_rt5651_quirk & BYT_RT5651_SSP2_AIF2) || (byt_rt5651_quirk & BYT_RT5651_SSP0_AIF2)) byt_rt5651_dais[dai_index].codecs->dai_name = ""rt5651-aif2""; if ((byt_rt5651_quirk & BYT_RT5651_SSP0_AIF1) || (byt_rt5651_quirk & BYT_RT5651_SSP0_AIF2)) byt_rt5651_dais[dai_index].cpus->dai_name = ""ssp0-port""; if (byt_rt5651_quirk & BYT_RT5651_MCLK_EN) { priv->mclk = devm_clk_get_optional(dev, ""pmc_plt_clk_3""); if (IS_ERR(priv->mclk)) { ret_val = dev_err_probe(dev, PTR_ERR(priv->mclk), ""Failed to get MCLK from pmc_plt_clk_3\n""); goto err; } if (!priv->mclk) byt_rt5651_quirk &= ~BYT_RT5651_MCLK_EN; } snprintf(byt_rt5651_components, sizeof(byt_rt5651_components), ""cfg-spk:%s cfg-mic:%s%s"", (byt_rt5651_quirk & BYT_RT5651_MONO_SPEAKER) ? ""1"" : ""2"", mic_name[BYT_RT5651_MAP(byt_rt5651_quirk)], (byt_rt5651_quirk & BYT_RT5651_HP_LR_SWAPPED) ? "" cfg-hp:lrswap"" : """"); byt_rt5651_card.components = byt_rt5651_components; #if !IS_ENABLED(CONFIG_SND_SOC_INTEL_USER_FRIENDLY_LONG_NAMES) snprintf(byt_rt5651_long_name, sizeof(byt_rt5651_long_name), ""bytcr-rt5651-%s-spk-%s-mic%s"", (byt_rt5651_quirk & BYT_RT5651_MONO_SPEAKER) ? ""mono"" : ""stereo"", mic_name[BYT_RT5651_MAP(byt_rt5651_quirk)], (byt_rt5651_quirk & BYT_RT5651_HP_LR_SWAPPED) ? ""-hp-swapped"" : """"); byt_rt5651_card.long_name = byt_rt5651_long_name; #endif platform_name = mach->mach_params.platform; ret_val = snd_soc_fixup_dai_links_platform_name(&byt_rt5651_card, platform_name); if (ret_val) goto err; sof_parent = snd_soc_acpi_sof_parent(dev); if (sof_parent) { byt_rt5651_card.name = SOF_CARD_NAME; byt_rt5651_card.driver_name = SOF_DRIVER_NAME; } else { byt_rt5651_card.name = CARD_NAME; byt_rt5651_card.driver_name = DRIVER_NAME; } if (sof_parent) dev->driver->pm = &snd_soc_pm_ops; ret_val = devm_snd_soc_register_card(dev, &byt_rt5651_card); if (ret_val) { dev_err(dev, ""devm_snd_soc_register_card failed %d\n"", ret_val); goto err; } platform_set_drvdata(pdev, &byt_rt5651_card); return ret_val; err: device_remove_software_node(priv->codec_dev); err_device: put_device(priv->codec_dev); return ret_val; }","- if (byt_rt5651_dais[i].codecs->name &&
+ if (byt_rt5651_dais[i].num_codecs &&","static int snd_byt_rt5651_mc_probe(struct platform_device *pdev) { struct device *dev = &pdev->dev; static const char * const mic_name[] = { ""dmic"", ""in1"", ""in2"", ""in12"" }; struct snd_soc_acpi_mach *mach = dev_get_platdata(dev); struct byt_rt5651_private *priv; const char *platform_name; struct acpi_device *adev; struct device *codec_dev; bool sof_parent; bool is_bytcr = false; int ret_val = 0; int dai_index = 0; int i; priv = devm_kzalloc(dev, sizeof(*priv), GFP_KERNEL); if (!priv) return -ENOMEM; byt_rt5651_card.dev = dev; snd_soc_card_set_drvdata(&byt_rt5651_card, priv); for (i = 0; i < ARRAY_SIZE(byt_rt5651_dais); i++) { if (byt_rt5651_dais[i].num_codecs && !strcmp(byt_rt5651_dais[i].codecs->name, ""i2c-10EC5651:00"")) { dai_index = i; break; } } adev = acpi_dev_get_first_match_dev(mach->id, NULL, -1); if (adev) { snprintf(byt_rt5651_codec_name, sizeof(byt_rt5651_codec_name), ""i2c-%s"", acpi_dev_name(adev)); byt_rt5651_dais[dai_index].codecs->name = byt_rt5651_codec_name; } else { dev_err(dev, ""Error cannot find '%s' dev\n"", mach->id); return -ENXIO; } codec_dev = acpi_get_first_physical_node(adev); acpi_dev_put(adev); if (!codec_dev) return -EPROBE_DEFER; priv->codec_dev = get_device(codec_dev); if (soc_intel_is_byt()) { if (mach->mach_params.acpi_ipc_irq_index == 0) is_bytcr = true; } if (is_bytcr) { struct acpi_chan_package chan_package = { 0 }; struct acpi_buffer format = {sizeof(""NN""), ""NN""}; struct acpi_buffer state = {0, NULL}; struct snd_soc_acpi_package_context pkg_ctx; bool pkg_found = false; state.length = sizeof(chan_package); state.pointer = &chan_package; pkg_ctx.name = ""CHAN""; pkg_ctx.length = 2; pkg_ctx.format = &format; pkg_ctx.state = &state; pkg_ctx.data_valid = false; pkg_found = snd_soc_acpi_find_package_from_hid(mach->id, &pkg_ctx); if (pkg_found) { if (chan_package.aif_value == 1) { dev_info(dev, ""BIOS Routing: AIF1 connected\n""); byt_rt5651_quirk |= BYT_RT5651_SSP0_AIF1; } else if (chan_package.aif_value == 2) { dev_info(dev, ""BIOS Routing: AIF2 connected\n""); byt_rt5651_quirk |= BYT_RT5651_SSP0_AIF2; } else { dev_info(dev, ""BIOS Routing isn't valid, ignored\n""); pkg_found = false; } } if (!pkg_found) { byt_rt5651_quirk |= BYT_RT5651_SSP0_AIF2; } } dmi_check_system(byt_rt5651_quirk_table); if (quirk_override != -1) { dev_info(dev, ""Overriding quirk 0x%lx => 0x%x\n"", byt_rt5651_quirk, quirk_override); byt_rt5651_quirk = quirk_override; } ret_val = byt_rt5651_add_codec_device_props(codec_dev, priv); if (ret_val) goto err_device; if (soc_intel_is_cht() && !byt_rt5651_gpios) byt_rt5651_gpios = cht_rt5651_gpios; if (byt_rt5651_gpios) { devm_acpi_dev_add_driver_gpios(codec_dev, byt_rt5651_gpios); priv->ext_amp_gpio = devm_fwnode_gpiod_get(dev, codec_dev->fwnode, ""ext-amp-enable"", GPIOD_OUT_LOW, ""speaker-amp""); if (IS_ERR(priv->ext_amp_gpio)) { ret_val = PTR_ERR(priv->ext_amp_gpio); switch (ret_val) { case -ENOENT: priv->ext_amp_gpio = NULL; break; default: dev_err(dev, ""Failed to get ext-amp-enable GPIO: %d\n"", ret_val); fallthrough; case -EPROBE_DEFER: goto err; } } priv->hp_detect = devm_fwnode_gpiod_get(dev, codec_dev->fwnode, ""hp-detect"", GPIOD_IN, ""hp-detect""); if (IS_ERR(priv->hp_detect)) { ret_val = PTR_ERR(priv->hp_detect); switch (ret_val) { case -ENOENT: priv->hp_detect = NULL; break; default: dev_err(dev, ""Failed to get hp-detect GPIO: %d\n"", ret_val); fallthrough; case -EPROBE_DEFER: goto err; } } } log_quirks(dev); if ((byt_rt5651_quirk & BYT_RT5651_SSP2_AIF2) || (byt_rt5651_quirk & BYT_RT5651_SSP0_AIF2)) byt_rt5651_dais[dai_index].codecs->dai_name = ""rt5651-aif2""; if ((byt_rt5651_quirk & BYT_RT5651_SSP0_AIF1) || (byt_rt5651_quirk & BYT_RT5651_SSP0_AIF2)) byt_rt5651_dais[dai_index].cpus->dai_name = ""ssp0-port""; if (byt_rt5651_quirk & BYT_RT5651_MCLK_EN) { priv->mclk = devm_clk_get_optional(dev, ""pmc_plt_clk_3""); if (IS_ERR(priv->mclk)) { ret_val = dev_err_probe(dev, PTR_ERR(priv->mclk), ""Failed to get MCLK from pmc_plt_clk_3\n""); goto err; } if (!priv->mclk) byt_rt5651_quirk &= ~BYT_RT5651_MCLK_EN; } snprintf(byt_rt5651_components, sizeof(byt_rt5651_components), ""cfg-spk:%s cfg-mic:%s%s"", (byt_rt5651_quirk & BYT_RT5651_MONO_SPEAKER) ? ""1"" : ""2"", mic_name[BYT_RT5651_MAP(byt_rt5651_quirk)], (byt_rt5651_quirk & BYT_RT5651_HP_LR_SWAPPED) ? "" cfg-hp:lrswap"" : """"); byt_rt5651_card.components = byt_rt5651_components; #if !IS_ENABLED(CONFIG_SND_SOC_INTEL_USER_FRIENDLY_LONG_NAMES) snprintf(byt_rt5651_long_name, sizeof(byt_rt5651_long_name), ""bytcr-rt5651-%s-spk-%s-mic%s"", (byt_rt5651_quirk & BYT_RT5651_MONO_SPEAKER) ? ""mono"" : ""stereo"", mic_name[BYT_RT5651_MAP(byt_rt5651_quirk)], (byt_rt5651_quirk & BYT_RT5651_HP_LR_SWAPPED) ? ""-hp-swapped"" : """"); byt_rt5651_card.long_name = byt_rt5651_long_name; #endif platform_name = mach->mach_params.platform; ret_val = snd_soc_fixup_dai_links_platform_name(&byt_rt5651_card, platform_name); if (ret_val) goto err; sof_parent = snd_soc_acpi_sof_parent(dev); if (sof_parent) { byt_rt5651_card.name = SOF_CARD_NAME; byt_rt5651_card.driver_name = SOF_DRIVER_NAME; } else { byt_rt5651_card.name = CARD_NAME; byt_rt5651_card.driver_name = DRIVER_NAME; } if (sof_parent) dev->driver->pm = &snd_soc_pm_ops; ret_val = devm_snd_soc_register_card(dev, &byt_rt5651_card); if (ret_val) { dev_err(dev, ""devm_snd_soc_register_card failed %d\n"", ret_val); goto err; } platform_set_drvdata(pdev, &byt_rt5651_card); return ret_val; err: device_remove_software_node(priv->codec_dev); err_device: put_device(priv->codec_dev); return ret_val; }"
970----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50211/bad/inode.c----udf_extend_file,"static int udf_extend_file(struct inode *inode, loff_t newsize) { struct extent_position epos; struct kernel_lb_addr eloc; uint32_t elen; int8_t etype; struct super_block *sb = inode->i_sb; sector_t first_block = newsize >> sb->s_blocksize_bits, offset; loff_t new_elen; int adsize; struct udf_inode_info *iinfo = UDF_I(inode); struct kernel_long_ad extent; int err = 0; bool within_last_ext; if (iinfo->i_alloc_type == ICBTAG_FLAG_AD_SHORT) adsize = sizeof(struct short_ad); else if (iinfo->i_alloc_type == ICBTAG_FLAG_AD_LONG) adsize = sizeof(struct long_ad); else BUG(); down_write(&iinfo->i_data_sem); udf_discard_prealloc(inode); <S2SV_StartVul> etype = inode_bmap(inode, first_block, &epos, &eloc, &elen, &offset); <S2SV_EndVul> <S2SV_StartVul> within_last_ext = (etype != -1); <S2SV_EndVul> WARN_ON_ONCE(within_last_ext && elen > ((loff_t)offset + 1) << inode->i_blkbits); if ((!epos.bh && epos.offset == udf_file_entry_alloc_offset(inode)) || (epos.bh && epos.offset == sizeof(struct allocExtDesc))) { extent.extLocation.logicalBlockNum = 0; extent.extLocation.partitionReferenceNum = 0; extent.extLength = EXT_NOT_RECORDED_NOT_ALLOCATED; } else { epos.offset -= adsize; err = udf_next_aext(inode, &epos, &extent.extLocation, &extent.extLength, &etype, 0); if (err <= 0) goto out; extent.extLength |= etype << 30; } new_elen = ((loff_t)offset << inode->i_blkbits) | (newsize & (sb->s_blocksize - 1)); if (within_last_ext) { udf_do_extend_final_block(inode, &epos, &extent, new_elen); } else { err = udf_do_extend_file(inode, &epos, &extent, new_elen); } if (err < 0) goto out; err = 0; out: brelse(epos.bh); up_write(&iinfo->i_data_sem); return err; }","- etype = inode_bmap(inode, first_block, &epos, &eloc, &elen, &offset);
- within_last_ext = (etype != -1);
+ err = inode_bmap(inode, first_block, &epos, &eloc, &elen, &offset, &etype);
+ if (err < 0)
+ goto out;
+ within_last_ext = (err == 1);","static int udf_extend_file(struct inode *inode, loff_t newsize) { struct extent_position epos; struct kernel_lb_addr eloc; uint32_t elen; int8_t etype; struct super_block *sb = inode->i_sb; sector_t first_block = newsize >> sb->s_blocksize_bits, offset; loff_t new_elen; int adsize; struct udf_inode_info *iinfo = UDF_I(inode); struct kernel_long_ad extent; int err = 0; bool within_last_ext; if (iinfo->i_alloc_type == ICBTAG_FLAG_AD_SHORT) adsize = sizeof(struct short_ad); else if (iinfo->i_alloc_type == ICBTAG_FLAG_AD_LONG) adsize = sizeof(struct long_ad); else BUG(); down_write(&iinfo->i_data_sem); udf_discard_prealloc(inode); err = inode_bmap(inode, first_block, &epos, &eloc, &elen, &offset, &etype); if (err < 0) goto out; within_last_ext = (err == 1); WARN_ON_ONCE(within_last_ext && elen > ((loff_t)offset + 1) << inode->i_blkbits); if ((!epos.bh && epos.offset == udf_file_entry_alloc_offset(inode)) || (epos.bh && epos.offset == sizeof(struct allocExtDesc))) { extent.extLocation.logicalBlockNum = 0; extent.extLocation.partitionReferenceNum = 0; extent.extLength = EXT_NOT_RECORDED_NOT_ALLOCATED; } else { epos.offset -= adsize; err = udf_next_aext(inode, &epos, &extent.extLocation, &extent.extLength, &etype, 0); if (err <= 0) goto out; extent.extLength |= etype << 30; } new_elen = ((loff_t)offset << inode->i_blkbits) | (newsize & (sb->s_blocksize - 1)); if (within_last_ext) { udf_do_extend_final_block(inode, &epos, &extent, new_elen); } else { err = udf_do_extend_file(inode, &epos, &extent, new_elen); } if (err < 0) goto out; err = 0; out: brelse(epos.bh); up_write(&iinfo->i_data_sem); return err; }"
747----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50025/bad/fnic_main.c----fnic_probe,"static int fnic_probe(struct pci_dev *pdev, const struct pci_device_id *ent) { struct Scsi_Host *host; struct fc_lport *lp; struct fnic *fnic; mempool_t *pool; int err = 0; int fnic_id = 0; int i; unsigned long flags; int hwq; lp = libfc_host_alloc(&fnic_host_template, sizeof(struct fnic)); if (!lp) { printk(KERN_ERR PFX ""Unable to alloc libfc local port\n""); err = -ENOMEM; goto err_out; } host = lp->host; fnic = lport_priv(lp); fnic_id = ida_alloc(&fnic_ida, GFP_KERNEL); if (fnic_id < 0) { pr_err(""Unable to alloc fnic ID\n""); err = fnic_id; goto err_out_ida_alloc; } fnic->lport = lp; fnic->ctlr.lp = lp; fnic->link_events = 0; fnic->pdev = pdev; snprintf(fnic->name, sizeof(fnic->name) - 1, ""%s%d"", DRV_NAME, host->host_no); host->transportt = fnic_fc_transport; fnic->fnic_num = fnic_id; fnic_stats_debugfs_init(fnic); err = pci_enable_device(pdev); if (err) { shost_printk(KERN_ERR, fnic->lport->host, ""Cannot enable PCI device, aborting.\n""); goto err_out_free_hba; } err = pci_request_regions(pdev, DRV_NAME); if (err) { shost_printk(KERN_ERR, fnic->lport->host, ""Cannot enable PCI resources, aborting\n""); goto err_out_disable_device; } pci_set_master(pdev); err = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(47)); if (err) { err = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(32)); if (err) { shost_printk(KERN_ERR, fnic->lport->host, ""No usable DMA configuration "" ""aborting\n""); goto err_out_release_regions; } } if (!(pci_resource_flags(pdev, 0) & IORESOURCE_MEM)) { shost_printk(KERN_ERR, fnic->lport->host, ""BAR0 not memory-map'able, aborting.\n""); err = -ENODEV; goto err_out_release_regions; } fnic->bar0.vaddr = pci_iomap(pdev, 0, 0); fnic->bar0.bus_addr = pci_resource_start(pdev, 0); fnic->bar0.len = pci_resource_len(pdev, 0); if (!fnic->bar0.vaddr) { shost_printk(KERN_ERR, fnic->lport->host, ""Cannot memory-map BAR0 res hdr, "" ""aborting.\n""); err = -ENODEV; goto err_out_release_regions; } fnic->vdev = vnic_dev_register(NULL, fnic, pdev, &fnic->bar0); if (!fnic->vdev) { shost_printk(KERN_ERR, fnic->lport->host, ""vNIC registration failed, "" ""aborting.\n""); err = -ENODEV; goto err_out_iounmap; } err = vnic_dev_cmd_init(fnic->vdev); if (err) { shost_printk(KERN_ERR, fnic->lport->host, ""vnic_dev_cmd_init() returns %d, aborting\n"", err); goto err_out_vnic_unregister; } err = fnic_dev_wait(fnic->vdev, vnic_dev_open, vnic_dev_open_done, CMD_OPENF_RQ_ENABLE_THEN_POST); if (err) { shost_printk(KERN_ERR, fnic->lport->host, ""vNIC dev open failed, aborting.\n""); goto err_out_dev_cmd_deinit; } err = vnic_dev_init(fnic->vdev, 0); if (err) { shost_printk(KERN_ERR, fnic->lport->host, ""vNIC dev init failed, aborting.\n""); goto err_out_dev_close; } err = vnic_dev_mac_addr(fnic->vdev, fnic->ctlr.ctl_src_addr); if (err) { shost_printk(KERN_ERR, fnic->lport->host, ""vNIC get MAC addr failed \n""); goto err_out_dev_close; } memcpy(fnic->data_src_addr, fnic->ctlr.ctl_src_addr, ETH_ALEN); err = fnic_get_vnic_config(fnic); if (err) { shost_printk(KERN_ERR, fnic->lport->host, ""Get vNIC configuration failed, "" ""aborting.\n""); goto err_out_dev_close; } pci_set_drvdata(pdev, fnic); fnic_get_res_counts(fnic); err = fnic_set_intr_mode(fnic); if (err) { shost_printk(KERN_ERR, fnic->lport->host, ""Failed to set intr mode, "" ""aborting.\n""); goto err_out_dev_close; } err = fnic_alloc_vnic_resources(fnic); if (err) { shost_printk(KERN_ERR, fnic->lport->host, ""Failed to alloc vNIC resources, "" ""aborting.\n""); goto err_out_clear_intr; } fnic_scsi_drv_init(fnic); for (hwq = 0; hwq < fnic->wq_copy_count; hwq++) { fnic->sw_copy_wq[hwq].ioreq_table_size = fnic->fnic_max_tag_id; fnic->sw_copy_wq[hwq].io_req_table = kzalloc((fnic->sw_copy_wq[hwq].ioreq_table_size + 1) * sizeof(struct fnic_io_req *), GFP_KERNEL); } shost_printk(KERN_INFO, fnic->lport->host, ""fnic copy wqs: %d, Q0 ioreq table size: %d\n"", fnic->wq_copy_count, fnic->sw_copy_wq[0].ioreq_table_size); spin_lock_init(&fnic->fnic_lock); for (i = 0; i < FNIC_WQ_MAX; i++) spin_lock_init(&fnic->wq_lock[i]); for (i = 0; i < FNIC_WQ_COPY_MAX; i++) { spin_lock_init(&fnic->wq_copy_lock[i]); fnic->wq_copy_desc_low[i] = DESC_CLEAN_LOW_WATERMARK; fnic->fw_ack_recd[i] = 0; fnic->fw_ack_index[i] = -1; } err = -ENOMEM; fnic->io_req_pool = mempool_create_slab_pool(2, fnic_io_req_cache); if (!fnic->io_req_pool) goto err_out_free_resources; pool = mempool_create_slab_pool(2, fnic_sgl_cache[FNIC_SGL_CACHE_DFLT]); if (!pool) goto err_out_free_ioreq_pool; fnic->io_sgl_pool[FNIC_SGL_CACHE_DFLT] = pool; pool = mempool_create_slab_pool(2, fnic_sgl_cache[FNIC_SGL_CACHE_MAX]); if (!pool) goto err_out_free_dflt_pool; fnic->io_sgl_pool[FNIC_SGL_CACHE_MAX] = pool; fnic->vlan_hw_insert = 1; fnic->vlan_id = 0; fnic->ctlr.send = fnic_eth_send; fnic->ctlr.update_mac = fnic_update_mac; fnic->ctlr.get_src_addr = fnic_get_mac; if (fnic->config.flags & VFCF_FIP_CAPABLE) { shost_printk(KERN_INFO, fnic->lport->host, ""firmware supports FIP\n""); vnic_dev_packet_filter(fnic->vdev, 1, 1, 0, 0, 0); vnic_dev_add_addr(fnic->vdev, FIP_ALL_ENODE_MACS); vnic_dev_add_addr(fnic->vdev, fnic->ctlr.ctl_src_addr); fnic->set_vlan = fnic_set_vlan; fcoe_ctlr_init(&fnic->ctlr, FIP_MODE_AUTO); timer_setup(&fnic->fip_timer, fnic_fip_notify_timer, 0); spin_lock_init(&fnic->vlans_lock); INIT_WORK(&fnic->fip_frame_work, fnic_handle_fip_frame); INIT_WORK(&fnic->event_work, fnic_handle_event); <S2SV_StartVul> INIT_WORK(&fnic->flush_work, fnic_flush_tx); <S2SV_EndVul> skb_queue_head_init(&fnic->fip_frame_queue); INIT_LIST_HEAD(&fnic->evlist); INIT_LIST_HEAD(&fnic->vlans); } else { shost_printk(KERN_INFO, fnic->lport->host, ""firmware uses non-FIP mode\n""); fcoe_ctlr_init(&fnic->ctlr, FIP_MODE_NON_FIP); fnic->ctlr.state = FIP_ST_NON_FIP; } fnic->state = FNIC_IN_FC_MODE; atomic_set(&fnic->in_flight, 0); fnic->state_flags = FNIC_FLAGS_NONE; fnic_set_nic_config(fnic, 0, 0, 0, 0, 0, 0, 1); err = fnic_notify_set(fnic); if (err) { shost_printk(KERN_ERR, fnic->lport->host, ""Failed to alloc notify buffer, aborting.\n""); goto err_out_free_max_pool; } if (vnic_dev_get_intr_mode(fnic->vdev) == VNIC_DEV_INTR_MODE_MSI) timer_setup(&fnic->notify_timer, fnic_notify_timer, 0); for (i = 0; i < fnic->rq_count; i++) { err = vnic_rq_fill(&fnic->rq[i], fnic_alloc_rq_frame); if (err) { shost_printk(KERN_ERR, fnic->lport->host, ""fnic_alloc_rq_frame can't alloc "" ""frame\n""); goto err_out_rq_buf; } } for (i = 0; i < fnic->raw_wq_count; i++) vnic_wq_enable(&fnic->wq[i]); for (i = 0; i < fnic->rq_count; i++) { if (!ioread32(&fnic->rq[i].ctrl->enable)) vnic_rq_enable(&fnic->rq[i]); } for (i = 0; i < fnic->wq_copy_count; i++) vnic_wq_copy_enable(&fnic->hw_copy_wq[i]); err = fnic_request_intr(fnic); if (err) { shost_printk(KERN_ERR, fnic->lport->host, ""Unable to request irq.\n""); goto err_out_request_intr; } err = scsi_add_host(lp->host, &pdev->dev); if (err) { shost_printk(KERN_ERR, fnic->lport->host, ""fnic: scsi_add_host failed...exiting\n""); goto err_out_scsi_add_host; } lp->link_up = 0; lp->max_retry_count = fnic->config.flogi_retries; lp->max_rport_retry_count = fnic->config.plogi_retries; lp->service_params = (FCP_SPPF_INIT_FCN | FCP_SPPF_RD_XRDY_DIS | FCP_SPPF_CONF_COMPL); if (fnic->config.flags & VFCF_FCP_SEQ_LVL_ERR) lp->service_params |= FCP_SPPF_RETRY; lp->boot_time = jiffies; lp->e_d_tov = fnic->config.ed_tov; lp->r_a_tov = fnic->config.ra_tov; lp->link_supported_speeds = FC_PORTSPEED_10GBIT; fc_set_wwnn(lp, fnic->config.node_wwn); fc_set_wwpn(lp, fnic->config.port_wwn); fcoe_libfc_config(lp, &fnic->ctlr, &fnic_transport_template, 0); if (!fc_exch_mgr_alloc(lp, FC_CLASS_3, FCPIO_HOST_EXCH_RANGE_START, FCPIO_HOST_EXCH_RANGE_END, NULL)) { err = -ENOMEM; goto err_out_fc_exch_mgr_alloc; } fc_lport_init_stats(lp); fnic->stats_reset_time = jiffies; fc_lport_config(lp); if (fc_set_mfs(lp, fnic->config.maxdatafieldsize + sizeof(struct fc_frame_header))) { err = -EINVAL; goto err_out_free_exch_mgr; } fc_host_maxframe_size(lp->host) = lp->mfs; fc_host_dev_loss_tmo(lp->host) = fnic->config.port_down_timeout / 1000; sprintf(fc_host_symbolic_name(lp->host), DRV_NAME "" v"" DRV_VERSION "" over %s"", fnic->name); spin_lock_irqsave(&fnic_list_lock, flags); list_add_tail(&fnic->list, &fnic_list); spin_unlock_irqrestore(&fnic_list_lock, flags); INIT_WORK(&fnic->link_work, fnic_handle_link); INIT_WORK(&fnic->frame_work, fnic_handle_frame); skb_queue_head_init(&fnic->frame_queue); skb_queue_head_init(&fnic->tx_queue); fc_fabric_login(lp); vnic_dev_enable(fnic->vdev); for (i = 0; i < fnic->intr_count; i++) vnic_intr_unmask(&fnic->intr[i]); fnic_notify_timer_start(fnic); return 0; err_out_free_exch_mgr: fc_exch_mgr_free(lp); err_out_fc_exch_mgr_alloc: fc_remove_host(lp->host); scsi_remove_host(lp->host); err_out_scsi_add_host: fnic_free_intr(fnic); err_out_request_intr: for (i = 0; i < fnic->rq_count; i++) vnic_rq_clean(&fnic->rq[i], fnic_free_rq_buf); err_out_rq_buf: vnic_dev_notify_unset(fnic->vdev); err_out_free_max_pool: mempool_destroy(fnic->io_sgl_pool[FNIC_SGL_CACHE_MAX]); err_out_free_dflt_pool: mempool_destroy(fnic->io_sgl_pool[FNIC_SGL_CACHE_DFLT]); err_out_free_ioreq_pool: mempool_destroy(fnic->io_req_pool); err_out_free_resources: for (hwq = 0; hwq < fnic->wq_copy_count; hwq++) kfree(fnic->sw_copy_wq[hwq].io_req_table); fnic_free_vnic_resources(fnic); err_out_clear_intr: fnic_clear_intr_mode(fnic); err_out_dev_close: vnic_dev_close(fnic->vdev); err_out_dev_cmd_deinit: err_out_vnic_unregister: vnic_dev_unregister(fnic->vdev); err_out_iounmap: fnic_iounmap(fnic); err_out_release_regions: pci_release_regions(pdev); err_out_disable_device: pci_disable_device(pdev); err_out_free_hba: fnic_stats_debugfs_remove(fnic); ida_free(&fnic_ida, fnic->fnic_num); err_out_ida_alloc: scsi_host_put(lp->host); err_out: return err; }","- INIT_WORK(&fnic->flush_work, fnic_flush_tx);
+ INIT_WORK(&fnic->flush_work, fnic_flush_tx);","static int fnic_probe(struct pci_dev *pdev, const struct pci_device_id *ent) { struct Scsi_Host *host; struct fc_lport *lp; struct fnic *fnic; mempool_t *pool; int err = 0; int fnic_id = 0; int i; unsigned long flags; int hwq; lp = libfc_host_alloc(&fnic_host_template, sizeof(struct fnic)); if (!lp) { printk(KERN_ERR PFX ""Unable to alloc libfc local port\n""); err = -ENOMEM; goto err_out; } host = lp->host; fnic = lport_priv(lp); fnic_id = ida_alloc(&fnic_ida, GFP_KERNEL); if (fnic_id < 0) { pr_err(""Unable to alloc fnic ID\n""); err = fnic_id; goto err_out_ida_alloc; } fnic->lport = lp; fnic->ctlr.lp = lp; fnic->link_events = 0; fnic->pdev = pdev; snprintf(fnic->name, sizeof(fnic->name) - 1, ""%s%d"", DRV_NAME, host->host_no); host->transportt = fnic_fc_transport; fnic->fnic_num = fnic_id; fnic_stats_debugfs_init(fnic); err = pci_enable_device(pdev); if (err) { shost_printk(KERN_ERR, fnic->lport->host, ""Cannot enable PCI device, aborting.\n""); goto err_out_free_hba; } err = pci_request_regions(pdev, DRV_NAME); if (err) { shost_printk(KERN_ERR, fnic->lport->host, ""Cannot enable PCI resources, aborting\n""); goto err_out_disable_device; } pci_set_master(pdev); err = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(47)); if (err) { err = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(32)); if (err) { shost_printk(KERN_ERR, fnic->lport->host, ""No usable DMA configuration "" ""aborting\n""); goto err_out_release_regions; } } if (!(pci_resource_flags(pdev, 0) & IORESOURCE_MEM)) { shost_printk(KERN_ERR, fnic->lport->host, ""BAR0 not memory-map'able, aborting.\n""); err = -ENODEV; goto err_out_release_regions; } fnic->bar0.vaddr = pci_iomap(pdev, 0, 0); fnic->bar0.bus_addr = pci_resource_start(pdev, 0); fnic->bar0.len = pci_resource_len(pdev, 0); if (!fnic->bar0.vaddr) { shost_printk(KERN_ERR, fnic->lport->host, ""Cannot memory-map BAR0 res hdr, "" ""aborting.\n""); err = -ENODEV; goto err_out_release_regions; } fnic->vdev = vnic_dev_register(NULL, fnic, pdev, &fnic->bar0); if (!fnic->vdev) { shost_printk(KERN_ERR, fnic->lport->host, ""vNIC registration failed, "" ""aborting.\n""); err = -ENODEV; goto err_out_iounmap; } err = vnic_dev_cmd_init(fnic->vdev); if (err) { shost_printk(KERN_ERR, fnic->lport->host, ""vnic_dev_cmd_init() returns %d, aborting\n"", err); goto err_out_vnic_unregister; } err = fnic_dev_wait(fnic->vdev, vnic_dev_open, vnic_dev_open_done, CMD_OPENF_RQ_ENABLE_THEN_POST); if (err) { shost_printk(KERN_ERR, fnic->lport->host, ""vNIC dev open failed, aborting.\n""); goto err_out_dev_cmd_deinit; } err = vnic_dev_init(fnic->vdev, 0); if (err) { shost_printk(KERN_ERR, fnic->lport->host, ""vNIC dev init failed, aborting.\n""); goto err_out_dev_close; } err = vnic_dev_mac_addr(fnic->vdev, fnic->ctlr.ctl_src_addr); if (err) { shost_printk(KERN_ERR, fnic->lport->host, ""vNIC get MAC addr failed \n""); goto err_out_dev_close; } memcpy(fnic->data_src_addr, fnic->ctlr.ctl_src_addr, ETH_ALEN); err = fnic_get_vnic_config(fnic); if (err) { shost_printk(KERN_ERR, fnic->lport->host, ""Get vNIC configuration failed, "" ""aborting.\n""); goto err_out_dev_close; } pci_set_drvdata(pdev, fnic); fnic_get_res_counts(fnic); err = fnic_set_intr_mode(fnic); if (err) { shost_printk(KERN_ERR, fnic->lport->host, ""Failed to set intr mode, "" ""aborting.\n""); goto err_out_dev_close; } err = fnic_alloc_vnic_resources(fnic); if (err) { shost_printk(KERN_ERR, fnic->lport->host, ""Failed to alloc vNIC resources, "" ""aborting.\n""); goto err_out_clear_intr; } fnic_scsi_drv_init(fnic); for (hwq = 0; hwq < fnic->wq_copy_count; hwq++) { fnic->sw_copy_wq[hwq].ioreq_table_size = fnic->fnic_max_tag_id; fnic->sw_copy_wq[hwq].io_req_table = kzalloc((fnic->sw_copy_wq[hwq].ioreq_table_size + 1) * sizeof(struct fnic_io_req *), GFP_KERNEL); } shost_printk(KERN_INFO, fnic->lport->host, ""fnic copy wqs: %d, Q0 ioreq table size: %d\n"", fnic->wq_copy_count, fnic->sw_copy_wq[0].ioreq_table_size); spin_lock_init(&fnic->fnic_lock); for (i = 0; i < FNIC_WQ_MAX; i++) spin_lock_init(&fnic->wq_lock[i]); for (i = 0; i < FNIC_WQ_COPY_MAX; i++) { spin_lock_init(&fnic->wq_copy_lock[i]); fnic->wq_copy_desc_low[i] = DESC_CLEAN_LOW_WATERMARK; fnic->fw_ack_recd[i] = 0; fnic->fw_ack_index[i] = -1; } err = -ENOMEM; fnic->io_req_pool = mempool_create_slab_pool(2, fnic_io_req_cache); if (!fnic->io_req_pool) goto err_out_free_resources; pool = mempool_create_slab_pool(2, fnic_sgl_cache[FNIC_SGL_CACHE_DFLT]); if (!pool) goto err_out_free_ioreq_pool; fnic->io_sgl_pool[FNIC_SGL_CACHE_DFLT] = pool; pool = mempool_create_slab_pool(2, fnic_sgl_cache[FNIC_SGL_CACHE_MAX]); if (!pool) goto err_out_free_dflt_pool; fnic->io_sgl_pool[FNIC_SGL_CACHE_MAX] = pool; fnic->vlan_hw_insert = 1; fnic->vlan_id = 0; fnic->ctlr.send = fnic_eth_send; fnic->ctlr.update_mac = fnic_update_mac; fnic->ctlr.get_src_addr = fnic_get_mac; if (fnic->config.flags & VFCF_FIP_CAPABLE) { shost_printk(KERN_INFO, fnic->lport->host, ""firmware supports FIP\n""); vnic_dev_packet_filter(fnic->vdev, 1, 1, 0, 0, 0); vnic_dev_add_addr(fnic->vdev, FIP_ALL_ENODE_MACS); vnic_dev_add_addr(fnic->vdev, fnic->ctlr.ctl_src_addr); fnic->set_vlan = fnic_set_vlan; fcoe_ctlr_init(&fnic->ctlr, FIP_MODE_AUTO); timer_setup(&fnic->fip_timer, fnic_fip_notify_timer, 0); spin_lock_init(&fnic->vlans_lock); INIT_WORK(&fnic->fip_frame_work, fnic_handle_fip_frame); INIT_WORK(&fnic->event_work, fnic_handle_event); skb_queue_head_init(&fnic->fip_frame_queue); INIT_LIST_HEAD(&fnic->evlist); INIT_LIST_HEAD(&fnic->vlans); } else { shost_printk(KERN_INFO, fnic->lport->host, ""firmware uses non-FIP mode\n""); fcoe_ctlr_init(&fnic->ctlr, FIP_MODE_NON_FIP); fnic->ctlr.state = FIP_ST_NON_FIP; } fnic->state = FNIC_IN_FC_MODE; atomic_set(&fnic->in_flight, 0); fnic->state_flags = FNIC_FLAGS_NONE; fnic_set_nic_config(fnic, 0, 0, 0, 0, 0, 0, 1); err = fnic_notify_set(fnic); if (err) { shost_printk(KERN_ERR, fnic->lport->host, ""Failed to alloc notify buffer, aborting.\n""); goto err_out_free_max_pool; } if (vnic_dev_get_intr_mode(fnic->vdev) == VNIC_DEV_INTR_MODE_MSI) timer_setup(&fnic->notify_timer, fnic_notify_timer, 0); for (i = 0; i < fnic->rq_count; i++) { err = vnic_rq_fill(&fnic->rq[i], fnic_alloc_rq_frame); if (err) { shost_printk(KERN_ERR, fnic->lport->host, ""fnic_alloc_rq_frame can't alloc "" ""frame\n""); goto err_out_rq_buf; } } for (i = 0; i < fnic->raw_wq_count; i++) vnic_wq_enable(&fnic->wq[i]); for (i = 0; i < fnic->rq_count; i++) { if (!ioread32(&fnic->rq[i].ctrl->enable)) vnic_rq_enable(&fnic->rq[i]); } for (i = 0; i < fnic->wq_copy_count; i++) vnic_wq_copy_enable(&fnic->hw_copy_wq[i]); err = fnic_request_intr(fnic); if (err) { shost_printk(KERN_ERR, fnic->lport->host, ""Unable to request irq.\n""); goto err_out_request_intr; } err = scsi_add_host(lp->host, &pdev->dev); if (err) { shost_printk(KERN_ERR, fnic->lport->host, ""fnic: scsi_add_host failed...exiting\n""); goto err_out_scsi_add_host; } lp->link_up = 0; lp->max_retry_count = fnic->config.flogi_retries; lp->max_rport_retry_count = fnic->config.plogi_retries; lp->service_params = (FCP_SPPF_INIT_FCN | FCP_SPPF_RD_XRDY_DIS | FCP_SPPF_CONF_COMPL); if (fnic->config.flags & VFCF_FCP_SEQ_LVL_ERR) lp->service_params |= FCP_SPPF_RETRY; lp->boot_time = jiffies; lp->e_d_tov = fnic->config.ed_tov; lp->r_a_tov = fnic->config.ra_tov; lp->link_supported_speeds = FC_PORTSPEED_10GBIT; fc_set_wwnn(lp, fnic->config.node_wwn); fc_set_wwpn(lp, fnic->config.port_wwn); fcoe_libfc_config(lp, &fnic->ctlr, &fnic_transport_template, 0); if (!fc_exch_mgr_alloc(lp, FC_CLASS_3, FCPIO_HOST_EXCH_RANGE_START, FCPIO_HOST_EXCH_RANGE_END, NULL)) { err = -ENOMEM; goto err_out_fc_exch_mgr_alloc; } fc_lport_init_stats(lp); fnic->stats_reset_time = jiffies; fc_lport_config(lp); if (fc_set_mfs(lp, fnic->config.maxdatafieldsize + sizeof(struct fc_frame_header))) { err = -EINVAL; goto err_out_free_exch_mgr; } fc_host_maxframe_size(lp->host) = lp->mfs; fc_host_dev_loss_tmo(lp->host) = fnic->config.port_down_timeout / 1000; sprintf(fc_host_symbolic_name(lp->host), DRV_NAME "" v"" DRV_VERSION "" over %s"", fnic->name); spin_lock_irqsave(&fnic_list_lock, flags); list_add_tail(&fnic->list, &fnic_list); spin_unlock_irqrestore(&fnic_list_lock, flags); INIT_WORK(&fnic->link_work, fnic_handle_link); INIT_WORK(&fnic->frame_work, fnic_handle_frame); INIT_WORK(&fnic->flush_work, fnic_flush_tx); skb_queue_head_init(&fnic->frame_queue); skb_queue_head_init(&fnic->tx_queue); fc_fabric_login(lp); vnic_dev_enable(fnic->vdev); for (i = 0; i < fnic->intr_count; i++) vnic_intr_unmask(&fnic->intr[i]); fnic_notify_timer_start(fnic); return 0; err_out_free_exch_mgr: fc_exch_mgr_free(lp); err_out_fc_exch_mgr_alloc: fc_remove_host(lp->host); scsi_remove_host(lp->host); err_out_scsi_add_host: fnic_free_intr(fnic); err_out_request_intr: for (i = 0; i < fnic->rq_count; i++) vnic_rq_clean(&fnic->rq[i], fnic_free_rq_buf); err_out_rq_buf: vnic_dev_notify_unset(fnic->vdev); err_out_free_max_pool: mempool_destroy(fnic->io_sgl_pool[FNIC_SGL_CACHE_MAX]); err_out_free_dflt_pool: mempool_destroy(fnic->io_sgl_pool[FNIC_SGL_CACHE_DFLT]); err_out_free_ioreq_pool: mempool_destroy(fnic->io_req_pool); err_out_free_resources: for (hwq = 0; hwq < fnic->wq_copy_count; hwq++) kfree(fnic->sw_copy_wq[hwq].io_req_table); fnic_free_vnic_resources(fnic); err_out_clear_intr: fnic_clear_intr_mode(fnic); err_out_dev_close: vnic_dev_close(fnic->vdev); err_out_dev_cmd_deinit: err_out_vnic_unregister: vnic_dev_unregister(fnic->vdev); err_out_iounmap: fnic_iounmap(fnic); err_out_release_regions: pci_release_regions(pdev); err_out_disable_device: pci_disable_device(pdev); err_out_free_hba: fnic_stats_debugfs_remove(fnic); ida_free(&fnic_ida, fnic->fnic_num); err_out_ida_alloc: scsi_host_put(lp->host); err_out: return err; }"
805----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50080/bad/ublk_drv.c----ublk_ctrl_add_dev,"static int ublk_ctrl_add_dev(struct io_uring_cmd *cmd) { const struct ublksrv_ctrl_cmd *header = io_uring_sqe_cmd(cmd->sqe); void __user *argp = (void __user *)(unsigned long)header->addr; struct ublksrv_ctrl_dev_info info; struct ublk_device *ub; int ret = -EINVAL; if (header->len < sizeof(info) || !header->addr) return -EINVAL; if (header->queue_id != (u16)-1) { pr_warn(""%s: queue_id is wrong %x\n"", __func__, header->queue_id); return -EINVAL; } if (copy_from_user(&info, argp, sizeof(info))) return -EFAULT; if (capable(CAP_SYS_ADMIN)) info.flags &= ~UBLK_F_UNPRIVILEGED_DEV; else if (!(info.flags & UBLK_F_UNPRIVILEGED_DEV)) return -EPERM; <S2SV_StartVul> if (info.flags & UBLK_F_UNPRIVILEGED_DEV) <S2SV_EndVul> info.flags &= ~(UBLK_F_USER_RECOVERY_REISSUE | UBLK_F_USER_RECOVERY); ublk_store_owner_uid_gid(&info.owner_uid, &info.owner_gid); if (header->dev_id != info.dev_id) { pr_warn(""%s: dev id not match %u %u\n"", __func__, header->dev_id, info.dev_id); return -EINVAL; } if (header->dev_id != U32_MAX && header->dev_id >= UBLK_MAX_UBLKS) { pr_warn(""%s: dev id is too large. Max supported is %d\n"", __func__, UBLK_MAX_UBLKS - 1); return -EINVAL; } ublk_dump_dev_info(&info); ret = mutex_lock_killable(&ublk_ctl_mutex); if (ret) return ret; ret = -EACCES; if (ublks_added >= ublks_max) goto out_unlock; ret = -ENOMEM; ub = kzalloc(sizeof(*ub), GFP_KERNEL); if (!ub) goto out_unlock; mutex_init(&ub->mutex); spin_lock_init(&ub->lock); INIT_WORK(&ub->quiesce_work, ublk_quiesce_work_fn); INIT_WORK(&ub->stop_work, ublk_stop_work_fn); ret = ublk_alloc_dev_number(ub, header->dev_id); if (ret < 0) goto out_free_ub; memcpy(&ub->dev_info, &info, sizeof(info)); ub->dev_info.dev_id = ub->ub_number; ub->dev_info.flags &= UBLK_F_ALL; ub->dev_info.flags |= UBLK_F_CMD_IOCTL_ENCODE | UBLK_F_URING_CMD_COMP_IN_TASK; if (ublk_dev_is_user_copy(ub)) ub->dev_info.flags &= ~UBLK_F_NEED_GET_DATA; if (ublk_dev_is_zoned(ub) && (!IS_ENABLED(CONFIG_BLK_DEV_ZONED) || !ublk_dev_is_user_copy(ub))) { ret = -EINVAL; goto out_free_dev_number; } ub->dev_info.flags &= ~UBLK_F_SUPPORT_ZERO_COPY; ub->dev_info.nr_hw_queues = min_t(unsigned int, ub->dev_info.nr_hw_queues, nr_cpu_ids); ublk_align_max_io_size(ub); ret = ublk_init_queues(ub); if (ret) goto out_free_dev_number; ret = ublk_add_tag_set(ub); if (ret) goto out_deinit_queues; ret = -EFAULT; if (copy_to_user(argp, &ub->dev_info, sizeof(info))) goto out_free_tag_set; ret = ublk_add_chdev(ub); goto out_unlock; out_free_tag_set: blk_mq_free_tag_set(&ub->tag_set); out_deinit_queues: ublk_deinit_queues(ub); out_free_dev_number: ublk_free_dev_number(ub); out_free_ub: mutex_destroy(&ub->mutex); kfree(ub); out_unlock: mutex_unlock(&ublk_ctl_mutex); return ret; }","- if (info.flags & UBLK_F_UNPRIVILEGED_DEV)
+ if (info.flags & UBLK_F_UNPRIVILEGED_DEV) {
+ if (info.flags & UBLK_F_USER_COPY)
+ return -EINVAL;
+ }","static int ublk_ctrl_add_dev(struct io_uring_cmd *cmd) { const struct ublksrv_ctrl_cmd *header = io_uring_sqe_cmd(cmd->sqe); void __user *argp = (void __user *)(unsigned long)header->addr; struct ublksrv_ctrl_dev_info info; struct ublk_device *ub; int ret = -EINVAL; if (header->len < sizeof(info) || !header->addr) return -EINVAL; if (header->queue_id != (u16)-1) { pr_warn(""%s: queue_id is wrong %x\n"", __func__, header->queue_id); return -EINVAL; } if (copy_from_user(&info, argp, sizeof(info))) return -EFAULT; if (capable(CAP_SYS_ADMIN)) info.flags &= ~UBLK_F_UNPRIVILEGED_DEV; else if (!(info.flags & UBLK_F_UNPRIVILEGED_DEV)) return -EPERM; if (info.flags & UBLK_F_UNPRIVILEGED_DEV) { info.flags &= ~(UBLK_F_USER_RECOVERY_REISSUE | UBLK_F_USER_RECOVERY); if (info.flags & UBLK_F_USER_COPY) return -EINVAL; } ublk_store_owner_uid_gid(&info.owner_uid, &info.owner_gid); if (header->dev_id != info.dev_id) { pr_warn(""%s: dev id not match %u %u\n"", __func__, header->dev_id, info.dev_id); return -EINVAL; } if (header->dev_id != U32_MAX && header->dev_id >= UBLK_MAX_UBLKS) { pr_warn(""%s: dev id is too large. Max supported is %d\n"", __func__, UBLK_MAX_UBLKS - 1); return -EINVAL; } ublk_dump_dev_info(&info); ret = mutex_lock_killable(&ublk_ctl_mutex); if (ret) return ret; ret = -EACCES; if (ublks_added >= ublks_max) goto out_unlock; ret = -ENOMEM; ub = kzalloc(sizeof(*ub), GFP_KERNEL); if (!ub) goto out_unlock; mutex_init(&ub->mutex); spin_lock_init(&ub->lock); INIT_WORK(&ub->quiesce_work, ublk_quiesce_work_fn); INIT_WORK(&ub->stop_work, ublk_stop_work_fn); ret = ublk_alloc_dev_number(ub, header->dev_id); if (ret < 0) goto out_free_ub; memcpy(&ub->dev_info, &info, sizeof(info)); ub->dev_info.dev_id = ub->ub_number; ub->dev_info.flags &= UBLK_F_ALL; ub->dev_info.flags |= UBLK_F_CMD_IOCTL_ENCODE | UBLK_F_URING_CMD_COMP_IN_TASK; if (ublk_dev_is_user_copy(ub)) ub->dev_info.flags &= ~UBLK_F_NEED_GET_DATA; if (ublk_dev_is_zoned(ub) && (!IS_ENABLED(CONFIG_BLK_DEV_ZONED) || !ublk_dev_is_user_copy(ub))) { ret = -EINVAL; goto out_free_dev_number; } ub->dev_info.flags &= ~UBLK_F_SUPPORT_ZERO_COPY; ub->dev_info.nr_hw_queues = min_t(unsigned int, ub->dev_info.nr_hw_queues, nr_cpu_ids); ublk_align_max_io_size(ub); ret = ublk_init_queues(ub); if (ret) goto out_free_dev_number; ret = ublk_add_tag_set(ub); if (ret) goto out_deinit_queues; ret = -EFAULT; if (copy_to_user(argp, &ub->dev_info, sizeof(info))) goto out_free_tag_set; ret = ublk_add_chdev(ub); goto out_unlock; out_free_tag_set: blk_mq_free_tag_set(&ub->tag_set); out_deinit_queues: ublk_deinit_queues(ub); out_free_dev_number: ublk_free_dev_number(ub); out_free_ub: mutex_destroy(&ub->mutex); kfree(ub); out_unlock: mutex_unlock(&ublk_ctl_mutex); return ret; }"
488----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47730/bad/qm.c----qm_soft_reset_prepare,"static int qm_soft_reset_prepare(struct hisi_qm *qm) { struct pci_dev *pdev = qm->pdev; int ret; ret = qm_check_req_recv(qm); if (ret) return ret; if (qm->vfs_num) { ret = qm_set_vf_mse(qm, false); if (ret) { pci_err(pdev, ""Fails to disable vf MSE bit.\n""); return ret; } } ret = qm->ops->set_msi(qm, false); if (ret) { pci_err(pdev, ""Fails to disable PEH MSI bit.\n""); return ret; <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> qm_dev_ecc_mbit_handle(qm); <S2SV_EndVul> ret = qm_master_ooo_check(qm); if (ret) return ret; if (qm->err_ini->close_sva_prefetch) qm->err_ini->close_sva_prefetch(qm); ret = qm_set_pf_mse(qm, false); if (ret) pci_err(pdev, ""Fails to disable pf MSE bit.\n""); return ret; }","- }
- qm_dev_ecc_mbit_handle(qm);
+ }","static int qm_soft_reset_prepare(struct hisi_qm *qm) { struct pci_dev *pdev = qm->pdev; int ret; ret = qm_check_req_recv(qm); if (ret) return ret; if (qm->vfs_num) { ret = qm_set_vf_mse(qm, false); if (ret) { pci_err(pdev, ""Fails to disable vf MSE bit.\n""); return ret; } } ret = qm->ops->set_msi(qm, false); if (ret) { pci_err(pdev, ""Fails to disable PEH MSI bit.\n""); return ret; } ret = qm_master_ooo_check(qm); if (ret) return ret; if (qm->err_ini->close_sva_prefetch) qm->err_ini->close_sva_prefetch(qm); ret = qm_set_pf_mse(qm, false); if (ret) pci_err(pdev, ""Fails to disable pf MSE bit.\n""); return ret; }"
908----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50154/bad/inet_connection_sock.c----reqsk_timer_handler,"static void reqsk_timer_handler(struct timer_list *t) { struct request_sock *req = from_timer(req, t, rsk_timer); struct request_sock *nreq = NULL, *oreq = req; struct sock *sk_listener = req->rsk_listener; struct inet_connection_sock *icsk; struct request_sock_queue *queue; struct net *net; int max_syn_ack_retries, qlen, expire = 0, resend = 0; if (inet_sk_state_load(sk_listener) != TCP_LISTEN) { struct sock *nsk; nsk = reuseport_migrate_sock(sk_listener, req_to_sk(req), NULL); if (!nsk) goto drop; nreq = inet_reqsk_clone(req, nsk); if (!nreq) goto drop; refcount_set(&nreq->rsk_refcnt, 2 + 1); timer_setup(&nreq->rsk_timer, reqsk_timer_handler, TIMER_PINNED); reqsk_queue_migrated(&inet_csk(nsk)->icsk_accept_queue, req); req = nreq; sk_listener = nsk; } icsk = inet_csk(sk_listener); net = sock_net(sk_listener); max_syn_ack_retries = READ_ONCE(icsk->icsk_syn_retries) ? : READ_ONCE(net->ipv4.sysctl_tcp_synack_retries); queue = &icsk->icsk_accept_queue; qlen = reqsk_queue_len(queue); if ((qlen << 1) > max(8U, READ_ONCE(sk_listener->sk_max_ack_backlog))) { int young = reqsk_queue_len_young(queue) << 1; while (max_syn_ack_retries > 2) { if (qlen < young) break; max_syn_ack_retries--; young <<= 1; } } syn_ack_recalc(req, max_syn_ack_retries, READ_ONCE(queue->rskq_defer_accept), &expire, &resend); req->rsk_ops->syn_ack_timeout(req); if (!expire && (!resend || !inet_rtx_syn_ack(sk_listener, req) || inet_rsk(req)->acked)) { if (req->num_timeout++ == 0) atomic_dec(&queue->young); mod_timer(&req->rsk_timer, jiffies + reqsk_timeout(req, TCP_RTO_MAX)); if (!nreq) return; if (!inet_ehash_insert(req_to_sk(nreq), req_to_sk(oreq), NULL)) { <S2SV_StartVul> inet_csk_reqsk_queue_drop(sk_listener, nreq); <S2SV_EndVul> goto no_ownership; } __NET_INC_STATS(net, LINUX_MIB_TCPMIGRATEREQSUCCESS); reqsk_migrate_reset(oreq); reqsk_queue_removed(&inet_csk(oreq->rsk_listener)->icsk_accept_queue, oreq); reqsk_put(oreq); reqsk_put(nreq); return; } if (nreq) { __NET_INC_STATS(net, LINUX_MIB_TCPMIGRATEREQFAILURE); no_ownership: reqsk_migrate_reset(nreq); reqsk_queue_removed(queue, nreq); __reqsk_free(nreq); } drop: <S2SV_StartVul> inet_csk_reqsk_queue_drop_and_put(oreq->rsk_listener, oreq); <S2SV_EndVul> }","- inet_csk_reqsk_queue_drop(sk_listener, nreq);
- inet_csk_reqsk_queue_drop_and_put(oreq->rsk_listener, oreq);
+ __inet_csk_reqsk_queue_drop(sk_listener, nreq, true);
+ }
+ __inet_csk_reqsk_queue_drop(sk_listener, oreq, true);
+ reqsk_put(req);
+ }","static void reqsk_timer_handler(struct timer_list *t) { struct request_sock *req = from_timer(req, t, rsk_timer); struct request_sock *nreq = NULL, *oreq = req; struct sock *sk_listener = req->rsk_listener; struct inet_connection_sock *icsk; struct request_sock_queue *queue; struct net *net; int max_syn_ack_retries, qlen, expire = 0, resend = 0; if (inet_sk_state_load(sk_listener) != TCP_LISTEN) { struct sock *nsk; nsk = reuseport_migrate_sock(sk_listener, req_to_sk(req), NULL); if (!nsk) goto drop; nreq = inet_reqsk_clone(req, nsk); if (!nreq) goto drop; refcount_set(&nreq->rsk_refcnt, 2 + 1); timer_setup(&nreq->rsk_timer, reqsk_timer_handler, TIMER_PINNED); reqsk_queue_migrated(&inet_csk(nsk)->icsk_accept_queue, req); req = nreq; sk_listener = nsk; } icsk = inet_csk(sk_listener); net = sock_net(sk_listener); max_syn_ack_retries = READ_ONCE(icsk->icsk_syn_retries) ? : READ_ONCE(net->ipv4.sysctl_tcp_synack_retries); queue = &icsk->icsk_accept_queue; qlen = reqsk_queue_len(queue); if ((qlen << 1) > max(8U, READ_ONCE(sk_listener->sk_max_ack_backlog))) { int young = reqsk_queue_len_young(queue) << 1; while (max_syn_ack_retries > 2) { if (qlen < young) break; max_syn_ack_retries--; young <<= 1; } } syn_ack_recalc(req, max_syn_ack_retries, READ_ONCE(queue->rskq_defer_accept), &expire, &resend); req->rsk_ops->syn_ack_timeout(req); if (!expire && (!resend || !inet_rtx_syn_ack(sk_listener, req) || inet_rsk(req)->acked)) { if (req->num_timeout++ == 0) atomic_dec(&queue->young); mod_timer(&req->rsk_timer, jiffies + reqsk_timeout(req, TCP_RTO_MAX)); if (!nreq) return; if (!inet_ehash_insert(req_to_sk(nreq), req_to_sk(oreq), NULL)) { __inet_csk_reqsk_queue_drop(sk_listener, nreq, true); goto no_ownership; } __NET_INC_STATS(net, LINUX_MIB_TCPMIGRATEREQSUCCESS); reqsk_migrate_reset(oreq); reqsk_queue_removed(&inet_csk(oreq->rsk_listener)->icsk_accept_queue, oreq); reqsk_put(oreq); reqsk_put(nreq); return; } if (nreq) { __NET_INC_STATS(net, LINUX_MIB_TCPMIGRATEREQFAILURE); no_ownership: reqsk_migrate_reset(nreq); reqsk_queue_removed(queue, nreq); __reqsk_free(nreq); } drop: __inet_csk_reqsk_queue_drop(sk_listener, oreq, true); reqsk_put(req); }"
439----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47677/bad/nls.c----exfat_create_upcase_table,"int exfat_create_upcase_table(struct super_block *sb) { int i, ret; unsigned int tbl_clu, type; sector_t sector; unsigned long long tbl_size, num_sectors; unsigned char blksize_bits = sb->s_blocksize_bits; struct exfat_chain clu; struct exfat_dentry *ep; struct exfat_sb_info *sbi = EXFAT_SB(sb); struct buffer_head *bh; clu.dir = sbi->root_dir; clu.flags = ALLOC_FAT_CHAIN; while (clu.dir != EXFAT_EOF_CLUSTER) { for (i = 0; i < sbi->dentries_per_clu; i++) { ep = exfat_get_dentry(sb, &clu, i, &bh); if (!ep) return -EIO; type = exfat_get_entry_type(ep); if (type == TYPE_UNUSED) { brelse(bh); break; } if (type != TYPE_UPCASE) { brelse(bh); continue; } tbl_clu = le32_to_cpu(ep->dentry.upcase.start_clu); tbl_size = le64_to_cpu(ep->dentry.upcase.size); sector = exfat_cluster_to_sector(sbi, tbl_clu); num_sectors = ((tbl_size - 1) >> blksize_bits) + 1; ret = exfat_load_upcase_table(sb, sector, num_sectors, le32_to_cpu(ep->dentry.upcase.checksum)); brelse(bh); <S2SV_StartVul> if (ret && ret != -EIO) <S2SV_EndVul> goto load_default; return ret; } if (exfat_get_next_cluster(sb, &(clu.dir))) return -EIO; } load_default: return exfat_load_default_upcase_table(sb); }","- if (ret && ret != -EIO)
+ if (ret && ret != -EIO) {
+ exfat_free_upcase_table(sbi);
+ }","int exfat_create_upcase_table(struct super_block *sb) { int i, ret; unsigned int tbl_clu, type; sector_t sector; unsigned long long tbl_size, num_sectors; unsigned char blksize_bits = sb->s_blocksize_bits; struct exfat_chain clu; struct exfat_dentry *ep; struct exfat_sb_info *sbi = EXFAT_SB(sb); struct buffer_head *bh; clu.dir = sbi->root_dir; clu.flags = ALLOC_FAT_CHAIN; while (clu.dir != EXFAT_EOF_CLUSTER) { for (i = 0; i < sbi->dentries_per_clu; i++) { ep = exfat_get_dentry(sb, &clu, i, &bh); if (!ep) return -EIO; type = exfat_get_entry_type(ep); if (type == TYPE_UNUSED) { brelse(bh); break; } if (type != TYPE_UPCASE) { brelse(bh); continue; } tbl_clu = le32_to_cpu(ep->dentry.upcase.start_clu); tbl_size = le64_to_cpu(ep->dentry.upcase.size); sector = exfat_cluster_to_sector(sbi, tbl_clu); num_sectors = ((tbl_size - 1) >> blksize_bits) + 1; ret = exfat_load_upcase_table(sb, sector, num_sectors, le32_to_cpu(ep->dentry.upcase.checksum)); brelse(bh); if (ret && ret != -EIO) { exfat_free_upcase_table(sbi); goto load_default; } return ret; } if (exfat_get_next_cluster(sb, &(clu.dir))) return -EIO; } load_default: return exfat_load_default_upcase_table(sb); }"
6----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-43891/bad/trace_events.c----event_enable_write,"event_enable_write(struct file *filp, const char __user *ubuf, size_t cnt, loff_t *ppos) { struct trace_event_file *file; unsigned long val; int ret; ret = kstrtoul_from_user(ubuf, cnt, 10, &val); if (ret) return ret; switch (val) { case 0: case 1: ret = -ENODEV; mutex_lock(&event_mutex); <S2SV_StartVul> file = event_file_data(filp); <S2SV_EndVul> <S2SV_StartVul> if (likely(file && !(file->flags & EVENT_FILE_FL_FREED))) { <S2SV_EndVul> ret = tracing_update_buffers(file->tr); if (ret < 0) { mutex_unlock(&event_mutex); return ret; } ret = ftrace_event_enable_disable(file, val); } mutex_unlock(&event_mutex); break; default: return -EINVAL; } *ppos += cnt; return ret ? ret : cnt; }","- file = event_file_data(filp);
- if (likely(file && !(file->flags & EVENT_FILE_FL_FREED))) {
+ file = event_file_file(filp);
+ if (likely(file)) {","event_enable_write(struct file *filp, const char __user *ubuf, size_t cnt, loff_t *ppos) { struct trace_event_file *file; unsigned long val; int ret; ret = kstrtoul_from_user(ubuf, cnt, 10, &val); if (ret) return ret; switch (val) { case 0: case 1: ret = -ENODEV; mutex_lock(&event_mutex); file = event_file_file(filp); if (likely(file)) { ret = tracing_update_buffers(file->tr); if (ret < 0) { mutex_unlock(&event_mutex); return ret; } ret = ftrace_event_enable_disable(file, val); } mutex_unlock(&event_mutex); break; default: return -EINVAL; } *ppos += cnt; return ret ? ret : cnt; }"
1407----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56730/bad/trans_usbg.c----*usb9pfs_alloc_instance,"static struct usb_function_instance *usb9pfs_alloc_instance(void) { struct f_usb9pfs_opts *usb9pfs_opts; struct f_usb9pfs_dev *dev; usb9pfs_opts = kzalloc(sizeof(*usb9pfs_opts), GFP_KERNEL); if (!usb9pfs_opts) return ERR_PTR(-ENOMEM); mutex_init(&usb9pfs_opts->lock); usb9pfs_opts->func_inst.set_inst_name = usb9pfs_set_inst_tag; usb9pfs_opts->func_inst.free_func_inst = usb9pfs_free_instance; usb9pfs_opts->buflen = DEFAULT_BUFLEN; dev = kzalloc(sizeof(*dev), GFP_KERNEL); <S2SV_StartVul> if (IS_ERR(dev)) { <S2SV_EndVul> kfree(usb9pfs_opts); <S2SV_StartVul> return ERR_CAST(dev); <S2SV_EndVul> } usb9pfs_opts->dev = dev; dev->opts = usb9pfs_opts; config_group_init_type_name(&usb9pfs_opts->func_inst.group, """", &usb9pfs_func_type); mutex_lock(&usb9pfs_lock); list_add_tail(&dev->usb9pfs_instance, &usbg_instance_list); mutex_unlock(&usb9pfs_lock); return &usb9pfs_opts->func_inst; }","- if (IS_ERR(dev)) {
- return ERR_CAST(dev);
+ if (!dev) {
+ return ERR_PTR(-ENOMEM);","static struct usb_function_instance *usb9pfs_alloc_instance(void) { struct f_usb9pfs_opts *usb9pfs_opts; struct f_usb9pfs_dev *dev; usb9pfs_opts = kzalloc(sizeof(*usb9pfs_opts), GFP_KERNEL); if (!usb9pfs_opts) return ERR_PTR(-ENOMEM); mutex_init(&usb9pfs_opts->lock); usb9pfs_opts->func_inst.set_inst_name = usb9pfs_set_inst_tag; usb9pfs_opts->func_inst.free_func_inst = usb9pfs_free_instance; usb9pfs_opts->buflen = DEFAULT_BUFLEN; dev = kzalloc(sizeof(*dev), GFP_KERNEL); if (!dev) { kfree(usb9pfs_opts); return ERR_PTR(-ENOMEM); } usb9pfs_opts->dev = dev; dev->opts = usb9pfs_opts; config_group_init_type_name(&usb9pfs_opts->func_inst.group, """", &usb9pfs_func_type); mutex_lock(&usb9pfs_lock); list_add_tail(&dev->usb9pfs_instance, &usbg_instance_list); mutex_unlock(&usb9pfs_lock); return &usb9pfs_opts->func_inst; }"
1059----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50300/bad/rtq2208-regulator.c----rtq2208_probe,"static int rtq2208_probe(struct i2c_client *i2c) { struct device *dev = &i2c->dev; struct regmap *regmap; struct rtq2208_regulator_desc *rdesc[RTQ2208_LDO_MAX]; struct regulator_dev *rdev; <S2SV_StartVul> struct regulator_config cfg; <S2SV_EndVul> struct rtq2208_rdev_map *rdev_map; int i, ret = 0, idx, n_regulator = 0; unsigned int regulator_idx_table[RTQ2208_LDO_MAX], buck_masks[RTQ2208_BUCK_NUM_IRQ_REGS] = {0x33, 0x33, 0x33, 0x33, 0x33}; rdev_map = devm_kzalloc(dev, sizeof(struct rtq2208_rdev_map), GFP_KERNEL); if (!rdev_map) return -ENOMEM; regmap = devm_regmap_init_i2c(i2c, &rtq2208_regmap_config); if (IS_ERR(regmap)) return dev_err_probe(dev, PTR_ERR(regmap), ""Failed to allocate regmap\n""); ret = rtq2208_regulator_check(i2c->addr, &n_regulator, regulator_idx_table, buck_masks); if (ret) return dev_err_probe(dev, ret, ""Failed to check used regulators\n""); rdev_map->regmap = regmap; rdev_map->dev = dev; cfg.dev = dev; ret = rtq2208_parse_regulator_dt_data(n_regulator, regulator_idx_table, rdesc, dev); if (ret) return ret; for (i = 0; i < n_regulator; i++) { idx = regulator_idx_table[i]; rdev = devm_regulator_register(dev, &rdesc[i]->desc, &cfg); if (IS_ERR(rdev)) return PTR_ERR(rdev); rdev_map->rdev[idx] = rdev; } ret = rtq2208_init_irq_mask(rdev_map, buck_masks); if (ret) return ret; return devm_request_threaded_irq(dev, i2c->irq, NULL, rtq2208_irq_handler, IRQF_ONESHOT, dev_name(dev), rdev_map); }","- struct regulator_config cfg;
+ struct regulator_config cfg = {};","static int rtq2208_probe(struct i2c_client *i2c) { struct device *dev = &i2c->dev; struct regmap *regmap; struct rtq2208_regulator_desc *rdesc[RTQ2208_LDO_MAX]; struct regulator_dev *rdev; struct regulator_config cfg = {}; struct rtq2208_rdev_map *rdev_map; int i, ret = 0, idx, n_regulator = 0; unsigned int regulator_idx_table[RTQ2208_LDO_MAX], buck_masks[RTQ2208_BUCK_NUM_IRQ_REGS] = {0x33, 0x33, 0x33, 0x33, 0x33}; rdev_map = devm_kzalloc(dev, sizeof(struct rtq2208_rdev_map), GFP_KERNEL); if (!rdev_map) return -ENOMEM; regmap = devm_regmap_init_i2c(i2c, &rtq2208_regmap_config); if (IS_ERR(regmap)) return dev_err_probe(dev, PTR_ERR(regmap), ""Failed to allocate regmap\n""); ret = rtq2208_regulator_check(i2c->addr, &n_regulator, regulator_idx_table, buck_masks); if (ret) return dev_err_probe(dev, ret, ""Failed to check used regulators\n""); rdev_map->regmap = regmap; rdev_map->dev = dev; cfg.dev = dev; ret = rtq2208_parse_regulator_dt_data(n_regulator, regulator_idx_table, rdesc, dev); if (ret) return ret; for (i = 0; i < n_regulator; i++) { idx = regulator_idx_table[i]; rdev = devm_regulator_register(dev, &rdesc[i]->desc, &cfg); if (IS_ERR(rdev)) return PTR_ERR(rdev); rdev_map->rdev[idx] = rdev; } ret = rtq2208_init_irq_mask(rdev_map, buck_masks); if (ret) return ret; return devm_request_threaded_irq(dev, i2c->irq, NULL, rtq2208_irq_handler, IRQF_ONESHOT, dev_name(dev), rdev_map); }"
1026----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50267/bad/io_edgeport.c----edge_bulk_out_cmd_callback,"static void edge_bulk_out_cmd_callback(struct urb *urb) { struct edgeport_port *edge_port = urb->context; int status = urb->status; atomic_dec(&CmdUrbs); <S2SV_StartVul> dev_dbg(&urb->dev->dev, ""%s - FREE URB %p (outstanding %d)\n"", <S2SV_EndVul> <S2SV_StartVul> __func__, urb, atomic_read(&CmdUrbs)); <S2SV_EndVul> kfree(urb->transfer_buffer); usb_free_urb(urb); if (status) { <S2SV_StartVul> dev_dbg(&urb->dev->dev, <S2SV_EndVul> <S2SV_StartVul> ""%s - nonzero write bulk status received: %d\n"", <S2SV_EndVul> __func__, status); return; } if (edge_port->open) tty_port_tty_wakeup(&edge_port->port->port); edge_port->commandPending = false; wake_up(&edge_port->wait_command); }","- dev_dbg(&urb->dev->dev, ""%s - FREE URB %p (outstanding %d)\n"",
- __func__, urb, atomic_read(&CmdUrbs));
- dev_dbg(&urb->dev->dev,
- ""%s - nonzero write bulk status received: %d\n"",
+ struct device *dev = &urb->dev->dev;
+ dev_dbg(dev, ""%s - FREE URB %p (outstanding %d)\n"", __func__, urb,
+ atomic_read(&CmdUrbs));
+ dev_dbg(dev, ""%s - nonzero write bulk status received: %d\n"",","static void edge_bulk_out_cmd_callback(struct urb *urb) { struct edgeport_port *edge_port = urb->context; struct device *dev = &urb->dev->dev; int status = urb->status; atomic_dec(&CmdUrbs); dev_dbg(dev, ""%s - FREE URB %p (outstanding %d)\n"", __func__, urb, atomic_read(&CmdUrbs)); kfree(urb->transfer_buffer); usb_free_urb(urb); if (status) { dev_dbg(dev, ""%s - nonzero write bulk status received: %d\n"", __func__, status); return; } if (edge_port->open) tty_port_tty_wakeup(&edge_port->port->port); edge_port->commandPending = false; wake_up(&edge_port->wait_command); }"
1078----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53061/bad/jpeg-core.c----s5p_jpeg_parse_hdr,"static bool s5p_jpeg_parse_hdr(struct s5p_jpeg_q_data *result, unsigned long buffer, unsigned long size, struct s5p_jpeg_ctx *ctx) { int c, components = 0, notfound, n_dht = 0, n_dqt = 0; unsigned int height = 0, width = 0, word, subsampling = 0; unsigned int sos = 0, sof = 0, sof_len = 0; unsigned int dht[S5P_JPEG_MAX_MARKER], dht_len[S5P_JPEG_MAX_MARKER]; unsigned int dqt[S5P_JPEG_MAX_MARKER], dqt_len[S5P_JPEG_MAX_MARKER]; long length; struct s5p_jpeg_buffer jpeg_buffer; jpeg_buffer.size = size; jpeg_buffer.data = buffer; jpeg_buffer.curr = 0; notfound = 1; while (notfound || !sos) { c = get_byte(&jpeg_buffer); if (c == -1) return false; if (c != 0xff) continue; do c = get_byte(&jpeg_buffer); while (c == 0xff); if (c == -1) return false; if (c == 0) continue; length = 0; switch (c) { case SOF0: if (get_word_be(&jpeg_buffer, &word)) break; length = (long)word - 2; <S2SV_StartVul> if (!length) <S2SV_EndVul> return false; sof = jpeg_buffer.curr; sof_len = length; if (get_byte(&jpeg_buffer) == -1) break; if (get_word_be(&jpeg_buffer, &height)) break; if (get_word_be(&jpeg_buffer, &width)) break; components = get_byte(&jpeg_buffer); if (components == -1) break; if (components == 1) { subsampling = 0x33; } else { skip(&jpeg_buffer, 1); subsampling = get_byte(&jpeg_buffer); skip(&jpeg_buffer, 1); } if (components > 3) return false; skip(&jpeg_buffer, components * 2); notfound = 0; break; case DQT: if (get_word_be(&jpeg_buffer, &word)) break; length = (long)word - 2; <S2SV_StartVul> if (!length) <S2SV_EndVul> return false; if (n_dqt >= S5P_JPEG_MAX_MARKER) return false; dqt[n_dqt] = jpeg_buffer.curr; dqt_len[n_dqt++] = length; skip(&jpeg_buffer, length); break; case DHT: if (get_word_be(&jpeg_buffer, &word)) break; length = (long)word - 2; <S2SV_StartVul> if (!length) <S2SV_EndVul> return false; if (n_dht >= S5P_JPEG_MAX_MARKER) return false; dht[n_dht] = jpeg_buffer.curr; dht_len[n_dht++] = length; skip(&jpeg_buffer, length); break; case SOS: sos = jpeg_buffer.curr - 2; break; case RST ... RST + 7: case SOI: case EOI: case TEM: break; default: if (get_word_be(&jpeg_buffer, &word)) break; length = (long)word - 2; skip(&jpeg_buffer, length); break; } } if (notfound || !sos || !s5p_jpeg_subsampling_decode(ctx, subsampling)) return false; result->w = width; result->h = height; result->sos = sos; result->dht.n = n_dht; while (n_dht--) { result->dht.marker[n_dht] = dht[n_dht]; result->dht.len[n_dht] = dht_len[n_dht]; } result->dqt.n = n_dqt; while (n_dqt--) { result->dqt.marker[n_dqt] = dqt[n_dqt]; result->dqt.len[n_dqt] = dqt_len[n_dqt]; } result->sof = sof; result->sof_len = sof_len; return true; }","- if (!length)
- if (!length)
- if (!length)
+ if (length <= 0)
+ if (length <= 0)
+ if (length <= 0)","static bool s5p_jpeg_parse_hdr(struct s5p_jpeg_q_data *result, unsigned long buffer, unsigned long size, struct s5p_jpeg_ctx *ctx) { int c, components = 0, notfound, n_dht = 0, n_dqt = 0; unsigned int height = 0, width = 0, word, subsampling = 0; unsigned int sos = 0, sof = 0, sof_len = 0; unsigned int dht[S5P_JPEG_MAX_MARKER], dht_len[S5P_JPEG_MAX_MARKER]; unsigned int dqt[S5P_JPEG_MAX_MARKER], dqt_len[S5P_JPEG_MAX_MARKER]; long length; struct s5p_jpeg_buffer jpeg_buffer; jpeg_buffer.size = size; jpeg_buffer.data = buffer; jpeg_buffer.curr = 0; notfound = 1; while (notfound || !sos) { c = get_byte(&jpeg_buffer); if (c == -1) return false; if (c != 0xff) continue; do c = get_byte(&jpeg_buffer); while (c == 0xff); if (c == -1) return false; if (c == 0) continue; length = 0; switch (c) { case SOF0: if (get_word_be(&jpeg_buffer, &word)) break; length = (long)word - 2; if (length <= 0) return false; sof = jpeg_buffer.curr; sof_len = length; if (get_byte(&jpeg_buffer) == -1) break; if (get_word_be(&jpeg_buffer, &height)) break; if (get_word_be(&jpeg_buffer, &width)) break; components = get_byte(&jpeg_buffer); if (components == -1) break; if (components == 1) { subsampling = 0x33; } else { skip(&jpeg_buffer, 1); subsampling = get_byte(&jpeg_buffer); skip(&jpeg_buffer, 1); } if (components > 3) return false; skip(&jpeg_buffer, components * 2); notfound = 0; break; case DQT: if (get_word_be(&jpeg_buffer, &word)) break; length = (long)word - 2; if (length <= 0) return false; if (n_dqt >= S5P_JPEG_MAX_MARKER) return false; dqt[n_dqt] = jpeg_buffer.curr; dqt_len[n_dqt++] = length; skip(&jpeg_buffer, length); break; case DHT: if (get_word_be(&jpeg_buffer, &word)) break; length = (long)word - 2; if (length <= 0) return false; if (n_dht >= S5P_JPEG_MAX_MARKER) return false; dht[n_dht] = jpeg_buffer.curr; dht_len[n_dht++] = length; skip(&jpeg_buffer, length); break; case SOS: sos = jpeg_buffer.curr - 2; break; case RST ... RST + 7: case SOI: case EOI: case TEM: break; default: if (get_word_be(&jpeg_buffer, &word)) break; length = (long)word - 2; skip(&jpeg_buffer, length); break; } } if (notfound || !sos || !s5p_jpeg_subsampling_decode(ctx, subsampling)) return false; result->w = width; result->h = height; result->sos = sos; result->dht.n = n_dht; while (n_dht--) { result->dht.marker[n_dht] = dht[n_dht]; result->dht.len[n_dht] = dht_len[n_dht]; } result->dqt.n = n_dqt; while (n_dqt--) { result->dqt.marker[n_dqt] = dqt[n_dqt]; result->dqt.len[n_dqt] = dqt_len[n_dqt]; } result->sof = sof; result->sof_len = sof_len; return true; }"
448----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47688/bad/module.c----module_add_driver,"int module_add_driver(struct module *mod, const struct device_driver *drv) { char *driver_name; struct module_kobject *mk = NULL; int ret; if (!drv) return 0; if (mod) mk = &mod->mkobj; else if (drv->mod_name) { struct kobject *mkobj; mkobj = kset_find_obj(module_kset, drv->mod_name); if (mkobj) { mk = container_of(mkobj, struct module_kobject, kobj); drv->p->mkobj = mk; kobject_put(mkobj); } } if (!mk) return 0; ret = sysfs_create_link(&drv->p->kobj, &mk->kobj, ""module""); if (ret) return ret; driver_name = make_driver_name(drv); if (!driver_name) { ret = -ENOMEM; <S2SV_StartVul> goto out; <S2SV_EndVul> } module_create_drivers_dir(mk); if (!mk->drivers_dir) { ret = -EINVAL; <S2SV_StartVul> goto out; <S2SV_EndVul> } ret = sysfs_create_link(mk->drivers_dir, &drv->p->kobj, driver_name); if (ret) <S2SV_StartVul> goto out; <S2SV_EndVul> kfree(driver_name); return 0; <S2SV_StartVul> out: <S2SV_EndVul> <S2SV_StartVul> sysfs_remove_link(&drv->p->kobj, ""module""); <S2SV_EndVul> sysfs_remove_link(mk->drivers_dir, driver_name); kfree(driver_name); return ret; }","- goto out;
- goto out;
- goto out;
- out:
- sysfs_remove_link(&drv->p->kobj, ""module"");
+ goto out_remove_kobj;
+ goto out_free_driver_name;
+ goto out_remove_drivers_dir;
+ out_remove_drivers_dir:
+ out_free_driver_name:
+ out_remove_kobj:
+ sysfs_remove_link(&drv->p->kobj, ""module"");","int module_add_driver(struct module *mod, const struct device_driver *drv) { char *driver_name; struct module_kobject *mk = NULL; int ret; if (!drv) return 0; if (mod) mk = &mod->mkobj; else if (drv->mod_name) { struct kobject *mkobj; mkobj = kset_find_obj(module_kset, drv->mod_name); if (mkobj) { mk = container_of(mkobj, struct module_kobject, kobj); drv->p->mkobj = mk; kobject_put(mkobj); } } if (!mk) return 0; ret = sysfs_create_link(&drv->p->kobj, &mk->kobj, ""module""); if (ret) return ret; driver_name = make_driver_name(drv); if (!driver_name) { ret = -ENOMEM; goto out_remove_kobj; } module_create_drivers_dir(mk); if (!mk->drivers_dir) { ret = -EINVAL; goto out_free_driver_name; } ret = sysfs_create_link(mk->drivers_dir, &drv->p->kobj, driver_name); if (ret) goto out_remove_drivers_dir; kfree(driver_name); return 0; out_remove_drivers_dir: sysfs_remove_link(mk->drivers_dir, driver_name); out_free_driver_name: kfree(driver_name); out_remove_kobj: sysfs_remove_link(&drv->p->kobj, ""module""); return ret; }"
520----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-47756/bad/pci-keystone.c----ks_pcie_quirk,"static void ks_pcie_quirk(struct pci_dev *dev) { struct pci_bus *bus = dev->bus; struct keystone_pcie *ks_pcie; struct device *bridge_dev; struct pci_dev *bridge; u32 val; static const struct pci_device_id rc_pci_devids[] = { { PCI_DEVICE(PCI_VENDOR_ID_TI, PCIE_RC_K2HK), .class = PCI_CLASS_BRIDGE_PCI << 8, .class_mask = ~0, }, { PCI_DEVICE(PCI_VENDOR_ID_TI, PCIE_RC_K2E), .class = PCI_CLASS_BRIDGE_PCI << 8, .class_mask = ~0, }, { PCI_DEVICE(PCI_VENDOR_ID_TI, PCIE_RC_K2L), .class = PCI_CLASS_BRIDGE_PCI << 8, .class_mask = ~0, }, { PCI_DEVICE(PCI_VENDOR_ID_TI, PCIE_RC_K2G), .class = PCI_CLASS_BRIDGE_PCI << 8, .class_mask = ~0, }, { 0, }, }; static const struct pci_device_id am6_pci_devids[] = { { PCI_DEVICE(PCI_VENDOR_ID_TI, PCI_DEVICE_ID_TI_AM654X), .class = PCI_CLASS_BRIDGE_PCI << 8, .class_mask = ~0, }, { 0, }, }; if (pci_is_root_bus(bus)) bridge = dev; while (!pci_is_root_bus(bus)) { bridge = bus->self; bus = bus->parent; } if (!bridge) return; if (pci_match_id(rc_pci_devids, bridge)) { if (pcie_get_readrq(dev) > 256) { dev_info(&dev->dev, ""limiting MRRS to 256 bytes\n""); pcie_set_readrq(dev, 256); } } if (pci_match_id(am6_pci_devids, bridge)) { bridge_dev = pci_get_host_bridge_device(dev); <S2SV_StartVul> if (!bridge_dev && !bridge_dev->parent) <S2SV_EndVul> return; ks_pcie = dev_get_drvdata(bridge_dev->parent); if (!ks_pcie) return; val = ks_pcie_app_readl(ks_pcie, PID); val &= RTL; val >>= RTL_SHIFT; if (val != AM6_PCI_PG1_RTL_VER) return; if (pcie_get_readrq(dev) > 128) { dev_info(&dev->dev, ""limiting MRRS to 128 bytes\n""); pcie_set_readrq(dev, 128); } } }","- if (!bridge_dev && !bridge_dev->parent)
+ if (!bridge_dev || !bridge_dev->parent)","static void ks_pcie_quirk(struct pci_dev *dev) { struct pci_bus *bus = dev->bus; struct keystone_pcie *ks_pcie; struct device *bridge_dev; struct pci_dev *bridge; u32 val; static const struct pci_device_id rc_pci_devids[] = { { PCI_DEVICE(PCI_VENDOR_ID_TI, PCIE_RC_K2HK), .class = PCI_CLASS_BRIDGE_PCI << 8, .class_mask = ~0, }, { PCI_DEVICE(PCI_VENDOR_ID_TI, PCIE_RC_K2E), .class = PCI_CLASS_BRIDGE_PCI << 8, .class_mask = ~0, }, { PCI_DEVICE(PCI_VENDOR_ID_TI, PCIE_RC_K2L), .class = PCI_CLASS_BRIDGE_PCI << 8, .class_mask = ~0, }, { PCI_DEVICE(PCI_VENDOR_ID_TI, PCIE_RC_K2G), .class = PCI_CLASS_BRIDGE_PCI << 8, .class_mask = ~0, }, { 0, }, }; static const struct pci_device_id am6_pci_devids[] = { { PCI_DEVICE(PCI_VENDOR_ID_TI, PCI_DEVICE_ID_TI_AM654X), .class = PCI_CLASS_BRIDGE_PCI << 8, .class_mask = ~0, }, { 0, }, }; if (pci_is_root_bus(bus)) bridge = dev; while (!pci_is_root_bus(bus)) { bridge = bus->self; bus = bus->parent; } if (!bridge) return; if (pci_match_id(rc_pci_devids, bridge)) { if (pcie_get_readrq(dev) > 256) { dev_info(&dev->dev, ""limiting MRRS to 256 bytes\n""); pcie_set_readrq(dev, 256); } } if (pci_match_id(am6_pci_devids, bridge)) { bridge_dev = pci_get_host_bridge_device(dev); if (!bridge_dev || !bridge_dev->parent) return; ks_pcie = dev_get_drvdata(bridge_dev->parent); if (!ks_pcie) return; val = ks_pcie_app_readl(ks_pcie, PID); val &= RTL; val >>= RTL_SHIFT; if (val != AM6_PCI_PG1_RTL_VER) return; if (pcie_get_readrq(dev) > 128) { dev_info(&dev->dev, ""limiting MRRS to 128 bytes\n""); pcie_set_readrq(dev, 128); } } }"
1328----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56646/bad/addrconf.c----modify_prefix_route,"static int modify_prefix_route(struct inet6_ifaddr *ifp, unsigned long expires, u32 flags, bool modify_peer) { struct fib6_table *table; struct fib6_info *f6i; u32 prio; f6i = addrconf_get_prefix_route(modify_peer ? &ifp->peer_addr : &ifp->addr, ifp->prefix_len, ifp->idev->dev, 0, RTF_DEFAULT, true); if (!f6i) return -ENOENT; prio = ifp->rt_priority ? : IP6_RT_PRIO_ADDRCONF; if (f6i->fib6_metric != prio) { ip6_del_rt(dev_net(ifp->idev->dev), f6i, false); addrconf_prefix_route(modify_peer ? &ifp->peer_addr : &ifp->addr, ifp->prefix_len, ifp->rt_priority, ifp->idev->dev, expires, flags, GFP_KERNEL); <S2SV_StartVul> } else { <S2SV_EndVul> table = f6i->fib6_table; spin_lock_bh(&table->tb6_lock); if (!(flags & RTF_EXPIRES)) { fib6_clean_expires(f6i); fib6_remove_gc_list(f6i); } else { fib6_set_expires(f6i, expires); fib6_add_gc_list(f6i); } spin_unlock_bh(&table->tb6_lock); <S2SV_StartVul> fib6_info_release(f6i); <S2SV_EndVul> } return 0; }","- } else {
- fib6_info_release(f6i);
+ #include <linux/types.h>
+ #include <linux/kernel.h>
+ #include <linux/socket.h>
+ #include <linux/sockios.h>","#define pr_fmt(fmt) ""IPv6: "" fmt #include <linux/errno.h> #include <linux/types.h> #include <linux/kernel.h> #include <linux/sched/signal.h> #include <linux/socket.h> #include <linux/sockios.h> #include <linux/net.h> #include <linux/inet.h> #include <linux/in6.h> #include <linux/netdevice.h> #include <linux/if_addr.h> #include <linux/if_arp.h> #include <linux/if_arcnet.h> #include <linux/if_infiniband.h> #include <linux/route.h> #include <linux/inetdevice.h> #include <linux/init.h> #include <linux/slab.h> #ifdef CONFIG_SYSCTL #include <linux/sysctl.h> #endif #include <linux/capability.h> #include <linux/delay.h> #include <linux/notifier.h> #include <linux/string.h> #include <linux/hash.h> #include <net/ip_tunnels.h> #include <net/net_namespace.h> #include <net/sock.h> #include <net/snmp.h> #include <net/6lowpan.h> #include <net/firewire.h> #include <net/ipv6.h> #include <net/protocol.h> #include <net/ndisc.h> #include <net/ip6_route.h> #include <net/addrconf.h> #include <net/tcp.h> #include <net/ip.h> #include <net/netlink.h> #include <net/pkt_sched.h> #include <net/l3mdev.h> #include <linux/if_tunnel.h> #include <linux/rtnetlink.h> #include <linux/netconf.h> #include <linux/random.h> #include <linux/uaccess.h> #include <linux/unaligned.h> #include <linux/proc_fs.h> #include <linux/seq_file.h> #include <linux/export.h> #include <linux/ioam6.h> #define IPV6_MAX_STRLEN \ sizeof(""ffff:ffff:ffff:ffff:ffff:ffff:255.255.255.255"") static inline u32 cstamp_delta(unsigned long cstamp) { return (cstamp - INITIAL_JIFFIES) * 100UL / HZ; }"
660----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49961/bad/ar0521.c----ar0521_power_on,"static int ar0521_power_on(struct device *dev) { struct v4l2_subdev *sd = dev_get_drvdata(dev); struct ar0521_dev *sensor = to_ar0521_dev(sd); unsigned int cnt; int ret; for (cnt = 0; cnt < ARRAY_SIZE(ar0521_supply_names); cnt++) if (sensor->supplies[cnt]) { ret = regulator_enable(sensor->supplies[cnt]); if (ret < 0) goto off; usleep_range(1000, 1500); } ret = clk_prepare_enable(sensor->extclk); if (ret < 0) { v4l2_err(&sensor->sd, ""error enabling sensor clock\n""); goto off; } usleep_range(1000, 1500); if (sensor->reset_gpio) <S2SV_StartVul> gpiod_set_value(sensor->reset_gpio, 0); <S2SV_EndVul> usleep_range(4500, 5000); for (cnt = 0; cnt < ARRAY_SIZE(initial_regs); cnt++) { ret = ar0521_write_regs(sensor, initial_regs[cnt].data, initial_regs[cnt].count); if (ret) goto off; } ret = ar0521_write_reg(sensor, AR0521_REG_SERIAL_FORMAT, AR0521_REG_SERIAL_FORMAT_MIPI | sensor->lane_count); if (ret) goto off; ret = ar0521_write_reg(sensor, AR0521_REG_HISPI_TEST_MODE, ((0x40 << sensor->lane_count) - 0x40) | AR0521_REG_HISPI_TEST_MODE_LP11); if (ret) goto off; ret = ar0521_write_reg(sensor, AR0521_REG_ROW_SPEED, 0x110 | 4 / sensor->lane_count); if (ret) goto off; return 0; off: ar0521_power_off(dev); return ret; }","- gpiod_set_value(sensor->reset_gpio, 0);
+ gpiod_set_value_cansleep(sensor->reset_gpio, 0);","static int ar0521_power_on(struct device *dev) { struct v4l2_subdev *sd = dev_get_drvdata(dev); struct ar0521_dev *sensor = to_ar0521_dev(sd); unsigned int cnt; int ret; for (cnt = 0; cnt < ARRAY_SIZE(ar0521_supply_names); cnt++) if (sensor->supplies[cnt]) { ret = regulator_enable(sensor->supplies[cnt]); if (ret < 0) goto off; usleep_range(1000, 1500); } ret = clk_prepare_enable(sensor->extclk); if (ret < 0) { v4l2_err(&sensor->sd, ""error enabling sensor clock\n""); goto off; } usleep_range(1000, 1500); if (sensor->reset_gpio) gpiod_set_value_cansleep(sensor->reset_gpio, 0); usleep_range(4500, 5000); for (cnt = 0; cnt < ARRAY_SIZE(initial_regs); cnt++) { ret = ar0521_write_regs(sensor, initial_regs[cnt].data, initial_regs[cnt].count); if (ret) goto off; } ret = ar0521_write_reg(sensor, AR0521_REG_SERIAL_FORMAT, AR0521_REG_SERIAL_FORMAT_MIPI | sensor->lane_count); if (ret) goto off; ret = ar0521_write_reg(sensor, AR0521_REG_HISPI_TEST_MODE, ((0x40 << sensor->lane_count) - 0x40) | AR0521_REG_HISPI_TEST_MODE_LP11); if (ret) goto off; ret = ar0521_write_reg(sensor, AR0521_REG_ROW_SPEED, 0x110 | 4 / sensor->lane_count); if (ret) goto off; return 0; off: ar0521_power_off(dev); return ret; }"
1370----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56702/bad/verifier.c----check_func_arg,"static int check_func_arg(struct bpf_verifier_env *env, u32 arg, struct bpf_call_arg_meta *meta, const struct bpf_func_proto *fn, int insn_idx) { u32 regno = BPF_REG_1 + arg; struct bpf_reg_state *regs = cur_regs(env), *reg = &regs[regno]; enum bpf_arg_type arg_type = fn->arg_type[arg]; enum bpf_reg_type type = reg->type; u32 *arg_btf_id = NULL; int err = 0; if (arg_type == ARG_DONTCARE) return 0; err = check_reg_arg(env, regno, SRC_OP); if (err) return err; if (arg_type == ARG_ANYTHING) { if (is_pointer_value(env, regno)) { verbose(env, ""R%d leaks addr into helper function\n"", regno); return -EACCES; } return 0; } if (type_is_pkt_pointer(type) && !may_access_direct_pkt_data(env, meta, BPF_READ)) { verbose(env, ""helper access to the packet is not allowed\n""); return -EACCES; } if (base_type(arg_type) == ARG_PTR_TO_MAP_VALUE) { err = resolve_map_arg_type(env, meta, &arg_type); if (err) return err; } if (register_is_null(reg) && type_may_be_null(arg_type)) goto skip_type_check; if (base_type(arg_type) == ARG_PTR_TO_BTF_ID || base_type(arg_type) == ARG_PTR_TO_SPIN_LOCK) arg_btf_id = fn->arg_btf_id[arg]; err = check_reg_type(env, regno, arg_type, arg_btf_id, meta); if (err) return err; err = check_func_arg_reg_off(env, reg, regno, arg_type); if (err) return err; skip_type_check: if (arg_type_is_release(arg_type)) { if (arg_type_is_dynptr(arg_type)) { struct bpf_func_state *state = func(env, reg); int spi; if (reg->type == PTR_TO_STACK) { spi = dynptr_get_spi(env, reg); if (spi < 0 || !state->stack[spi].spilled_ptr.ref_obj_id) { verbose(env, ""arg %d is an unacquired reference\n"", regno); return -EINVAL; } } else { verbose(env, ""cannot release unowned const bpf_dynptr\n""); return -EINVAL; } } else if (!reg->ref_obj_id && !register_is_null(reg)) { verbose(env, ""R%d must be referenced when passed to release function\n"", regno); return -EINVAL; } if (meta->release_regno) { verbose(env, ""verifier internal error: more than one release argument\n""); return -EFAULT; } meta->release_regno = regno; } if (reg->ref_obj_id && base_type(arg_type) != ARG_KPTR_XCHG_DEST) { if (meta->ref_obj_id) { verbose(env, ""verifier internal error: more than one arg with ref_obj_id R%d %u %u\n"", regno, reg->ref_obj_id, meta->ref_obj_id); return -EFAULT; } meta->ref_obj_id = reg->ref_obj_id; } switch (base_type(arg_type)) { case ARG_CONST_MAP_PTR: if (meta->map_ptr) { if (meta->map_ptr != reg->map_ptr || meta->map_uid != reg->map_uid) { verbose(env, ""timer pointer in R1 map_uid=%d doesn't match map pointer in R2 map_uid=%d\n"", meta->map_uid, reg->map_uid); return -EINVAL; } } meta->map_ptr = reg->map_ptr; meta->map_uid = reg->map_uid; break; case ARG_PTR_TO_MAP_KEY: if (!meta->map_ptr) { verbose(env, ""invalid map_ptr to access map->key\n""); return -EACCES; } err = check_helper_mem_access(env, regno, meta->map_ptr->key_size, BPF_READ, false, NULL); break; case ARG_PTR_TO_MAP_VALUE: if (type_may_be_null(arg_type) && register_is_null(reg)) return 0; if (!meta->map_ptr) { verbose(env, ""invalid map_ptr to access map->value\n""); return -EACCES; } meta->raw_mode = arg_type & MEM_UNINIT; err = check_helper_mem_access(env, regno, meta->map_ptr->value_size, arg_type & MEM_WRITE ? BPF_WRITE : BPF_READ, false, meta); break; case ARG_PTR_TO_PERCPU_BTF_ID: if (!reg->btf_id) { verbose(env, ""Helper has invalid btf_id in R%d\n"", regno); return -EACCES; } meta->ret_btf = reg->btf; meta->ret_btf_id = reg->btf_id; break; case ARG_PTR_TO_SPIN_LOCK: if (in_rbtree_lock_required_cb(env)) { verbose(env, ""can't spin_{lock,unlock} in rbtree cb\n""); return -EACCES; } if (meta->func_id == BPF_FUNC_spin_lock) { err = process_spin_lock(env, regno, true); if (err) return err; } else if (meta->func_id == BPF_FUNC_spin_unlock) { err = process_spin_lock(env, regno, false); if (err) return err; } else { verbose(env, ""verifier internal error\n""); return -EFAULT; } break; case ARG_PTR_TO_TIMER: err = process_timer_func(env, regno, meta); if (err) return err; break; case ARG_PTR_TO_FUNC: meta->subprogno = reg->subprogno; break; case ARG_PTR_TO_MEM: meta->raw_mode = arg_type & MEM_UNINIT; if (arg_type & MEM_FIXED_SIZE) { err = check_helper_mem_access(env, regno, fn->arg_size[arg], arg_type & MEM_WRITE ? BPF_WRITE : BPF_READ, false, meta); if (err) return err; if (arg_type & MEM_ALIGNED) err = check_ptr_alignment(env, reg, 0, fn->arg_size[arg], true); } break; case ARG_CONST_SIZE: err = check_mem_size_reg(env, reg, regno, fn->arg_type[arg - 1] & MEM_WRITE ? BPF_WRITE : BPF_READ, false, meta); break; case ARG_CONST_SIZE_OR_ZERO: err = check_mem_size_reg(env, reg, regno, fn->arg_type[arg - 1] & MEM_WRITE ? BPF_WRITE : BPF_READ, true, meta); break; case ARG_PTR_TO_DYNPTR: err = process_dynptr_func(env, regno, insn_idx, arg_type, 0); if (err) return err; break; case ARG_CONST_ALLOC_SIZE_OR_ZERO: if (!tnum_is_const(reg->var_off)) { verbose(env, ""R%d is not a known constant'\n"", regno); return -EACCES; } meta->mem_size = reg->var_off.value; err = mark_chain_precision(env, regno); <S2SV_StartVul> if (err) <S2SV_EndVul> <S2SV_StartVul> return err; <S2SV_EndVul> break; <S2SV_StartVul> case ARG_PTR_TO_CONST_STR: <S2SV_EndVul> <S2SV_StartVul> { <S2SV_EndVul> <S2SV_StartVul> err = check_reg_const_str(env, reg, regno); <S2SV_EndVul> if (err) return err; break; } case ARG_KPTR_XCHG_DEST: err = process_kptr_func(env, regno, meta); if (err) return err; break; } return err; }","- if (err)
- return err;
- case ARG_PTR_TO_CONST_STR:
- {
- err = check_reg_const_str(env, reg, regno);
+ meta->subprogno = reg->subprogno;
+ err = mark_chain_precision(env, regno);
+ break;
+ case ARG_PTR_TO_CONST_STR:","static int check_func_arg(struct bpf_verifier_env *env, u32 arg, struct bpf_call_arg_meta *meta, const struct bpf_func_proto *fn, int insn_idx) { u32 regno = BPF_REG_1 + arg; struct bpf_reg_state *regs = cur_regs(env), *reg = &regs[regno]; enum bpf_arg_type arg_type = fn->arg_type[arg]; enum bpf_reg_type type = reg->type; u32 *arg_btf_id = NULL; int err = 0; bool mask; if (arg_type == ARG_DONTCARE) return 0; err = check_reg_arg(env, regno, SRC_OP); if (err) return err; if (arg_type == ARG_ANYTHING) { if (is_pointer_value(env, regno)) { verbose(env, ""R%d leaks addr into helper function\n"", regno); return -EACCES; } return 0; } if (type_is_pkt_pointer(type) && !may_access_direct_pkt_data(env, meta, BPF_READ)) { verbose(env, ""helper access to the packet is not allowed\n""); return -EACCES; } if (base_type(arg_type) == ARG_PTR_TO_MAP_VALUE) { err = resolve_map_arg_type(env, meta, &arg_type); if (err) return err; } if (register_is_null(reg) && type_may_be_null(arg_type)) goto skip_type_check; if (base_type(arg_type) == ARG_PTR_TO_BTF_ID || base_type(arg_type) == ARG_PTR_TO_SPIN_LOCK) arg_btf_id = fn->arg_btf_id[arg]; mask = mask_raw_tp_reg(env, reg); err = check_reg_type(env, regno, arg_type, arg_btf_id, meta); err = err ?: check_func_arg_reg_off(env, reg, regno, arg_type); unmask_raw_tp_reg(reg, mask); if (err) return err; skip_type_check: if (arg_type_is_release(arg_type)) { if (arg_type_is_dynptr(arg_type)) { struct bpf_func_state *state = func(env, reg); int spi; if (reg->type == PTR_TO_STACK) { spi = dynptr_get_spi(env, reg); if (spi < 0 || !state->stack[spi].spilled_ptr.ref_obj_id) { verbose(env, ""arg %d is an unacquired reference\n"", regno); return -EINVAL; } } else { verbose(env, ""cannot release unowned const bpf_dynptr\n""); return -EINVAL; } } else if (!reg->ref_obj_id && !register_is_null(reg)) { verbose(env, ""R%d must be referenced when passed to release function\n"", regno); return -EINVAL; } if (meta->release_regno) { verbose(env, ""verifier internal error: more than one release argument\n""); return -EFAULT; } meta->release_regno = regno; } if (reg->ref_obj_id && base_type(arg_type) != ARG_KPTR_XCHG_DEST) { if (meta->ref_obj_id) { verbose(env, ""verifier internal error: more than one arg with ref_obj_id R%d %u %u\n"", regno, reg->ref_obj_id, meta->ref_obj_id); return -EFAULT; } meta->ref_obj_id = reg->ref_obj_id; } switch (base_type(arg_type)) { case ARG_CONST_MAP_PTR: if (meta->map_ptr) { if (meta->map_ptr != reg->map_ptr || meta->map_uid != reg->map_uid) { verbose(env, ""timer pointer in R1 map_uid=%d doesn't match map pointer in R2 map_uid=%d\n"", meta->map_uid, reg->map_uid); return -EINVAL; } } meta->map_ptr = reg->map_ptr; meta->map_uid = reg->map_uid; break; case ARG_PTR_TO_MAP_KEY: if (!meta->map_ptr) { verbose(env, ""invalid map_ptr to access map->key\n""); return -EACCES; } err = check_helper_mem_access(env, regno, meta->map_ptr->key_size, BPF_READ, false, NULL); break; case ARG_PTR_TO_MAP_VALUE: if (type_may_be_null(arg_type) && register_is_null(reg)) return 0; if (!meta->map_ptr) { verbose(env, ""invalid map_ptr to access map->value\n""); return -EACCES; } meta->raw_mode = arg_type & MEM_UNINIT; err = check_helper_mem_access(env, regno, meta->map_ptr->value_size, arg_type & MEM_WRITE ? BPF_WRITE : BPF_READ, false, meta); break; case ARG_PTR_TO_PERCPU_BTF_ID: if (!reg->btf_id) { verbose(env, ""Helper has invalid btf_id in R%d\n"", regno); return -EACCES; } meta->ret_btf = reg->btf; meta->ret_btf_id = reg->btf_id; break; case ARG_PTR_TO_SPIN_LOCK: if (in_rbtree_lock_required_cb(env)) { verbose(env, ""can't spin_{lock,unlock} in rbtree cb\n""); return -EACCES; } if (meta->func_id == BPF_FUNC_spin_lock) { err = process_spin_lock(env, regno, true); if (err) return err; } else if (meta->func_id == BPF_FUNC_spin_unlock) { err = process_spin_lock(env, regno, false); if (err) return err; } else { verbose(env, ""verifier internal error\n""); return -EFAULT; } break; case ARG_PTR_TO_TIMER: err = process_timer_func(env, regno, meta); if (err) return err; break; case ARG_PTR_TO_FUNC: meta->subprogno = reg->subprogno; break; case ARG_PTR_TO_MEM: meta->raw_mode = arg_type & MEM_UNINIT; if (arg_type & MEM_FIXED_SIZE) { err = check_helper_mem_access(env, regno, fn->arg_size[arg], arg_type & MEM_WRITE ? BPF_WRITE : BPF_READ, false, meta); if (err) return err; if (arg_type & MEM_ALIGNED) err = check_ptr_alignment(env, reg, 0, fn->arg_size[arg], true); } break; case ARG_CONST_SIZE: err = check_mem_size_reg(env, reg, regno, fn->arg_type[arg - 1] & MEM_WRITE ? BPF_WRITE : BPF_READ, false, meta); break; case ARG_CONST_SIZE_OR_ZERO: err = check_mem_size_reg(env, reg, regno, fn->arg_type[arg - 1] & MEM_WRITE ? BPF_WRITE : BPF_READ, true, meta); break; case ARG_PTR_TO_DYNPTR: err = process_dynptr_func(env, regno, insn_idx, arg_type, 0); if (err) return err; break; case ARG_CONST_ALLOC_SIZE_OR_ZERO: if (!tnum_is_const(reg->var_off)) { verbose(env, ""R%d is not a known constant'\n"", regno); return -EACCES; } meta->mem_size = reg->var_off.value; err = mark_chain_precision(env, regno); if (err) return err; break; case ARG_PTR_TO_CONST_STR: { err = check_reg_const_str(env, reg, regno); if (err) return err; break; } case ARG_KPTR_XCHG_DEST: err = process_kptr_func(env, regno, meta); if (err) return err; break; } return err; }"
1172----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53146/bad/nfs4callback.c----decode_cb_compound4res,"static int decode_cb_compound4res(struct xdr_stream *xdr, struct nfs4_cb_compound_hdr *hdr) { u32 length; __be32 *p; <S2SV_StartVul> p = xdr_inline_decode(xdr, 4 + 4); <S2SV_EndVul> <S2SV_StartVul> if (unlikely(p == NULL)) <S2SV_EndVul> goto out_overflow; <S2SV_StartVul> hdr->status = be32_to_cpup(p++); <S2SV_EndVul> <S2SV_StartVul> length = be32_to_cpup(p++); <S2SV_EndVul> <S2SV_StartVul> p = xdr_inline_decode(xdr, length + 4); <S2SV_EndVul> <S2SV_StartVul> if (unlikely(p == NULL)) <S2SV_EndVul> goto out_overflow; <S2SV_StartVul> p += XDR_QUADLEN(length); <S2SV_EndVul> <S2SV_StartVul> hdr->nops = be32_to_cpup(p); <S2SV_EndVul> return 0; out_overflow: return -EIO; }","- p = xdr_inline_decode(xdr, 4 + 4);
- if (unlikely(p == NULL))
- hdr->status = be32_to_cpup(p++);
- length = be32_to_cpup(p++);
- p = xdr_inline_decode(xdr, length + 4);
- if (unlikely(p == NULL))
- p += XDR_QUADLEN(length);
- hdr->nops = be32_to_cpup(p);
+ p = xdr_inline_decode(xdr, XDR_UNIT);
+ goto out_overflow;
+ hdr->status = be32_to_cpup(p);
+ if (xdr_stream_decode_u32(xdr, &length) < 0)
+ goto out_overflow;
+ if (xdr_inline_decode(xdr, length) == NULL)
+ goto out_overflow;
+ if (xdr_stream_decode_u32(xdr, &hdr->nops) < 0)
+ goto out_overflow;","static int decode_cb_compound4res(struct xdr_stream *xdr, struct nfs4_cb_compound_hdr *hdr) { u32 length; __be32 *p; p = xdr_inline_decode(xdr, XDR_UNIT); if (unlikely(p == NULL)) goto out_overflow; hdr->status = be32_to_cpup(p); if (xdr_stream_decode_u32(xdr, &length) < 0) goto out_overflow; if (xdr_inline_decode(xdr, length) == NULL) goto out_overflow; if (xdr_stream_decode_u32(xdr, &hdr->nops) < 0) goto out_overflow; return 0; out_overflow: return -EIO; }"
679----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49979/bad/tcpv6_offload.c----*tcp6_gso_segment,"static struct sk_buff *tcp6_gso_segment(struct sk_buff *skb, netdev_features_t features) { struct tcphdr *th; if (!(skb_shinfo(skb)->gso_type & SKB_GSO_TCPV6)) return ERR_PTR(-EINVAL); if (!pskb_may_pull(skb, sizeof(*th))) return ERR_PTR(-EINVAL); <S2SV_StartVul> if (skb_shinfo(skb)->gso_type & SKB_GSO_FRAGLIST) <S2SV_EndVul> <S2SV_StartVul> return __tcp6_gso_segment_list(skb, features); <S2SV_EndVul> if (unlikely(skb->ip_summed != CHECKSUM_PARTIAL)) { const struct ipv6hdr *ipv6h = ipv6_hdr(skb); struct tcphdr *th = tcp_hdr(skb); th->check = 0; skb->ip_summed = CHECKSUM_PARTIAL; __tcp_v6_send_check(skb, &ipv6h->saddr, &ipv6h->daddr); } return tcp_gso_segment(skb, features); }","- if (skb_shinfo(skb)->gso_type & SKB_GSO_FRAGLIST)
- return __tcp6_gso_segment_list(skb, features);
+ if (skb_shinfo(skb)->gso_type & SKB_GSO_FRAGLIST) {
+ struct tcphdr *th = tcp_hdr(skb);
+ if (skb_pagelen(skb) - th->doff * 4 == skb_shinfo(skb)->gso_size)
+ return __tcp6_gso_segment_list(skb, features);
+ skb->ip_summed = CHECKSUM_NONE;
+ }","static struct sk_buff *tcp6_gso_segment(struct sk_buff *skb, netdev_features_t features) { struct tcphdr *th; if (!(skb_shinfo(skb)->gso_type & SKB_GSO_TCPV6)) return ERR_PTR(-EINVAL); if (!pskb_may_pull(skb, sizeof(*th))) return ERR_PTR(-EINVAL); if (skb_shinfo(skb)->gso_type & SKB_GSO_FRAGLIST) { struct tcphdr *th = tcp_hdr(skb); if (skb_pagelen(skb) - th->doff * 4 == skb_shinfo(skb)->gso_size) return __tcp6_gso_segment_list(skb, features); skb->ip_summed = CHECKSUM_NONE; } if (unlikely(skb->ip_summed != CHECKSUM_PARTIAL)) { const struct ipv6hdr *ipv6h = ipv6_hdr(skb); struct tcphdr *th = tcp_hdr(skb); th->check = 0; skb->ip_summed = CHECKSUM_PARTIAL; __tcp_v6_send_check(skb, &ipv6h->saddr, &ipv6h->daddr); } return tcp_gso_segment(skb, features); }"
162----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-45017/bad/ipsec_fs_roce.c----ipsec_fs_roce_rx_mpv_create,"static int ipsec_fs_roce_rx_mpv_create(struct mlx5_core_dev *mdev, struct mlx5_ipsec_fs *ipsec_roce, struct mlx5_flow_namespace *ns, u32 family, u32 level, u32 prio) { struct mlx5_flow_namespace *roce_ns, *nic_ns; struct mlx5_flow_table_attr ft_attr = {}; struct mlx5_devcom_comp_dev *tmp = NULL; struct mlx5_ipsec_rx_roce *roce; struct mlx5_flow_table next_ft; struct mlx5_flow_table *ft; struct mlx5_flow_group *g; struct mlx5e_priv *peer_priv; int ix = 0; u32 *in; int err; roce = (family == AF_INET) ? &ipsec_roce->ipv4_rx : &ipsec_roce->ipv6_rx; if (!mlx5_devcom_for_each_peer_begin(*ipsec_roce->devcom)) return -EOPNOTSUPP; peer_priv = mlx5_devcom_get_next_peer_data(*ipsec_roce->devcom, &tmp); <S2SV_StartVul> if (!peer_priv) { <S2SV_EndVul> err = -EOPNOTSUPP; goto release_peer; } roce_ns = mlx5_get_flow_namespace(peer_priv->mdev, MLX5_FLOW_NAMESPACE_RDMA_RX_IPSEC); if (!roce_ns) { err = -EOPNOTSUPP; goto release_peer; } nic_ns = mlx5_get_flow_namespace(peer_priv->mdev, MLX5_FLOW_NAMESPACE_KERNEL); if (!nic_ns) { err = -EOPNOTSUPP; goto release_peer; } in = kvzalloc(MLX5_ST_SZ_BYTES(create_flow_group_in), GFP_KERNEL); if (!in) { err = -ENOMEM; goto release_peer; } ft_attr.level = (family == AF_INET) ? MLX5_IPSEC_RX_IPV4_FT_LEVEL : MLX5_IPSEC_RX_IPV6_FT_LEVEL; ft_attr.max_fte = 1; ft = mlx5_create_flow_table(roce_ns, &ft_attr); if (IS_ERR(ft)) { err = PTR_ERR(ft); mlx5_core_err(mdev, ""Fail to create RoCE IPsec rx ft at rdma master err=%d\n"", err); goto free_in; } roce->ft_rdma = ft; ft_attr.max_fte = 1; ft_attr.prio = prio; ft_attr.level = level + 2; ft = mlx5_create_flow_table(nic_ns, &ft_attr); if (IS_ERR(ft)) { err = PTR_ERR(ft); mlx5_core_err(mdev, ""Fail to create RoCE IPsec rx ft at NIC master err=%d\n"", err); goto destroy_ft_rdma; } roce->nic_master_ft = ft; MLX5_SET_CFG(in, start_flow_index, ix); ix += 1; MLX5_SET_CFG(in, end_flow_index, ix - 1); g = mlx5_create_flow_group(roce->nic_master_ft, in); if (IS_ERR(g)) { err = PTR_ERR(g); mlx5_core_err(mdev, ""Fail to create RoCE IPsec rx group aliased err=%d\n"", err); goto destroy_nic_master_ft; } roce->nic_master_group = g; err = ipsec_fs_create_aliased_ft(peer_priv->mdev, mdev, roce->nic_master_ft, &roce->alias_id, roce->key, false); if (err) { mlx5_core_err(mdev, ""Fail to create RoCE IPsec rx alias FT err=%d\n"", err); goto destroy_group; } next_ft.id = roce->alias_id; ft_attr.max_fte = 1; ft_attr.prio = prio; ft_attr.level = roce->ft->level + 1; ft_attr.flags = MLX5_FLOW_TABLE_UNMANAGED; ft_attr.next_ft = &next_ft; ft = mlx5_create_flow_table(ns, &ft_attr); if (IS_ERR(ft)) { err = PTR_ERR(ft); mlx5_core_err(mdev, ""Fail to create RoCE IPsec rx ft at NIC slave err=%d\n"", err); goto destroy_alias; } roce->goto_alias_ft = ft; kvfree(in); mlx5_devcom_for_each_peer_end(*ipsec_roce->devcom); return 0; destroy_alias: mlx5_cmd_alias_obj_destroy(mdev, roce->alias_id, MLX5_GENERAL_OBJECT_TYPES_FLOW_TABLE_ALIAS); destroy_group: mlx5_destroy_flow_group(roce->nic_master_group); destroy_nic_master_ft: mlx5_destroy_flow_table(roce->nic_master_ft); destroy_ft_rdma: mlx5_destroy_flow_table(roce->ft_rdma); free_in: kvfree(in); release_peer: mlx5_devcom_for_each_peer_end(*ipsec_roce->devcom); return err; }","- if (!peer_priv) {
+ if (!peer_priv || !peer_priv->ipsec) {
+ mlx5_core_err(mdev, ""IPsec not supported on master device\n"");","static int ipsec_fs_roce_rx_mpv_create(struct mlx5_core_dev *mdev, struct mlx5_ipsec_fs *ipsec_roce, struct mlx5_flow_namespace *ns, u32 family, u32 level, u32 prio) { struct mlx5_flow_namespace *roce_ns, *nic_ns; struct mlx5_flow_table_attr ft_attr = {}; struct mlx5_devcom_comp_dev *tmp = NULL; struct mlx5_ipsec_rx_roce *roce; struct mlx5_flow_table next_ft; struct mlx5_flow_table *ft; struct mlx5_flow_group *g; struct mlx5e_priv *peer_priv; int ix = 0; u32 *in; int err; roce = (family == AF_INET) ? &ipsec_roce->ipv4_rx : &ipsec_roce->ipv6_rx; if (!mlx5_devcom_for_each_peer_begin(*ipsec_roce->devcom)) return -EOPNOTSUPP; peer_priv = mlx5_devcom_get_next_peer_data(*ipsec_roce->devcom, &tmp); if (!peer_priv || !peer_priv->ipsec) { mlx5_core_err(mdev, ""IPsec not supported on master device\n""); err = -EOPNOTSUPP; goto release_peer; } roce_ns = mlx5_get_flow_namespace(peer_priv->mdev, MLX5_FLOW_NAMESPACE_RDMA_RX_IPSEC); if (!roce_ns) { err = -EOPNOTSUPP; goto release_peer; } nic_ns = mlx5_get_flow_namespace(peer_priv->mdev, MLX5_FLOW_NAMESPACE_KERNEL); if (!nic_ns) { err = -EOPNOTSUPP; goto release_peer; } in = kvzalloc(MLX5_ST_SZ_BYTES(create_flow_group_in), GFP_KERNEL); if (!in) { err = -ENOMEM; goto release_peer; } ft_attr.level = (family == AF_INET) ? MLX5_IPSEC_RX_IPV4_FT_LEVEL : MLX5_IPSEC_RX_IPV6_FT_LEVEL; ft_attr.max_fte = 1; ft = mlx5_create_flow_table(roce_ns, &ft_attr); if (IS_ERR(ft)) { err = PTR_ERR(ft); mlx5_core_err(mdev, ""Fail to create RoCE IPsec rx ft at rdma master err=%d\n"", err); goto free_in; } roce->ft_rdma = ft; ft_attr.max_fte = 1; ft_attr.prio = prio; ft_attr.level = level + 2; ft = mlx5_create_flow_table(nic_ns, &ft_attr); if (IS_ERR(ft)) { err = PTR_ERR(ft); mlx5_core_err(mdev, ""Fail to create RoCE IPsec rx ft at NIC master err=%d\n"", err); goto destroy_ft_rdma; } roce->nic_master_ft = ft; MLX5_SET_CFG(in, start_flow_index, ix); ix += 1; MLX5_SET_CFG(in, end_flow_index, ix - 1); g = mlx5_create_flow_group(roce->nic_master_ft, in); if (IS_ERR(g)) { err = PTR_ERR(g); mlx5_core_err(mdev, ""Fail to create RoCE IPsec rx group aliased err=%d\n"", err); goto destroy_nic_master_ft; } roce->nic_master_group = g; err = ipsec_fs_create_aliased_ft(peer_priv->mdev, mdev, roce->nic_master_ft, &roce->alias_id, roce->key, false); if (err) { mlx5_core_err(mdev, ""Fail to create RoCE IPsec rx alias FT err=%d\n"", err); goto destroy_group; } next_ft.id = roce->alias_id; ft_attr.max_fte = 1; ft_attr.prio = prio; ft_attr.level = roce->ft->level + 1; ft_attr.flags = MLX5_FLOW_TABLE_UNMANAGED; ft_attr.next_ft = &next_ft; ft = mlx5_create_flow_table(ns, &ft_attr); if (IS_ERR(ft)) { err = PTR_ERR(ft); mlx5_core_err(mdev, ""Fail to create RoCE IPsec rx ft at NIC slave err=%d\n"", err); goto destroy_alias; } roce->goto_alias_ft = ft; kvfree(in); mlx5_devcom_for_each_peer_end(*ipsec_roce->devcom); return 0; destroy_alias: mlx5_cmd_alias_obj_destroy(mdev, roce->alias_id, MLX5_GENERAL_OBJECT_TYPES_FLOW_TABLE_ALIAS); destroy_group: mlx5_destroy_flow_group(roce->nic_master_group); destroy_nic_master_ft: mlx5_destroy_flow_table(roce->nic_master_ft); destroy_ft_rdma: mlx5_destroy_flow_table(roce->ft_rdma); free_in: kvfree(in); release_peer: mlx5_devcom_for_each_peer_end(*ipsec_roce->devcom); return err; }"
733----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50008/bad/scan.c----mwifiex_ret_802_11_scan_ext,"int mwifiex_ret_802_11_scan_ext(struct mwifiex_private *priv, struct host_cmd_ds_command *resp) { struct mwifiex_adapter *adapter = priv->adapter; struct host_cmd_ds_802_11_scan_ext *ext_scan_resp; struct mwifiex_ie_types_header *tlv; struct mwifiex_ietypes_chanstats *tlv_stat; u16 buf_left, type, len; struct host_cmd_ds_command *cmd_ptr; struct cmd_ctrl_node *cmd_node; bool complete_scan = false; mwifiex_dbg(adapter, INFO, ""info: EXT scan returns successfully\n""); ext_scan_resp = &resp->params.ext_scan; tlv = (void *)ext_scan_resp->tlv_buffer; <S2SV_StartVul> buf_left = le16_to_cpu(resp->size) - (sizeof(*ext_scan_resp) + S_DS_GEN <S2SV_EndVul> <S2SV_StartVul> - 1); <S2SV_EndVul> while (buf_left >= sizeof(struct mwifiex_ie_types_header)) { type = le16_to_cpu(tlv->type); len = le16_to_cpu(tlv->len); if (buf_left < (sizeof(struct mwifiex_ie_types_header) + len)) { mwifiex_dbg(adapter, ERROR, ""error processing scan response TLVs""); break; } switch (type) { case TLV_TYPE_CHANNEL_STATS: tlv_stat = (void *)tlv; mwifiex_update_chan_statistics(priv, tlv_stat); break; default: break; } buf_left -= len + sizeof(struct mwifiex_ie_types_header); tlv = (void *)((u8 *)tlv + len + sizeof(struct mwifiex_ie_types_header)); } spin_lock_bh(&adapter->cmd_pending_q_lock); spin_lock_bh(&adapter->scan_pending_q_lock); if (list_empty(&adapter->scan_pending_q)) { complete_scan = true; list_for_each_entry(cmd_node, &adapter->cmd_pending_q, list) { cmd_ptr = (void *)cmd_node->cmd_skb->data; if (le16_to_cpu(cmd_ptr->command) == HostCmd_CMD_802_11_SCAN_EXT) { mwifiex_dbg(adapter, INFO, ""Scan pending in command pending list""); complete_scan = false; break; } } } spin_unlock_bh(&adapter->scan_pending_q_lock); spin_unlock_bh(&adapter->cmd_pending_q_lock); if (complete_scan) mwifiex_complete_scan(priv); return 0; }","- buf_left = le16_to_cpu(resp->size) - (sizeof(*ext_scan_resp) + S_DS_GEN
- - 1);
+ buf_left = le16_to_cpu(resp->size) - (sizeof(*ext_scan_resp) + S_DS_GEN);","int mwifiex_ret_802_11_scan_ext(struct mwifiex_private *priv, struct host_cmd_ds_command *resp) { struct mwifiex_adapter *adapter = priv->adapter; struct host_cmd_ds_802_11_scan_ext *ext_scan_resp; struct mwifiex_ie_types_header *tlv; struct mwifiex_ietypes_chanstats *tlv_stat; u16 buf_left, type, len; struct host_cmd_ds_command *cmd_ptr; struct cmd_ctrl_node *cmd_node; bool complete_scan = false; mwifiex_dbg(adapter, INFO, ""info: EXT scan returns successfully\n""); ext_scan_resp = &resp->params.ext_scan; tlv = (void *)ext_scan_resp->tlv_buffer; buf_left = le16_to_cpu(resp->size) - (sizeof(*ext_scan_resp) + S_DS_GEN); while (buf_left >= sizeof(struct mwifiex_ie_types_header)) { type = le16_to_cpu(tlv->type); len = le16_to_cpu(tlv->len); if (buf_left < (sizeof(struct mwifiex_ie_types_header) + len)) { mwifiex_dbg(adapter, ERROR, ""error processing scan response TLVs""); break; } switch (type) { case TLV_TYPE_CHANNEL_STATS: tlv_stat = (void *)tlv; mwifiex_update_chan_statistics(priv, tlv_stat); break; default: break; } buf_left -= len + sizeof(struct mwifiex_ie_types_header); tlv = (void *)((u8 *)tlv + len + sizeof(struct mwifiex_ie_types_header)); } spin_lock_bh(&adapter->cmd_pending_q_lock); spin_lock_bh(&adapter->scan_pending_q_lock); if (list_empty(&adapter->scan_pending_q)) { complete_scan = true; list_for_each_entry(cmd_node, &adapter->cmd_pending_q, list) { cmd_ptr = (void *)cmd_node->cmd_skb->data; if (le16_to_cpu(cmd_ptr->command) == HostCmd_CMD_802_11_SCAN_EXT) { mwifiex_dbg(adapter, INFO, ""Scan pending in command pending list""); complete_scan = false; break; } } } spin_unlock_bh(&adapter->scan_pending_q_lock); spin_unlock_bh(&adapter->cmd_pending_q_lock); if (complete_scan) mwifiex_complete_scan(priv); return 0; }"
100----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44963/bad/free-space-tree.c----btrfs_delete_free_space_tree,"int btrfs_delete_free_space_tree(struct btrfs_fs_info *fs_info) { struct btrfs_trans_handle *trans; struct btrfs_root *tree_root = fs_info->tree_root; struct btrfs_key key = { .objectid = BTRFS_FREE_SPACE_TREE_OBJECTID, .type = BTRFS_ROOT_ITEM_KEY, .offset = 0, }; struct btrfs_root *free_space_root = btrfs_global_root(fs_info, &key); int ret; trans = btrfs_start_transaction(tree_root, 0); if (IS_ERR(trans)) return PTR_ERR(trans); btrfs_clear_fs_compat_ro(fs_info, FREE_SPACE_TREE); btrfs_clear_fs_compat_ro(fs_info, FREE_SPACE_TREE_VALID); ret = clear_free_space_tree(trans, free_space_root); if (ret) { btrfs_abort_transaction(trans, ret); btrfs_end_transaction(trans); return ret; } ret = btrfs_del_root(trans, &free_space_root->root_key); if (ret) { btrfs_abort_transaction(trans, ret); btrfs_end_transaction(trans); return ret; } btrfs_global_root_delete(free_space_root); spin_lock(&fs_info->trans_lock); list_del(&free_space_root->dirty_list); spin_unlock(&fs_info->trans_lock); btrfs_tree_lock(free_space_root->node); btrfs_clear_buffer_dirty(trans, free_space_root->node); btrfs_tree_unlock(free_space_root->node); <S2SV_StartVul> btrfs_free_tree_block(trans, btrfs_root_id(free_space_root), <S2SV_EndVul> <S2SV_StartVul> free_space_root->node, 0, 1); <S2SV_EndVul> btrfs_put_root(free_space_root); return btrfs_commit_transaction(trans); }","- btrfs_free_tree_block(trans, btrfs_root_id(free_space_root),
- free_space_root->node, 0, 1);
+ ret = btrfs_free_tree_block(trans, btrfs_root_id(free_space_root),
+ free_space_root->node, 0, 1);
+ if (ret < 0) {
+ btrfs_abort_transaction(trans, ret);
+ btrfs_end_transaction(trans);
+ return ret;
+ }
+ }","int btrfs_delete_free_space_tree(struct btrfs_fs_info *fs_info) { struct btrfs_trans_handle *trans; struct btrfs_root *tree_root = fs_info->tree_root; struct btrfs_key key = { .objectid = BTRFS_FREE_SPACE_TREE_OBJECTID, .type = BTRFS_ROOT_ITEM_KEY, .offset = 0, }; struct btrfs_root *free_space_root = btrfs_global_root(fs_info, &key); int ret; trans = btrfs_start_transaction(tree_root, 0); if (IS_ERR(trans)) return PTR_ERR(trans); btrfs_clear_fs_compat_ro(fs_info, FREE_SPACE_TREE); btrfs_clear_fs_compat_ro(fs_info, FREE_SPACE_TREE_VALID); ret = clear_free_space_tree(trans, free_space_root); if (ret) { btrfs_abort_transaction(trans, ret); btrfs_end_transaction(trans); return ret; } ret = btrfs_del_root(trans, &free_space_root->root_key); if (ret) { btrfs_abort_transaction(trans, ret); btrfs_end_transaction(trans); return ret; } btrfs_global_root_delete(free_space_root); spin_lock(&fs_info->trans_lock); list_del(&free_space_root->dirty_list); spin_unlock(&fs_info->trans_lock); btrfs_tree_lock(free_space_root->node); btrfs_clear_buffer_dirty(trans, free_space_root->node); btrfs_tree_unlock(free_space_root->node); ret = btrfs_free_tree_block(trans, btrfs_root_id(free_space_root), free_space_root->node, 0, 1); btrfs_put_root(free_space_root); if (ret < 0) { btrfs_abort_transaction(trans, ret); btrfs_end_transaction(trans); return ret; } return btrfs_commit_transaction(trans); }"
1321----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56631/bad/sg.c----sg_release,"sg_release(struct inode *inode, struct file *filp) { Sg_device *sdp; Sg_fd *sfp; if ((!(sfp = (Sg_fd *) filp->private_data)) || (!(sdp = sfp->parentdp))) return -ENXIO; SCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sdp, ""sg_release\n"")); mutex_lock(&sdp->open_rel_lock); scsi_autopm_put_device(sdp->device); <S2SV_StartVul> kref_put(&sfp->f_ref, sg_remove_sfp); <S2SV_EndVul> sdp->open_cnt--; if (sdp->exclude) { sdp->exclude = false; wake_up_interruptible_all(&sdp->open_wait); } else if (0 == sdp->open_cnt) { wake_up_interruptible(&sdp->open_wait); } mutex_unlock(&sdp->open_rel_lock); return 0; }","- kref_put(&sfp->f_ref, sg_remove_sfp);
+ kref_put(&sfp->f_ref, sg_remove_sfp);","sg_release(struct inode *inode, struct file *filp) { Sg_device *sdp; Sg_fd *sfp; if ((!(sfp = (Sg_fd *) filp->private_data)) || (!(sdp = sfp->parentdp))) return -ENXIO; SCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sdp, ""sg_release\n"")); mutex_lock(&sdp->open_rel_lock); scsi_autopm_put_device(sdp->device); sdp->open_cnt--; if (sdp->exclude) { sdp->exclude = false; wake_up_interruptible_all(&sdp->open_wait); } else if (0 == sdp->open_cnt) { wake_up_interruptible(&sdp->open_wait); } mutex_unlock(&sdp->open_rel_lock); kref_put(&sfp->f_ref, sg_remove_sfp); return 0; }"
711----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49991/bad/kfd_device_queue_manager.c----deallocate_hiq_sdma_mqd,"static void deallocate_hiq_sdma_mqd(struct kfd_node *dev, struct kfd_mem_obj *mqd) { WARN(!mqd, ""No hiq sdma mqd trunk to free""); <S2SV_StartVul> amdgpu_amdkfd_free_gtt_mem(dev->adev, mqd->gtt_mem); <S2SV_EndVul> }","- amdgpu_amdkfd_free_gtt_mem(dev->adev, mqd->gtt_mem);
+ amdgpu_amdkfd_free_gtt_mem(dev->adev, &mqd->gtt_mem);","static void deallocate_hiq_sdma_mqd(struct kfd_node *dev, struct kfd_mem_obj *mqd) { WARN(!mqd, ""No hiq sdma mqd trunk to free""); amdgpu_amdkfd_free_gtt_mem(dev->adev, &mqd->gtt_mem); }"
58----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44941/bad/inode.c----do_read_inode,"static int do_read_inode(struct inode *inode) { struct f2fs_sb_info *sbi = F2FS_I_SB(inode); struct f2fs_inode_info *fi = F2FS_I(inode); struct page *node_page; struct f2fs_inode *ri; projid_t i_projid; if (f2fs_check_nid_range(sbi, inode->i_ino)) return -EINVAL; node_page = f2fs_get_node_page(sbi, inode->i_ino); if (IS_ERR(node_page)) return PTR_ERR(node_page); ri = F2FS_INODE(node_page); inode->i_mode = le16_to_cpu(ri->i_mode); i_uid_write(inode, le32_to_cpu(ri->i_uid)); i_gid_write(inode, le32_to_cpu(ri->i_gid)); set_nlink(inode, le32_to_cpu(ri->i_links)); inode->i_size = le64_to_cpu(ri->i_size); inode->i_blocks = SECTOR_FROM_BLOCK(le64_to_cpu(ri->i_blocks) - 1); inode_set_atime(inode, le64_to_cpu(ri->i_atime), le32_to_cpu(ri->i_atime_nsec)); inode_set_ctime(inode, le64_to_cpu(ri->i_ctime), le32_to_cpu(ri->i_ctime_nsec)); inode_set_mtime(inode, le64_to_cpu(ri->i_mtime), le32_to_cpu(ri->i_mtime_nsec)); inode->i_generation = le32_to_cpu(ri->i_generation); if (S_ISDIR(inode->i_mode)) fi->i_current_depth = le32_to_cpu(ri->i_current_depth); else if (S_ISREG(inode->i_mode)) fi->i_gc_failures = le16_to_cpu(ri->i_gc_failures); fi->i_xattr_nid = le32_to_cpu(ri->i_xattr_nid); fi->i_flags = le32_to_cpu(ri->i_flags); if (S_ISREG(inode->i_mode)) fi->i_flags &= ~F2FS_PROJINHERIT_FL; bitmap_zero(fi->flags, FI_MAX); fi->i_advise = ri->i_advise; fi->i_pino = le32_to_cpu(ri->i_pino); fi->i_dir_level = ri->i_dir_level; get_inline_info(inode, ri); fi->i_extra_isize = f2fs_has_extra_attr(inode) ? le16_to_cpu(ri->i_extra_isize) : 0; if (f2fs_sb_has_flexible_inline_xattr(sbi)) { fi->i_inline_xattr_size = le16_to_cpu(ri->i_inline_xattr_size); } else if (f2fs_has_inline_xattr(inode) || f2fs_has_inline_dentry(inode)) { fi->i_inline_xattr_size = DEFAULT_INLINE_XATTR_ADDRS; } else { fi->i_inline_xattr_size = 0; } if (!sanity_check_inode(inode, node_page)) { f2fs_put_page(node_page, 1); set_sbi_flag(sbi, SBI_NEED_FSCK); f2fs_handle_error(sbi, ERROR_CORRUPTED_INODE); return -EFSCORRUPTED; } if (f2fs_has_inline_data(inode) && !f2fs_exist_data(inode)) __recover_inline_status(inode, node_page); if (!S_ISDIR(inode->i_mode) && !is_cold_node(node_page)) { f2fs_wait_on_page_writeback(node_page, NODE, true, true); set_cold_node(node_page, false); set_page_dirty(node_page); } __get_inode_rdev(inode, node_page); if (!f2fs_need_inode_block_update(sbi, inode->i_ino)) fi->last_disk_size = inode->i_size; if (fi->i_flags & F2FS_PROJINHERIT_FL) set_inode_flag(inode, FI_PROJ_INHERIT); if (f2fs_has_extra_attr(inode) && f2fs_sb_has_project_quota(sbi) && F2FS_FITS_IN_INODE(ri, fi->i_extra_isize, i_projid)) i_projid = (projid_t)le32_to_cpu(ri->i_projid); else i_projid = F2FS_DEF_PROJID; fi->i_projid = make_kprojid(&init_user_ns, i_projid); if (f2fs_has_extra_attr(inode) && f2fs_sb_has_inode_crtime(sbi) && F2FS_FITS_IN_INODE(ri, fi->i_extra_isize, i_crtime)) { fi->i_crtime.tv_sec = le64_to_cpu(ri->i_crtime); fi->i_crtime.tv_nsec = le32_to_cpu(ri->i_crtime_nsec); } if (f2fs_has_extra_attr(inode) && f2fs_sb_has_compression(sbi) && (fi->i_flags & F2FS_COMPR_FL)) { if (F2FS_FITS_IN_INODE(ri, fi->i_extra_isize, i_compress_flag)) { unsigned short compress_flag; atomic_set(&fi->i_compr_blocks, le64_to_cpu(ri->i_compr_blocks)); fi->i_compress_algorithm = ri->i_compress_algorithm; fi->i_log_cluster_size = ri->i_log_cluster_size; compress_flag = le16_to_cpu(ri->i_compress_flag); fi->i_compress_level = compress_flag >> COMPRESS_LEVEL_OFFSET; fi->i_compress_flag = compress_flag & GENMASK(COMPRESS_LEVEL_OFFSET - 1, 0); fi->i_cluster_size = BIT(fi->i_log_cluster_size); set_inode_flag(inode, FI_COMPRESSED_FILE); } } init_idisk_time(inode); <S2SV_StartVul> f2fs_init_read_extent_tree(inode, node_page); <S2SV_EndVul> <S2SV_StartVul> f2fs_init_age_extent_tree(inode); <S2SV_EndVul> <S2SV_StartVul> if (!sanity_check_extent_cache(inode)) { <S2SV_EndVul> f2fs_put_page(node_page, 1); f2fs_handle_error(sbi, ERROR_CORRUPTED_INODE); return -EFSCORRUPTED; } f2fs_put_page(node_page, 1); stat_inc_inline_xattr(inode); stat_inc_inline_inode(inode); stat_inc_inline_dir(inode); stat_inc_compr_inode(inode); stat_add_compr_blocks(inode, atomic_read(&fi->i_compr_blocks)); return 0; }","- f2fs_init_read_extent_tree(inode, node_page);
- f2fs_init_age_extent_tree(inode);
- if (!sanity_check_extent_cache(inode)) {
+ if (!sanity_check_extent_cache(inode, node_page)) {
+ f2fs_init_read_extent_tree(inode, node_page);
+ f2fs_init_age_extent_tree(inode);","static int do_read_inode(struct inode *inode) { struct f2fs_sb_info *sbi = F2FS_I_SB(inode); struct f2fs_inode_info *fi = F2FS_I(inode); struct page *node_page; struct f2fs_inode *ri; projid_t i_projid; if (f2fs_check_nid_range(sbi, inode->i_ino)) return -EINVAL; node_page = f2fs_get_node_page(sbi, inode->i_ino); if (IS_ERR(node_page)) return PTR_ERR(node_page); ri = F2FS_INODE(node_page); inode->i_mode = le16_to_cpu(ri->i_mode); i_uid_write(inode, le32_to_cpu(ri->i_uid)); i_gid_write(inode, le32_to_cpu(ri->i_gid)); set_nlink(inode, le32_to_cpu(ri->i_links)); inode->i_size = le64_to_cpu(ri->i_size); inode->i_blocks = SECTOR_FROM_BLOCK(le64_to_cpu(ri->i_blocks) - 1); inode_set_atime(inode, le64_to_cpu(ri->i_atime), le32_to_cpu(ri->i_atime_nsec)); inode_set_ctime(inode, le64_to_cpu(ri->i_ctime), le32_to_cpu(ri->i_ctime_nsec)); inode_set_mtime(inode, le64_to_cpu(ri->i_mtime), le32_to_cpu(ri->i_mtime_nsec)); inode->i_generation = le32_to_cpu(ri->i_generation); if (S_ISDIR(inode->i_mode)) fi->i_current_depth = le32_to_cpu(ri->i_current_depth); else if (S_ISREG(inode->i_mode)) fi->i_gc_failures = le16_to_cpu(ri->i_gc_failures); fi->i_xattr_nid = le32_to_cpu(ri->i_xattr_nid); fi->i_flags = le32_to_cpu(ri->i_flags); if (S_ISREG(inode->i_mode)) fi->i_flags &= ~F2FS_PROJINHERIT_FL; bitmap_zero(fi->flags, FI_MAX); fi->i_advise = ri->i_advise; fi->i_pino = le32_to_cpu(ri->i_pino); fi->i_dir_level = ri->i_dir_level; get_inline_info(inode, ri); fi->i_extra_isize = f2fs_has_extra_attr(inode) ? le16_to_cpu(ri->i_extra_isize) : 0; if (f2fs_sb_has_flexible_inline_xattr(sbi)) { fi->i_inline_xattr_size = le16_to_cpu(ri->i_inline_xattr_size); } else if (f2fs_has_inline_xattr(inode) || f2fs_has_inline_dentry(inode)) { fi->i_inline_xattr_size = DEFAULT_INLINE_XATTR_ADDRS; } else { fi->i_inline_xattr_size = 0; } if (!sanity_check_inode(inode, node_page)) { f2fs_put_page(node_page, 1); set_sbi_flag(sbi, SBI_NEED_FSCK); f2fs_handle_error(sbi, ERROR_CORRUPTED_INODE); return -EFSCORRUPTED; } if (f2fs_has_inline_data(inode) && !f2fs_exist_data(inode)) __recover_inline_status(inode, node_page); if (!S_ISDIR(inode->i_mode) && !is_cold_node(node_page)) { f2fs_wait_on_page_writeback(node_page, NODE, true, true); set_cold_node(node_page, false); set_page_dirty(node_page); } __get_inode_rdev(inode, node_page); if (!f2fs_need_inode_block_update(sbi, inode->i_ino)) fi->last_disk_size = inode->i_size; if (fi->i_flags & F2FS_PROJINHERIT_FL) set_inode_flag(inode, FI_PROJ_INHERIT); if (f2fs_has_extra_attr(inode) && f2fs_sb_has_project_quota(sbi) && F2FS_FITS_IN_INODE(ri, fi->i_extra_isize, i_projid)) i_projid = (projid_t)le32_to_cpu(ri->i_projid); else i_projid = F2FS_DEF_PROJID; fi->i_projid = make_kprojid(&init_user_ns, i_projid); if (f2fs_has_extra_attr(inode) && f2fs_sb_has_inode_crtime(sbi) && F2FS_FITS_IN_INODE(ri, fi->i_extra_isize, i_crtime)) { fi->i_crtime.tv_sec = le64_to_cpu(ri->i_crtime); fi->i_crtime.tv_nsec = le32_to_cpu(ri->i_crtime_nsec); } if (f2fs_has_extra_attr(inode) && f2fs_sb_has_compression(sbi) && (fi->i_flags & F2FS_COMPR_FL)) { if (F2FS_FITS_IN_INODE(ri, fi->i_extra_isize, i_compress_flag)) { unsigned short compress_flag; atomic_set(&fi->i_compr_blocks, le64_to_cpu(ri->i_compr_blocks)); fi->i_compress_algorithm = ri->i_compress_algorithm; fi->i_log_cluster_size = ri->i_log_cluster_size; compress_flag = le16_to_cpu(ri->i_compress_flag); fi->i_compress_level = compress_flag >> COMPRESS_LEVEL_OFFSET; fi->i_compress_flag = compress_flag & GENMASK(COMPRESS_LEVEL_OFFSET - 1, 0); fi->i_cluster_size = BIT(fi->i_log_cluster_size); set_inode_flag(inode, FI_COMPRESSED_FILE); } } init_idisk_time(inode); if (!sanity_check_extent_cache(inode, node_page)) { f2fs_put_page(node_page, 1); f2fs_handle_error(sbi, ERROR_CORRUPTED_INODE); return -EFSCORRUPTED; } f2fs_init_read_extent_tree(inode, node_page); f2fs_init_age_extent_tree(inode); f2fs_put_page(node_page, 1); stat_inc_inline_xattr(inode); stat_inc_inline_inode(inode); stat_inc_inline_dir(inode); stat_inc_compr_inode(inode); stat_add_compr_blocks(inode, atomic_read(&fi->i_compr_blocks)); return 0; }"
1234----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-53689/bad/blk-mq.c----blk_mq_elv_switch_none,"static bool blk_mq_elv_switch_none(struct list_head *head, struct request_queue *q) { struct blk_mq_qe_pair *qe; qe = kmalloc(sizeof(*qe), GFP_NOIO | __GFP_NOWARN | __GFP_NORETRY); if (!qe) return false; <S2SV_StartVul> mutex_lock(&q->sysfs_lock); <S2SV_EndVul> if (!q->elevator) { kfree(qe); <S2SV_StartVul> goto unlock; <S2SV_EndVul> } INIT_LIST_HEAD(&qe->node); qe->q = q; qe->type = q->elevator->type; __elevator_get(qe->type); list_add(&qe->node, head); elevator_disable(q); <S2SV_StartVul> unlock: <S2SV_EndVul> <S2SV_StartVul> mutex_unlock(&q->sysfs_lock); <S2SV_EndVul> return true; }","- mutex_lock(&q->sysfs_lock);
- goto unlock;
- unlock:
- mutex_unlock(&q->sysfs_lock);
+ lockdep_assert_held(&q->sysfs_lock);
+ goto out;
+ }
+ out:
+ }","static bool blk_mq_elv_switch_none(struct list_head *head, struct request_queue *q) { struct blk_mq_qe_pair *qe; qe = kmalloc(sizeof(*qe), GFP_NOIO | __GFP_NOWARN | __GFP_NORETRY); if (!qe) return false; lockdep_assert_held(&q->sysfs_lock); if (!q->elevator) { kfree(qe); goto out; } INIT_LIST_HEAD(&qe->node); qe->q = q; qe->type = q->elevator->type; __elevator_get(qe->type); list_add(&qe->node, head); elevator_disable(q); out: return true; }"
1286----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-56587/bad/led-class.c----max_brightness_show,"static ssize_t max_brightness_show(struct device *dev, struct device_attribute *attr, char *buf) { struct led_classdev *led_cdev = dev_get_drvdata(dev); <S2SV_StartVul> return sprintf(buf, ""%u\n"", led_cdev->max_brightness); <S2SV_EndVul> }","- return sprintf(buf, ""%u\n"", led_cdev->max_brightness);
+ unsigned int max_brightness;
+ mutex_lock(&led_cdev->led_access);
+ max_brightness = led_cdev->max_brightness;
+ mutex_unlock(&led_cdev->led_access);
+ return sprintf(buf, ""%u\n"", max_brightness);","static ssize_t max_brightness_show(struct device *dev, struct device_attribute *attr, char *buf) { struct led_classdev *led_cdev = dev_get_drvdata(dev); unsigned int max_brightness; mutex_lock(&led_cdev->led_access); max_brightness = led_cdev->max_brightness; mutex_unlock(&led_cdev->led_access); return sprintf(buf, ""%u\n"", max_brightness); }"
98----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-44963/bad/ctree.c----btrfs_free_tree_block,"void btrfs_free_tree_block(struct btrfs_trans_handle *trans, u64 root_id, struct extent_buffer *buf, u64 parent, int last_ref) { struct btrfs_fs_info *fs_info = trans->fs_info; struct btrfs_block_group *bg; int ret; if (root_id != BTRFS_TREE_LOG_OBJECTID) { struct btrfs_ref generic_ref = { .action = BTRFS_DROP_DELAYED_REF, .bytenr = buf->start, .num_bytes = buf->len, .parent = parent, .owning_root = btrfs_header_owner(buf), .ref_root = root_id, }; ASSERT(btrfs_header_bytenr(buf) != 0); btrfs_init_tree_ref(&generic_ref, btrfs_header_level(buf), 0, false); <S2SV_StartVul> btrfs_ref_tree_mod(fs_info, &generic_ref); <S2SV_EndVul> <S2SV_StartVul> ret = btrfs_add_delayed_tree_ref(trans, &generic_ref, NULL); <S2SV_EndVul> BUG_ON(ret); } if (!last_ref) return; if (btrfs_header_generation(buf) != trans->transid) goto out; if (root_id != BTRFS_TREE_LOG_OBJECTID) { ret = check_ref_cleanup(trans, buf->start); if (!ret) goto out; } bg = btrfs_lookup_block_group(fs_info, buf->start); if (btrfs_header_flag(buf, BTRFS_HEADER_FLAG_WRITTEN)) { pin_down_extent(trans, bg, buf->start, buf->len, 1); btrfs_put_block_group(bg); goto out; } if (test_bit(BTRFS_FS_TREE_MOD_LOG_USERS, &fs_info->flags) || btrfs_is_zoned(fs_info)) { pin_down_extent(trans, bg, buf->start, buf->len, 1); btrfs_put_block_group(bg); goto out; } WARN_ON(test_bit(EXTENT_BUFFER_DIRTY, &buf->bflags)); btrfs_add_free_space(bg, buf->start, buf->len); btrfs_free_reserved_bytes(bg, buf->len, 0); btrfs_put_block_group(bg); trace_btrfs_reserved_extent_free(fs_info, buf->start, buf->len); out: clear_bit(EXTENT_BUFFER_CORRUPT, &buf->bflags); }","- btrfs_ref_tree_mod(fs_info, &generic_ref);
- ret = btrfs_add_delayed_tree_ref(trans, &generic_ref, NULL);
+ #include ""discard.h""
+ #include ""zoned.h""
+ #include ""dev-replace.h""
+ #include ""fs.h""
+ #include ""accessors.h""
+ #include ""root-tree.h""
+ #include ""file-item.h""
+ #include ""orphan.h""
+ #include ""tree-checker.h""
+ #include ""raid-stripe-tree.h""","#include <linux/sched.h> #include <linux/sched/signal.h> #include <linux/pagemap.h> #include <linux/writeback.h> #include <linux/blkdev.h> #include <linux/sort.h> #include <linux/rcupdate.h> #include <linux/kthread.h> #include <linux/slab.h> #include <linux/ratelimit.h> #include <linux/percpu_counter.h> #include <linux/lockdep.h> #include <linux/crc32c.h> #include ""ctree.h"" #include ""extent-tree.h"" #include ""transaction.h"" #include ""disk-io.h"" #include ""print-tree.h"" #include ""volumes.h"" #include ""raid56.h"" #include ""locking.h"" #include ""free-space-cache.h"" #include ""free-space-tree.h"" #include ""qgroup.h"" #include ""ref-verify.h"" #include ""space-info.h"" #include ""block-rsv.h"" #include ""discard.h"" #include ""zoned.h"" #include ""dev-replace.h"" #include ""fs.h"" #include ""accessors.h"" #include ""root-tree.h"" #include ""file-item.h"" #include ""orphan.h"" #include ""tree-checker.h"" #include ""raid-stripe-tree.h"" #undef SCRAMBLE_DELAYED_REFS static int __btrfs_free_extent(struct btrfs_trans_handle *trans, struct btrfs_delayed_ref_head *href, struct btrfs_delayed_ref_node *node, struct btrfs_delayed_extent_op *extra_op); static void __run_delayed_extent_op(struct btrfs_delayed_extent_op *extent_op, struct extent_buffer *leaf, struct btrfs_extent_item *ei); static int alloc_reserved_file_extent(struct btrfs_trans_handle *trans, u64 parent, u64 root_objectid, u64 flags, u64 owner, u64 offset, struct btrfs_key *ins, int ref_mod, u64 oref_root); static int alloc_reserved_tree_block(struct btrfs_trans_handle *trans, struct btrfs_delayed_ref_node *node, struct btrfs_delayed_extent_op *extent_op); static int find_next_key(struct btrfs_path *path, int level, struct btrfs_key *key); static int block_group_bits(struct btrfs_block_group *cache, u64 bits) { return (cache->flags & bits) == bits; }"
267----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46750/bad/pci.c----pci_bus_lock,"static void pci_bus_lock(struct pci_bus *bus) { struct pci_dev *dev; list_for_each_entry(dev, &bus->devices, bus_list) { <S2SV_StartVul> pci_dev_lock(dev); <S2SV_EndVul> if (dev->subordinate) pci_bus_lock(dev->subordinate); <S2SV_StartVul> } <S2SV_EndVul> <S2SV_StartVul> } <S2SV_EndVul>","- pci_dev_lock(dev);
- }
- }
+ pci_dev_lock(bus->self);
+ else
+ pci_dev_lock(dev);","static void pci_bus_lock(struct pci_bus *bus) { struct pci_dev *dev; pci_dev_lock(bus->self); list_for_each_entry(dev, &bus->devices, bus_list) { if (dev->subordinate) pci_bus_lock(dev->subordinate); else pci_dev_lock(dev); } }"
650----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-49952/bad/nf_dup_ipv4.c----nf_dup_ipv4,"void nf_dup_ipv4(struct net *net, struct sk_buff *skb, unsigned int hooknum, const struct in_addr *gw, int oif) { struct iphdr *iph; if (this_cpu_read(nf_skb_duplicated)) return; skb = pskb_copy(skb, GFP_ATOMIC); if (skb == NULL) return; #if IS_ENABLED(CONFIG_NF_CONNTRACK) nf_reset_ct(skb); nf_ct_set(skb, NULL, IP_CT_UNTRACKED); #endif iph = ip_hdr(skb); iph->frag_off |= htons(IP_DF); if (hooknum == NF_INET_PRE_ROUTING || <S2SV_StartVul> hooknum == NF_INET_LOCAL_IN) <S2SV_EndVul> --iph->ttl; if (nf_dup_ipv4_route(net, skb, gw, oif)) { __this_cpu_write(nf_skb_duplicated, true); ip_local_out(net, skb->sk, skb); __this_cpu_write(nf_skb_duplicated, false); } else { <S2SV_StartVul> kfree_skb(skb); <S2SV_EndVul> } }","- hooknum == NF_INET_LOCAL_IN)
- kfree_skb(skb);
+ iph->frag_off |= htons(IP_DF);
+ hooknum == NF_INET_LOCAL_IN)
+ kfree_skb(skb);","void nf_dup_ipv4(struct net *net, struct sk_buff *skb, unsigned int hooknum, const struct in_addr *gw, int oif) { struct iphdr *iph; local_bh_disable(); if (this_cpu_read(nf_skb_duplicated)) goto out; skb = pskb_copy(skb, GFP_ATOMIC); if (skb == NULL) goto out; #if IS_ENABLED(CONFIG_NF_CONNTRACK) nf_reset_ct(skb); nf_ct_set(skb, NULL, IP_CT_UNTRACKED); #endif iph = ip_hdr(skb); iph->frag_off |= htons(IP_DF); if (hooknum == NF_INET_PRE_ROUTING || hooknum == NF_INET_LOCAL_IN) --iph->ttl; if (nf_dup_ipv4_route(net, skb, gw, oif)) { __this_cpu_write(nf_skb_duplicated, true); ip_local_out(net, skb->sk, skb); __this_cpu_write(nf_skb_duplicated, false); } else { kfree_skb(skb); } out: local_bh_enable(); }"
751----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50028/bad/thermal_netlink.c----thermal_genl_cmd_tz_get_trip,"static int thermal_genl_cmd_tz_get_trip(struct param *p) { struct sk_buff *msg = p->msg; const struct thermal_trip_desc *td; <S2SV_StartVul> struct thermal_zone_device *tz; <S2SV_EndVul> struct nlattr *start_trip; int id; if (!p->attrs[THERMAL_GENL_ATTR_TZ_ID]) return -EINVAL; id = nla_get_u32(p->attrs[THERMAL_GENL_ATTR_TZ_ID]); <S2SV_StartVul> tz = thermal_zone_get_by_id(id); <S2SV_EndVul> if (!tz) return -EINVAL; start_trip = nla_nest_start(msg, THERMAL_GENL_ATTR_TZ_TRIP); if (!start_trip) return -EMSGSIZE; mutex_lock(&tz->lock); for_each_trip_desc(tz, td) { const struct thermal_trip *trip = &td->trip; if (nla_put_u32(msg, THERMAL_GENL_ATTR_TZ_TRIP_ID, thermal_zone_trip_id(tz, trip)) || nla_put_u32(msg, THERMAL_GENL_ATTR_TZ_TRIP_TYPE, trip->type) || nla_put_u32(msg, THERMAL_GENL_ATTR_TZ_TRIP_TEMP, trip->temperature) || nla_put_u32(msg, THERMAL_GENL_ATTR_TZ_TRIP_HYST, trip->hysteresis)) goto out_cancel_nest; } mutex_unlock(&tz->lock); nla_nest_end(msg, start_trip); return 0; out_cancel_nest: mutex_unlock(&tz->lock); return -EMSGSIZE; }","- struct thermal_zone_device *tz;
- tz = thermal_zone_get_by_id(id);
+ CLASS(thermal_zone_get_by_id, tz)(id);","static int thermal_genl_cmd_tz_get_trip(struct param *p) { struct sk_buff *msg = p->msg; const struct thermal_trip_desc *td; struct nlattr *start_trip; int id; if (!p->attrs[THERMAL_GENL_ATTR_TZ_ID]) return -EINVAL; id = nla_get_u32(p->attrs[THERMAL_GENL_ATTR_TZ_ID]); CLASS(thermal_zone_get_by_id, tz)(id); if (!tz) return -EINVAL; start_trip = nla_nest_start(msg, THERMAL_GENL_ATTR_TZ_TRIP); if (!start_trip) return -EMSGSIZE; mutex_lock(&tz->lock); for_each_trip_desc(tz, td) { const struct thermal_trip *trip = &td->trip; if (nla_put_u32(msg, THERMAL_GENL_ATTR_TZ_TRIP_ID, thermal_zone_trip_id(tz, trip)) || nla_put_u32(msg, THERMAL_GENL_ATTR_TZ_TRIP_TYPE, trip->type) || nla_put_u32(msg, THERMAL_GENL_ATTR_TZ_TRIP_TEMP, trip->temperature) || nla_put_u32(msg, THERMAL_GENL_ATTR_TZ_TRIP_HYST, trip->hysteresis)) goto out_cancel_nest; } mutex_unlock(&tz->lock); nla_nest_end(msg, start_trip); return 0; out_cancel_nest: mutex_unlock(&tz->lock); return -EMSGSIZE; }"
386----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-46847/bad/vmalloc.c----*new_vmap_block,"static void *new_vmap_block(unsigned int order, gfp_t gfp_mask) { struct vmap_block_queue *vbq; struct vmap_block *vb; struct vmap_area *va; struct xarray *xa; unsigned long vb_idx; int node, err; void *vaddr; node = numa_node_id(); vb = kmalloc_node(sizeof(struct vmap_block), gfp_mask & GFP_RECLAIM_MASK, node); if (unlikely(!vb)) return ERR_PTR(-ENOMEM); va = alloc_vmap_area(VMAP_BLOCK_SIZE, VMAP_BLOCK_SIZE, VMALLOC_START, VMALLOC_END, node, gfp_mask, VMAP_RAM|VMAP_BLOCK); if (IS_ERR(va)) { kfree(vb); return ERR_CAST(va); } vaddr = vmap_block_vaddr(va->va_start, 0); spin_lock_init(&vb->lock); vb->va = va; BUG_ON(VMAP_BBMAP_BITS <= (1UL << order)); bitmap_zero(vb->used_map, VMAP_BBMAP_BITS); vb->free = VMAP_BBMAP_BITS - (1UL << order); vb->dirty = 0; vb->dirty_min = VMAP_BBMAP_BITS; vb->dirty_max = 0; bitmap_set(vb->used_map, 0, (1UL << order)); INIT_LIST_HEAD(&vb->free_list); xa = addr_to_vb_xa(va->va_start); vb_idx = addr_to_vb_idx(va->va_start); err = xa_insert(xa, vb_idx, vb, gfp_mask); if (err) { kfree(vb); free_vmap_area(va); return ERR_PTR(err); } <S2SV_StartVul> vb->cpu = raw_smp_processor_id(); <S2SV_EndVul> vbq = per_cpu_ptr(&vmap_block_queue, vb->cpu); spin_lock(&vbq->lock); list_add_tail_rcu(&vb->free_list, &vbq->free); spin_unlock(&vbq->lock); return vaddr; }","- vb->cpu = raw_smp_processor_id();
+ vb->cpu = raw_smp_processor_id();","static void *new_vmap_block(unsigned int order, gfp_t gfp_mask) { struct vmap_block_queue *vbq; struct vmap_block *vb; struct vmap_area *va; struct xarray *xa; unsigned long vb_idx; int node, err; void *vaddr; node = numa_node_id(); vb = kmalloc_node(sizeof(struct vmap_block), gfp_mask & GFP_RECLAIM_MASK, node); if (unlikely(!vb)) return ERR_PTR(-ENOMEM); va = alloc_vmap_area(VMAP_BLOCK_SIZE, VMAP_BLOCK_SIZE, VMALLOC_START, VMALLOC_END, node, gfp_mask, VMAP_RAM|VMAP_BLOCK); if (IS_ERR(va)) { kfree(vb); return ERR_CAST(va); } vaddr = vmap_block_vaddr(va->va_start, 0); spin_lock_init(&vb->lock); vb->va = va; BUG_ON(VMAP_BBMAP_BITS <= (1UL << order)); bitmap_zero(vb->used_map, VMAP_BBMAP_BITS); vb->free = VMAP_BBMAP_BITS - (1UL << order); vb->dirty = 0; vb->dirty_min = VMAP_BBMAP_BITS; vb->dirty_max = 0; bitmap_set(vb->used_map, 0, (1UL << order)); INIT_LIST_HEAD(&vb->free_list); vb->cpu = raw_smp_processor_id(); xa = addr_to_vb_xa(va->va_start); vb_idx = addr_to_vb_idx(va->va_start); err = xa_insert(xa, vb_idx, vb, gfp_mask); if (err) { kfree(vb); free_vmap_area(va); return ERR_PTR(err); } vbq = per_cpu_ptr(&vmap_block_queue, vb->cpu); spin_lock(&vbq->lock); list_add_tail_rcu(&vb->free_list, &vbq->free); spin_unlock(&vbq->lock); return vaddr; }"
852----None----E:/smm/code_security_analysis/漏洞检测/实验/my_model/new_data/CVE-2024-50109/bad/raid10.c----raid10_run,"static int raid10_run(struct mddev *mddev) { struct r10conf *conf; int i, disk_idx; struct raid10_info *disk; struct md_rdev *rdev; sector_t size; sector_t min_offset_diff = 0; int first = 1; int ret = -EIO; if (mddev->private == NULL) { conf = setup_conf(mddev); if (IS_ERR(conf)) return PTR_ERR(conf); mddev->private = conf; } conf = mddev->private; if (!conf) goto out; rcu_assign_pointer(mddev->thread, conf->thread); rcu_assign_pointer(conf->thread, NULL); if (mddev_is_clustered(conf->mddev)) { int fc, fo; fc = (mddev->layout >> 8) & 255; fo = mddev->layout & (1<<16); if (fc > 1 || fo > 0) { pr_err(""only near layout is supported by clustered"" "" raid10\n""); goto out_free_conf; } } rdev_for_each(rdev, mddev) { long long diff; disk_idx = rdev->raid_disk; if (disk_idx < 0) continue; if (disk_idx >= conf->geo.raid_disks && disk_idx >= conf->prev.raid_disks) continue; disk = conf->mirrors + disk_idx; if (test_bit(Replacement, &rdev->flags)) { if (disk->replacement) goto out_free_conf; disk->replacement = rdev; } else { if (disk->rdev) goto out_free_conf; disk->rdev = rdev; } diff = (rdev->new_data_offset - rdev->data_offset); if (!mddev->reshape_backwards) diff = -diff; if (diff < 0) diff = 0; if (first || diff < min_offset_diff) min_offset_diff = diff; disk->head_position = 0; first = 0; } if (!mddev_is_dm(conf->mddev)) { <S2SV_StartVul> ret = raid10_set_queue_limits(mddev); <S2SV_EndVul> <S2SV_StartVul> if (ret) <S2SV_EndVul> goto out_free_conf; } if (!enough(conf, -1)) { pr_err(""md/raid10:%s: not enough operational mirrors.\n"", mdname(mddev)); goto out_free_conf; } if (conf->reshape_progress != MaxSector) { if (conf->geo.far_copies != 1 && conf->geo.far_offset == 0) goto out_free_conf; if (conf->prev.far_copies != 1 && conf->prev.far_offset == 0) goto out_free_conf; } mddev->degraded = 0; for (i = 0; i < conf->geo.raid_disks || i < conf->prev.raid_disks; i++) { disk = conf->mirrors + i; if (!disk->rdev && disk->replacement) { disk->rdev = disk->replacement; disk->replacement = NULL; clear_bit(Replacement, &disk->rdev->flags); } if (!disk->rdev || !test_bit(In_sync, &disk->rdev->flags)) { disk->head_position = 0; mddev->degraded++; if (disk->rdev && disk->rdev->saved_raid_disk < 0) conf->fullsync = 1; } if (disk->replacement && !test_bit(In_sync, &disk->replacement->flags) && disk->replacement->saved_raid_disk < 0) { conf->fullsync = 1; } disk->recovery_disabled = mddev->recovery_disabled - 1; } if (mddev->recovery_cp != MaxSector) pr_notice(""md/raid10:%s: not clean -- starting background reconstruction\n"", mdname(mddev)); pr_info(""md/raid10:%s: active with %d out of %d devices\n"", mdname(mddev), conf->geo.raid_disks - mddev->degraded, conf->geo.raid_disks); mddev->dev_sectors = conf->dev_sectors; size = raid10_size(mddev, 0, 0); md_set_array_sectors(mddev, size); mddev->resync_max_sectors = size; set_bit(MD_FAILFAST_SUPPORTED, &mddev->flags); if (md_integrity_register(mddev)) goto out_free_conf; if (conf->reshape_progress != MaxSector) { unsigned long before_length, after_length; before_length = ((1 << conf->prev.chunk_shift) * conf->prev.far_copies); after_length = ((1 << conf->geo.chunk_shift) * conf->geo.far_copies); if (max(before_length, after_length) > min_offset_diff) { pr_warn(""md/raid10: offset difference not enough to continue reshape\n""); goto out_free_conf; } conf->offset_diff = min_offset_diff; clear_bit(MD_RECOVERY_SYNC, &mddev->recovery); clear_bit(MD_RECOVERY_CHECK, &mddev->recovery); set_bit(MD_RECOVERY_RESHAPE, &mddev->recovery); set_bit(MD_RECOVERY_NEEDED, &mddev->recovery); } return 0; out_free_conf: md_unregister_thread(mddev, &mddev->thread); raid10_free_conf(conf); mddev->private = NULL; out: return ret; }","- ret = raid10_set_queue_limits(mddev);
- if (ret)
+ int err = raid10_set_queue_limits(mddev);
+ if (err) {
+ ret = err;
+ }
+ }","static int raid10_run(struct mddev *mddev) { struct r10conf *conf; int i, disk_idx; struct raid10_info *disk; struct md_rdev *rdev; sector_t size; sector_t min_offset_diff = 0; int first = 1; int ret = -EIO; if (mddev->private == NULL) { conf = setup_conf(mddev); if (IS_ERR(conf)) return PTR_ERR(conf); mddev->private = conf; } conf = mddev->private; if (!conf) goto out; rcu_assign_pointer(mddev->thread, conf->thread); rcu_assign_pointer(conf->thread, NULL); if (mddev_is_clustered(conf->mddev)) { int fc, fo; fc = (mddev->layout >> 8) & 255; fo = mddev->layout & (1<<16); if (fc > 1 || fo > 0) { pr_err(""only near layout is supported by clustered"" "" raid10\n""); goto out_free_conf; } } rdev_for_each(rdev, mddev) { long long diff; disk_idx = rdev->raid_disk; if (disk_idx < 0) continue; if (disk_idx >= conf->geo.raid_disks && disk_idx >= conf->prev.raid_disks) continue; disk = conf->mirrors + disk_idx; if (test_bit(Replacement, &rdev->flags)) { if (disk->replacement) goto out_free_conf; disk->replacement = rdev; } else { if (disk->rdev) goto out_free_conf; disk->rdev = rdev; } diff = (rdev->new_data_offset - rdev->data_offset); if (!mddev->reshape_backwards) diff = -diff; if (diff < 0) diff = 0; if (first || diff < min_offset_diff) min_offset_diff = diff; disk->head_position = 0; first = 0; } if (!mddev_is_dm(conf->mddev)) { int err = raid10_set_queue_limits(mddev); if (err) { ret = err; goto out_free_conf; } } if (!enough(conf, -1)) { pr_err(""md/raid10:%s: not enough operational mirrors.\n"", mdname(mddev)); goto out_free_conf; } if (conf->reshape_progress != MaxSector) { if (conf->geo.far_copies != 1 && conf->geo.far_offset == 0) goto out_free_conf; if (conf->prev.far_copies != 1 && conf->prev.far_offset == 0) goto out_free_conf; } mddev->degraded = 0; for (i = 0; i < conf->geo.raid_disks || i < conf->prev.raid_disks; i++) { disk = conf->mirrors + i; if (!disk->rdev && disk->replacement) { disk->rdev = disk->replacement; disk->replacement = NULL; clear_bit(Replacement, &disk->rdev->flags); } if (!disk->rdev || !test_bit(In_sync, &disk->rdev->flags)) { disk->head_position = 0; mddev->degraded++; if (disk->rdev && disk->rdev->saved_raid_disk < 0) conf->fullsync = 1; } if (disk->replacement && !test_bit(In_sync, &disk->replacement->flags) && disk->replacement->saved_raid_disk < 0) { conf->fullsync = 1; } disk->recovery_disabled = mddev->recovery_disabled - 1; } if (mddev->recovery_cp != MaxSector) pr_notice(""md/raid10:%s: not clean -- starting background reconstruction\n"", mdname(mddev)); pr_info(""md/raid10:%s: active with %d out of %d devices\n"", mdname(mddev), conf->geo.raid_disks - mddev->degraded, conf->geo.raid_disks); mddev->dev_sectors = conf->dev_sectors; size = raid10_size(mddev, 0, 0); md_set_array_sectors(mddev, size); mddev->resync_max_sectors = size; set_bit(MD_FAILFAST_SUPPORTED, &mddev->flags); if (md_integrity_register(mddev)) goto out_free_conf; if (conf->reshape_progress != MaxSector) { unsigned long before_length, after_length; before_length = ((1 << conf->prev.chunk_shift) * conf->prev.far_copies); after_length = ((1 << conf->geo.chunk_shift) * conf->geo.far_copies); if (max(before_length, after_length) > min_offset_diff) { pr_warn(""md/raid10: offset difference not enough to continue reshape\n""); goto out_free_conf; } conf->offset_diff = min_offset_diff; clear_bit(MD_RECOVERY_SYNC, &mddev->recovery); clear_bit(MD_RECOVERY_CHECK, &mddev->recovery); set_bit(MD_RECOVERY_RESHAPE, &mddev->recovery); set_bit(MD_RECOVERY_NEEDED, &mddev->recovery); } return 0; out_free_conf: md_unregister_thread(mddev, &mddev->thread); raid10_free_conf(conf); mddev->private = NULL; out: return ret; }"
